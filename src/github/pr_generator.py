# github/pr_generator.py
"""
Minimal PR Generator for CI/CD AI Assistant.

Creates pull requests from FixPlan objects generated by agent_handler.

Configuration via environment variables (use .env file locally):
    GITHUB_TOKEN              - GitHub PAT with repo permissions
    TARGET_REPO_OWNER         - Owner of target repository (e.g., "my-org")
    TARGET_REPO_NAME          - Name of target repository (e.g., "backend")
    TARGET_REPO_DEFAULT_BRANCH - Branch to create PRs against (default: "main")
    PR_BRANCH_PREFIX          - Prefix for branch names (default: "ai-fix")
    PR_LABELS                 - Comma-separated labels (default: "ai-generated")
    PR_DRAFT_MODE             - Create as draft PR (default: "false")
"""
from __future__ import annotations

import base64
import hashlib
import os
import logging

from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Optional

import httpx
from dotenv import load_dotenv

from agents.agent_handler import CodeEdit, EditType, FileEdit, FixPlan, SignalError
from github.client import (
    GitHubError,
    github_request,
    TARGET_REPO_OWNER,
    TARGET_REPO_NAME,
    TARGET_REPO_DEFAULT_BRANCH,
)

DEFAULT_CONFIDENCE_THRESHOLD = 0.7


# Load environment variables
load_dotenv()

# ============================================================================
# Configuration (PR-specific settings)
# ============================================================================
PR_BRANCH_PREFIX = os.getenv("PR_BRANCH_PREFIX", "cicd-agent-fix").strip()
PR_LABELS = [l.strip() for l in os.getenv("PR_LABELS", "cicd-agent-generated").split(",") if l.strip()]
PR_DRAFT_MODE = os.getenv("PR_DRAFT_MODE", "false").lower() in ("true", "1", "yes")

# Logging Mode Environment Settings
log_level = os.getenv("LOG_LEVEL", "info").strip().lower()
debug_mode = log_level == "debug"

# ============================================================================
# Result Type
# ============================================================================

@dataclass
class SkippedFix:
    """A fix that was skipped due to low confidence."""
    file_path: str
    confidence: float
    reasoning: str
    threshold: float


@dataclass
class UnchangedFix:
    """A fix where the LLM returned identical code."""
    file_path: str
    reasoning: str
    signal_errors: list[SignalError]


@dataclass
class PRResult:
    """Result of PR creation."""
    success: bool
    pr_url: Optional[str] = None
    pr_number: Optional[int] = None
    branch_name: Optional[str] = None
    error: Optional[str] = None
    files_changed: list[str] = field(default_factory=list)
    skipped_fixes: list[SkippedFix] = field(default_factory=list)
    unchanged_fixes: list[UnchangedFix] = field(default_factory=list)


# ============================================================================
# Edit Application
# ============================================================================

def apply_edits_to_content(content: str, edits: list[CodeEdit]) -> str:
    """
    Apply CodeEdit operations to file content.

    Edits are applied bottom-to-top to preserve line numbers.
    """
    if not edits:
        return content

    lines = content.split("\n")

    # Sort edits by position descending (apply bottom-to-top)
    sorted_edits = sorted(
        edits,
        key=lambda e: (e.span.start.row, e.span.start.column),
        reverse=True,
    )

    for edit in sorted_edits:
        lines = _apply_edit(lines, edit)

    return "\n".join(lines)


def _apply_edit(lines: list[str], edit: CodeEdit) -> list[str]:
    """Apply a single edit to lines."""
    # Convert to 0-based index
    start_row = max(0, edit.span.start.row - 1)
    end_row = max(0, edit.span.end.row - 1)
    start_col = max(0, edit.span.start.column - 1)
    end_col = max(0, edit.span.end.column -1)

    if not lines:
        if edit.edit_type in (EditType.INSERT, EditType.REPLACE):
            return edit.content.split("\n")
        return lines

    start_row = min(start_row, len(lines) - 1)
    end_row = min(end_row, len(lines) - 1)

    if edit.edit_type == EditType.DELETE:
        if start_row == end_row:
            line = lines[start_row]
            lines[start_row] = line[:start_col] + line[end_col:]
        else:
            prefix = lines[start_row][:start_col]
            suffix = lines[end_row][end_col:]
            lines[start_row] = prefix + suffix
            del lines[start_row + 1:end_row + 1]

    elif edit.edit_type == EditType.INSERT:
        line = lines[start_row]
        new_content = line[:start_col] + edit.content + line[start_col:]
        # Use splitlines() to avoid empty trailing element from trailing \n
        lines[start_row:start_row + 1] = new_content.splitlines()

    elif edit.edit_type == EditType.REPLACE:
        prefix = lines[start_row][:start_col]
        suffix = lines[end_row][end_col:]
        new_content = prefix + edit.content + suffix
        # Use splitlines() to avoid empty trailing element from trailing \n
        lines[start_row:end_row + 1] = new_content.splitlines()

    return lines


# ============================================================================
# PR Generator
# ============================================================================

class PRGenerator:
    """
    Creates pull requests from FixPlan objects.

    Usage:
        generator = PRGenerator()
        result = generator.create_pr(fix_plan)

        if result.success:
            print(f"PR created: {result.pr_url}")
    """

    def __init__(
        self,
        *,
        github_client: httpx.Client,
        confidence_threshold: float = DEFAULT_CONFIDENCE_THRESHOLD,
    ) -> None:
        """Initialize PR generator.

        Args:
            github_client: httpx.Client pre-configured with GitHub auth headers.
            confidence_threshold: Minimum confidence level for a fix to be
                included in the PR. Fixes below this threshold are skipped
                and reported in PRResult.skipped_fixes. Defaults to 0.7.
        """
        self._client = github_client
        self._confidence_threshold = confidence_threshold

    def create_pr(self, fix_plan: FixPlan, base_branch: Optional[str] = None) -> PRResult:
        """
        Create a PR from a FixPlan.

        Fixes with confidence below the threshold are skipped and reported
        in PRResult.skipped_fixes. The PR description includes per-fix
        confidence levels and an average across all included fixes.

        Args:
            fix_plan: FixPlan from agent_handler containing file edits
            base_branch: Branch to create PR against (uses default if None)

        Returns:
            PRResult with PR URL and details, or error information
        """
        if not fix_plan.file_edits:
            return PRResult(success=False, error="No file edits in fix plan")

        # Filter fixes by confidence threshold
        accepted_edits: list[FileEdit] = []
        skipped_fixes: list[SkippedFix] = []

        for file_edit in fix_plan.file_edits:
            if file_edit.confidence >= self._confidence_threshold:
                accepted_edits.append(file_edit)
            else:
                skipped_fixes.append(SkippedFix(
                    file_path=file_edit.file_path,
                    confidence=file_edit.confidence,
                    reasoning=file_edit.reasoning,
                    threshold=self._confidence_threshold,
                ))

        if not accepted_edits:
            return PRResult(
                success=True,
                error="All fixes were below confidence threshold — no PR created",
                skipped_fixes=skipped_fixes,
            )

        base = base_branch or TARGET_REPO_DEFAULT_BRANCH
        owner = TARGET_REPO_OWNER
        repo = TARGET_REPO_NAME

        try:
            client = self._client

            # 1. Get base branch SHA
            ref_data = github_request(client, "GET", f"/repos/{owner}/{repo}/git/ref/heads/{base}")
            base_sha = ref_data["object"]["sha"]

            # 2. Create new branch
            branch_name = self._generate_branch_name(fix_plan)
            github_request(client, "POST", f"/repos/{owner}/{repo}/git/refs", {
                "ref": f"refs/heads/{branch_name}",
                "sha": base_sha,
            })

            # 3. Group edits by file (to handle multiple FileEdits for same file)
            merged_file_edits = self._merge_file_edits(accepted_edits)

            # 4. Apply edits and commit each unique file
            files_changed: list[str] = []
            unchanged_fixes: list[UnchangedFix] = []
            for file_edit in merged_file_edits:
                if self._commit_file_edit(client, owner, repo, file_edit, branch_name, base):
                    files_changed.append(file_edit.file_path)
                else:
                    unchanged_fixes.append(UnchangedFix(
                        file_path=file_edit.file_path,
                        reasoning=file_edit.reasoning,
                        signal_errors=file_edit.signal_errors,
                    ))

            if not files_changed:
                return PRResult(
                    success=False,
                    error="No files were modified",
                    branch_name=branch_name,
                    skipped_fixes=skipped_fixes,
                    unchanged_fixes=unchanged_fixes,
                )

            # 5. Create pull request
            title = self._generate_title(fix_plan)
            body = self._generate_body(fix_plan, files_changed, accepted_edits, skipped_fixes, unchanged_fixes)

            pr_data = github_request(client, "POST", f"/repos/{owner}/{repo}/pulls", {
                "title": title,
                "body": body,
                "head": branch_name,
                "base": base,
                "draft": PR_DRAFT_MODE,
            })

            pr_number = pr_data.get("number")
            pr_url = pr_data.get("html_url")

            # 6. Add labels
            if pr_number and PR_LABELS:
                labels = list(PR_LABELS)
                if fix_plan.group_signal_type:
                    labels.append(fix_plan.group_signal_type)
                try:
                    github_request(client, "POST", f"/repos/{owner}/{repo}/issues/{pr_number}/labels", {
                        "labels": labels
                    })
                except GitHubError:
                    pass  # Labels are non-critical

            return PRResult(
                success=True,
                pr_url=pr_url,
                pr_number=pr_number,
                branch_name=branch_name,
                files_changed=files_changed,
                skipped_fixes=skipped_fixes,
                unchanged_fixes=unchanged_fixes,
            )

        except GitHubError as e:
            return PRResult(success=False, error=str(e), skipped_fixes=skipped_fixes)
        except Exception as e:
            return PRResult(success=False, error=f"Unexpected error: {e}", skipped_fixes=skipped_fixes)

    def _commit_file_edit(
        self,
        client: httpx.Client,
        owner: str,
        repo: str,
        file_edit: FileEdit,
        branch: str,
        base_branch: str,
    ) -> bool:
        """Apply a FileEdit and commit it. Returns True if successful."""
        try:
            # Get current file content from the branch being built (to chain commits)
            file_data = github_request(
                client, "GET",
                f"/repos/{owner}/{repo}/contents/{file_edit.file_path}?ref={branch}"
            )
            original_content = base64.b64decode(file_data["content"]).decode("utf-8")
            file_sha = file_data["sha"]

            # Apply edits
            new_content = apply_edits_to_content(original_content, file_edit.edits)

            if new_content == original_content:
                return False  # No changes

            # Log the details before committing
            if debug_mode: 
                logging.info(f"Attempting to commit {file_edit.file_path}")
                logging.info(f"  Branch: {branch}")
                logging.info(f"  File SHA: {file_sha}")
                logging.info(f"  Original length: {len(original_content)} bytes")
                logging.info(f"  New length: {len(new_content)} bytes")
                logging.info(f"  Content changed: {original_content != new_content}")

            # Commit updated file
            commit_msg = self._generate_commit_message(file_edit)

            encoded_content = base64.b64encode(new_content.encode("utf-8")).decode("utf-8")

            try:
                github_request(client,"PUT", f"/repos/{owner}/{repo}/contents/{file_edit.file_path}", {
                    "message": commit_msg,
                    "content": encoded_content,
                    "sha": file_sha,
                    "branch": branch,
                })
                logging.info(f"✓ Successfully committed {file_edit.file_path}")
                return True
            except GitHubError as e:
                logging.error(f"✗ Failed to commit {file_edit.file_path}: {e}")
                # Re-raise to preserve original behavior
                raise

        except GitHubError:
            return False

    def _merge_file_edits(self, file_edits: list[FileEdit]) -> list[FileEdit]:
        """
        Merge multiple FileEdit objects for the same file into a single FileEdit.

        This prevents line number conflicts when multiple edits target the same file.
        All edits for a file are combined and will be applied in a single commit.
        Confidence is set to the minimum across merged edits (most conservative).
        """
        from collections import defaultdict

        # Group edits by file path
        edits_by_file: dict[str, list[FileEdit]] = defaultdict(list)
        for file_edit in file_edits:
            edits_by_file[file_edit.file_path].append(file_edit)

        # Merge edits for each file
        merged: list[FileEdit] = []
        for file_path, edits_list in edits_by_file.items():
            if len(edits_list) == 1:
                # Only one FileEdit for this file, use as-is
                merged.append(edits_list[0])
            else:
                # Multiple FileEdits for same file - merge them
                all_edits: list[CodeEdit] = []
                reasonings: list[str] = []

                for file_edit in edits_list:
                    all_edits.extend(file_edit.edits)
                    if file_edit.reasoning:
                        reasonings.append(file_edit.reasoning)

                merged.append(FileEdit(
                    file_path=file_path,
                    edits=all_edits,
                    reasoning=" | ".join(reasonings) if reasonings else "",
                    confidence=min(fe.confidence for fe in edits_list),
                ))

        return merged

    def _generate_commit_message(self, file_edit: FileEdit) -> str:
        """
        Generate commit message that includes all edit descriptions.

        For merged FileEdits with multiple fixes, lists each one.
        """
        if not file_edit.edits:
            return "fix: apply automated fixes"

        if len(file_edit.edits) == 1:
            return f"fix: {file_edit.edits[0].description}"

        # Multiple edits - create a structured commit message
        title = f"fix: {len(file_edit.edits)} fixes in {file_edit.file_path}"
        details = []
        for edit in file_edit.edits:
            details.append(f"- Line {edit.span.start.row}: {edit.description}")

        return f"{title}\n{chr(10).join(details)}"

    def _generate_branch_name(self, fix_plan: FixPlan) -> str:
        """Generate unique branch name."""
        signal_type = fix_plan.group_signal_type or "fix"
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
        short_hash = hashlib.sha256(f"{fix_plan.group_tool_id}{timestamp}".encode()).hexdigest()[:6]
        return f"{PR_BRANCH_PREFIX}/{signal_type}/{timestamp}-{short_hash}"

    def _generate_title(self, fix_plan: FixPlan) -> str:
        """Generate PR title."""
        num_files = len(fix_plan.file_edits)
        total_edits = sum(len(fe.edits) for fe in fix_plan.file_edits)
        signal_type = fix_plan.group_signal_type or "code"

        if num_files == 1:
            return f"Fix {total_edits} {signal_type} issue(s) in {fix_plan.file_edits[0].file_path}"
        return f"Fix {total_edits} {signal_type} issue(s) across {num_files} files"

    def _generate_body(
        self,
        fix_plan: FixPlan,
        files_changed: list[str],
        accepted_edits: list[FileEdit],
        skipped_fixes: list[SkippedFix],
        unchanged_fixes: list[UnchangedFix] | None = None,
    ) -> str:
        """Generate PR body with per-fix confidence levels and averages."""
        unchanged_fixes = unchanged_fixes or []

        # Calculate average confidence across accepted fixes only
        if accepted_edits:
            avg_confidence = sum(fe.confidence for fe in accepted_edits) / len(accepted_edits)
        else:
            avg_confidence = 0.0

        lines = [
            "## Summary",
            fix_plan.summary or f"Automated fixes for {fix_plan.group_signal_type or 'code'} issues.",
            "",
            f"**Average Confidence:** {avg_confidence:.0%}",
            f"**Confidence Threshold:** {self._confidence_threshold:.0%}",
            (
                f"**Fixes Applied:** {len(files_changed)} | "
                f"**Unchanged:** {len(unchanged_fixes)} | "
                f"**Skipped:** {len(skipped_fixes)}"
            ),
            "",
            "## Files Changed",
        ]

        for f in files_changed:
            lines.append(f"- `{f}`")

        lines.extend(["", "## Changes"])

        for fe in accepted_edits:
            if fe.file_path in files_changed:
                lines.append(f"### `{fe.file_path}` (confidence: {fe.confidence:.0%})")

                if fe.signal_errors:
                    lines.append("")
                    lines.append("**Errors addressed:**")
                    for se in fe.signal_errors:
                        rule_tag = f"`[{se.rule_code}]` " if se.rule_code else ""
                        lines.append(f"- Line {se.line}, Col {se.column} — {rule_tag}{se.message}")
                    lines.append("")
                    if fe.reasoning:
                        lines.append(f"**Fix:** {fe.reasoning}")
                else:
                    if fe.reasoning:
                        lines.append(f"**Reasoning:** {fe.reasoning}")
                    for edit in fe.edits:
                        lines.append(f"- (line {edit.span.start.row}): {edit.description}")

                lines.append("")

        if unchanged_fixes:
            lines.extend(["## Unchanged Fixes (LLM returned identical code)", ""])
            for uf in unchanged_fixes:
                lines.append(f"### `{uf.file_path}`")
                if uf.signal_errors:
                    for se in uf.signal_errors:
                        rule_tag = f"`[{se.rule_code}]` " if se.rule_code else ""
                        lines.append(f"- Line {se.line}, Col {se.column} — {rule_tag}{se.message}")
                if uf.reasoning:
                    lines.append(f"- **Reasoning:** {uf.reasoning}")
                lines.append("")

        if skipped_fixes:
            lines.extend(["## Skipped Fixes (Below Confidence Threshold)", ""])
            for sf in skipped_fixes:
                lines.append(
                    f"- `{sf.file_path}` — confidence {sf.confidence:.0%} "
                    f"(threshold: {sf.threshold:.0%}): {sf.reasoning}"
                )
            lines.append("")

        if fix_plan.warnings:
            lines.extend(["## Warnings"])
            for w in fix_plan.warnings:
                lines.append(f"- {w}")

        lines.extend([
            "",
            "---",
            "*Generated by CI/CD AI Assistant*",
        ])

        return "\n".join(lines)
