"""
Report Generation.

Generates JSON and Markdown reports from evaluation results.
"""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .harness import EvaluationRun
    from .metrics import EvaluationMetrics
    from .test_case import TestCaseResult


class ReportGenerator:
    """
    Generates evaluation reports in JSON and Markdown formats.

    Usage:
        generator = ReportGenerator(output_dir="evaluation/results")
        generator.generate_all(run)
    """

    def __init__(self, output_dir: str | Path) -> None:
        """Initialize the report generator."""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def generate_all(self, run: "EvaluationRun") -> dict[str, Path]:
        """
        Generate all report formats.

        Args:
            run: The evaluation run to report on

        Returns:
            Dictionary mapping format names to file paths
        """
        return {
            "json": self.generate_json_report(run),
            "markdown": self.generate_markdown_report(run),
        }

    def generate_json_report(self, run: "EvaluationRun") -> Path:
        """
        Generate a JSON report.

        Args:
            run: The evaluation run

        Returns:
            Path to the generated JSON file
        """
        output_path = self.output_dir / f"report_{run.run_id}.json"

        report_data = run.to_dict()

        with open(output_path, "w") as f:
            json.dump(report_data, f, indent=2, default=str)

        return output_path

    def generate_markdown_report(self, run: "EvaluationRun") -> Path:
        """
        Generate a Markdown report with tables and summaries.

        Args:
            run: The evaluation run

        Returns:
            Path to the generated Markdown file
        """
        output_path = self.output_dir / f"report_{run.run_id}.md"

        lines = []

        # Header
        lines.append("# CICD AI Assistant Evaluation Report")
        lines.append("")
        lines.append(f"**Run ID:** {run.run_id}")
        lines.append(f"**Started:** {run.started_at.strftime('%Y-%m-%d %H:%M:%S')}")
        if run.completed_at:
            lines.append(f"**Completed:** {run.completed_at.strftime('%Y-%m-%d %H:%M:%S')}")
            duration = (run.completed_at - run.started_at).total_seconds()
            lines.append(f"**Duration:** {duration:.1f} seconds")
        lines.append("")

        # Configuration
        lines.append("## Configuration")
        lines.append("")
        lines.append(f"- **Target Repository:** {run.config.test_repo_owner}/{run.config.test_repo_name}")
        lines.append(f"- **Base Branch:** {run.config.test_repo_branch}")
        lines.append(f"- **LLM Provider:** {run.config.llm_provider}")
        lines.append(f"- **Review Enabled:** {'No' if run.config.skip_review else 'Yes'}")
        lines.append(f"- **Regression Tests:** {'Disabled' if run.config.skip_regression else 'Enabled'}")
        lines.append(f"- **Dry Run:** {'Yes' if run.config.dry_run else 'No'}")
        lines.append("")

        # Summary metrics
        if run.metrics:
            lines.extend(self._format_metrics_section(run.metrics))

        # Results table
        lines.extend(self._format_results_table(run.results))

        # Failures section
        failures = [r for r in run.results if not r.is_successful]
        if failures:
            lines.extend(self._format_failures_section(failures))

        # Footer
        lines.append("---")
        lines.append(f"*Generated by CICD AI Assistant Evaluation Framework at {datetime.now().isoformat()}*")

        with open(output_path, "w") as f:
            f.write("\n".join(lines))

        return output_path

    def _format_metrics_section(self, metrics: "EvaluationMetrics") -> list[str]:
        """Format the metrics summary section."""
        lines = []

        lines.append("## Summary Metrics")
        lines.append("")

        # Main metrics table
        lines.append("| Metric | Value |")
        lines.append("|--------|-------|")
        lines.append(f"| Total Test Cases | {metrics.total_cases} |")
        lines.append(f"| PRs Created | {metrics.prs_created} ({metrics.pr_creation_rate:.1f}%) |")
        lines.append(f"| Successful Fixes | {metrics.successful_fixes} ({metrics.fix_success_rate:.1f}%) |")

        if metrics.review_approval_rate is not None:
            lines.append(f"| Review Approval Rate | {metrics.review_approval_rate:.1f}% |")

        if metrics.regression_pass_rate is not None:
            lines.append(f"| Regression Pass Rate | {metrics.regression_pass_rate:.1f}% |")

        lines.append(f"| Average Confidence | {metrics.avg_confidence:.2f} |")
        lines.append(f"| Average Duration | {metrics.avg_duration_seconds:.1f}s |")
        lines.append("")

        # By tool breakdown
        if metrics.by_tool:
            lines.append("### Results by Tool")
            lines.append("")
            lines.append("| Tool | Total | Success Rate |")
            lines.append("|------|-------|--------------|")
            for tool, count in sorted(metrics.by_tool.items()):
                rate = metrics.success_rate_by_tool.get(tool, 0)
                lines.append(f"| {tool} | {count} | {rate:.1f}% |")
            lines.append("")

        # By signal type breakdown
        if metrics.by_signal_type:
            lines.append("### Results by Signal Type")
            lines.append("")
            lines.append("| Signal Type | Count |")
            lines.append("|-------------|-------|")
            for signal_type, count in sorted(metrics.by_signal_type.items()):
                lines.append(f"| {signal_type} | {count} |")
            lines.append("")

        # Top error codes
        if metrics.by_error_code:
            lines.append("### Top Error Codes")
            lines.append("")
            lines.append("| Error Code | Total | Success | Rate |")
            lines.append("|------------|-------|---------|------|")

            # Sort by total count descending
            sorted_codes = sorted(
                metrics.by_error_code.items(),
                key=lambda x: x[1]["total"],
                reverse=True,
            )[:10]  # Top 10

            for code, stats in sorted_codes:
                total = stats["total"]
                success = stats["success"]
                rate = (success / total * 100) if total > 0 else 0
                lines.append(f"| {code} | {total} | {success} | {rate:.0f}% |")
            lines.append("")

        return lines

    def _format_results_table(self, results: list["TestCaseResult"]) -> list[str]:
        """Format the detailed results table."""
        lines = []

        lines.append("## Detailed Results")
        lines.append("")
        lines.append("| Test Case | Tool | Error | Status | PR | Review | Confidence |")
        lines.append("|-----------|------|-------|--------|----|----|------------|")

        for result in results:
            tc = result.test_case
            status = "✓" if result.is_successful else "✗"
            pr_link = f"[PR]({result.pr_url})" if result.pr_url else "-"
            review = "-"
            if result.review_result:
                verdict = result.review_result.verdict.value
                review = verdict[:10]  # Truncate long verdicts
            confidence = f"{result.fix_confidence:.2f}" if result.fix_confidence else "-"

            lines.append(f"| {tc.id[:30]} | {tc.tool} | {tc.error_code} | {status} | {pr_link} | {review} | {confidence} |")

        lines.append("")

        return lines

    def _format_failures_section(self, failures: list["TestCaseResult"]) -> list[str]:
        """Format the failures section with details."""
        lines = []

        lines.append("## Failures")
        lines.append("")
        lines.append(f"**{len(failures)} test cases failed:**")
        lines.append("")

        for result in failures:
            tc = result.test_case
            lines.append(f"### {tc.id}")
            lines.append("")
            lines.append(f"- **Tool:** {tc.tool}")
            lines.append(f"- **Error Code:** {tc.error_code}")
            lines.append(f"- **File:** {tc.file_path}")
            lines.append(f"- **Status:** {result.status.value}")

            if result.error_message:
                lines.append(f"- **Error:** {result.error_message}")

            if result.error_stage:
                lines.append(f"- **Stage:** {result.error_stage}")

            if result.review_result and result.review_result.feedback:
                lines.append("")
                lines.append("**Review Feedback:**")
                lines.append("```")
                lines.append(result.review_result.feedback[:500])  # Truncate long feedback
                lines.append("```")

            if result.regression_result and result.regression_result.failure_details:
                lines.append("")
                lines.append("**Regression Failures:**")
                for detail in result.regression_result.failure_details[:3]:  # First 3
                    if "test" in detail:
                        lines.append(f"- {detail['test']}: {detail.get('message', 'No message')}")

            lines.append("")

        return lines


def generate_comparison_report(
    run1: "EvaluationRun",
    run2: "EvaluationRun",
    output_dir: str | Path,
) -> Path:
    """
    Generate a comparison report between two evaluation runs.

    Useful for measuring improvement over time or A/B testing configurations.

    Args:
        run1: The baseline run
        run2: The comparison run
        output_dir: Directory to write the report

    Returns:
        Path to the generated report
    """
    from .metrics import compare_runs

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / f"comparison_{run1.run_id}_vs_{run2.run_id}.md"

    lines = []

    lines.append("# Evaluation Comparison Report")
    lines.append("")
    lines.append(f"**Baseline Run:** {run1.run_id} ({run1.started_at.strftime('%Y-%m-%d')})")
    lines.append(f"**Comparison Run:** {run2.run_id} ({run2.started_at.strftime('%Y-%m-%d')})")
    lines.append("")

    if run1.metrics and run2.metrics:
        comparison = compare_runs(run1.metrics, run2.metrics)

        lines.append("## Metrics Comparison")
        lines.append("")
        lines.append("| Metric | Baseline | Comparison | Delta | Change |")
        lines.append("|--------|----------|------------|-------|--------|")

        m1, m2 = run1.metrics, run2.metrics

        # Fix success rate
        delta = comparison["fix_success_rate"]
        arrow = "↑" if delta["delta"] > 0 else "↓" if delta["delta"] < 0 else "→"
        lines.append(f"| Fix Success Rate | {m1.fix_success_rate:.1f}% | {m2.fix_success_rate:.1f}% | {delta['delta']:+.1f}% | {arrow} {abs(delta['percent_change']):.1f}% |")

        # PR creation rate
        delta = comparison["pr_creation_rate"]
        arrow = "↑" if delta["delta"] > 0 else "↓" if delta["delta"] < 0 else "→"
        lines.append(f"| PR Creation Rate | {m1.pr_creation_rate:.1f}% | {m2.pr_creation_rate:.1f}% | {delta['delta']:+.1f}% | {arrow} {abs(delta['percent_change']):.1f}% |")

        # Confidence
        delta = comparison["avg_confidence"]
        arrow = "↑" if delta["delta"] > 0 else "↓" if delta["delta"] < 0 else "→"
        lines.append(f"| Avg Confidence | {m1.avg_confidence:.2f} | {m2.avg_confidence:.2f} | {delta['delta']:+.2f} | {arrow} {abs(delta['percent_change']):.1f}% |")

        lines.append("")

        # By tool comparison
        if comparison.get("by_tool"):
            lines.append("### By Tool")
            lines.append("")
            lines.append("| Tool | Baseline | Comparison | Change |")
            lines.append("|------|----------|------------|--------|")

            for tool, delta in sorted(comparison["by_tool"].items()):
                baseline = m1.success_rate_by_tool.get(tool, 0)
                comparison_val = m2.success_rate_by_tool.get(tool, 0)
                arrow = "↑" if delta["delta"] > 0 else "↓" if delta["delta"] < 0 else "→"
                lines.append(f"| {tool} | {baseline:.1f}% | {comparison_val:.1f}% | {arrow} {delta['delta']:+.1f}% |")

            lines.append("")

    # Summary
    lines.append("## Summary")
    lines.append("")

    if run1.metrics and run2.metrics:
        improvement = run2.metrics.fix_success_rate - run1.metrics.fix_success_rate
        if improvement > 0:
            lines.append(f"The comparison run shows **improvement** with a {improvement:.1f}% increase in fix success rate.")
        elif improvement < 0:
            lines.append(f"The comparison run shows **regression** with a {abs(improvement):.1f}% decrease in fix success rate.")
        else:
            lines.append("The comparison run shows **no change** in fix success rate.")

    lines.append("")
    lines.append("---")
    lines.append(f"*Generated at {datetime.now().isoformat()}*")

    with open(output_path, "w") as f:
        f.write("\n".join(lines))

    return output_path
