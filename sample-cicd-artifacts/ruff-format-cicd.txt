--- app/agents/prospecting_orchestrator.py
+++ app/agents/prospecting_orchestrator.py
@@ -38,10 +38,11 @@
 
 logger = get_logger(__name__)
 
+
 # Session context data structure
 class SessionContext:
     """Session context for managing user interactions within a session."""
-    
+
     def __init__(self, session_id: str, user_id: str):
         self.session_id = session_id
         self.user_id = user_id
@@ -50,49 +51,50 @@
         self.status = "active"
         self.last_activity = datetime.now()
         self.ria_detection_result: Optional[Dict[str, Any]] = None
-    
+
     def increment_interaction(self) -> str:
         """Increment interaction counter and return run_id for this interaction."""
         self.interaction_count += 1
         self.last_activity = datetime.now()
         return f"{self.session_id}_{self.interaction_count:03d}"
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert session context to dictionary for storage."""
         return {
-            'session_id': self.session_id,
-            'user_id': self.user_id,
-            'start_time': self.start_time.isoformat(),
-            'interaction_count': self.interaction_count,
-            'status': self.status,
-            'last_activity': self.last_activity.isoformat()
+            "session_id": self.session_id,
+            "user_id": self.user_id,
+            "start_time": self.start_time.isoformat(),
+            "interaction_count": self.interaction_count,
+            "status": self.status,
+            "last_activity": self.last_activity.isoformat(),
         }
-    
+
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'SessionContext':
+    def from_dict(cls, data: Dict[str, Any]) -> "SessionContext":
         """Create session context from dictionary."""
-        session = cls(data['session_id'], data['user_id'])
-        session.start_time = datetime.fromisoformat(data['start_time'])
-        session.interaction_count = data['interaction_count']
-        session.status = data['status']
-        session.last_activity = datetime.fromisoformat(data['last_activity'])
+        session = cls(data["session_id"], data["user_id"])
+        session.start_time = datetime.fromisoformat(data["start_time"])
+        session.interaction_count = data["interaction_count"]
+        session.status = data["status"]
+        session.last_activity = datetime.fromisoformat(data["last_activity"])
         return session
 
+
 class PromptAnalyzer:
     """
     Analyzes user prompts to determine if they contain specific company names or general search requests.
     """
-    
+
     def __init__(self):
         self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=get_openai_api_key())
-        
+
     async def analyze_prompt(self, prompt: str) -> Dict[str, Any]:
         """
         Analyze a user prompt to determine the type of request using LLM.
-        
-        Args:  
+
+        Args:
             prompt: User's input prompt
-            
+
         Returns:
             Dictionary with analysis results:
             - prompt_type: "specific_company", "general_search", or "off_topic"
@@ -104,67 +106,62 @@
             result = await self._llm_based_classification(prompt)
             logger.info("Prompt analysis completed", extra={"prompt_type": result.get("prompt_type"), "confidence": result.get("confidence", 0.0)})
             return result
-            
+
         except Exception as e:
             logger.exception("Prompt analysis failed")
             print(f"‚ö†Ô∏è Prompt analysis failed: {e}")
             # Fallback to off_topic classification
-            return {
-                'prompt_type': 'off_topic',
-                'confidence': 0.0,
-                'extracted_data': {}
-            }
-    
+            return {"prompt_type": "off_topic", "confidence": 0.0, "extracted_data": {}}
+
     async def _llm_based_classification(self, prompt: str) -> Dict[str, Any]:
         """
         Use LLM to classify the prompt into one of four categories.
         """
         system_prompt = get_prompt_analysis_system_prompt()
         user_prompt = f"Analyze this prompt: {prompt}"
-        
+
         try:
-            with trace_operation("prompt_analysis", {
-                "prompt_length": len(prompt),
-                "model": "gpt-4o-mini"
-            }):
+            with trace_operation("prompt_analysis", {"prompt_length": len(prompt), "model": "gpt-4o-mini"}):
                 messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]
                 response = await self.llm.ainvoke(messages)
-            
+
             # Parse JSON response
             response_text = response.content.strip()
-            if '```json' in response_text:
-                json_start = response_text.find('```json') + 7
-                json_end = response_text.find('```', json_start)
+            if "```json" in response_text:
+                json_start = response_text.find("```json") + 7
+                json_end = response_text.find("```", json_start)
                 response_text = response_text[json_start:json_end].strip()
-            
+
             result = json.loads(response_text)
-            
+
             # Add off_topic message if needed
-            if result['prompt_type'] == 'off_topic':
-                result['extracted_data']['message'] = "I can help you with company research in two ways:\n\n1. **Specific Company Research**: Provide a company name and I'll research and enrich that specific company (e.g., 'Research Sequoia Capital' or 'Tell me about BlackRock')\n\n2. **General Company Search**: Search for companies matching specific criteria (e.g., 'Find VC firms in London focusing on fintech' or 'Show me private equity firms in healthcare')\n\nPlease provide either a specific company name to research or search criteria for finding companies."
-            
+            if result["prompt_type"] == "off_topic":
+                result["extracted_data"]["message"] = (
+                    "I can help you with company research in two ways:\n\n1. **Specific Company Research**: Provide a company name and I'll research and enrich that specific company (e.g., 'Research Sequoia Capital' or 'Tell me about BlackRock')\n\n2. **General Company Search**: Search for companies matching specific criteria (e.g., 'Find VC firms in London focusing on fintech' or 'Show me private equity firms in healthcare')\n\nPlease provide either a specific company name to research or search criteria for finding companies."
+                )
+
             return result
-            
+
         except Exception as e:
             print(f"‚ö†Ô∏è LLM classification failed: {e}")
             return {
-                'prompt_type': 'off_topic',
-                'confidence': 0.0,
-                'extracted_data': {
-                    'message': "I can help you with company research in two ways:\n\n1. **Specific Company Research**: Provide a company name and I'll research and enrich that specific company (e.g., 'Research Sequoia Capital' or 'Tell me about BlackRock')\n\n2. **General Company Search**: Search for companies matching specific criteria (e.g., 'Find VC firms in London focusing on fintech' or 'Show me private equity firms in healthcare')\n\nPlease provide either a specific company name to research or search criteria for finding companies."
-                }
+                "prompt_type": "off_topic",
+                "confidence": 0.0,
+                "extracted_data": {
+                    "message": "I can help you with company research in two ways:\n\n1. **Specific Company Research**: Provide a company name and I'll research and enrich that specific company (e.g., 'Research Sequoia Capital' or 'Tell me about BlackRock')\n\n2. **General Company Search**: Search for companies matching specific criteria (e.g., 'Find VC firms in London focusing on fintech' or 'Show me private equity firms in healthcare')\n\nPlease provide either a specific company name to research or search criteria for finding companies."
+                },
             }
 
 
 class ProspectingOrchestrator:
     """
     Enhanced prospecting orchestrator that coordinates multiple sub-agents for comprehensive company research.
-    
+
     This orchestrator provides three main execution modes:
     1. execute_from_prompt(): Process natural language user prompts
     2. execute_from_data(): Process structured company data
     3. execute_hybrid(): Automatically choose the best approach
-    
+
     Features:
     - Intelligent prompt analysis and classification
     - Multi-agent coordination with error handling
@@ -172,11 +169,11 @@
     - Interactive mode for testing
     - MCP tool integration for enhanced capabilities
     """
-    
+
     def __init__(self, db=None, extractor_llm=None, output_dir="output/prospecting", enable_debugging: Optional[bool] = None):
         """
         Initialize the prospecting orchestrator with sub-agents and configuration.
-        
+
         Args:
             db: Optional database instance (will use global if not provided)
             extractor_llm: LangChain LLM instance for data extraction
@@ -185,7 +182,7 @@
         """
         # Initialize LangSmith first
         initialize_langsmith()
-        
+
         # Resolve debugging flag from env if not explicitly provided
         resolved_enable_debugging = get_enable_debugging() if enable_debugging is None else enable_debugging
 
@@ -194,124 +191,128 @@
         self.output_dir = effective_output_dir
         self.extractor_llm = extractor_llm
         self.enable_debugging = resolved_enable_debugging
-        
+
         # Always use None for database - sub-agents will get global instance
         self.db = None
         print("üîó ProspectingOrchestrator: Initialized to use global database instance")
-        
+
         # Initialize MCP tools first
         self.mcp_tools = {}
-        
+
         # Initialize sub-agents without database - they will get global instance
         self.sub_agents = {
-            'company_search': CompanySearchAgent(output_dir=effective_output_dir, db=None),
-            'coresignal': CoreSignalSubAgent(output_dir=effective_output_dir, mcp_tools=self.mcp_tools, db=None),
-            'web_research': WebResearchAgent(output_dir=effective_output_dir, mcp_tools=self.mcp_tools, db=None),
-            'person_enrich': PersonEnrichAgent(output_dir=effective_output_dir, num_people=2, db=None),
-            'company_enrich': CompanyEnrichAgent(output_dir=effective_output_dir, db=None),
-            'youtube_media': YouTubeMediaAgent(output_dir=effective_output_dir, db=None)
+            "company_search": CompanySearchAgent(output_dir=effective_output_dir, db=None),
+            "coresignal": CoreSignalSubAgent(output_dir=effective_output_dir, mcp_tools=self.mcp_tools, db=None),
+            "web_research": WebResearchAgent(output_dir=effective_output_dir, mcp_tools=self.mcp_tools, db=None),
+            "person_enrich": PersonEnrichAgent(output_dir=effective_output_dir, num_people=2, db=None),
+            "company_enrich": CompanyEnrichAgent(output_dir=effective_output_dir, db=None),
+            "youtube_media": YouTubeMediaAgent(output_dir=effective_output_dir, db=None),
         }
-        
+
         # Initialize prompt analyzer
         self.prompt_analyzer = PromptAnalyzer()
-        
+
         # Session management
         self.active_sessions: Dict[str, SessionContext] = {}
         self.session_timeout_hours = 24  # Default session timeout
-        
+
         # Register cleanup handlers
         atexit.register(self._cleanup_all_sessions)
         signal.signal(signal.SIGINT, self._signal_handler)
         signal.signal(signal.SIGTERM, self._signal_handler)
-        
+
         debug_status = "with debugging enabled" if self.enable_debugging else "without debugging"
         print(f"‚úÖ ProspectingOrchestrator initialized {debug_status}")
-        logger.info("ProspectingOrchestrator initialized", extra={"output_dir": output_dir, "enable_debugging": self.enable_debugging, "sub_agents_count": len(self.sub_agents)})
+        logger.info(
+            "ProspectingOrchestrator initialized",
+            extra={"output_dir": output_dir, "enable_debugging": self.enable_debugging, "sub_agents_count": len(self.sub_agents)},
+        )
 
     def _signal_handler(self, signum, frame):
         """Handle termination signals to ensure session cleanup."""
         print(f"\nüõë Received signal {signum}, cleaning up sessions...")
         self._cleanup_all_sessions()
         exit(0)
-    
+
     async def start_session(self, user_id: str = None) -> SessionContext:
         """
         Start a new session for a user.
-        
+
         Args:
             user_id: User identifier (generated if not provided)
-            
+
         Returns:
             SessionContext object with session details
         """
         if not user_id:
             user_id = f"user_{uuid.uuid4().hex[:8]}"
-            
+
         session_id = f"session_{uuid.uuid4().hex[:12]}"
         start_time = datetime.now()
-        
-        session_context = SessionContext(
-            session_id=session_id,
-            user_id=user_id
-        )
-        
+
+        session_context = SessionContext(session_id=session_id, user_id=user_id)
+
         # Always get global database instance
         self.db = await get_global_db()
         print(f"üîó ProspectingOrchestrator: Using global database instance: {id(self.db)}")
-        
+
         # Store session in database
         session_data = {
-            'session_id': session_id,
-            'user_id': user_id,
-            'start_time': start_time,
-            'last_activity': start_time,
-            'interaction_count': 0,
-            'status': 'active'
+            "session_id": session_id,
+            "user_id": user_id,
+            "start_time": start_time,
+            "last_activity": start_time,
+            "interaction_count": 0,
+            "status": "active",
         }
-        
+
         await self.db.store_session(session_data)
-        
+
         print(f"üöÄ Started new session: {session_id}")
         print(f"üë§ User ID: {user_id}")
         print(f"‚è∞ Start time: {start_time}")
-        
+
         logger.info("Session started", extra={"session_id": session_id, "user_id": user_id, "start_time": start_time.isoformat()})
-        
+
         return session_context
-    
+
     async def end_session(self, session_context: SessionContext) -> None:
         """
         End the current session.
-        
+
         Args:
             session_context: Session context to end
         """
         if not session_context:
             return
-            
+
         end_time = datetime.now()
         session_context.status = "ended"
-        
+
         # Always get global database instance
         self.db = await get_global_db()
-        
+
         # Update session in database
-        await self.db.update_session_status(
-            session_context.session_id, 
-            "ended", 
-            end_time
-        )
-        
+        await self.db.update_session_status(session_context.session_id, "ended", end_time)
+
         print(f"üèÅ Ended session: {session_context.session_id}")
         print(f"üìä Total interactions: {session_context.interaction_count}")
         print(f"‚è∞ End time: {end_time}")
-        
-        logger.info("Session ended", extra={"session_id": session_context.session_id, "user_id": session_context.user_id, "interaction_count": session_context.interaction_count, "end_time": end_time.isoformat()})
-    
+
+        logger.info(
+            "Session ended",
+            extra={
+                "session_id": session_context.session_id,
+                "user_id": session_context.user_id,
+                "interaction_count": session_context.interaction_count,
+                "end_time": end_time.isoformat(),
+            },
+        )
+
     async def store_session_interaction(self, session_context: SessionContext, run_id: str, prompt: str, workflow_type: str) -> None:
         """
         Store a session interaction in the database.
-        
+
         Args:
             session_context: Current session context
             run_id: Unique run identifier
@@ -320,42 +321,50 @@
         """
         if not session_context:
             return
-            
+
         # Always get global database instance
         self.db = await get_global_db()
-            
+
         interaction_id = f"interaction_{uuid.uuid4().hex[:12]}"
         start_time = datetime.now()
-        
+
         interaction_data = {
-            'interaction_id': interaction_id,
-            'session_id': session_context.session_id,
-            'run_id': run_id,
-            'interaction_number': session_context.interaction_count,
-            'prompt': prompt,
-            'workflow_type': workflow_type,
-            'start_time': start_time
+            "interaction_id": interaction_id,
+            "session_id": session_context.session_id,
+            "run_id": run_id,
+            "interaction_number": session_context.interaction_count,
+            "prompt": prompt,
+            "workflow_type": workflow_type,
+            "start_time": start_time,
         }
-        
+
         await self.db.store_session_interaction(interaction_data)
-        
+
         # Update session interaction count and last activity
         session_context.interaction_count += 1
         session_context.last_activity = start_time
-        
+
         session_data = {
-            'session_id': session_context.session_id,
-            'user_id': session_context.user_id,
-            'start_time': session_context.start_time,
-            'last_activity': start_time,
-            'interaction_count': session_context.interaction_count,
-            'status': session_context.status
+            "session_id": session_context.session_id,
+            "user_id": session_context.user_id,
+            "start_time": session_context.start_time,
+            "last_activity": start_time,
+            "interaction_count": session_context.interaction_count,
+            "status": session_context.status,
         }
-        
+
         await self.db.store_session(session_data)
-        
-        logger.info("Session interaction stored", extra={"session_id": session_context.session_id, "run_id": run_id, "workflow_type": workflow_type, "interaction_number": session_context.interaction_count})
 
+        logger.info(
+            "Session interaction stored",
+            extra={
+                "session_id": session_context.session_id,
+                "run_id": run_id,
+                "workflow_type": workflow_type,
+                "interaction_number": session_context.interaction_count,
+            },
+        )
+
     def _cleanup_all_sessions(self):
         """Clean up all active sessions (called on exit)."""
         if self.active_sessions:
@@ -364,6 +373,7 @@
                 try:
                     # Use asyncio.run if we're not already in an async context
                     import asyncio
+
                     try:
                         loop = asyncio.get_running_loop()
                         # We're in an async context, create a task
@@ -373,76 +383,74 @@
                         asyncio.run(self.end_session(self.active_sessions[session_id]))
                 except Exception as e:
                     print(f"‚ö†Ô∏è Failed to cleanup session {session_id}: {e}")
-    
+
     async def cleanup_expired_sessions(self) -> int:
         """
         Clean up expired sessions based on timeout.
-        
+
         Returns:
             Number of sessions cleaned up
         """
         cleaned_count = 0
         timeout_threshold = datetime.now() - timedelta(hours=self.session_timeout_hours)
-        
+
         for session_id, session_context in list(self.active_sessions.items()):
             if session_context.last_activity < timeout_threshold:
                 await self.end_session(session_context)
                 cleaned_count += 1
-        
+
         if cleaned_count > 0:
             print(f"üßπ Cleaned up {cleaned_count} expired sessions")
-        
+
         return cleaned_count
 
     async def execute(self, prompt: str, user_id: str, session_context: SessionContext, run_id: str | None = None) -> Dict[str, Any]:
         """
         Execute the prospecting orchestrator with session-aware run_id generation.
-        
+
         Args:
             prompt: User's input prompt
             user_id: User identifier for multi-tenant isolation
             session_context: Session context for session-aware execution
-            
+
         Returns:
             Dictionary with execution results including session information
         """
         start_time = datetime.now()
-        
+
         # Always get global database instance
         self.db = await get_global_db()
         print(f"üîó ProspectingOrchestrator.execute: Using global database instance: {id(self.db)}")
-        
+
         # Use provided run_id from router if available; otherwise generate from session context
         if not run_id:
             run_id = f"{session_context.session_id}_run_{session_context.interaction_count + 1:03d}"
         output_file = f"prospecting_results_{session_context.session_id}_{session_context.interaction_count:03d}.json"
-        
+
         print(f"üîÑ Session: {session_context.session_id}")
         print(f"üë§ User: {session_context.user_id}")
         print(f"üìù Interaction: {session_context.interaction_count}")
         print(f"üÜî Run ID: {run_id}")
-        
+
         print(f"üìÑ Output file: {output_file}")
         print(f"üí¨ User prompt: {prompt}")
-        
-        logger.info("Orchestrator execution started", extra={"run_id": run_id, "user_id": user_id, "session_id": session_context.session_id, "prompt_length": len(prompt)})
+
+        logger.info(
+            "Orchestrator execution started",
+            extra={"run_id": run_id, "user_id": user_id, "session_id": session_context.session_id, "prompt_length": len(prompt)},
+        )
         try:
             await ProgressStore.instance().set_progress(self.db, run_id, 10)
         except Exception:
             pass
-        
+
         # Store session interaction if session context is provided
-        await self.store_session_interaction(
-            session_context, 
-            run_id, 
-            prompt, 
-            "prospecting_orchestration"
-        )
-        
+        await self.store_session_interaction(session_context, run_id, prompt, "prospecting_orchestration")
+
         # Ensure user_id is available
         if not user_id:
             user_id = session_context.user_id
-        
+
         # Create shared output file
         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
         if self.enable_debugging:
@@ -453,19 +461,19 @@
             print(f"üêõ Debug file created: {shared_output_file}")
         else:
             shared_output_file = None
-        
+
         # Ensure output directory exists only when debugging output is enabled
         if self.enable_debugging:
             os.makedirs(self.output_dir, exist_ok=True)
-        
+
         # Create main LangSmith run context
         run_metadata = {
-            'prompt': prompt,
-            'workflow_type': 'prospecting_orchestration',
-            'session_id': session_context.session_id,
-            'interaction_count': session_context.interaction_count
+            "prompt": prompt,
+            "workflow_type": "prospecting_orchestration",
+            "session_id": session_context.session_id,
+            "interaction_count": session_context.interaction_count,
         }
-        
+
         with create_main_run(run_id, user_id, session_context.session_id, run_metadata):
             try:
                 # Retrieve and display user bio
@@ -477,145 +485,176 @@
                         print("=" * 60)
                         print(f"üè¢ Firm Description ({len(user_bio['firm_description'])} characters):")
                         print("-" * 40)
-                        print(user_bio['firm_description'])
+                        print(user_bio["firm_description"])
                         print(f"\nüéØ Key Differentiators ({len(user_bio['key_differentiators'])} characters):")
                         print("-" * 40)
-                        print(user_bio['key_differentiators'])
+                        print(user_bio["key_differentiators"])
                         print(f"\nüéØ Key Objectives ({len(user_bio['key_objectives'])} characters):")
                         print("-" * 40)
-                        print(user_bio['key_objectives'])
+                        print(user_bio["key_objectives"])
                         print("=" * 60)
                     else:
                         print(f"‚ÑπÔ∏è No user profile found for user: {user_id}")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Error retrieving user profile: {e}")
-                
+
                 # Step 1: Analyze prompt and determine workflow
                 print(f"üîç Step 1: Analyzing user prompt")
                 prompt_analysis = await self.prompt_analyzer.analyze_prompt(prompt)
-                workflow_type = prompt_analysis['prompt_type']
+                workflow_type = prompt_analysis["prompt_type"]
                 print(f"‚úÖ Prompt analysis complete: {workflow_type} (confidence: {prompt_analysis['confidence']:.2f})")
-                
-                logger.info("Workflow routing", extra={"run_id": run_id, "workflow_type": workflow_type, "confidence": prompt_analysis.get("confidence", 0.0)})
-                
+
+                logger.info(
+                    "Workflow routing", extra={"run_id": run_id, "workflow_type": workflow_type, "confidence": prompt_analysis.get("confidence", 0.0)}
+                )
+
                 # Step 2: Route to appropriate workflow
-                if workflow_type == 'specific_company':
+                if workflow_type == "specific_company":
                     return await self._execute_specific_company_workflow(
-                        prompt_analysis['extracted_data'], run_id, user_id, shared_output_file, self.db, get_enable_postgres_storage(), session_context, workflow_type
+                        prompt_analysis["extracted_data"],
+                        run_id,
+                        user_id,
+                        shared_output_file,
+                        self.db,
+                        get_enable_postgres_storage(),
+                        session_context,
+                        workflow_type,
                     )
-                elif workflow_type == 'general_search':
+                elif workflow_type == "general_search":
                     return await self._execute_company_search_workflow(
-                        prompt_analysis['extracted_data'], run_id, user_id, shared_output_file, self.db, get_enable_postgres_storage(), session_context, workflow_type
+                        prompt_analysis["extracted_data"],
+                        run_id,
+                        user_id,
+                        shared_output_file,
+                        self.db,
+                        get_enable_postgres_storage(),
+                        session_context,
+                        workflow_type,
                     )
-                elif workflow_type == 'off_topic':
+                elif workflow_type == "off_topic":
                     return {
-                        'success': True,
-                        'workflow_type': 'off_topic',
-                        'message': prompt_analysis['extracted_data'].get('message', 'Off-topic request'),
-                        'prompt_analysis': prompt_analysis,
-                        'run_id': run_id,
-                        'user_id': user_id,
-                        'session_id': session_context.session_id,
-                        'interaction_number': session_context.interaction_count,
-                        'execution_time_ms': int((datetime.now() - start_time).total_seconds() * 1000),
-                        'next_step': 'wait_for_user_input'
+                        "success": True,
+                        "workflow_type": "off_topic",
+                        "message": prompt_analysis["extracted_data"].get("message", "Off-topic request"),
+                        "prompt_analysis": prompt_analysis,
+                        "run_id": run_id,
+                        "user_id": user_id,
+                        "session_id": session_context.session_id,
+                        "interaction_number": session_context.interaction_count,
+                        "execution_time_ms": int((datetime.now() - start_time).total_seconds() * 1000),
+                        "next_step": "wait_for_user_input",
                     }
                 else:
                     # Fallback for any unexpected types
                     return {
-                        'success': False,
-                        'error': 'Unexpected prompt type - please provide a specific company name or clear search criteria',
-                        'prompt_analysis': prompt_analysis,
-                        'run_id': run_id,
-                        'user_id': user_id,
-                        'session_id': session_context.session_id,
-                        'execution_time_ms': int((datetime.now() - start_time).total_seconds() * 1000)
+                        "success": False,
+                        "error": "Unexpected prompt type - please provide a specific company name or clear search criteria",
+                        "prompt_analysis": prompt_analysis,
+                        "run_id": run_id,
+                        "user_id": user_id,
+                        "session_id": session_context.session_id,
+                        "execution_time_ms": int((datetime.now() - start_time).total_seconds() * 1000),
                     }
-                        
+
             except Exception as e:
                 import traceback
+
                 print(f"\n‚ùå [ProspectingOrchestrator ERROR] Exception in execute():\n{traceback.format_exc()}")
-                
+
                 execution_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
-                
-                logger.exception("Orchestrator execution failed", extra={"run_id": run_id, "user_id": user_id, "session_id": session_context.session_id})
-                
+
+                logger.exception(
+                    "Orchestrator execution failed", extra={"run_id": run_id, "user_id": user_id, "session_id": session_context.session_id}
+                )
+
                 return {
-                    'success': False,
-                    'error': str(e),
-                    'run_id': run_id,
-                    'user_id': user_id,
-                    'session_id': session_context.session_id,
-                    'execution_time_ms': execution_time_ms
+                    "success": False,
+                    "error": str(e),
+                    "run_id": run_id,
+                    "user_id": user_id,
+                    "session_id": session_context.session_id,
+                    "execution_time_ms": execution_time_ms,
                 }
             finally:
                 # Note: Database connection is managed globally, no need to close here
                 pass
 
-    async def _execute_specific_company_workflow(self, company_data: Dict[str, Any], run_id: str, user_id: str, 
-                                               shared_output_file: str, db: ProspectingDB, postgres_enabled: bool, session_context: SessionContext, workflow_type: str) -> Dict[str, Any]:
+    async def _execute_specific_company_workflow(
+        self,
+        company_data: Dict[str, Any],
+        run_id: str,
+        user_id: str,
+        shared_output_file: str,
+        db: ProspectingDB,
+        postgres_enabled: bool,
+        session_context: SessionContext,
+        workflow_type: str,
+    ) -> Dict[str, Any]:
         """
         Execute the traditional prospecting workflow for a specific company.
         """
         start_time = datetime.now()
-        
-        company_name = company_data.get('company_name', 'Unknown Company')
+
+        company_name = company_data.get("company_name", "Unknown Company")
         logger.info("Specific company workflow started", extra={"run_id": run_id, "company_name": company_name, "workflow_type": workflow_type})
-        
+
         print(f"üéØ Executing specific company workflow for: {company_data.get('company_name', 'Unknown')}")
-        
+
         # Prepare company data for existing workflow
-        company_name = company_data.get('company_name', 'Unknown Company')
-        location = company_data.get('location')
-        hq_city = company_data.get('hq_city')
-        hq_country = company_data.get('hq_country')
-        focus_area = company_data.get('focus_area')
-        
+        company_name = company_data.get("company_name", "Unknown Company")
+        location = company_data.get("location")
+        hq_city = company_data.get("hq_city")
+        hq_country = company_data.get("hq_country")
+        focus_area = company_data.get("focus_area")
+
         # Set up company_data like in original orchestrator
         company_data_for_workflow = {
-            'name': company_name,
-            'location': location or 'Unknown',
-            'hq_city': hq_city,
-            'hq_country': hq_country,
-            'focus_area': focus_area or 'General prospecting',
-            'output_file': shared_output_file
+            "name": company_name,
+            "location": location or "Unknown",
+            "hq_city": hq_city,
+            "hq_country": hq_country,
+            "focus_area": focus_area or "General prospecting",
+            "output_file": shared_output_file,
         }
-        
+
         # Execute the original workflow steps
         try:
             # Step 1: Domain Discovery and Company ID Generation
             print(f"üîç Step 1: Domain discovery for {company_name}")
             domain_discovery_result = await self._discover_domain_and_generate_id(company_data_for_workflow, shared_output_file)
-            
+
             # Early exit if no valid domain found
-            found_url = domain_discovery_result.get('found_url')
-            if (not domain_discovery_result['success'] or 
-                found_url is None or 
-                (isinstance(found_url, str) and found_url.strip().lower() in ('null', '[null]', 'none', '')) or
-                (isinstance(found_url, list) and (not found_url or found_url[0] in (None, 'null', '[null]', 'none', '')))):
+            found_url = domain_discovery_result.get("found_url")
+            if (
+                not domain_discovery_result["success"]
+                or found_url is None
+                or (isinstance(found_url, str) and found_url.strip().lower() in ("null", "[null]", "none", ""))
+                or (isinstance(found_url, list) and (not found_url or found_url[0] in (None, "null", "[null]", "none", "")))
+            ):
                 error_msg = "No valid company domain could be found. Sub-agents were not executed."
                 print(f"‚ùå {error_msg}")
                 logger.warning("Domain discovery failed", extra={"run_id": run_id, "company_name": company_name, "error": error_msg})
                 try:
                     # Mark component states for FE polling stop logic
-                    await ProgressStore.instance().update_component_status(self.db, run_id, "domain_resolution", "failed", "DOMAIN_NOT_FOUND", error_msg)
-                    await ProgressStore.instance().update_component_status(self.db, run_id, "company_enrichment", "failed", "DOMAIN_NOT_FOUND", error_msg)
+                    await ProgressStore.instance().update_component_status(
+                        self.db, run_id, "domain_resolution", "failed", "DOMAIN_NOT_FOUND", error_msg
+                    )
+                    await ProgressStore.instance().update_component_status(
+                        self.db, run_id, "company_enrichment", "failed", "DOMAIN_NOT_FOUND", error_msg
+                    )
                 except Exception:
                     pass
-                return {
-                    'success': False,
-                    'error': error_msg,
-                    'run_id': run_id,
-                    'company_name': company_name,
-                    'execution_time_ms': 0
-                }
-            
-            company_id = domain_discovery_result['company_id']
-            found_url = domain_discovery_result['found_url']
-            found_by_perplexity = domain_discovery_result['found_by_perplexity']
-            
-            logger.info("Domain discovery completed", extra={"run_id": run_id, "company_id": company_id, "found_url": found_url, "found_by_perplexity": found_by_perplexity})
-            
+                return {"success": False, "error": error_msg, "run_id": run_id, "company_name": company_name, "execution_time_ms": 0}
+
+            company_id = domain_discovery_result["company_id"]
+            found_url = domain_discovery_result["found_url"]
+            found_by_perplexity = domain_discovery_result["found_by_perplexity"]
+
+            logger.info(
+                "Domain discovery completed",
+                extra={"run_id": run_id, "company_id": company_id, "found_url": found_url, "found_by_perplexity": found_by_perplexity},
+            )
+
             # Start the prospecting run to ensure the run_id exists in the database
             if postgres_enabled and db:
                 try:
@@ -626,78 +665,88 @@
                         search_params=None,  # No search params for specific company
                         company_name=company_name,
                         company_id=company_id,
-                        workflow_type=workflow_type
+                        workflow_type=workflow_type,
                     )
                     print(f"‚úÖ Started prospecting run: {run_id}")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Warning: Failed to start prospecting run: {e}")
                     # Continue anyway - the workflow might still work
-            
+
             # Store company in PostgreSQL if enabled
             if postgres_enabled and db:
                 try:
                     company_data_for_db = {
-                        'name': company_name,
-                        'website_url': found_url,
-                        'location': location,  # Keep for backward compatibility
-                        'hq_city': hq_city,
-                        'hq_country': hq_country,
-                        'focus_area': focus_area
+                        "name": company_name,
+                        "website_url": found_url,
+                        "location": location,  # Keep for backward compatibility
+                        "hq_city": hq_city,
+                        "hq_country": hq_country,
+                        "focus_area": focus_area,
                     }
                     await db.store_company(
-                        run_id=run_id,
-                        company_data=company_data_for_db,
-                        company_id=company_id,
-                        user_id=user_id,
-                        session_id=session_context.session_id
+                        run_id=run_id, company_data=company_data_for_db, company_id=company_id, user_id=user_id, session_id=session_context.session_id
                     )
                     print(f"‚úÖ Company stored in PostgreSQL with ID: {company_id}")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Error storing company in PostgreSQL: {e}")
-            
+
             # Step 2: Phase 1 - Parallel Data Collection
             print(f"üîÑ Step 2: Phase 1 - Parallel data collection")
             logger.info("Phase 1 data collection started", extra={"run_id": run_id, "company_id": company_id})
             phase1_result = await self._run_phase1_parallel(
-                company_data_for_workflow, run_id, company_id, domain_discovery_result['found_url'], 
-                domain_discovery_result['found_urls'], domain_discovery_result['found_by_perplexity'], 
-                shared_output_file, user_id, session_id=session_context.session_id
+                company_data_for_workflow,
+                run_id,
+                company_id,
+                domain_discovery_result["found_url"],
+                domain_discovery_result["found_urls"],
+                domain_discovery_result["found_by_perplexity"],
+                shared_output_file,
+                user_id,
+                session_id=session_context.session_id,
             )
-            
-            if not phase1_result['success']:
-                logger.error("Phase 1 data collection failed", extra={"run_id": run_id, "company_id": company_id, "error": phase1_result.get("error")})
+
+            if not phase1_result["success"]:
+                logger.error(
+                    "Phase 1 data collection failed", extra={"run_id": run_id, "company_id": company_id, "error": phase1_result.get("error")}
+                )
                 return {
-                    'success': False,
-                    'error': f"Phase 1 failed: {phase1_result['error']}",
-                    'run_id': run_id,
-                    'company_name': company_name,
-                    'execution_time_ms': 0
+                    "success": False,
+                    "error": f"Phase 1 failed: {phase1_result['error']}",
+                    "run_id": run_id,
+                    "company_name": company_name,
+                    "execution_time_ms": 0,
                 }
-            
+
             logger.info("Phase 1 data collection completed", extra={"run_id": run_id, "company_id": company_id})
-            
+
             # Step 3: Phase 2 - Parallel Enrichment
             print(f"üîÑ Step 3: Phase 2 - Parallel enrichment")
             logger.info("Phase 2 enrichment started", extra={"run_id": run_id, "company_id": company_id})
             phase2_result = await self._run_phase2_parallel(
-                company_data_for_workflow, run_id, company_id, shared_output_file, user_id, session_id=session_context.session_id, session_context=session_context
+                company_data_for_workflow,
+                run_id,
+                company_id,
+                shared_output_file,
+                user_id,
+                session_id=session_context.session_id,
+                session_context=session_context,
             )
-            
-            if not phase2_result['success']:
+
+            if not phase2_result["success"]:
                 logger.error("Phase 2 enrichment failed", extra={"run_id": run_id, "company_id": company_id, "error": phase2_result.get("error")})
                 return {
-                    'success': False,
-                    'error': f"Phase 2 failed: {phase2_result['error']}",
-                    'run_id': run_id,
-                    'company_name': company_name,
-                    'execution_time_ms': 0
+                    "success": False,
+                    "error": f"Phase 2 failed: {phase2_result['error']}",
+                    "run_id": run_id,
+                    "company_name": company_name,
+                    "execution_time_ms": 0,
                 }
-            
+
             logger.info("Phase 2 enrichment completed", extra={"run_id": run_id, "company_id": company_id})
-            
+
             # Step 4: Generate Summary Report
             execution_time_ms = int((datetime.now() - datetime.now()).total_seconds() * 1000)  # Will be calculated properly
-            
+
             summary_result = await self._generate_summary_report(
                 company_data=company_data_for_workflow,
                 run_id=run_id,
@@ -706,63 +755,69 @@
                 phase2_result=phase2_result,
                 execution_time_ms=execution_time_ms,
                 shared_output_file=shared_output_file,
-                session_context=session_context
+                session_context=session_context,
             )
-            
+
             print(f"‚úÖ Specific company workflow completed successfully")
-            
+
             # At the end of successful execution, before the return:
             # Note: Tally increment is handled in execute_workflow_background to avoid double-counting
             if postgres_enabled and db:
                 execution_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                 await db.complete_prospecting_run(run_id, user_id, session_context.session_id, execution_time_ms)
-            
-            logger.info("Specific company workflow completed successfully", extra={"run_id": run_id, "company_id": company_id, "execution_time_ms": execution_time_ms})
-            
+
+            logger.info(
+                "Specific company workflow completed successfully",
+                extra={"run_id": run_id, "company_id": company_id, "execution_time_ms": execution_time_ms},
+            )
+
             return {
-                'success': True,
-                'workflow_type': workflow_type,
-                'run_id': run_id,
-                'company_id': company_id,
-                'company_name': company_name,
-                'execution_time_ms': execution_time_ms,
-                'output_file': shared_output_file,
-                'phase1_results': phase1_result,
-                'phase2_results': phase2_result,
-                'summary': summary_result,
-                'postgres_enabled': postgres_enabled
+                "success": True,
+                "workflow_type": workflow_type,
+                "run_id": run_id,
+                "company_id": company_id,
+                "company_name": company_name,
+                "execution_time_ms": execution_time_ms,
+                "output_file": shared_output_file,
+                "phase1_results": phase1_result,
+                "phase2_results": phase2_result,
+                "summary": summary_result,
+                "postgres_enabled": postgres_enabled,
             }
-            
+
         except Exception as e:
             error_msg = f"Specific company workflow failed: {str(e)}"
             print(f"‚ùå {error_msg}")
             logger.exception("Specific company workflow failed", extra={"run_id": run_id, "company_name": company_name})
-            return {
-                'success': False,
-                'error': error_msg,
-                'run_id': run_id,
-                'company_name': company_name,
-                'execution_time_ms': 0
-            }
+            return {"success": False, "error": error_msg, "run_id": run_id, "company_name": company_name, "execution_time_ms": 0}
 
-    async def _execute_company_search_workflow(self, search_params: Dict[str, Any], run_id: str, user_id: str,
-                                             shared_output_file: str, db: ProspectingDB, postgres_enabled: bool, session_context: SessionContext, workflow_type: str) -> Dict[str, Any]:
+    async def _execute_company_search_workflow(
+        self,
+        search_params: Dict[str, Any],
+        run_id: str,
+        user_id: str,
+        shared_output_file: str,
+        db: ProspectingDB,
+        postgres_enabled: bool,
+        session_context: SessionContext,
+        workflow_type: str,
+    ) -> Dict[str, Any]:
         """
         Execute the company search workflow for general search requests.
         Returns search results for user selection.
         """
         logger.info("Company search workflow started", extra={"run_id": run_id, "search_params": search_params, "workflow_type": workflow_type})
         print(f"üîç Executing company search workflow")
-        
+
         # Start the prospecting run to ensure the run_id exists in the database
         if postgres_enabled and db:
             # Prepare search parameters for storage
             search_params_for_storage = {
-                'investor_type': search_params.get('investor_type', 'VC'),
-                'investor_focus': search_params.get('investor_focus', 'Technology'),
-                'investment_stage': search_params.get('investment_stage', 'Series A'),
-                'location': search_params.get('location', 'United States'),
-                'additional_company_info': search_params.get('additional_company_info', '')
+                "investor_type": search_params.get("investor_type", "VC"),
+                "investor_focus": search_params.get("investor_focus", "Technology"),
+                "investment_stage": search_params.get("investment_stage", "Series A"),
+                "location": search_params.get("location", "United States"),
+                "additional_company_info": search_params.get("additional_company_info", ""),
             }
             try:
                 await db.start_run(
@@ -771,8 +826,8 @@
                     session_id=session_context.session_id,
                     search_params=search_params_for_storage,  # Store search parameters
                     company_name=None,  # NULL during search phase
-                    company_id=None,     # NULL during search phase
-                    workflow_type=workflow_type
+                    company_id=None,  # NULL during search phase
+                    workflow_type=workflow_type,
                 )
                 print(f"‚úÖ Started prospecting run: {run_id}")
             except Exception as e:
@@ -780,79 +835,81 @@
 
             # Prepare search parameters - include additional_company_info
             search_params_for_agent = {
-                'investor_type': search_params.get('investor_type', 'VC'),
-                'investor_focus': search_params.get('investor_focus', 'Technology'),
-                'investment_stage': search_params.get('investment_stage', 'Series A'),
-                'location': search_params.get('location', 'United States'),
-                'additional_company_info': search_params.get('additional_company_info', '')  # Pass additional info
+                "investor_type": search_params.get("investor_type", "VC"),
+                "investor_focus": search_params.get("investor_focus", "Technology"),
+                "investment_stage": search_params.get("investment_stage", "Series A"),
+                "location": search_params.get("location", "United States"),
+                "additional_company_info": search_params.get("additional_company_info", ""),  # Pass additional info
             }
-            
+
             # Execute company search
             print(f"üîç Step 1: Company search with parameters: {search_params_for_agent}")
-            search_result = await self.sub_agents['company_search'].execute(
+            search_result = await self.sub_agents["company_search"].execute(
                 company_search_params=search_params_for_agent,
                 run_id=run_id,
                 user_id=user_id,
                 shared_output_file=shared_output_file,
                 db=db,
                 postgres_enabled=postgres_enabled,
-                session_id=session_context.session_id
+                session_id=session_context.session_id,
             )
-            
-            if not search_result['success']:
-                logger.error("Company search failed", extra={"run_id": run_id, "error": search_result.get('error', 'Unknown error')})
+
+            if not search_result["success"]:
+                logger.error("Company search failed", extra={"run_id": run_id, "error": search_result.get("error", "Unknown error")})
                 return {
-                    'success': False,
-                    'error': f"Company search failed: {search_result.get('error', 'Unknown error')}",
-                    'run_id': run_id,
-                    'user_id': user_id,
-                    'execution_time_ms': search_result.get('execution_time_ms', 0)
+                    "success": False,
+                    "error": f"Company search failed: {search_result.get('error', 'Unknown error')}",
+                    "run_id": run_id,
+                    "user_id": user_id,
+                    "execution_time_ms": search_result.get("execution_time_ms", 0),
                 }
-            
+
             # Format search results
-            companies = search_result.get('search_results', {}).get('companies', [])
-            
+            companies = search_result.get("search_results", {}).get("companies", [])
+
             if not companies:
                 logger.warning("No companies found in search results", extra={"run_id": run_id})
                 return {
-                    'success': False,
-                    'error': 'No companies found in search results',
-                    'run_id': run_id,
-                    'user_id': user_id,
-                    'execution_time_ms': search_result.get('execution_time_ms', 0)
+                    "success": False,
+                    "error": "No companies found in search results",
+                    "run_id": run_id,
+                    "user_id": user_id,
+                    "execution_time_ms": search_result.get("execution_time_ms", 0),
                 }
-            
+
             print(f"‚úÖ Company search completed - found {len(companies)} companies")
             print(f"üìã Please select a company (1-{len(companies)}) for enrichment")
-            
+
             logger.info("Company search completed successfully", extra={"run_id": run_id, "companies_found": len(companies)})
         # Do NOT set 100% here; reserve 100% for final workflow completion
         # Ensure we always return results to the caller
         return {
-            'success': True,
-            'workflow_type': 'company_search',
-            'run_id': run_id,
-            'user_id': user_id,
-            'companies': companies,
-            'companies_found': len(companies),
-            'search_params': search_params_for_agent,
-            'execution_time_ms': search_result.get('execution_time_ms', 0),
-            'search_output_file': shared_output_file,
-            'next_step': 'user_selection_required'
+            "success": True,
+            "workflow_type": "company_search",
+            "run_id": run_id,
+            "user_id": user_id,
+            "companies": companies,
+            "companies_found": len(companies),
+            "search_params": search_params_for_agent,
+            "execution_time_ms": search_result.get("execution_time_ms", 0),
+            "search_output_file": shared_output_file,
+            "next_step": "user_selection_required",
         }
-            
+
         # No broad catch here; exceptions bubble to caller which already handles
 
-    async def handle_user_company_selection(self, run_id: str, user_id: str, selected_company_index: int = None, session_context: SessionContext = None) -> Dict[str, Any]:
+    async def handle_user_company_selection(
+        self, run_id: str, user_id: str, selected_company_index: int = None, session_context: SessionContext = None
+    ) -> Dict[str, Any]:
         """
         Handle user company selection and proceed with enrichment.
-        
+
         Args:
             run_id: The run ID from the search phase
             user_id: User identifier for multi-tenant isolation
             selected_company_index: Company index to select (1-based). If None, prompts for console input.
             session_context: Session context for session-aware execution
-            
+
         Returns:
             Dict containing enrichment results with web app compatible format
         """
@@ -863,60 +920,67 @@
             if not self.db:
                 self.db = await get_global_db()
             from app.utils.progress_store import ProgressStore
+
             await ProgressStore.instance().reset_progress(self.db, run_id)
             hard_after_reset = await ProgressStore.instance().get_progress(self.db, run_id)
-            display_after_reset = await ProgressStore.instance().get_display_progress(self.db, run_id, workflow_type='specific_company', status='processing')
+            display_after_reset = await ProgressStore.instance().get_display_progress(
+                self.db, run_id, workflow_type="specific_company", status="processing"
+            )
             print(f"üß™ Progress reset verification ‚Äî hard: {hard_after_reset}%, display: {display_after_reset}%")
         except Exception as e:
             print(f"‚ö†Ô∏è Progress reset verification failed: {e}")
-        
+
         try:
             # Always get global database instance for selection path
             self.db = await get_global_db()
             print(f"üîó ProspectingOrchestrator.selection: Using global database instance: {id(self.db)}")
-            
+
             # Extract parent run_id if this is a company selection run_id
             parent_run_id = run_id
             if "_sel_" in run_id:
                 # Extract parent run_id from selection run_id (e.g., "session_abc_run_001_sel_001" -> "session_abc_run_001")
                 parent_run_id = run_id.split("_sel_")[0]
                 print(f"üéØ Detected company selection run_id, using parent: {parent_run_id}")
-            
+
             # Retrieve search results from database using parent run_id
             search_results_list = await self.db.get_company_search_results(user_id, parent_run_id, session_context.session_id) if self.db else None
-            
+
             if not search_results_list or not isinstance(search_results_list, list) or len(search_results_list) == 0:
                 return {
-                    'success': False,
-                    'error': 'No search results found for this run',
-                    'run_id': run_id,
-                    'user_id': user_id,
-                    'workflow_type': 'company_selection_error'
+                    "success": False,
+                    "error": "No search results found for this run",
+                    "run_id": run_id,
+                    "user_id": user_id,
+                    "workflow_type": "company_selection_error",
                 }
-            
+
             # Get the most recent search result (first in the list)
             search_results = search_results_list[0]
-            
+
             # Extract companies from the search_results JSONB field
             companies = []
-            if search_results.get('search_results'):
+            if search_results.get("search_results"):
                 try:
-                    search_results_data = json.loads(search_results['search_results']) if isinstance(search_results['search_results'], str) else search_results['search_results']
-                    companies = search_results_data.get('companies', [])
+                    search_results_data = (
+                        json.loads(search_results["search_results"])
+                        if isinstance(search_results["search_results"], str)
+                        else search_results["search_results"]
+                    )
+                    companies = search_results_data.get("companies", [])
                     print(f"üîç Extracted {len(companies)} companies from search_results field")
                 except (json.JSONDecodeError, TypeError) as e:
                     print(f"‚ö†Ô∏è Error parsing search_results from database: {e}")
                     companies = []
-            
+
             if not companies:
                 return {
-                    'success': False,
-                    'error': f'No companies found in search results. Search Results field: {type(search_results.get("search_results"))}',
-                    'run_id': run_id,
-                    'user_id': user_id,
-                    'workflow_type': 'company_selection_error'
+                    "success": False,
+                    "error": f"No companies found in search results. Search Results field: {type(search_results.get('search_results'))}",
+                    "run_id": run_id,
+                    "user_id": user_id,
+                    "workflow_type": "company_selection_error",
                 }
-            
+
             # Display companies for selection (console-friendly format)
             print(f"\nüìã Available Companies ({len(companies)} found):")
             print("-" * 60)
@@ -927,7 +991,7 @@
                 print(f"     Focus: {company.get('investor_focus', 'N/A')}")
                 print(f"     Location: {company.get('location', 'N/A')}")
                 print()
-            
+
             # Handle company selection
             if selected_company_index is None:
                 # Console input mode (testing)
@@ -935,7 +999,7 @@
                     try:
                         user_input = input(f"üéØ Select a company (1-{len(companies)}): ").strip()
                         selected_company_index = int(user_input)
-                        
+
                         if 1 <= selected_company_index <= len(companies):
                             break
                         else:
@@ -945,55 +1009,51 @@
                     except KeyboardInterrupt:
                         print("\n‚ùå Selection cancelled")
                         return {
-                            'success': False,
-                            'error': 'User cancelled selection',
-                            'run_id': run_id,
-                            'user_id': user_id,
-                            'workflow_type': 'company_selection_cancelled'
+                            "success": False,
+                            "error": "User cancelled selection",
+                            "run_id": run_id,
+                            "user_id": user_id,
+                            "workflow_type": "company_selection_cancelled",
                         }
             else:
                 # Programmatic selection (web app mode)
                 if not (1 <= selected_company_index <= len(companies)):
                     return {
-                        'success': False,
-                        'error': f'Invalid company index. Must be between 1 and {len(companies)}',
-                        'run_id': run_id,
-                        'user_id': user_id,
-                        'workflow_type': 'company_selection_error'
+                        "success": False,
+                        "error": f"Invalid company index. Must be between 1 and {len(companies)}",
+                        "run_id": run_id,
+                        "user_id": user_id,
+                        "workflow_type": "company_selection_error",
                     }
-            
+
             # Get selected company data
             selected_company = companies[selected_company_index - 1]
-            company_name = selected_company.get('company_name', 'Unknown Company')
-            
+            company_name = selected_company.get("company_name", "Unknown Company")
+
             print(f"‚úÖ Selected company: {company_name}")
             print(f"üîÑ Starting enrichment process...")
-            
+
             # Update prospecting run with selected company information
             if get_enable_postgres_storage() and self.db:
                 try:
                     # Generate a company_id for the selected company
                     company_id_for_run = f"selected_{selected_company.get('company_domain', 'unknown').replace('.', '_')}"
-                    
-                    await self.db.update_run_company(
-                        run_id=run_id,
-                        company_name=company_name,
-                        company_id=company_id_for_run
-                    )
+
+                    await self.db.update_run_company(run_id=run_id, company_name=company_name, company_id=company_id_for_run)
                     print(f"‚úÖ Updated prospecting run with selected company: {company_name}")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Warning: Failed to update prospecting run with company selection: {e}")
-            
+
             # Store session interaction if session context is provided
             await self.store_session_interaction(
                 session_context,
                 f"{run_id}_selection_{selected_company_index}",
                 f"Company selection: {company_name} (index {selected_company_index})",
-                "company_selection"
+                "company_selection",
             )
-            
+
             # Create output file for enrichment when debugging is enabled
-            safe_company_name = company_name.replace('/', '-').replace(' ', '_')[:50]
+            safe_company_name = company_name.replace("/", "-").replace(" ", "_")[:50]
             enrichment_output_file = None
             if self.enable_debugging:
                 try:
@@ -1004,125 +1064,137 @@
                     print(f"üêõ Debug file created for selection flow: {enrichment_output_file}")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Failed to create debug file for selection flow: {e}")
-            
+
             # Prepare company data for enrichment workflow
             # Use separate hq_city and hq_country_region from search results
             # Only extract metadata domain if it's non-null and non-empty
-            raw_domain = selected_company.get('company_domain')
+            raw_domain = selected_company.get("company_domain")
             search_metadata_domain = None
             if raw_domain and raw_domain.strip():  # Check for non-null and non-empty
                 # Normalize search metadata domain to URL format if it's just a domain
-                if not raw_domain.startswith(('http://', 'https://')):
+                if not raw_domain.startswith(("http://", "https://")):
                     search_metadata_domain = f"https://{raw_domain}"
                 else:
                     search_metadata_domain = raw_domain
-            
+
             company_data_for_workflow = {
-                'name': company_name,
-                'location': selected_company.get('location', 'Unknown'),  # Keep for backward compatibility
-                'hq_city': selected_company.get('hq_city'),
-                'hq_country': selected_company.get('hq_country_region'),  # Map hq_country_region to hq_country
-                'focus_area': selected_company.get('investor_focus', 'General prospecting'),
-                'investor_type': selected_company.get('investor_type', ''),
-                'search_metadata_domain': search_metadata_domain,
-                'output_file': enrichment_output_file
+                "name": company_name,
+                "location": selected_company.get("location", "Unknown"),  # Keep for backward compatibility
+                "hq_city": selected_company.get("hq_city"),
+                "hq_country": selected_company.get("hq_country_region"),  # Map hq_country_region to hq_country
+                "focus_area": selected_company.get("investor_focus", "General prospecting"),
+                "investor_type": selected_company.get("investor_type", ""),
+                "search_metadata_domain": search_metadata_domain,
+                "output_file": enrichment_output_file,
             }
-            
+
             # Execute the enrichment workflow for the selected company
             print(f"üîç Step 1: Domain discovery for {company_name}")
-            
+
             # Update status to processing
             if get_enable_postgres_storage() and self.db:
                 try:
-                    await self.db.update_run_status(run_id, 'processing')
+                    await self.db.update_run_status(run_id, "processing")
                     print(f"‚úÖ Updated prospecting run status to: processing")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Warning: Failed to update run status: {e}")
-            
+
             domain_discovery_result = await self._discover_domain_and_generate_id(company_data_for_workflow, enrichment_output_file)
-            
+
             # Early exit if no valid domain found
-            found_url = domain_discovery_result.get('found_url')
-            if (not domain_discovery_result['success'] or 
-                found_url is None or 
-                (isinstance(found_url, str) and found_url.strip().lower() in ('null', '[null]', 'none', '')) or
-                (isinstance(found_url, list) and (not found_url or found_url[0] in (None, 'null', '[null]', 'none', '')))):
+            found_url = domain_discovery_result.get("found_url")
+            if (
+                not domain_discovery_result["success"]
+                or found_url is None
+                or (isinstance(found_url, str) and found_url.strip().lower() in ("null", "[null]", "none", ""))
+                or (isinstance(found_url, list) and (not found_url or found_url[0] in (None, "null", "[null]", "none", "")))
+            ):
                 error_msg = "No valid company domain could be found. Sub-agents were not executed."
                 print(f"‚ùå {error_msg}")
                 return {
-                    'success': False,
-                    'error': error_msg,
-                    'run_id': run_id,
-                    'company_name': company_name,
-                    'execution_time_ms': 0,
-                    'workflow_type': 'company_selection_enrichment_failed'
+                    "success": False,
+                    "error": error_msg,
+                    "run_id": run_id,
+                    "company_name": company_name,
+                    "execution_time_ms": 0,
+                    "workflow_type": "company_selection_enrichment_failed",
                 }
-            
-            company_id = domain_discovery_result['company_id']
-            found_url = domain_discovery_result['found_url']
-            found_by_perplexity = domain_discovery_result['found_by_perplexity']
-            
+
+            company_id = domain_discovery_result["company_id"]
+            found_url = domain_discovery_result["found_url"]
+            found_by_perplexity = domain_discovery_result["found_by_perplexity"]
+
             # Store company in PostgreSQL if enabled
             if get_enable_postgres_storage() and self.db:
                 try:
                     company_data_for_db = {
-                        'name': company_name,
-                        'website_url': found_url,
-                        'location': selected_company.get('location'),  # Keep for backward compatibility
-                        'hq_city': selected_company.get('hq_city'),
-                        'hq_country': selected_company.get('hq_country_region'),
-                        'focus_area': selected_company.get('investor_focus')
+                        "name": company_name,
+                        "website_url": found_url,
+                        "location": selected_company.get("location"),  # Keep for backward compatibility
+                        "hq_city": selected_company.get("hq_city"),
+                        "hq_country": selected_company.get("hq_country_region"),
+                        "focus_area": selected_company.get("investor_focus"),
                     }
                     await self.db.store_company(
-                        run_id=run_id,
-                        company_data=company_data_for_db,
-                        company_id=company_id,
-                        user_id=user_id,
-                        session_id=session_context.session_id
+                        run_id=run_id, company_data=company_data_for_db, company_id=company_id, user_id=user_id, session_id=session_context.session_id
                     )
                     print(f"‚úÖ Company stored in PostgreSQL with ID: {company_id}")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Error storing company in PostgreSQL: {e}")
-            
+
             # Step 2: Phase 1 - Parallel Data Collection
             print(f"üîÑ Step 2: Phase 1 - Parallel data collection")
             phase1_result = await self._run_phase1_parallel(
-                company_data_for_workflow, run_id, company_id, domain_discovery_result['found_url'], 
-                domain_discovery_result['found_urls'], domain_discovery_result['found_by_perplexity'], 
-                enrichment_output_file, user_id, session_id=session_context.session_id
+                company_data_for_workflow,
+                run_id,
+                company_id,
+                domain_discovery_result["found_url"],
+                domain_discovery_result["found_urls"],
+                domain_discovery_result["found_by_perplexity"],
+                enrichment_output_file,
+                user_id,
+                session_id=session_context.session_id,
             )
-            
-            if not phase1_result['success']:
+
+            if not phase1_result["success"]:
                 return {
-                    'success': False,
-                    'error': f"Phase 1 failed: {phase1_result['error']}",
-                    'run_id': run_id,
-                    'company_name': company_name,
-                    'execution_time_ms': 0,
-                    'workflow_type': 'company_selection_enrichment_failed'
+                    "success": False,
+                    "error": f"Phase 1 failed: {phase1_result['error']}",
+                    "run_id": run_id,
+                    "company_name": company_name,
+                    "execution_time_ms": 0,
+                    "workflow_type": "company_selection_enrichment_failed",
                 }
-            
+
             # Step 3: Phase 2 - Parallel Enrichment
             print(f"üîÑ Step 3: Phase 2 - Parallel enrichment")
             phase2_result = await self._run_phase2_parallel(
-                company_data_for_workflow, run_id, company_id, enrichment_output_file, user_id, session_id=session_context.session_id, session_context=session_context
+                company_data_for_workflow,
+                run_id,
+                company_id,
+                enrichment_output_file,
+                user_id,
+                session_id=session_context.session_id,
+                session_context=session_context,
             )
-            
-            if not phase2_result['success']:
+
+            if not phase2_result["success"]:
                 return {
-                    'success': False,
-                    'error': f"Phase 2 failed: {phase2_result['error']}",
-                    'run_id': run_id,
-                    'company_name': company_name,
-                    'execution_time_ms': 0,
-                    'workflow_type': 'company_selection_enrichment_failed'
+                    "success": False,
+                    "error": f"Phase 2 failed: {phase2_result['error']}",
+                    "run_id": run_id,
+                    "company_name": company_name,
+                    "execution_time_ms": 0,
+                    "workflow_type": "company_selection_enrichment_failed",
                 }
-            
+
             # Step 4: Generate Summary Report
-            total_execution_time_ms = domain_discovery_result.get('execution_time_ms', 0) + \
-                                    phase1_result.get('execution_time_ms', 0) + \
-                                    phase2_result.get('execution_time_ms', 0)
-            
+            total_execution_time_ms = (
+                domain_discovery_result.get("execution_time_ms", 0)
+                + phase1_result.get("execution_time_ms", 0)
+                + phase2_result.get("execution_time_ms", 0)
+            )
+
             summary_result = await self._generate_summary_report(
                 company_data=company_data_for_workflow,
                 run_id=run_id,
@@ -1131,71 +1203,63 @@
                 phase2_result=phase2_result,
                 execution_time_ms=total_execution_time_ms,
                 shared_output_file=enrichment_output_file,
-                session_context=session_context
+                session_context=session_context,
             )
-            
+
             print(f"‚úÖ Company selection and enrichment completed successfully!")
-            
+
             # Update status to completed and set end_time
             if get_enable_postgres_storage() and self.db:
                 try:
                     await self.db.complete_prospecting_run(run_id, user_id, session_context.session_id, total_execution_time_ms)
                     print(f"‚úÖ Completed prospecting run with end_time set")
-                    
+
                     # Increment agent run tally for completed company_selection workflow
                     try:
                         rate_limiter = RateLimiter()
                         await rate_limiter.increment_request_count(user_id)
                         logger.info(
                             "Agent run tally incremented for company selection",
-                            extra={
-                                "run_id": run_id,
-                                "user_id": user_id,
-                                "workflow_type": "company_selection"
-                            }
+                            extra={"run_id": run_id, "user_id": user_id, "workflow_type": "company_selection"},
                         )
                     except Exception as tally_error:
                         # Non-fatal: log but don't fail the workflow
                         logger.warning(
                             "Failed to increment agent run tally for company selection",
-                            extra={
-                                "run_id": run_id,
-                                "user_id": user_id,
-                                "error": str(tally_error)
-                            }
+                            extra={"run_id": run_id, "user_id": user_id, "error": str(tally_error)},
                         )
                 except Exception as e:
                     print(f"‚ö†Ô∏è Warning: Failed to complete prospecting run: {e}")
-            
+
             # Return web app compatible response format
             return {
-                'success': True,
-                'workflow_type': 'company_selection_enrichment',
-                'run_id': run_id,
-                'user_id': user_id,
-                'selected_company_index': selected_company_index,
-                'selected_company': selected_company,
-                'company_name': company_name,
-                'company_id': company_id,
-                'execution_time_ms': total_execution_time_ms,
-                'output_file': enrichment_output_file,
-                'phase1_results': phase1_result,
-                'phase2_results': phase2_result,
-                'summary': summary_result,
-                'enrichment_status': 'completed',
-                'postgres_enabled': get_enable_postgres_storage()
+                "success": True,
+                "workflow_type": "company_selection_enrichment",
+                "run_id": run_id,
+                "user_id": user_id,
+                "selected_company_index": selected_company_index,
+                "selected_company": selected_company,
+                "company_name": company_name,
+                "company_id": company_id,
+                "execution_time_ms": total_execution_time_ms,
+                "output_file": enrichment_output_file,
+                "phase1_results": phase1_result,
+                "phase2_results": phase2_result,
+                "summary": summary_result,
+                "enrichment_status": "completed",
+                "postgres_enabled": get_enable_postgres_storage(),
             }
-            
+
         except Exception as e:
             error_msg = f"Company selection and enrichment failed: {str(e)}"
             print(f"‚ùå {error_msg}")
             return {
-                'success': False,
-                'error': error_msg,
-                'run_id': run_id,
-                'user_id': user_id,
-                'execution_time_ms': 0,
-                'workflow_type': 'company_selection_enrichment_failed'
+                "success": False,
+                "error": error_msg,
+                "run_id": run_id,
+                "user_id": user_id,
+                "execution_time_ms": 0,
+                "workflow_type": "company_selection_enrichment_failed",
             }
         finally:
             # Note: Database connection is managed globally, no need to close here
@@ -1207,44 +1271,46 @@
         Discover company domain and generate company_id once at the beginning.
         """
         try:
-            company_name = company_data['name']
-            
+            company_name = company_data["name"]
+
             # Attempt to find company homepage using Perplexity
             print(f"üîç Searching for {company_name} homepage...")
-            
+
             try:
                 # Prepare web search config for Perplexity
-                investor_type = company_data.get('investor_type', '')
+                investor_type = company_data.get("investor_type", "")
                 investor_type_context = f" (this company is an {investor_type})" if investor_type else ""
                 web_search_config = {
                     "query_template": "official website {company_name} {location}{investor_type_context}",
                     "search_type_description": "company homepage or official website for {company_name}",
                     "output_format_example": '["https://company.com"]',
                     "max_results": 1,
-                    "investor_type": investor_type
+                    "investor_type": investor_type,
                 }
-                
+
                 # Use location for domain discovery (backward compatibility)
-                location_for_domain = company_data.get('location') or ""
+                location_for_domain = company_data.get("location") or ""
                 # Also include hq_city and hq_country if available for better domain discovery
-                if company_data.get('hq_city') and company_data.get('hq_country'):
+                if company_data.get("hq_city") and company_data.get("hq_country"):
                     location_for_domain = f"{company_data.get('hq_city')}, {company_data.get('hq_country')}"
-                elif company_data.get('hq_city'):
-                    location_for_domain = company_data.get('hq_city')
-                elif company_data.get('hq_country'):
-                    location_for_domain = company_data.get('hq_country')
-                
-                domain_result = await find_company_homepage_url_perplexity.ainvoke({
-                    "company_name": company_name,
-                    "location": location_for_domain,
-                    "focus_area": company_data['focus_area'] or "",
-                    "investor_type": company_data.get('investor_type', ''),
-                    "web_search_config": web_search_config
-                })
-                
+                elif company_data.get("hq_city"):
+                    location_for_domain = company_data.get("hq_city")
+                elif company_data.get("hq_country"):
+                    location_for_domain = company_data.get("hq_country")
+
+                domain_result = await find_company_homepage_url_perplexity.ainvoke(
+                    {
+                        "company_name": company_name,
+                        "location": location_for_domain,
+                        "focus_area": company_data["focus_area"] or "",
+                        "investor_type": company_data.get("investor_type", ""),
+                        "web_search_config": web_search_config,
+                    }
+                )
+
                 # Get domain from search metadata (if available)
-                search_metadata_domain = company_data.get('search_metadata_domain', '')
-                
+                search_metadata_domain = company_data.get("search_metadata_domain", "")
+
                 # Normalize domain for comparison (remove protocol, www., trailing slashes)
                 def normalize_domain_for_comparison(domain: str) -> str:
                     """Normalize domain for deduplication comparison."""
@@ -1252,28 +1318,28 @@
                         return ""
                     domain = domain.lower().strip()
                     # Remove protocol
-                    if domain.startswith('http://'):
+                    if domain.startswith("http://"):
                         domain = domain[7:]
-                    elif domain.startswith('https://'):
+                    elif domain.startswith("https://"):
                         domain = domain[8:]
                     # Remove www.
-                    if domain.startswith('www.'):
+                    if domain.startswith("www."):
                         domain = domain[4:]
                     # Remove trailing slash and path
-                    domain = domain.split('/')[0].split('?')[0]
+                    domain = domain.split("/")[0].split("?")[0]
                     return domain
-                
+
                 # Handle both single domain (string) and multiple domains (list) from Perplexity
                 perplexity_domains = []
                 if isinstance(domain_result, list):
                     perplexity_domains = domain_result
                 elif domain_result:
                     perplexity_domains = [domain_result]
-                
+
                 # Build final domain list: Perplexity domains first, then search metadata domain (deduplicated)
                 found_urls = []
                 normalized_seen = set()
-                
+
                 # Add Perplexity domains first (deduplicated)
                 for domain in perplexity_domains:
                     if domain:
@@ -1281,7 +1347,7 @@
                         if normalized and normalized not in normalized_seen:
                             found_urls.append(domain)
                             normalized_seen.add(normalized)
-                
+
                 # Add search metadata domain after Perplexity domains if available
                 if search_metadata_domain:
                     normalized_metadata = normalize_domain_for_comparison(search_metadata_domain)
@@ -1289,7 +1355,7 @@
                         found_urls.append(search_metadata_domain)
                         normalized_seen.add(normalized_metadata)
                         print(f"‚úÖ Added search metadata domain: {search_metadata_domain}")
-                
+
                 # Set primary domain (first in list)
                 if found_urls:
                     found_url = found_urls[0]
@@ -1298,9 +1364,9 @@
                 else:
                     found_url = None
                     print(f"‚ö†Ô∏è No domains found")
-                
+
                 found_by_perplexity = True
-                
+
             except Exception as e:
                 print(f"‚ö†Ô∏è Perplexity homepage search failed: {e}")
                 # Fallback to basic URL guess
@@ -1308,103 +1374,130 @@
                 found_urls = [found_url]
                 found_by_perplexity = False
                 print(f"üîÑ Using fallback URL: {found_url}")
-            
+
             # Generate company_id from primary domain
             company_id = generate_company_id_from_domain(found_url)
             print(f"üÜî Generated company_id: {company_id}")
-            
+
             return {
-                'success': True,
-                'company_id': company_id,
-                'found_url': found_url,
-                'found_urls': found_urls,
-                'domain_count': len(found_urls),
-                'found_by_perplexity': found_by_perplexity
+                "success": True,
+                "company_id": company_id,
+                "found_url": found_url,
+                "found_urls": found_urls,
+                "domain_count": len(found_urls),
+                "found_by_perplexity": found_by_perplexity,
             }
-            
+
         except Exception as e:
             error_msg = f"Domain discovery failed: {str(e)}"
             print(f"‚ùå {error_msg}")
-            
-            return {
-                'success': False,
-                'error': error_msg
-            }
 
-    async def _run_phase1_parallel(self, company_data: Dict, run_id: str, company_id: str, 
-                                 found_url: str, found_urls: List[str], found_by_perplexity: bool, 
-                                 shared_output_file: str, user_id: str = None, session_id: str = None) -> Dict[str, Any]:
+            return {"success": False, "error": error_msg}
+
+    async def _run_phase1_parallel(
+        self,
+        company_data: Dict,
+        run_id: str,
+        company_id: str,
+        found_url: str,
+        found_urls: List[str],
+        found_by_perplexity: bool,
+        shared_output_file: str,
+        user_id: str = None,
+        session_id: str = None,
+    ) -> Dict[str, Any]:
         """
         Phase 1: Run Web Research and CoreSignal agents in parallel.
         """
         logger.info("Phase 1 parallel execution started", extra={"run_id": run_id, "company_id": company_id})
         print("üîÑ Running Phase 1 agents in parallel...")
-        
+
         try:
             # Run both agents in parallel using asyncio.gather
-            web_research_task = self.sub_agents['web_research'].execute(
-                company_data, run_id=run_id, company_id=company_id,
-                found_url=found_url, found_urls=found_urls, found_by_perplexity=found_by_perplexity,
-                shared_output_file=shared_output_file, user_id=user_id, session_id=session_id
+            web_research_task = self.sub_agents["web_research"].execute(
+                company_data,
+                run_id=run_id,
+                company_id=company_id,
+                found_url=found_url,
+                found_urls=found_urls,
+                found_by_perplexity=found_by_perplexity,
+                shared_output_file=shared_output_file,
+                user_id=user_id,
+                session_id=session_id,
             )
-            
-            coresignal_task = self.sub_agents['coresignal'].execute(
-                company_data, run_id=run_id, company_id=company_id, 
-                found_url=found_url, found_urls=found_urls, user_id=user_id, session_id=session_id
+
+            coresignal_task = self.sub_agents["coresignal"].execute(
+                company_data, run_id=run_id, company_id=company_id, found_url=found_url, found_urls=found_urls, user_id=user_id, session_id=session_id
             )
 
-            youtube_task = self.sub_agents['youtube_media'].execute(
-                company_data, run_id=run_id, company_id=company_id,
-                shared_output_file=shared_output_file, db=self.db, postgres_enabled=get_enable_postgres_storage(),
-                user_id=user_id, session_id=session_id, youtube_url=found_url
+            youtube_task = self.sub_agents["youtube_media"].execute(
+                company_data,
+                run_id=run_id,
+                company_id=company_id,
+                shared_output_file=shared_output_file,
+                db=self.db,
+                postgres_enabled=get_enable_postgres_storage(),
+                user_id=user_id,
+                session_id=session_id,
+                youtube_url=found_url,
             )
-            
+
             # Wait for both to complete
             web_research_result, coresignal_result, youtube_result = await asyncio.gather(
                 web_research_task, coresignal_task, youtube_task, return_exceptions=True
             )
-            
+
             # Handle any exceptions
             if isinstance(web_research_result, Exception):
                 print(f"‚ùå Web Research Agent failed: {web_research_result}")
-                web_research_result = {'success': False, 'error': str(web_research_result)}
-            
+                web_research_result = {"success": False, "error": str(web_research_result)}
+
             if isinstance(coresignal_result, Exception):
                 print(f"‚ùå CoreSignal Agent failed: {coresignal_result}")
-                coresignal_result = {'success': False, 'error': str(coresignal_result)}
+                coresignal_result = {"success": False, "error": str(coresignal_result)}
 
             if isinstance(youtube_result, Exception):
                 print(f"‚ùå YouTube Media Agent failed: {youtube_result}")
-                youtube_result = {'success': False, 'error': str(youtube_result)}
-            
-            print(f"‚úÖ Phase 1 completed - Web Research: {'‚úÖ' if web_research_result.get('success') else '‚ùå'}, CoreSignal: {'‚úÖ' if coresignal_result.get('success') else '‚ùå'}, YouTube: {'‚úÖ' if youtube_result.get('success') else '‚ùå'}")
+                youtube_result = {"success": False, "error": str(youtube_result)}
 
-            logger.info("Phase 1 parallel execution completed", extra={
-                "run_id": run_id, 
-                "company_id": company_id, 
-                "web_research_success": web_research_result.get('success', False),
-                "coresignal_success": coresignal_result.get('success', False),
-                "youtube_success": youtube_result.get('success', False)
-            })
+            print(
+                f"‚úÖ Phase 1 completed - Web Research: {'‚úÖ' if web_research_result.get('success') else '‚ùå'}, CoreSignal: {'‚úÖ' if coresignal_result.get('success') else '‚ùå'}, YouTube: {'‚úÖ' if youtube_result.get('success') else '‚ùå'}"
+            )
 
+            logger.info(
+                "Phase 1 parallel execution completed",
+                extra={
+                    "run_id": run_id,
+                    "company_id": company_id,
+                    "web_research_success": web_research_result.get("success", False),
+                    "coresignal_success": coresignal_result.get("success", False),
+                    "youtube_success": youtube_result.get("success", False),
+                },
+            )
+
             return {
-                'success': True,
-                'web_research_result': web_research_result,
-                'coresignal_result': coresignal_result,
-                'youtube_result': youtube_result
+                "success": True,
+                "web_research_result": web_research_result,
+                "coresignal_result": coresignal_result,
+                "youtube_result": youtube_result,
             }
-            
+
         except Exception as e:
             error_msg = f"Phase 1 parallel execution failed: {str(e)}"
             print(f"‚ùå {error_msg}")
             logger.exception("Phase 1 parallel execution failed", extra={"run_id": run_id, "company_id": company_id})
-            return {
-                'success': False,
-                'error': error_msg
-            }
+            return {"success": False, "error": error_msg}
 
-    async def _run_phase2_parallel(self, company_data: Dict, run_id: str, company_id: str, 
-                                 shared_output_file: str, user_id: str, session_id: str, session_context: SessionContext) -> Dict[str, Any]:
+    async def _run_phase2_parallel(
+        self,
+        company_data: Dict,
+        run_id: str,
+        company_id: str,
+        shared_output_file: str,
+        user_id: str,
+        session_id: str,
+        session_context: SessionContext,
+    ) -> Dict[str, Any]:
         """
         Phase 2: Run Company Enrichment and Person Enrichment agents in parallel.
         """
@@ -1412,165 +1505,195 @@
         print("üîÑ Running Phase 2 agents in parallel...")
         try:
             # Run all enrichment agents in parallel using asyncio.gather
-            company_enrich_task = self.sub_agents['company_enrich'].execute(
+            company_enrich_task = self.sub_agents["company_enrich"].execute(
                 company_data, run_id, company_id, shared_output_file, self.db, get_enable_postgres_storage(), user_id, session_id
             )
-            person_enrich_task = self.sub_agents['person_enrich'].execute(
+            person_enrich_task = self.sub_agents["person_enrich"].execute(
                 company_data, run_id, company_id, shared_output_file, self.db, get_enable_postgres_storage(), user_id, session_id
             )
-            ria_detection_task = self._ria_detection_task(
-                company_data, run_id, user_id, session_id, company_id
-            )
-            
+            ria_detection_task = self._ria_detection_task(company_data, run_id, user_id, session_id, company_id)
+
             # Wait for all to complete
             company_enrich_result, person_enrich_result, ria_detection_result = await asyncio.gather(
                 company_enrich_task, person_enrich_task, ria_detection_task, return_exceptions=True
             )
-            
+
             # Handle any exceptions
             if isinstance(company_enrich_result, Exception):
                 print(f"‚ùå Company Enrich Agent failed: {company_enrich_result}")
-                company_enrich_result = {'success': False, 'error': str(company_enrich_result)}
-            
+                company_enrich_result = {"success": False, "error": str(company_enrich_result)}
+
             if isinstance(person_enrich_result, Exception):
                 print(f"‚ùå Person Enrich Agent failed: {person_enrich_result}")
-                person_enrich_result = {'success': False, 'error': str(person_enrich_result)}
-            
+                person_enrich_result = {"success": False, "error": str(person_enrich_result)}
+
             if isinstance(ria_detection_result, Exception):
                 print(f"‚ùå RIA Detection Agent failed: {ria_detection_result}")
-                ria_detection_result = {'success': False, 'error': str(ria_detection_result)}
-            
-            print(f"‚úÖ Phase 2 completed - Company Enrich: {'‚úÖ' if company_enrich_result.get('success') else '‚ùå'}, Person Enrich: {'‚úÖ' if person_enrich_result.get('success') else '‚ùå'}, RIA Detection: {'‚úÖ' if ria_detection_result.get('success') else '‚ùå'}")
-        
+                ria_detection_result = {"success": False, "error": str(ria_detection_result)}
+
+            print(
+                f"‚úÖ Phase 2 completed - Company Enrich: {'‚úÖ' if company_enrich_result.get('success') else '‚ùå'}, Person Enrich: {'‚úÖ' if person_enrich_result.get('success') else '‚ùå'}, RIA Detection: {'‚úÖ' if ria_detection_result.get('success') else '‚ùå'}"
+            )
+
             # Store RIA detection result in session context
-            if ria_detection_result.get('success'):
-                session_context.ria_detection_result = ria_detection_result.get('result', {})
+            if ria_detection_result.get("success"):
+                session_context.ria_detection_result = ria_detection_result.get("result", {})
             else:
-                session_context.ria_detection_result = {
-                    'is_ria': False,
-                    'crd_number': None,
-                    'method': 'error'
-                }
-        
-            logger.info("Phase 2 parallel execution completed", extra={
-                "run_id": run_id, 
-                "company_id": company_id, 
-                "company_enrich_success": company_enrich_result.get('success', False),
-                "person_enrich_success": person_enrich_result.get('success', False),
-                "ria_detection_success": ria_detection_result.get('success', False)
-            })
+                session_context.ria_detection_result = {"is_ria": False, "crd_number": None, "method": "error"}
+
+            logger.info(
+                "Phase 2 parallel execution completed",
+                extra={
+                    "run_id": run_id,
+                    "company_id": company_id,
+                    "company_enrich_success": company_enrich_result.get("success", False),
+                    "person_enrich_success": person_enrich_result.get("success", False),
+                    "ria_detection_success": ria_detection_result.get("success", False),
+                },
+            )
 
             # Update component statuses for FE early-stop logic
             try:
-                await ProgressStore.instance().update_component_status(self.db, run_id, "company_enrichment", "success" if company_enrich_result.get('success') else "failed", None if company_enrich_result.get('success') else "COMPANY_ENRICH_FAILED", None)
-                await ProgressStore.instance().update_component_status(self.db, run_id, "person_enrichment", "success" if person_enrich_result.get('success') else "failed", None if person_enrich_result.get('success') else "PEOPLE_NOT_FOUND", None)
-                await ProgressStore.instance().update_component_status(self.db, run_id, "ria_detection", "success" if ria_detection_result.get('success') else "failed", None if ria_detection_result.get('success') else "RIA_DETECTION_FAILED", None)
+                await ProgressStore.instance().update_component_status(
+                    self.db,
+                    run_id,
+                    "company_enrichment",
+                    "success" if company_enrich_result.get("success") else "failed",
+                    None if company_enrich_result.get("success") else "COMPANY_ENRICH_FAILED",
+                    None,
+                )
+                await ProgressStore.instance().update_component_status(
+                    self.db,
+                    run_id,
+                    "person_enrichment",
+                    "success" if person_enrich_result.get("success") else "failed",
+                    None if person_enrich_result.get("success") else "PEOPLE_NOT_FOUND",
+                    None,
+                )
+                await ProgressStore.instance().update_component_status(
+                    self.db,
+                    run_id,
+                    "ria_detection",
+                    "success" if ria_detection_result.get("success") else "failed",
+                    None if ria_detection_result.get("success") else "RIA_DETECTION_FAILED",
+                    None,
+                )
             except Exception:
                 pass
 
             # Ensure persisted results exist; if not, mark as failed with NO_PERSISTED_RESULTS
             try:
                 # Company verification
-                if company_enrich_result.get('success'):
+                if company_enrich_result.get("success"):
                     try:
                         company_data_check = await self.db.get_company_enrichment_by_run(run_id, user_id)
                         if not company_data_check:
-                            await ProgressStore.instance().update_component_status(self.db, run_id, "company_enrichment", "failed", "NO_PERSISTED_RESULTS", "Company enrichment reported success but no results were stored")
+                            await ProgressStore.instance().update_component_status(
+                                self.db,
+                                run_id,
+                                "company_enrichment",
+                                "failed",
+                                "NO_PERSISTED_RESULTS",
+                                "Company enrichment reported success but no results were stored",
+                            )
                     except Exception:
                         # If check fails, do not override
                         pass
                 # Person verification
-                if person_enrich_result.get('success'):
+                if person_enrich_result.get("success"):
                     try:
                         person_data_check = await self.db.get_person_enrichment_by_run(run_id, user_id)
                         has_person = bool(person_data_check) and isinstance(person_data_check, list) and len(person_data_check) > 0
                         if not has_person:
-                            await ProgressStore.instance().update_component_status(self.db, run_id, "person_enrichment", "failed", "NO_PERSISTED_RESULTS", "Person enrichment reported success but no results were stored")
+                            await ProgressStore.instance().update_component_status(
+                                self.db,
+                                run_id,
+                                "person_enrichment",
+                                "failed",
+                                "NO_PERSISTED_RESULTS",
+                                "Person enrichment reported success but no results were stored",
+                            )
                     except Exception:
                         pass
             except Exception:
                 pass
-        
+
             return {
-                'success': True,
-                'company_enrich_result': company_enrich_result,
-                'person_enrich_result': person_enrich_result,
-                'ria_detection_result': ria_detection_result
+                "success": True,
+                "company_enrich_result": company_enrich_result,
+                "person_enrich_result": person_enrich_result,
+                "ria_detection_result": ria_detection_result,
             }
-            
+
         except Exception as e:
             error_msg = f"Phase 2 parallel execution failed: {str(e)}"
             print(f"‚ùå {error_msg}")
             logger.exception("Phase 2 parallel execution failed", extra={"run_id": run_id, "company_id": company_id})
-            return {
-                'success': False,
-                'error': error_msg
-            }
+            return {"success": False, "error": error_msg}
 
-    async def _ria_detection_task(self, company_data: Dict[str, Any], run_id: str, 
-                                 user_id: str, session_id: str, company_id: str) -> Dict[str, Any]:
+    async def _ria_detection_task(self, company_data: Dict[str, Any], run_id: str, user_id: str, session_id: str, company_id: str) -> Dict[str, Any]:
         """Run RIA detection in parallel with other enrichment tasks."""
         try:
             ria_agent = RIADetectionAgent()
             result = await ria_agent.execute(
-                company_data=company_data,
-                run_id=run_id,
-                user_id=user_id,
-                session_id=session_id,
-                company_id=company_id,
-                db=self.db
+                company_data=company_data, run_id=run_id, user_id=user_id, session_id=session_id, company_id=company_id, db=self.db
             )
             return result
         except Exception as e:
-            return {
-                'success': False,
-                'error': f'RIA detection failed: {str(e)}',
-                'agent_name': 'RIA Detection'
-            }
+            return {"success": False, "error": f"RIA detection failed: {str(e)}", "agent_name": "RIA Detection"}
 
-    async def _generate_summary_report(self, company_data: Dict, run_id: str, company_id: str,
-                                     phase1_result: Dict, phase2_result: Dict, 
-                                     execution_time_ms: int, shared_output_file: str, session_context: SessionContext) -> Dict[str, Any]:
+    async def _generate_summary_report(
+        self,
+        company_data: Dict,
+        run_id: str,
+        company_id: str,
+        phase1_result: Dict,
+        phase2_result: Dict,
+        execution_time_ms: int,
+        shared_output_file: str,
+        session_context: SessionContext,
+    ) -> Dict[str, Any]:
         """
         Generate a comprehensive summary report of the prospecting results.
         """
         try:
-            company_name = company_data['name']
-            
+            company_name = company_data["name"]
+
             print(f"‚úÖ Summary report generated for {company_name}")
-        
+
             return {
-                'success': True,
-                'summary_file': shared_output_file,
-                'total_agents_successful': sum([
-                    1 for result in [
-                        phase1_result.get('web_research_result', {}),
-                        phase1_result.get('coresignal_result', {}),
-                        phase2_result.get('company_enrich_result', {}),
-                        phase2_result.get('person_enrich_result', {}),
-                        phase2_result.get('ria_detection_result', {})
-                    ] if result.get('success')
-                ]),
-                'ria_summary': {
-                    'is_ria': getattr(session_context, 'ria_detection_result', {}).get('is_ria', False),
-                    'crd_number': getattr(session_context, 'ria_detection_result', {}).get('crd_number'),
-                    'has_ria_data': bool(getattr(session_context, 'ria_detection_result', {}).get('crd_number'))
-                }
+                "success": True,
+                "summary_file": shared_output_file,
+                "total_agents_successful": sum(
+                    [
+                        1
+                        for result in [
+                            phase1_result.get("web_research_result", {}),
+                            phase1_result.get("coresignal_result", {}),
+                            phase2_result.get("company_enrich_result", {}),
+                            phase2_result.get("person_enrich_result", {}),
+                            phase2_result.get("ria_detection_result", {}),
+                        ]
+                        if result.get("success")
+                    ]
+                ),
+                "ria_summary": {
+                    "is_ria": getattr(session_context, "ria_detection_result", {}).get("is_ria", False),
+                    "crd_number": getattr(session_context, "ria_detection_result", {}).get("crd_number"),
+                    "has_ria_data": bool(getattr(session_context, "ria_detection_result", {}).get("crd_number")),
+                },
             }
-            
+
         except Exception as e:
             error_msg = f"Summary report generation failed: {str(e)}"
             print(f"‚ùå {error_msg}")
-            return {
-                'success': False,
-                'error': error_msg
-            }
+            return {"success": False, "error": error_msg}
 
     async def execute_interactive(self, user_id: str = None) -> None:
         """
         Execute the orchestrator in interactive mode with complete workflow handling and session management.
         This method runs a continuous loop that handles all workflow types including company selection.
-        
+
         Args:
             user_id: User identifier for multi-tenant isolation. If None, generates a unique ID.
         """
@@ -1584,52 +1707,52 @@
         print("   ‚Ä¢ General search: 'Find VC firms in London focusing on fintech'")
         print("   ‚Ä¢ Off-topic: 'What's the weather like?'")
         print()
-        
+
         # Generate a unique user ID if not provided
         if not user_id:
             user_id = f"interactive_user_{int(time.time())}"
-        
+
         # Start a new session
         session_context = await self.start_session(user_id)
-        
+
         print(f"üë§ User ID: {user_id}")
         print(f"üÜî Session ID: {session_context.session_id}")
         print()
-        
+
         try:
             while True:
                 try:
                     # Get user input
                     prompt = input("üìù Enter your prompt: ").strip()
-                    
-                    if prompt.lower() in ['quit', 'exit', 'q']:
+
+                    if prompt.lower() in ["quit", "exit", "q"]:
                         print("üëã Goodbye!")
                         break
-                    
+
                     if not prompt:
                         print("‚ö†Ô∏è Please enter a valid prompt.")
                         continue
-                    
+
                     print(f"\nüîç Processing: '{prompt}'")
                     print("-" * 40)
-                    
+
                     # Execute the orchestrator with session context
                     result = await self.execute(prompt=prompt, user_id=user_id, session_context=session_context)
-                    
-                    if result['success']:
+
+                    if result["success"]:
                         print("‚úÖ Success!")
                         print(f"   Workflow Type: {result.get('workflow_type', 'unknown')}")
                         print(f"   Run ID: {result['run_id']}")
                         print(f"   Session ID: {result.get('session_id', 'N/A')}")
                         print(f"   Interaction #: {result.get('interaction_number', 'N/A')}")
-                        
-                        if result.get('workflow_type') == 'company_search':
+
+                        if result.get("workflow_type") == "company_search":
                             print(f"   Companies Found: {len(result.get('companies', []))}")
                             print(f"   Next Step: {result.get('next_step', 'N/A')}")
                             print(f"   Status: Waiting for company selection...")
-                            
+
                             # Handle company selection
-                            companies = result.get('companies', [])
+                            companies = result.get("companies", [])
                             if companies:
                                 print(f"\nüìã Available Companies ({len(companies)} found):")
                                 print("-" * 60)
@@ -1640,30 +1763,30 @@
                                     print(f"     Focus: {company.get('investor_focus', 'N/A')}")
                                     print(f"     Location: {company.get('location', 'N/A')}")
                                     print()
-                                
+
                                 # Get user selection
                                 while True:
                                     try:
                                         selection = input(f"üéØ Select a company to enrich (1-{len(companies)}) or 'skip': ").strip()
-                                        
-                                        if selection.lower() in ['skip', 's', 'no']:
+
+                                        if selection.lower() in ["skip", "s", "no"]:
                                             print("‚è≠Ô∏è Skipping company selection.")
                                             break
-                                        
+
                                         selected_index = int(selection)
                                         if 1 <= selected_index <= len(companies):
-                                            print(f"‚úÖ Selected company: {companies[selected_index-1].get('company_name')}")
+                                            print(f"‚úÖ Selected company: {companies[selected_index - 1].get('company_name')}")
                                             print("üîÑ Starting enrichment process...")
-                                            
+
                                             # Call the company selection handler with session context
                                             selection_result = await self.handle_user_company_selection(
-                                                run_id=result['run_id'],
+                                                run_id=result["run_id"],
                                                 user_id=user_id,
                                                 selected_company_index=selected_index,
-                                                session_context=session_context
+                                                session_context=session_context,
                                             )
-                                            
-                                            if selection_result['success']:
+
+                                            if selection_result["success"]:
                                                 print("‚úÖ Company enrichment completed!")
                                                 print(f"   Company: {selection_result.get('company_name', 'Unknown')}")
                                                 print(f"   Company ID: {selection_result.get('company_id', 'N/A')}")
@@ -1673,7 +1796,7 @@
                                             else:
                                                 print("‚ùå Company enrichment failed!")
                                                 print(f"   Error: {selection_result.get('error', 'Unknown error')}")
-                                            
+
                                             break
                                         else:
                                             print(f"‚ùå Please enter a number between 1 and {len(companies)}")
@@ -1682,35 +1805,36 @@
                                     except KeyboardInterrupt:
                                         print("\n‚è≠Ô∏è Skipping company selection.")
                                         break
-                        
-                        elif result.get('workflow_type') == 'off_topic':
+
+                        elif result.get("workflow_type") == "off_topic":
                             print(f"   Message: {result.get('message', 'No message provided')}")
                             print(f"   Next Step: {result.get('next_step', 'N/A')}")
                             print(f"   Status: Waiting for user input...")
                             print()
                             print("üí° The system is now waiting for your next prompt.")
                             print("   Try asking about a specific company or search criteria.")
-                            
-                        elif result.get('workflow_type') == 'specific_company':
+
+                        elif result.get("workflow_type") == "specific_company":
                             print(f"   Company: {result.get('company_name', 'Unknown')}")
                             print(f"   Company ID: {result.get('company_id', 'N/A')}")
                             print(f"   Output File: {result.get('output_file', 'N/A')}")
                             print(f"   Status: Enrichment completed!")
-                            
+
                     else:
                         print("‚ùå Failed!")
                         print(f"   Error: {result.get('error', 'Unknown error')}")
-                        
+
                 except KeyboardInterrupt:
                     print("\nüëã Goodbye!")
                     break
                 except Exception as e:
                     print(f"‚ùå Exception: {e}")
                     import traceback
+
                     traceback.print_exc()
-                
+
                 print("\n" + "=" * 60)
-        
+
         finally:
             # End the session when the loop exits
             await self.end_session(session_context)
@@ -1725,14 +1849,15 @@
     print("=" * 60)
     print("Starting interactive mode...")
     print()
-    
+
     # Generate a unique user ID for this session
     user_id = f"session_user_{int(time.time())}"
-    
+
     # Run interactive mode
     orchestrator = ProspectingOrchestrator()
     await orchestrator.execute_interactive(user_id=user_id)
 
+
 if __name__ == "__main__":
     try:
         asyncio.run(main())
@@ -1741,4 +1866,5 @@
     except Exception as e:
         print(f"‚ùå Error: {e}")
         import sys
-        sys.exit(1) 
+
+        sys.exit(1)

--- app/agents/sub_agents/__init__.py
+++ app/agents/sub_agents/__init__.py
@@ -7,4 +7,13 @@
 from .company_search_agent import CompanySearchAgent
 from .youtube_media_agent import YouTubeMediaAgent
 
-__all__ = ["WebResearchAgent", "UserPromptExtractorAgent", "BaseSubAgent", "CoreSignalSubAgent", "CompanyEnrichAgent", "PersonEnrichAgent", "CompanySearchAgent", "YouTubeMediaAgent"] 
+__all__ = [
+    "WebResearchAgent",
+    "UserPromptExtractorAgent",
+    "BaseSubAgent",
+    "CoreSignalSubAgent",
+    "CompanyEnrichAgent",
+    "PersonEnrichAgent",
+    "CompanySearchAgent",
+    "YouTubeMediaAgent",
+]

--- app/agents/sub_agents/base_sub_agent.py
+++ app/agents/sub_agents/base_sub_agent.py
@@ -8,12 +8,13 @@
 from bs4 import BeautifulSoup
 from app.utils.db_utils import ProspectingDB
 
+
 class BaseSubAgent(ABC):
     """
     Base class for modular sub-agents that perform specific prospecting tasks.
     Each sub-agent should be self-contained and output results to files.
     """
-    
+
     def __init__(self, output_dir: str = "output", db: Optional[ProspectingDB] = None):
         self.output_dir = Path(output_dir)
         self.output_dir.mkdir(exist_ok=True)
@@ -22,54 +23,54 @@
             print(f"üîó {self.__class__.__name__}: Initialized with database instance: {id(self.db)}")
         else:
             print(f"üîó {self.__class__.__name__}: Initialized without database, will use global instance when needed")
-    
+
     @abstractmethod
     async def execute(self, company_data: Dict[str, Any]) -> Dict[str, Any]:
         """
         Execute the sub-agent's specific task.
-        
+
         Args:
             company_data: Dictionary containing company information
                          (name, website, location, etc.)
-        
+
         Returns:
             Dictionary with execution results and output file paths
         """
         pass
-    
+
     async def save_output(self, content: str, filename: str) -> str:
         """
         Save content to a markdown file.
-        
+
         Args:
             content: Content to save
             filename: Name of the output file
-            
+
         Returns:
             Path to the saved file
         """
         file_path = self.output_dir / filename
-        async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
+        async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
             await f.write(content)
         return str(file_path)
-    
+
     @property
     @abstractmethod
     def agent_name(self) -> str:
         """Return the name of this sub-agent."""
         pass
-    
+
     async def append_markdown(self, output_file: str, section_title: str, content: str):
         """
         Helper to append content to a markdown file with proper formatting and a datetime stamp.
-        
+
         Args:
             output_file: Path to the markdown file
             section_title: Title for this section
             content: Content to append
         """
         timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
-        
+
         # Format content based on type or patterns
         formatted_content = content
         if "```json" in content and "\n```" in content:
@@ -93,7 +94,7 @@
             except:
                 # Not valid JSON, leave as is
                 pass
-        
+
         async with asyncio.Lock():
             with open(output_file, "a", encoding="utf-8") as f:
                 f.write(f"\n### {section_title}\n")
@@ -103,4 +104,4 @@
 {formatted_content}
 ```
 ---
-""") 
\ No newline at end of file
+""")

--- app/agents/sub_agents/company_enrich_agent.py
+++ app/agents/sub_agents/company_enrich_agent.py
@@ -9,21 +9,30 @@
 from .base_sub_agent import BaseSubAgent
 from app.utils.db_utils import ProspectingDB
 from app.prompts.data_process_prompts import (
-    get_llm_data_consolidation_system_prompt, 
+    get_llm_data_consolidation_system_prompt,
     get_llm_data_consolidation_user_prompt,
     get_narrative_generation_system_prompt,
-    get_narrative_generation_user_prompt
+    get_narrative_generation_user_prompt,
 )
 from app.prompts.data_retrieval_prompts import get_linkedin_analysis_prompt
 from app.utils.qdrant_utils import search_chunks_in_qdrant
 from app.utils.global_db import get_global_db
 from app.utils.langsmith_config import trace_operation, initialize_langsmith
 from app.utils.logging_config import get_logger
-from app.utils.config import get_openai_api_key, get_gemini_api_key, get_google_api_key, get_enable_postgres_storage, get_enable_linkedin_rag, get_enable_llm_consolidation, get_enable_narrative_generation
+from app.utils.config import (
+    get_openai_api_key,
+    get_gemini_api_key,
+    get_google_api_key,
+    get_enable_postgres_storage,
+    get_enable_linkedin_rag,
+    get_enable_llm_consolidation,
+    get_enable_narrative_generation,
+)
 import os
 from app.utils.progress_store import ProgressStore
 # import google.generativeai as genai
 
+
 class CompanyEnrichAgent(BaseSubAgent):
     """
     Company Enrichment Agent that intelligently blends Perplexity and Coresignal data
@@ -33,190 +42,97 @@
     # Source mapping configuration - defines field sources and paths
     SOURCE_MAPPINGS = {
         "core_profile": {
-            "firm_name": {
-                "coresignal_path": ["company_name"]
-            },
-            "legal_entity_name": {
-                "coresignal_path": ["company_legal_name"]
-            },
-            "website": {
-                "coresignal_path": ["website"]
-            },
-            "year_founded": {
-                "coresignal_path": ["founded_year"]
-            },
-            "headquarters_location": {
-                "coresignal_path": ["hq_full_address"]
-            },
-            "additional_office_locations": {
-                "coresignal_path": ["company_locations_full"]
-            },
-            "employee_count": {
-                "coresignal_path": ["employees_count"]
-            },
-            "annual_revenue": {
-                "coresignal_path": ["revenue_annual"]
-            },
-            "employee_distribution": {
-                "coresignal_path": ["employees_count_by_country"]
-            }
+            "firm_name": {"coresignal_path": ["company_name"]},
+            "legal_entity_name": {"coresignal_path": ["company_legal_name"]},
+            "website": {"coresignal_path": ["website"]},
+            "year_founded": {"coresignal_path": ["founded_year"]},
+            "headquarters_location": {"coresignal_path": ["hq_full_address"]},
+            "additional_office_locations": {"coresignal_path": ["company_locations_full"]},
+            "employee_count": {"coresignal_path": ["employees_count"]},
+            "annual_revenue": {"coresignal_path": ["revenue_annual"]},
+            "employee_distribution": {"coresignal_path": ["employees_count_by_country"]},
         },
-        
         "social_media_presence": {
-            "twitter_url": {
-                "coresignal_path": ["twitter_url"]
-            },
-            "youtube_url": {
-                "coresignal_path": ["youtube_url"]
-            },
-            "facebook_url": {
-                "coresignal_path": ["facebook_url"]
-            },
-            "instagram_url": {
-                "coresignal_path": ["instagram_url"]
-            },
-            "linkedin_url": {
-                "coresignal_path": ["linkedin_url"]
-            }
+            "twitter_url": {"coresignal_path": ["twitter_url"]},
+            "youtube_url": {"coresignal_path": ["youtube_url"]},
+            "facebook_url": {"coresignal_path": ["facebook_url"]},
+            "instagram_url": {"coresignal_path": ["instagram_url"]},
+            "linkedin_url": {"coresignal_path": ["linkedin_url"]},
         },
-        
         "investment_strategy_mandate": {
-            "primary_investment_strategy": {
-                "perplexity_path": ["investment_strategy_mandate", "primary_investment_strategy"]
-            },
-            "primary_investment_asset_class": {
-                "perplexity_path": ["investment_strategy_mandate", "primary_investment_asset_class"]
-            },
-            "sector_focus": {
-                "perplexity_path": ["investment_strategy_mandate", "sector_focus"]
-            },
-            "geographic_focus": {
-                "perplexity_path": ["investment_strategy_mandate", "geographic_focus"]
-            },
-            "stage_preference": {
-                "perplexity_path": ["investment_strategy_mandate", "stage_preference"]
-            },
-            "typical_check_size": {
-                "perplexity_path": ["investment_strategy_mandate", "typical_check_size"]
-            }
+            "primary_investment_strategy": {"perplexity_path": ["investment_strategy_mandate", "primary_investment_strategy"]},
+            "primary_investment_asset_class": {"perplexity_path": ["investment_strategy_mandate", "primary_investment_asset_class"]},
+            "sector_focus": {"perplexity_path": ["investment_strategy_mandate", "sector_focus"]},
+            "geographic_focus": {"perplexity_path": ["investment_strategy_mandate", "geographic_focus"]},
+            "stage_preference": {"perplexity_path": ["investment_strategy_mandate", "stage_preference"]},
+            "typical_check_size": {"perplexity_path": ["investment_strategy_mandate", "typical_check_size"]},
         },
-        
         "fund_information": {
-            "current_funds_active": {
-                "perplexity_path": ["fund_information", "current_funds_active"]
-            },
-            "historical_fund_performance": {
-                "perplexity_path": ["fund_information", "historical_fund_performance"]
-            },
-            "recent_fund_launches": {
-                "perplexity_path": ["fund_information", "recent_fund_launches"]
-            },
-            "recent_fund_closures": {
-                "perplexity_path": ["fund_information", "recent_fund_closures"]
-            },
-            "recent_fund_activities": {
-                "perplexity_path": ["fund_information", "recent_fund_activities"]
-            }
+            "current_funds_active": {"perplexity_path": ["fund_information", "current_funds_active"]},
+            "historical_fund_performance": {"perplexity_path": ["fund_information", "historical_fund_performance"]},
+            "recent_fund_launches": {"perplexity_path": ["fund_information", "recent_fund_launches"]},
+            "recent_fund_closures": {"perplexity_path": ["fund_information", "recent_fund_closures"]},
+            "recent_fund_activities": {"perplexity_path": ["fund_information", "recent_fund_activities"]},
         },
-        
         "capital_commitments": {
-            "recent_LP_commitments": {
-                "perplexity_path": ["capital_commitments", "recent_LP_commitments"]
-            },
-            "co_investment_activity": {
-                "perplexity_path": ["capital_commitments", "co_investment_activity"]
-            },
-            "history_investing_similar_funds": {
-                "perplexity_path": ["capital_commitments", "history_investing_similar_funds"]
-            },
-            "public_RFPs_allocation_shifts": {
-                "perplexity_path": ["capital_commitments", "public_RFPs_allocation_shifts"]
-            }
+            "recent_LP_commitments": {"perplexity_path": ["capital_commitments", "recent_LP_commitments"]},
+            "co_investment_activity": {"perplexity_path": ["capital_commitments", "co_investment_activity"]},
+            "history_investing_similar_funds": {"perplexity_path": ["capital_commitments", "history_investing_similar_funds"]},
+            "public_RFPs_allocation_shifts": {"perplexity_path": ["capital_commitments", "public_RFPs_allocation_shifts"]},
         },
-        
         "assets_under_management": {
-            "aum": {
-                "perplexity_path": ["assets_under_management", "aum"]
-            },
-            "breakdown_by_asset_class": {
-                "perplexity_path": ["assets_under_management", "breakdown_by_asset_class"]
-            },
-            "growth_decline_trajectory": {
-                "perplexity_path": ["assets_under_management", "growth_decline_trajectory"]
-            },
-            "esg_dei_strategy": {
-                "perplexity_path": ["assets_under_management", "esg_dei_strategy"]
-            }
+            "aum": {"perplexity_path": ["assets_under_management", "aum"]},
+            "breakdown_by_asset_class": {"perplexity_path": ["assets_under_management", "breakdown_by_asset_class"]},
+            "growth_decline_trajectory": {"perplexity_path": ["assets_under_management", "growth_decline_trajectory"]},
+            "esg_dei_strategy": {"perplexity_path": ["assets_under_management", "esg_dei_strategy"]},
         },
-        
         "organization_decision_making": {
-            "key_decision_makers": {
-                "coresignal_path": ["key_executives"],
-                "perplexity_path": ["organization_decision_making", "c_suite_members"]
-            },
+            "key_decision_makers": {"coresignal_path": ["key_executives"], "perplexity_path": ["organization_decision_making", "c_suite_members"]},
             "firm_ownership_type": {
                 "perplexity_path": ["organization_decision_making", "firm_ownership_type"],
-                "coresignal_path": ["ownership_status"]
+                "coresignal_path": ["ownership_status"],
             },
             "recent_leadership_changes": {
                 "perplexity_path": ["organization_decision_making", "recent_leadership_changes"],
-                "coresignal_path": ["key_executive_arrivals", "key_executive_departures"]
+                "coresignal_path": ["key_executive_arrivals", "key_executive_departures"],
             },
-            "parent_company": {
-                "coresignal_path": ["parent_company_information"]
-            }
+            "parent_company": {"coresignal_path": ["parent_company_information"]},
         },
-        
         "distribution_coverage_relevance": {
-            "target_investor_type": {
-                "perplexity_path": ["distribution_coverage_relevance", "target_investor_type"]
-        },
-            "regulated_by": {
-                "perplexity_path": ["distribution_coverage_relevance", "regulated_by"]
-            },
-            "litigation_regulatory_risk": {
-                "perplexity_path": ["distribution_coverage_relevance", "litigation_regulatory_risk"]
-            }
+            "target_investor_type": {"perplexity_path": ["distribution_coverage_relevance", "target_investor_type"]},
+            "regulated_by": {"perplexity_path": ["distribution_coverage_relevance", "regulated_by"]},
+            "litigation_regulatory_risk": {"perplexity_path": ["distribution_coverage_relevance", "litigation_regulatory_risk"]},
         },
-        
         "activity_sentiment_signals": {
-            "recent_hires": {
-                "perplexity_path": ["activity_sentiment_signals", "recent_hires"],
-                "coresignal_path": ["employees_count_change"]
-            },
-            "new_offices_expansion": {
-                "perplexity_path": ["new_offices_expansion"]
-            },
+            "recent_hires": {"perplexity_path": ["activity_sentiment_signals", "recent_hires"], "coresignal_path": ["employees_count_change"]},
+            "new_offices_expansion": {"perplexity_path": ["new_offices_expansion"]},
             "M&A_fund_spinouts": {
                 "perplexity_path": ["activity_sentiment_signals", "M&A_fund_spinouts"],
-                "coresignal_path": ["acquisition_list_source_1", "acquisition_list_source_2", "acquisition_list_source_5"]
+                "coresignal_path": ["acquisition_list_source_1", "acquisition_list_source_2", "acquisition_list_source_5"],
             },
-            "awards_events_press": {
-                "perplexity_path": ["activity_sentiment_signals", "awards_events_press"]
-            },
-            "public_fund_strategy_commentary": {
-                "perplexity_path": ["activity_sentiment_signals", "public_fund_strategy_commentary"]
-            },
-            "recent_news": {
-                "perplexity_path": ["activity_sentiment_signals", "recent_news"]
-            },
-            "competitive_landscape": {
-                "coresignal_path": ["competitors"]
-            },
+            "awards_events_press": {"perplexity_path": ["activity_sentiment_signals", "awards_events_press"]},
+            "public_fund_strategy_commentary": {"perplexity_path": ["activity_sentiment_signals", "public_fund_strategy_commentary"]},
+            "recent_news": {"perplexity_path": ["activity_sentiment_signals", "recent_news"]},
+            "competitive_landscape": {"coresignal_path": ["competitors"]},
             "job_market_activity": {
                 "coresignal_path": ["active_job_postings_count", "active_job_postings_titles", "active_job_postings_count_change"]
             },
-            "social_media_engagement": {
-                "coresignal_path": ["linkedin_followers_count_change"]
-            },
+            "social_media_engagement": {"coresignal_path": ["linkedin_followers_count_change"]},
             "social_media_activity_summarised": {
-                "linkedin_rag_path": ["strategic_insights", "portfolio_activity", "leadership_updates", 
-                                     "market_commentary", "company_culture", "recognition", "esg_dei_insights"]
+                "linkedin_rag_path": [
+                    "strategic_insights",
+                    "portfolio_activity",
+                    "leadership_updates",
+                    "market_commentary",
+                    "company_culture",
+                    "recognition",
+                    "esg_dei_insights",
+                ]
             },
             "company_updates": {
                 "linkedin_rag_path": ["top_posts_analyzed"]  # Only the top 12 analyzed posts
-            }
-        }
+            },
+        },
     }
 
     class LinkedInRAGService:
@@ -224,97 +140,222 @@
         Service for LinkedIn RAG (Retrieval-Augmented Generation) analysis.
         Performs semantic search and analysis of LinkedIn posts for strategic insights.
         """
-        
+
         def __init__(self):
             # Initialize logger for LinkedIn RAG service
             self.logger = get_logger(__name__)
-            
+
             # Strategic categories with keywords for semantic search
             self.strategic_categories = {
-                'market_insights': [
-                    "healthcare", "technology", "energy", "finance", "consumer", "industrial", 
-                    "real estate", "infrastructure", "trends", "insights", "analysis", "commentary", 
-                    "outlook", "valuations", "strategy", "growth", "headwinds", "tailwinds", 
-                    "performance", "economic", "macro", "micro", "capital", "fundraising", 
-                    "opportunities", "innovation", "disruption", "regulation", "geopolitical", 
-                    "exit", "IPO", "M&A", "acquisition", "dealflow", "portfolio", "thematic", 
-                    "investment"
+                "market_insights": [
+                    "healthcare",
+                    "technology",
+                    "energy",
+                    "finance",
+                    "consumer",
+                    "industrial",
+                    "real estate",
+                    "infrastructure",
+                    "trends",
+                    "insights",
+                    "analysis",
+                    "commentary",
+                    "outlook",
+                    "valuations",
+                    "strategy",
+                    "growth",
+                    "headwinds",
+                    "tailwinds",
+                    "performance",
+                    "economic",
+                    "macro",
+                    "micro",
+                    "capital",
+                    "fundraising",
+                    "opportunities",
+                    "innovation",
+                    "disruption",
+                    "regulation",
+                    "geopolitical",
+                    "exit",
+                    "IPO",
+                    "M&A",
+                    "acquisition",
+                    "dealflow",
+                    "portfolio",
+                    "thematic",
+                    "investment",
+                ],
+                "portfolio_investments": [
+                    "investment",
+                    "portfolio",
+                    "company",
+                    "acquired",
+                    "invested",
+                    "backed",
+                    "supported",
+                    "funding",
+                    "deal",
+                    "acquisition",
+                    "stake",
+                    "equity",
+                    "partnership",
+                    "strategic",
+                    "venture capital",
+                    "seed",
+                    "round",
+                    "series",
+                    "financing",
+                    "valuation",
+                    "exit",
+                    "IPO",
+                    "buy-and-build",
+                    "platform",
+                    "majority",
+                    "minority",
+                    "shareholding",
+                    "divestment",
+                    "co-investment",
+                    "syndicate",
                 ],
-                'portfolio_investments': [
-                    "investment", "portfolio", "company", "acquired", "invested", "backed", 
-                    "supported", "funding", "deal", "acquisition", "stake", "equity", 
-                    "partnership", "strategic", "venture capital", "seed", "round", "series", 
-                    "financing", "valuation", "exit", "IPO", "buy-and-build", "platform", 
-                    "majority", "minority", "shareholding", "divestment", "co-investment", 
-                    "syndicate"
+                "esg_dei_investments": [
+                    "ESG",
+                    "environmental",
+                    "social",
+                    "governance",
+                    "sustainability",
+                    "sustainable",
+                    "renewable",
+                    "renewables",
+                    "clean energy",
+                    "solar",
+                    "wind",
+                    "green",
+                    "climate",
+                    "carbon",
+                    "net zero",
+                    "social housing",
+                    "affordable housing",
+                    "impact investing",
+                    "impact",
+                    "diversity",
+                    "inclusion",
+                    "DEI",
+                    "social impact",
+                    "responsible investing",
+                    "ethical",
+                    "TCFD",
+                    "UN SDG",
+                    "sustainable development",
+                    "decarbonization",
+                    "energy transition",
+                    "circular economy",
+                    "biodiversity",
+                    "water",
+                    "waste",
+                    "social infrastructure",
+                    "healthcare access",
+                    "education",
+                    "financial inclusion",
                 ],
-                'esg_dei_investments': [
-                    "ESG", "environmental", "social", "governance", "sustainability", "sustainable", 
-                    "renewable", "renewables", "clean energy", "solar", "wind", "green", "climate", 
-                    "carbon", "net zero", "social housing", "affordable housing", "impact investing", 
-                    "impact", "diversity", "inclusion", "DEI", "social impact", "responsible investing", 
-                    "ethical", "TCFD", "UN SDG", "sustainable development", "decarbonization", 
-                    "energy transition", "circular economy", "biodiversity", "water", "waste", 
-                    "social infrastructure", "healthcare access", "education", "financial inclusion"
+                "awards_recognition": [
+                    "award",
+                    "recognition",
+                    "achievement",
+                    "milestone",
+                    "accolade",
+                    "honor",
+                    "excellence",
+                    "best",
+                    "top",
+                    "winner",
+                    "finalist",
+                    "ranking",
+                    "league table",
                 ],
-                'awards_recognition': [
-                    "award", "recognition", "achievement", "milestone", "accolade", "honor", 
-                    "excellence", "best", "top", "winner", "finalist", "ranking", "league table"
+                "leadership_appointments": [
+                    "appointment",
+                    "promotion",
+                    "hire",
+                    "join",
+                    "welcome",
+                    "new",
+                    "director",
+                    "partner",
+                    "managing",
+                    "CEO",
+                    "CFO",
+                    "CTO",
+                    "leadership",
+                    "team",
+                    "executive",
                 ],
-                'leadership_appointments': [
-                    "appointment", "promotion", "hire", "join", "welcome", "new", "director", 
-                    "partner", "managing", "CEO", "CFO", "CTO", "leadership", "team", "executive"
+                "company_announcements": [
+                    "announce",
+                    "launch",
+                    "expansion",
+                    "growth",
+                    "milestone",
+                    "partnership",
+                    "collaboration",
+                    "initiative",
+                    "program",
+                    "office",
+                    "location",
+                    "strategy",
                 ],
-                'company_announcements': [
-                    "announce", "launch", "expansion", "growth", "milestone", "partnership", 
-                    "collaboration", "initiative", "program", "office", "location", "strategy"
+                "team_culture": [
+                    "team",
+                    "culture",
+                    "values",
+                    "people",
+                    "employee",
+                    "workplace",
+                    "diversity",
+                    "inclusion",
+                    "celebration",
+                    "event",
+                    "charity",
+                    "community",
+                    "volunteer",
                 ],
-                'team_culture': [
-                    "team", "culture", "values", "people", "employee", "workplace", "diversity", 
-                    "inclusion", "celebration", "event", "charity", "community", "volunteer"
-                ]
             }
-        
-        async def analyze_posts(self, company_id: str, run_id: str, company_name: str, 
-                               shared_output_file: str = None, user_id: str = None) -> Dict[str, Any]:
+
+        async def analyze_posts(
+            self, company_id: str, run_id: str, company_name: str, shared_output_file: str = None, user_id: str = None
+        ) -> Dict[str, Any]:
             """
             Analyze LinkedIn posts using RAG approach with deduplication.
-            
+
             Returns:
                 Dict containing both summarized insights and deduplicated raw posts
             """
             try:
                 print(f"üîç Starting LinkedIn RAG analysis for company_id: {company_id}")
-                
+
                 # Step 1: Search and categorize posts from Qdrant
                 all_posts = await self._search_and_categorize_posts(company_id, run_id, user_id)
-                
+
                 if not all_posts:
                     print(f"‚ö†Ô∏è No LinkedIn posts found for company_id: {company_id}")
                     return {
                         "summarized_insights": None,
                         "raw_posts_deduplicated": [],
                         "top_posts_analyzed": [],  # Empty when no posts found
-                        "analysis_metadata": {
-                            "total_posts_found": 0,
-                            "posts_analyzed": 0,
-                            "company_name": company_name
-                        }
+                        "analysis_metadata": {"total_posts_found": 0, "posts_analyzed": 0, "company_name": company_name},
                     }
-                
+
                 # Step 2: Deduplicate posts based on description content
                 deduplicated_posts = self._deduplicate_posts_by_description(all_posts)
-                
+
                 # Step 3: Select top posts for LLM analysis from deduplicated posts (limit 12)
                 top_posts = self._select_top_posts_for_analysis(deduplicated_posts, limit=12)
-                
+
                 # Step 4: Generate LLM analysis using GPT-4o
-                summarized_insights = await self._generate_llm_analysis(
-                    top_posts, company_name, shared_output_file, run_id, user_id
-                )
-                
+                summarized_insights = await self._generate_llm_analysis(top_posts, company_name, shared_output_file, run_id, user_id)
+
                 print(f"‚úÖ LinkedIn RAG analysis completed: {len(deduplicated_posts)} unique posts, {len(top_posts)} analyzed")
-                
+
                 return {
                     "summarized_insights": summarized_insights,
                     "raw_posts_deduplicated": deduplicated_posts,
@@ -323,89 +364,78 @@
                         "total_posts_found": len(all_posts),
                         "posts_analyzed": len(top_posts),
                         "posts_after_deduplication": len(deduplicated_posts),
-                        "company_name": company_name
-                    }
+                        "company_name": company_name,
+                    },
                 }
-                
+
             except Exception as e:
                 print(f"‚ùå LinkedIn RAG analysis failed: {e}")
                 return {
                     "summarized_insights": None,
                     "raw_posts_deduplicated": [],
                     "top_posts_analyzed": [],  # Empty for error case
-                    "analysis_metadata": {
-                        "error": str(e),
-                        "company_name": company_name
-                    }
+                    "analysis_metadata": {"error": str(e), "company_name": company_name},
                 }
-        
+
         async def _search_and_categorize_posts(self, company_id: str, run_id: str, user_id: str = None) -> List[Dict]:
             """Search Qdrant for LinkedIn posts and categorize them."""
             all_posts = []
-            
+
             for category, keywords in self.strategic_categories.items():
                 try:
                     # Create search query from keywords
                     query = " ".join(keywords[:10])  # Use first 10 keywords to avoid too long queries
-                    
+
                     # Search Qdrant with company, run, and user filtering
                     search_results = search_chunks_in_qdrant(
-                        query_text=query,
-                        collection_name="linkedin_posts",
-                        company_id=company_id,
-                        run_id=run_id,
-                        user_id=user_id,
-                        limit=50
+                        query_text=query, collection_name="linkedin_posts", company_id=company_id, run_id=run_id, user_id=user_id, limit=50
                     )
-                    
+
                     # Process and filter relevant posts
                     for result in search_results:
-                        content = result.get('text', '')
+                        content = result.get("text", "")
                         if content and self._is_relevant_to_category(content, category):
-                            all_posts.append({
-                                'content': content,
-                                'metadata': result.get('metadata', {}),
-                                'score': result.get('score', 0),
-                                'category': category
-                            })
-                    
+                            all_posts.append(
+                                {"content": content, "metadata": result.get("metadata", {}), "score": result.get("score", 0), "category": category}
+                            )
+
                     print(f"üîç Found {len(search_results)} posts for category: {category}")
-                    
+
                 except Exception as e:
                     print(f"‚ö†Ô∏è Error searching category {category}: {e}")
                     continue
-            
+
             # Remove duplicates and sort by score
             unique_posts = self._remove_duplicate_posts(all_posts)
-            return sorted(unique_posts, key=lambda x: x['score'], reverse=True)
-        
+            return sorted(unique_posts, key=lambda x: x["score"], reverse=True)
+
         def _is_relevant_to_category(self, content: str, category: str) -> bool:
             """Check if post content is relevant to the category."""
             if not content:
                 return False
-                
+
             content_lower = content.lower()
             keywords = self.strategic_categories.get(category, [])
-            
+
             # Check for keyword matches (at least 1 keyword match required)
             matches = sum(1 for keyword in keywords if keyword.lower() in content_lower)
             return matches > 0
-        
+
         def _remove_duplicate_posts(self, posts: List[Dict]) -> List[Dict]:
             """Remove duplicate posts based on content similarity."""
             seen_hashes = set()
             unique_posts = []
-            
+
             for post in posts:
-                content = post.get('content', '')
+                content = post.get("content", "")
                 content_hash = hashlib.md5(content.encode()).hexdigest()
-                
+
                 if content_hash not in seen_hashes:
                     seen_hashes.add(content_hash)
                     unique_posts.append(post)
-            
+
             return unique_posts
-        
+
         def _deduplicate_posts_by_description(self, posts: List[Dict]) -> List[Dict]:
             """
             Deduplicate posts based on the 'description' field from metadata.
@@ -415,34 +445,34 @@
             seen_descriptions = set()
             deduplicated = []
             duplicates_removed = 0
-            
+
             for post in posts:
                 # Get description from various possible locations with multiple fallbacks
                 description = ""
-                
+
                 # Try to get description from metadata first
-                metadata = post.get('metadata', {})
+                metadata = post.get("metadata", {})
                 if isinstance(metadata, dict):
-                    description = metadata.get('description', '')
-                
+                    description = metadata.get("description", "")
+
                 # If no description in metadata, try other fields
                 if not description:
-                    description = metadata.get('text', '') if isinstance(metadata, dict) else ''
-                
+                    description = metadata.get("text", "") if isinstance(metadata, dict) else ""
+
                 # If still no description, use content as fallback
                 if not description:
-                    description = post.get('content', '')
-                
+                    description = post.get("content", "")
+
                 # If still no description, use the entire post as a string (last resort)
                 if not description:
                     description = str(post)
-                
+
                 # Create hash of description content only
                 if description:
                     # Normalize the description: strip whitespace, convert to lowercase
                     normalized_description = description.strip().lower()
                     description_hash = hashlib.md5(normalized_description.encode()).hexdigest()
-                    
+
                     if description_hash not in seen_descriptions:
                         seen_descriptions.add(description_hash)
                         deduplicated.append(post)
@@ -452,303 +482,330 @@
                     # If somehow we still have no description, keep the post but warn
                     print(f"‚ö†Ô∏è Post with no description found, keeping anyway")
                     deduplicated.append(post)
-            
+
             print(f"üìä Deduplication complete: {len(posts)} ‚Üí {len(deduplicated)} posts (removed {duplicates_removed} duplicates)")
             return deduplicated
-        
+
         def _select_top_posts_for_analysis(self, posts: List[Dict], limit: int = 12) -> List[Dict]:
             """Select top posts for LLM analysis based on scores."""
             # Apply caps for specific categories first
             capped_posts = []
             category_counts = {}
-            
+
             for post in posts:
-                category = post.get('category', '')
+                category = post.get("category", "")
                 count = category_counts.get(category, 0)
-                
+
                 # Apply category caps
-                if category in ['team_culture', 'awards_recognition'] and count >= 5:
+                if category in ["team_culture", "awards_recognition"] and count >= 5:
                     continue  # Skip if category limit reached
-                
+
                 capped_posts.append(post)
                 category_counts[category] = count + 1
-            
+
             # Return top posts up to limit
             return capped_posts[:limit]
-        
-        async def _generate_llm_analysis(self, posts: List[Dict], company_name: str, 
-                                       shared_output_file: str = None, run_id: str = None, user_id: str = None) -> Dict[str, Any]:
+
+        async def _generate_llm_analysis(
+            self, posts: List[Dict], company_name: str, shared_output_file: str = None, run_id: str = None, user_id: str = None
+        ) -> Dict[str, Any]:
             """Generate LLM analysis using GPT-5."""
             if not posts:
                 print("‚ö†Ô∏è No posts provided for LLM analysis")
                 return None
-            
+
             try:
                 # Prepare posts text for LLM
                 posts_text = ""
                 for i, post in enumerate(posts, 1):
-                    content = post['content'][:400]  # Limit content length
-                    metadata = post['metadata']
+                    content = post["content"][:400]  # Limit content length
+                    metadata = post["metadata"]
                     posts_text += f"""
 Post {i}:
 Content: {content}
-Date: {metadata.get('post_date', 'N/A')}
-Engagement: {metadata.get('reactions_count', 0)} reactions, {metadata.get('comments_count', 0)} comments
+Date: {metadata.get("post_date", "N/A")}
+Engagement: {metadata.get("reactions_count", 0)} reactions, {metadata.get("comments_count", 0)} comments
 ---"""
-                
+
                 print(f"ü§ñ Preparing GPT-5 analysis for {len(posts)} posts ({len(posts_text)} characters)")
-                
+
                 # Use centralized prompt from data_retrieval_prompts.py
                 prompt = get_linkedin_analysis_prompt(company_name, posts_text, len(posts))
-                
+
                 # Log prompt to shared output file
                 if shared_output_file:
-                    with open(shared_output_file, 'a', encoding='utf-8') as f:
+                    with open(shared_output_file, "a", encoding="utf-8") as f:
                         f.write(f"\n## LinkedIn RAG - LLM Analysis Prompt\n\n```\n{prompt}\n```\n\n")
-                
+
                 # Call GPT-5-mini with better error handling, retry logic, and LangSmith tracing
                 api_call_start_time = time.time()
                 response = None
                 response_text = None
                 max_retries = 1  # Retry up to 1 time for empty responses
                 retry_count = 0
-                
+
                 while retry_count <= max_retries:
                     try:
                         # Wrap LangSmith tracing in its own try-except to prevent generator issues
                         try:
-                            with trace_operation("linkedin_rag_analysis", {
-                                "company_name": company_name,
-                                "model": "gpt-5-mini",
-                                "posts_count": len(posts),
-                                "analysis_type": "linkedin_posts",
-                                "run_id": run_id,
-                                "user_id": user_id,
-                                "retry_attempt": retry_count
-                            }):
+                            with trace_operation(
+                                "linkedin_rag_analysis",
+                                {
+                                    "company_name": company_name,
+                                    "model": "gpt-5-mini",
+                                    "posts_count": len(posts),
+                                    "analysis_type": "linkedin_posts",
+                                    "run_id": run_id,
+                                    "user_id": user_id,
+                                    "retry_attempt": retry_count,
+                                },
+                            ):
                                 chat = ChatOpenAI(
                                     model="gpt-5-mini",
                                     max_tokens=5000,  # Increased to handle large JSON responses from LinkedIn RAG analysis
-                                    api_key=get_openai_api_key()
+                                    api_key=get_openai_api_key(),
                                 )
-                                
+
                                 if retry_count > 0:
                                     print(f"üîÑ Retrying GPT-5-mini API call (attempt {retry_count + 1}/{max_retries + 1})...")
                                 else:
                                     print(f"ü§ñ Calling GPT-5-mini API for LinkedIn RAG analysis...")
-                                
-                                self.logger.info("Initiating GPT-5-mini API call", extra={
-                                    "model": "gpt-5-mini",
-                                    "posts_count": len(posts),
-                                    "company_name": company_name,
-                                    "run_id": run_id,
-                                    "prompt_length": len(prompt),
-                                    "retry_attempt": retry_count
-                                })
-                                
-                                response = await chat.ainvoke([
-                                    ("user", prompt)
-                                ])
-                                
+
+                                self.logger.info(
+                                    "Initiating GPT-5-mini API call",
+                                    extra={
+                                        "model": "gpt-5-mini",
+                                        "posts_count": len(posts),
+                                        "company_name": company_name,
+                                        "run_id": run_id,
+                                        "prompt_length": len(prompt),
+                                        "retry_attempt": retry_count,
+                                    },
+                                )
+
+                                response = await chat.ainvoke([("user", prompt)])
+
                                 print(f"‚úÖ GPT-5-mini API call completed")
-                                self.logger.info("GPT-5-mini API call completed", extra={
-                                    "model": "gpt-5-mini",
-                                    "response_type": type(response).__name__,
-                                    "has_content": hasattr(response, 'content'),
-                                    "content_length": len(response.content) if hasattr(response, 'content') and response.content else 0,
-                                    "retry_attempt": retry_count
-                                })
+                                self.logger.info(
+                                    "GPT-5-mini API call completed",
+                                    extra={
+                                        "model": "gpt-5-mini",
+                                        "response_type": type(response).__name__,
+                                        "has_content": hasattr(response, "content"),
+                                        "content_length": len(response.content) if hasattr(response, "content") and response.content else 0,
+                                        "retry_attempt": retry_count,
+                                    },
+                                )
                         except Exception as langsmith_error:
                             # Log LangSmith error but continue processing if we have a response
                             print(f"‚ö†Ô∏è LangSmith tracing error: {langsmith_error}")
-                            self.logger.warning("LangSmith tracing error", extra={
-                                "error": str(langsmith_error),
-                                "model": "gpt-5-mini",
-                                "company_name": company_name,
-                                "has_response": response is not None,
-                                "retry_attempt": retry_count
-                            })
+                            self.logger.warning(
+                                "LangSmith tracing error",
+                                extra={
+                                    "error": str(langsmith_error),
+                                    "model": "gpt-5-mini",
+                                    "company_name": company_name,
+                                    "has_response": response is not None,
+                                    "retry_attempt": retry_count,
+                                },
+                            )
                             # If we don't have a response yet, re-raise
                             if response is None:
                                 raise
-                        
+
                         api_response_time = time.time() - api_call_start_time
-                        
+
                         # Validate response BEFORE logging success
                         if not response:
                             error_msg = "No response object from GPT-5-mini API"
                             print(f"‚ùå {error_msg}")
-                            self.logger.error(error_msg, extra={
-                                "model": "gpt-5-mini",
-                                "response_time": api_response_time,
-                                "company_name": company_name
-                            })
+                            self.logger.error(
+                                error_msg, extra={"model": "gpt-5-mini", "response_time": api_response_time, "company_name": company_name}
+                            )
                             raise Exception(error_msg)
-                        
-                        if not hasattr(response, 'content'):
+
+                        if not hasattr(response, "content"):
                             error_msg = f"Response object missing 'content' attribute. Response type: {type(response)}"
                             print(f"‚ùå {error_msg}")
-                            self.logger.error(error_msg, extra={
-                                "model": "gpt-5-mini",
-                                "response_type": type(response).__name__,
-                                "response_attrs": dir(response) if response else None
-                            })
+                            self.logger.error(
+                                error_msg,
+                                extra={
+                                    "model": "gpt-5-mini",
+                                    "response_type": type(response).__name__,
+                                    "response_attrs": dir(response) if response else None,
+                                },
+                            )
                             raise Exception(error_msg)
-                        
+
                         if not response.content:
                             # Enhanced error logging for empty responses
-                            response_metadata = getattr(response, 'response_metadata', None) or {}
-                            additional_kwargs = getattr(response, 'additional_kwargs', None) or {}
-                            
-                            finish_reason = response_metadata.get('finish_reason', 'unknown')
-                            refusal = additional_kwargs.get('refusal', None)
-                            token_usage = response_metadata.get('token_usage', {})
-                            
+                            response_metadata = getattr(response, "response_metadata", None) or {}
+                            additional_kwargs = getattr(response, "additional_kwargs", None) or {}
+
+                            finish_reason = response_metadata.get("finish_reason", "unknown")
+                            refusal = additional_kwargs.get("refusal", None)
+                            token_usage = response_metadata.get("token_usage", {})
+
                             # Don't retry if content was filtered - it won't change
-                            should_retry = (finish_reason != 'content_filter' and 
-                                           retry_count < max_retries and
-                                           not refusal)
-                            
+                            should_retry = finish_reason != "content_filter" and retry_count < max_retries and not refusal
+
                             if should_retry:
                                 retry_count += 1
                                 print(f"‚ö†Ô∏è Empty response (finish_reason: {finish_reason}), retrying... (attempt {retry_count + 1}/{max_retries + 1})")
-                                self.logger.warning("Empty response, retrying API call", extra={
-                                    "model": "gpt-5-mini",
-                                    "finish_reason": finish_reason,
-                                    "retry_attempt": retry_count,
-                                    "company_name": company_name
-                                })
+                                self.logger.warning(
+                                    "Empty response, retrying API call",
+                                    extra={
+                                        "model": "gpt-5-mini",
+                                        "finish_reason": finish_reason,
+                                        "retry_attempt": retry_count,
+                                        "company_name": company_name,
+                                    },
+                                )
                                 import asyncio
+
                                 await asyncio.sleep(1)  # Brief delay before retry
                                 continue  # Retry the API call
-                            
+
                             # Log detailed diagnostics before raising error
                             error_msg = f"Empty response content from GPT-5-mini API (finish_reason: {finish_reason})"
                             print(f"‚ùå {error_msg}")
-                            
-                            self.logger.error(error_msg, extra={
-                                "model": "gpt-5-mini",
-                                "response_time": api_response_time,
-                                "company_name": company_name,
-                                "posts_count": len(posts),
-                                "run_id": run_id,
-                                "user_id": user_id,
-                                "response_type": type(response).__name__,
-                                "finish_reason": finish_reason,
-                                "refusal": refusal,
-                                "token_usage": token_usage,
-                                "response_metadata": response_metadata,
-                                "additional_kwargs": additional_kwargs,
-                                "prompt_length": len(prompt),
-                                "posts_text_length": len(posts_text),
-                                "has_content_filter": finish_reason == 'content_filter',
-                                "has_length_limit": finish_reason == 'length',
-                                "has_stop": finish_reason == 'stop',
-                                "retry_attempt": retry_count
-                            })
-                            
+
+                            self.logger.error(
+                                error_msg,
+                                extra={
+                                    "model": "gpt-5-mini",
+                                    "response_time": api_response_time,
+                                    "company_name": company_name,
+                                    "posts_count": len(posts),
+                                    "run_id": run_id,
+                                    "user_id": user_id,
+                                    "response_type": type(response).__name__,
+                                    "finish_reason": finish_reason,
+                                    "refusal": refusal,
+                                    "token_usage": token_usage,
+                                    "response_metadata": response_metadata,
+                                    "additional_kwargs": additional_kwargs,
+                                    "prompt_length": len(prompt),
+                                    "posts_text_length": len(posts_text),
+                                    "has_content_filter": finish_reason == "content_filter",
+                                    "has_length_limit": finish_reason == "length",
+                                    "has_stop": finish_reason == "stop",
+                                    "retry_attempt": retry_count,
+                                },
+                            )
+
                             # If content was filtered, provide more context
-                            if finish_reason == 'content_filter':
+                            if finish_reason == "content_filter":
                                 error_msg += " - Content was filtered by OpenAI's safety system"
-                            elif finish_reason == 'length':
+                            elif finish_reason == "length":
                                 error_msg += " - Response was truncated due to token limit"
                             elif refusal:
                                 error_msg += f" - API refused to generate content: {refusal}"
-                            
+
                             raise Exception(error_msg)
-                        
+
                         # Check if response has .content attribute first (preferred method)
                         # Some response objects have both .content and iterator methods, but .content is more reliable
-                        if hasattr(response, 'content') and response.content:
+                        if hasattr(response, "content") and response.content:
                             # Regular response with .content attribute (preferred)
                             # Ensure response.content is a string before processing
                             content_str = str(response.content) if response.content else ""
                             response_text = content_str.strip()
-                        elif hasattr(response, '__aiter__') or hasattr(response, '__iter__'):
+                        elif hasattr(response, "__aiter__") or hasattr(response, "__iter__"):
                             # Handle streaming/generator responses (fallback if no .content)
                             # If it's a generator/stream, collect all chunks
                             content_parts = []
                             try:
-                                if hasattr(response, '__aiter__'):
+                                if hasattr(response, "__aiter__"):
                                     async for chunk in response:
-                                        if hasattr(chunk, 'content'):
+                                        if hasattr(chunk, "content"):
                                             content_parts.append(chunk.content)
                                         elif isinstance(chunk, str):
                                             content_parts.append(chunk)
                                 else:
                                     for chunk in response:
-                                        if hasattr(chunk, 'content'):
+                                        if hasattr(chunk, "content"):
                                             content_parts.append(chunk.content)
                                         elif isinstance(chunk, str):
                                             content_parts.append(chunk)
-                                response_text = ''.join(content_parts).strip()
+                                response_text = "".join(content_parts).strip()
                             except Exception as stream_error:
                                 raise Exception(f"Error reading streaming response: {stream_error}")
                         else:
                             # No content and no iterator - empty response
                             response_text = ""
-                        
+
                         if not response_text:
                             # Enhanced diagnostics for empty response_text (after processing)
-                            response_metadata = getattr(response, 'response_metadata', None) or {}
-                            additional_kwargs = getattr(response, 'additional_kwargs', None) or {}
-                            
-                            finish_reason = response_metadata.get('finish_reason', 'unknown')
-                            refusal = additional_kwargs.get('refusal', None)
-                            token_usage = response_metadata.get('token_usage', {})
-                            
+                            response_metadata = getattr(response, "response_metadata", None) or {}
+                            additional_kwargs = getattr(response, "additional_kwargs", None) or {}
+
+                            finish_reason = response_metadata.get("finish_reason", "unknown")
+                            refusal = additional_kwargs.get("refusal", None)
+                            token_usage = response_metadata.get("token_usage", {})
+
                             # Log detailed diagnostics
-                            raw_content = response.content if hasattr(response, 'content') else None
+                            raw_content = response.content if hasattr(response, "content") else None
                             content_length = len(raw_content) if raw_content else 0
                             content_preview = repr(raw_content[:200]) if raw_content else "None"
-                            
+
                             # Check if we should retry (same logic as empty response.content)
-                            should_retry = (finish_reason != 'content_filter' and 
-                                           retry_count < max_retries and
-                                           not refusal and
-                                           content_length > 0)  # Only retry if there was some content
-                            
+                            should_retry = (
+                                finish_reason != "content_filter" and retry_count < max_retries and not refusal and content_length > 0
+                            )  # Only retry if there was some content
+
                             if should_retry:
                                 retry_count += 1
-                                print(f"‚ö†Ô∏è Empty response_text after processing (raw_length: {content_length}, finish_reason: {finish_reason}), retrying... (attempt {retry_count + 1}/{max_retries + 1})")
-                                self.logger.warning("Empty response_text after processing, retrying API call", extra={
-                                    "model": "gpt-5-mini",
-                                    "finish_reason": finish_reason,
-                                    "raw_content_length": content_length,
-                                    "retry_attempt": retry_count,
-                                    "company_name": company_name
-                                })
+                                print(
+                                    f"‚ö†Ô∏è Empty response_text after processing (raw_length: {content_length}, finish_reason: {finish_reason}), retrying... (attempt {retry_count + 1}/{max_retries + 1})"
+                                )
+                                self.logger.warning(
+                                    "Empty response_text after processing, retrying API call",
+                                    extra={
+                                        "model": "gpt-5-mini",
+                                        "finish_reason": finish_reason,
+                                        "raw_content_length": content_length,
+                                        "retry_attempt": retry_count,
+                                        "company_name": company_name,
+                                    },
+                                )
                                 import asyncio
+
                                 await asyncio.sleep(1)  # Brief delay before retry
                                 continue  # Retry the API call
-                            
+
                             # Log detailed diagnostics before raising error
-                            self.logger.error("Empty response_text after processing", extra={
-                                "model": "gpt-5-mini",
-                                "response_time": api_response_time,
-                                "company_name": company_name,
-                                "posts_count": len(posts),
-                                "run_id": run_id,
-                                "user_id": user_id,
-                                "response_type": type(response).__name__,
-                                "has_content_attr": hasattr(response, 'content'),
-                                "raw_content_length": content_length,
-                                "raw_content_preview": content_preview,
-                                "content_is_none": raw_content is None,
-                                "content_is_empty": raw_content == "",
-                                "content_is_whitespace": raw_content and raw_content.strip() == "",
-                                "finish_reason": finish_reason,
-                                "refusal": refusal,
-                                "token_usage": token_usage,
-                                "response_metadata": response_metadata,
-                                "additional_kwargs": additional_kwargs,
-                                "retry_attempt": retry_count
-                            })
-                            
-                            error_msg = f"Empty response_text from GPT-5-mini API (raw_content_length: {content_length}, finish_reason: {finish_reason})"
-                            if finish_reason == 'content_filter':
+                            self.logger.error(
+                                "Empty response_text after processing",
+                                extra={
+                                    "model": "gpt-5-mini",
+                                    "response_time": api_response_time,
+                                    "company_name": company_name,
+                                    "posts_count": len(posts),
+                                    "run_id": run_id,
+                                    "user_id": user_id,
+                                    "response_type": type(response).__name__,
+                                    "has_content_attr": hasattr(response, "content"),
+                                    "raw_content_length": content_length,
+                                    "raw_content_preview": content_preview,
+                                    "content_is_none": raw_content is None,
+                                    "content_is_empty": raw_content == "",
+                                    "content_is_whitespace": raw_content and raw_content.strip() == "",
+                                    "finish_reason": finish_reason,
+                                    "refusal": refusal,
+                                    "token_usage": token_usage,
+                                    "response_metadata": response_metadata,
+                                    "additional_kwargs": additional_kwargs,
+                                    "retry_attempt": retry_count,
+                                },
+                            )
+
+                            error_msg = (
+                                f"Empty response_text from GPT-5-mini API (raw_content_length: {content_length}, finish_reason: {finish_reason})"
+                            )
+                            if finish_reason == "content_filter":
                                 error_msg += " - Content was filtered by OpenAI's safety system"
-                            elif finish_reason == 'length':
+                            elif finish_reason == "length":
                                 error_msg += " - Response was truncated due to token limit"
                             elif refusal:
                                 error_msg += f" - API refused to generate content: {refusal}"
@@ -756,46 +813,49 @@
                                 error_msg += " - Response content is completely empty"
                             elif raw_content and raw_content.strip() == "":
                                 error_msg += " - Response content is whitespace-only"
-                            
+
                             raise Exception(error_msg)
-                        
+
                         # Only log success after validation
-                        self.logger.info("OpenAI API call successful", extra={
-                            "model": "gpt-5-mini",
-                            "response_time": api_response_time,
-                            "posts_count": len(posts),
-                            "company_name": company_name,
-                            "run_id": run_id,
-                            "user_id": user_id,
-                            "analysis_type": "linkedin_rag",
-                            "response_length": len(response_text)
-                        })
-                        
+                        self.logger.info(
+                            "OpenAI API call successful",
+                            extra={
+                                "model": "gpt-5-mini",
+                                "response_time": api_response_time,
+                                "posts_count": len(posts),
+                                "company_name": company_name,
+                                "run_id": run_id,
+                                "user_id": user_id,
+                                "analysis_type": "linkedin_rag",
+                                "response_length": len(response_text),
+                            },
+                        )
+
                         print(f"‚úÖ GPT-5-mini response received: {len(response_text)} characters")
-                        
+
                         # Successfully got response with content - break out of retry loop
                         break
-                        
+
                     except Exception as api_error:
                         # Check if we should retry this error
-                        should_retry_error = (retry_count < max_retries and 
-                                             "Empty response" not in str(api_error))  # Don't retry empty responses here (handled above)
-                        
+                        should_retry_error = retry_count < max_retries and "Empty response" not in str(
+                            api_error
+                        )  # Don't retry empty responses here (handled above)
+
                         if should_retry_error:
                             retry_count += 1
                             print(f"‚ö†Ô∏è API error occurred, retrying... (attempt {retry_count + 1}/{max_retries + 1}): {api_error}")
-                            self.logger.warning("API error, retrying", extra={
-                                "error": str(api_error),
-                                "retry_attempt": retry_count,
-                                "company_name": company_name
-                            })
+                            self.logger.warning(
+                                "API error, retrying", extra={"error": str(api_error), "retry_attempt": retry_count, "company_name": company_name}
+                            )
                             import asyncio
+
                             await asyncio.sleep(1)  # Brief delay before retry
                             continue  # Retry the API call
-                        
+
                         # All retries exhausted or non-retryable error
                         api_response_time = time.time() - api_call_start_time
-                        
+
                         # Enhanced error logging with response metadata
                         error_extra = {
                             "model": "gpt-5-mini",
@@ -809,64 +869,66 @@
                             "error_type": type(api_error).__name__,
                             "has_response": response is not None,
                             "response_type": type(response).__name__ if response else None,
-                            "retry_attempt": retry_count
+                            "retry_attempt": retry_count,
                         }
-                        
+
                         # Add response metadata if available
                         if response is not None:
-                            response_metadata = getattr(response, 'response_metadata', None) or {}
-                            additional_kwargs = getattr(response, 'additional_kwargs', None) or {}
-                            error_extra.update({
-                                "finish_reason": response_metadata.get('finish_reason', 'unknown'),
-                                "refusal": additional_kwargs.get('refusal', None),
-                                "token_usage": response_metadata.get('token_usage', {}),
-                                "response_metadata": response_metadata,
-                                "additional_kwargs": additional_kwargs,
-                                "has_content": hasattr(response, 'content'),
-                                "content_length": len(response.content) if hasattr(response, 'content') and response.content else 0
-                            })
-                        
+                            response_metadata = getattr(response, "response_metadata", None) or {}
+                            additional_kwargs = getattr(response, "additional_kwargs", None) or {}
+                            error_extra.update(
+                                {
+                                    "finish_reason": response_metadata.get("finish_reason", "unknown"),
+                                    "refusal": additional_kwargs.get("refusal", None),
+                                    "token_usage": response_metadata.get("token_usage", {}),
+                                    "response_metadata": response_metadata,
+                                    "additional_kwargs": additional_kwargs,
+                                    "has_content": hasattr(response, "content"),
+                                    "content_length": len(response.content) if hasattr(response, "content") and response.content else 0,
+                                }
+                            )
+
                         # API monitoring: Track failed OpenAI API call
                         self.logger.error("OpenAI API call failed", extra=error_extra)
-                        
+
                         print(f"‚ùå GPT-5-mini API call failed after {retry_count + 1} attempts: {api_error}")
                         if shared_output_file:
-                            with open(shared_output_file, 'a', encoding='utf-8') as f:
+                            with open(shared_output_file, "a", encoding="utf-8") as f:
                                 f.write(f"## LinkedIn RAG - GPT-5 API Error\n\n{str(api_error)}\n\n")
                                 if response is not None:
-                                    response_metadata = getattr(response, 'response_metadata', None) or {}
+                                    response_metadata = getattr(response, "response_metadata", None) or {}
                                     f.write(f"**Response Metadata:**\n```json\n{response_metadata}\n```\n\n")
                         return None
-                
+
                 # Log response to shared output file
                 if shared_output_file:
-                    with open(shared_output_file, 'a', encoding='utf-8') as f:
+                    with open(shared_output_file, "a", encoding="utf-8") as f:
                         f.write(f"## LinkedIn RAG - LLM Response\n\n```json\n{response_text}\n```\n\n")
-                
+
                 # Try to parse JSON with retry logic
                 max_retries = 3
                 final_result = None
-                
+
                 for attempt in range(1, max_retries + 1):
                     try:
                         # Strip markdown code block formatting if present
                         cleaned_text = response_text
-                        if cleaned_text.startswith('```'):
+                        if cleaned_text.startswith("```"):
                             # Remove opening ```[language] and closing ```
-                            lines = cleaned_text.split('\n')
+                            lines = cleaned_text.split("\n")
                             if len(lines) > 2:  # At least 3 lines for opening, content, closing
-                                cleaned_text = '\n'.join(lines[1:-1])  # Remove first and last lines
+                                cleaned_text = "\n".join(lines[1:-1])  # Remove first and last lines
                             cleaned_text = cleaned_text.strip()
-                        
+
                         # Remove any remaining "json" prefix
-                        if cleaned_text.startswith('json'):
+                        if cleaned_text.startswith("json"):
                             cleaned_text = cleaned_text[4:].strip()
-                        
+
                         # Try to parse the cleaned JSON
                         final_result = json.loads(cleaned_text)
                         print(f"‚úÖ JSON parsing successful on attempt {attempt}")
                         break
-                        
+
                     except json.JSONDecodeError as json_error:
                         print(f"‚ö†Ô∏è JSON parsing failed on attempt {attempt}: {json_error}")
                         if attempt == max_retries:
@@ -879,30 +941,27 @@
                                 "market_commentary": None,
                                 "company_culture": None,
                                 "recognition": None,
-                                "analysis_metadata": {
-                                    "error": str(json_error),
-                                    "company_name": company_name
-                                }
+                                "analysis_metadata": {"error": str(json_error), "company_name": company_name},
                             }
-                
+
                 return final_result
-                
+
             except Exception as e:
                 print(f"‚ùå LLM analysis failed: {e}")
                 if shared_output_file:
-                    with open(shared_output_file, 'a', encoding='utf-8') as f:
+                    with open(shared_output_file, "a", encoding="utf-8") as f:
                         f.write(f"## LinkedIn RAG - LLM Analysis Error\n\n{str(e)}\n\n")
                 return None
 
     def __init__(self, output_dir: str = "app/data", db: Optional[ProspectingDB] = None):
         # Initialize LangSmith
         initialize_langsmith()
-        
+
         super().__init__(output_dir, db)
-        
+
         # Initialize logger for CompanyEnrichAgent
         self.logger = get_logger(__name__)
-        
+
         # Initialize Gemini for LLM-based fusion
         try:
             # Try both environment variable names for compatibility
@@ -914,7 +973,7 @@
                 self.model = ChatGoogleGenerativeAI(
                     temperature=0.1,
                     model="gemini-2.0-flash",  # Updated to gemini-2.0-flash (gemini-1.5-flash is deprecated)
-                    google_api_key=api_key
+                    google_api_key=api_key,
                 )
                 print("‚úÖ Gemini model initialized successfully")
         except Exception as e:
@@ -922,21 +981,30 @@
             self.model = None
         # Initialize LinkedIn RAG service
         self.linkedin_rag = self.LinkedInRAGService()
-        
-        self.logger.info("CompanyEnrichAgent initialized", extra={
-            "output_dir": output_dir,
-            "gemini_model_available": self.model is not None,
-            "linkedin_rag_enabled": True
-        })
 
+        self.logger.info(
+            "CompanyEnrichAgent initialized",
+            extra={"output_dir": output_dir, "gemini_model_available": self.model is not None, "linkedin_rag_enabled": True},
+        )
+
     @property
     def agent_name(self) -> str:
         return "Company Enrichment Agent"
 
-    async def execute(self, company_data: Dict[str, Any], run_id: str, company_id: str, shared_output_file: str, db: ProspectingDB, postgres_enabled: bool, user_id: str, session_id: str) -> Dict[str, Any]:
+    async def execute(
+        self,
+        company_data: Dict[str, Any],
+        run_id: str,
+        company_id: str,
+        shared_output_file: str,
+        db: ProspectingDB,
+        postgres_enabled: bool,
+        user_id: str,
+        session_id: str,
+    ) -> Dict[str, Any]:
         """
         Execute company enrichment using V2 prompts.
-        
+
         Args:
             company_data: Company information dictionary
             run_id: Unique run identifier
@@ -946,24 +1014,27 @@
             postgres_enabled: Whether PostgreSQL is enabled
             user_id: User identifier
             session_id: Session identifier
-            
+
         Returns:
             Dictionary containing company enrichment results
         """
         start_time = time.time()
-        
-        self.logger.info("Company enrichment execution started", extra={
-            "run_id": run_id,
-            "user_id": user_id,
-            "session_id": session_id,
-            "company_name": company_data.get('name', ''),
-            "company_id": company_id
-        })
-        
+
+        self.logger.info(
+            "Company enrichment execution started",
+            extra={
+                "run_id": run_id,
+                "user_id": user_id,
+                "session_id": session_id,
+                "company_name": company_data.get("name", ""),
+                "company_id": company_id,
+            },
+        )
+
         # Use provided database or fall back to self.db
         if db is None:
             db = self.db
-        
+
         # Initialize PostgreSQL connection if not provided
         if not db:
             postgres_enabled = get_enable_postgres_storage()
@@ -979,17 +1050,18 @@
                     db = None
 
         try:
-            company_name = company_data.get('name', '')
+            company_name = company_data.get("name", "")
             print(f"üîÑ Starting Company Enrichment Agent for {company_name}")
 
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 5: Company Enrichment", 
-                                         f"Starting intelligent company enrichment for {company_name}")
+                await self.append_markdown(
+                    shared_output_file, "Step 5: Company Enrichment", f"Starting intelligent company enrichment for {company_name}"
+                )
 
             # Progress: company enrichment mid (approximate early checkpoint)
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'company_enrichment_progress', 0.50)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "company_enrichment_progress", 0.50)
             except Exception:
                 pass
 
@@ -999,111 +1071,110 @@
             # Step 1: Retrieve source data from PostgreSQL using company_id
             if not company_id:
                 raise ValueError("company_id is required for Company Enrichment Agent - cannot proceed without it")
-            
+
             print(f"üìä Retrieving source data from PostgreSQL for company_id: {company_id}")
             self.logger.info("Retrieving source data from PostgreSQL", extra={"run_id": run_id, "company_id": company_id})
-            
+
             # Direct lookup by company_id using the new unified method
-            perplexity_data = await db.get_agent_data_by_company_id(company_id, 'Web Research Agent', run_id, 'perplexity_merged_data', user_id)
-            coresignal_data = await db.get_agent_data_by_company_id(company_id, 'Coresignal Agent', run_id, 'coresignal_extracted_fields', user_id)
+            perplexity_data = await db.get_agent_data_by_company_id(company_id, "Web Research Agent", run_id, "perplexity_merged_data", user_id)
+            coresignal_data = await db.get_agent_data_by_company_id(company_id, "Coresignal Agent", run_id, "coresignal_extracted_fields", user_id)
             youtube_media = None
             try:
                 youtube_media = await db.get_youtube_media_by_run(run_id, user_id)
             except Exception as youtube_error:
-                self.logger.warning("Failed to fetch YouTube media result", extra={
+                self.logger.warning(
+                    "Failed to fetch YouTube media result", extra={"run_id": run_id, "company_id": company_id, "error": str(youtube_error)}
+                )
+
+            self.logger.info(
+                "Source data retrieval completed",
+                extra={
                     "run_id": run_id,
-                    "company_id": company_id,
-                    "error": str(youtube_error)
-                })
-            
-            self.logger.info("Source data retrieval completed", extra={
-                "run_id": run_id,
-                "perplexity_data_found": perplexity_data is not None,
-                "coresignal_data_found": coresignal_data is not None,
-                "youtube_media_found": youtube_media is not None
-            })
-            
+                    "perplexity_data_found": perplexity_data is not None,
+                    "coresignal_data_found": coresignal_data is not None,
+                    "youtube_media_found": youtube_media is not None,
+                },
+            )
+
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 5: Company Enrichment - Source Data Retrieved",
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 5: Company Enrichment - Source Data Retrieved",
                     f"**Perplexity Data:** {'‚úÖ Found' if perplexity_data else '‚ùå Not found'}\n"
                     f"**Coresignal Data:** {'‚úÖ Found' if coresignal_data else '‚ùå Not found'}\n"
-                    f"**YouTube Media:** {'‚úÖ Found' if youtube_media else '‚ùå Not found'}")
+                    f"**YouTube Media:** {'‚úÖ Found' if youtube_media else '‚ùå Not found'}",
+                )
 
             # Step 2: Initialize enrichment result with schema structure
             enriched_result = self._initialize_enrichment_result()
-            
+
             # Step 3: LinkedIn RAG Analysis (if enabled and data available)
             linkedin_insights = None
-            if (get_enable_linkedin_rag() and 
-                coresignal_data and company_id and run_id):
+            if get_enable_linkedin_rag() and coresignal_data and company_id and run_id:
                 try:
                     print(f"üîç Starting LinkedIn RAG analysis...")
-                    self.logger.info("Starting LinkedIn RAG analysis", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "company_name": company_name
-                    })
-                    linkedin_insights = await self.linkedin_rag.analyze_posts(
-                        company_id, run_id, company_name, shared_output_file, user_id
+                    self.logger.info(
+                        "Starting LinkedIn RAG analysis", extra={"run_id": run_id, "company_id": company_id, "company_name": company_name}
                     )
-                    self.logger.info("LinkedIn RAG analysis completed", extra={
-                        "run_id": run_id,
-                        "posts_found": linkedin_insights.get('analysis_metadata', {}).get('total_posts_found', 0) if linkedin_insights else 0,
-                        "posts_analyzed": linkedin_insights.get('analysis_metadata', {}).get('posts_analyzed', 0) if linkedin_insights else 0,
-                        "posts_after_deduplication": linkedin_insights.get('analysis_metadata', {}).get('posts_after_deduplication', 0) if linkedin_insights else 0
-                    })
-                    
+                    linkedin_insights = await self.linkedin_rag.analyze_posts(company_id, run_id, company_name, shared_output_file, user_id)
+                    self.logger.info(
+                        "LinkedIn RAG analysis completed",
+                        extra={
+                            "run_id": run_id,
+                            "posts_found": linkedin_insights.get("analysis_metadata", {}).get("total_posts_found", 0) if linkedin_insights else 0,
+                            "posts_analyzed": linkedin_insights.get("analysis_metadata", {}).get("posts_analyzed", 0) if linkedin_insights else 0,
+                            "posts_after_deduplication": linkedin_insights.get("analysis_metadata", {}).get("posts_after_deduplication", 0)
+                            if linkedin_insights
+                            else 0,
+                        },
+                    )
+
                     if shared_output_file:
-                        await self.append_markdown(shared_output_file, "Step 3: LinkedIn RAG Analysis",
+                        await self.append_markdown(
+                            shared_output_file,
+                            "Step 3: LinkedIn RAG Analysis",
                             f"**Status:** {'‚úÖ Completed' if linkedin_insights else '‚ùå Failed'}\n"
                             f"**Posts Found:** {linkedin_insights.get('analysis_metadata', {}).get('total_posts_found', 0) if linkedin_insights else 0}\n"
                             f"**Posts Analyzed:** {linkedin_insights.get('analysis_metadata', {}).get('posts_analyzed', 0) if linkedin_insights else 0}\n"
-                            f"**After Deduplication:** {linkedin_insights.get('analysis_metadata', {}).get('posts_after_deduplication', 0) if linkedin_insights else 0}")
-                    
+                            f"**After Deduplication:** {linkedin_insights.get('analysis_metadata', {}).get('posts_after_deduplication', 0) if linkedin_insights else 0}",
+                        )
+
                 except Exception as rag_error:
                     print(f"‚ö†Ô∏è LinkedIn RAG analysis failed, continuing without it: {rag_error}")
-                    self.logger.error("LinkedIn RAG analysis failed", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "error": str(rag_error)
-                    })
+                    self.logger.error("LinkedIn RAG analysis failed", extra={"run_id": run_id, "company_id": company_id, "error": str(rag_error)})
                     if shared_output_file:
-                        await self.append_markdown(shared_output_file, "Step 3: LinkedIn RAG Analysis - Error", 
-                                                 f"‚ùå LinkedIn RAG failed: {str(rag_error)}")
-            
+                        await self.append_markdown(
+                            shared_output_file, "Step 3: LinkedIn RAG Analysis - Error", f"‚ùå LinkedIn RAG failed: {str(rag_error)}"
+                        )
+
             # Step 4: Process each field using the new field-based mapping with LinkedIn RAG
             field_stats = {
                 "perplexity_only": {"processed": 0, "total": 0},
                 "coresignal_only": {"processed": 0, "total": 0},
-                "both_sources": {"processed": 0, "total": 0}
+                "both_sources": {"processed": 0, "total": 0},
             }
-            
+
             # Process each field in the mappings
             print(f"üîß Processing fields using new field-based mapping with LinkedIn RAG...")
-            self.logger.info("Starting field processing with SOURCE_MAPPINGS", extra={
-                "run_id": run_id,
-                "linkedin_rag_enabled": linkedin_insights is not None
-            })
-            
+            self.logger.info(
+                "Starting field processing with SOURCE_MAPPINGS", extra={"run_id": run_id, "linkedin_rag_enabled": linkedin_insights is not None}
+            )
+
             # Process field mappings using the existing method that handles nested structures correctly
             await self._process_field_mappings(enriched_result, perplexity_data, coresignal_data, field_stats, linkedin_insights)
-            
+
             # Step 4b: Merge YouTube media insights into social media activity (if available)
             if youtube_media:
                 try:
                     self._merge_youtube_media(enriched_result, youtube_media)
                 except Exception as yt_merge_error:
-                    self.logger.warning("Failed to merge YouTube media into enrichment result", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "error": str(yt_merge_error)
-                    })
-            
-            self.logger.info("Field processing completed", extra={
-                "run_id": run_id,
-                "field_stats": field_stats
-            })
-            
+                    self.logger.warning(
+                        "Failed to merge YouTube media into enrichment result",
+                        extra={"run_id": run_id, "company_id": company_id, "error": str(yt_merge_error)},
+                    )
+
+            self.logger.info("Field processing completed", extra={"run_id": run_id, "field_stats": field_stats})
+
             # Print joined data to shared output file for review using append_markdown
             if shared_output_file:
                 joined_data_content = f"""
@@ -1121,12 +1192,12 @@
 
 ### Perplexity Data
 ```json
-{json.dumps(perplexity_data, indent=2, ensure_ascii=False) if perplexity_data else 'null'}
+{json.dumps(perplexity_data, indent=2, ensure_ascii=False) if perplexity_data else "null"}
 ```
 
 ### Coresignal Data  
 ```json
-{json.dumps(coresignal_data, indent=2, ensure_ascii=False) if coresignal_data else 'null'}
+{json.dumps(coresignal_data, indent=2, ensure_ascii=False) if coresignal_data else "null"}
 ```
 
 ## Final Enriched Result
@@ -1135,15 +1206,15 @@
 ```
 
 ## Field Statistics
-- **Perplexity Only**: {field_stats['perplexity_only']['processed']}/{field_stats['perplexity_only']['total']} fields processed
-- **Coresignal Only**: {field_stats['coresignal_only']['processed']}/{field_stats['coresignal_only']['total']} fields processed  
-- **Both Sources**: {field_stats['both_sources']['processed']}/{field_stats['both_sources']['total']} fields processed
+- **Perplexity Only**: {field_stats["perplexity_only"]["processed"]}/{field_stats["perplexity_only"]["total"]} fields processed
+- **Coresignal Only**: {field_stats["coresignal_only"]["processed"]}/{field_stats["coresignal_only"]["total"]} fields processed  
+- **Both Sources**: {field_stats["both_sources"]["processed"]}/{field_stats["both_sources"]["total"]} fields processed
 
 ## Summary
 This shows the complete company enrichment process where fields from both Perplexity and Coresignal sources are joined using the SOURCE_MAPPINGS configuration. When both sources provide data for a field, they are combined with their respective citations (linkedin_url from Coresignal).
 """
                 await self.append_markdown(shared_output_file, "Step 5: Company Enrichment - Results", joined_data_content)
-            
+
             # Step 4: Add metadata and statistics
             execution_time_ms = int((time.time() - start_time) * 1000)
             enriched_result["enrichment_metadata"] = {
@@ -1155,31 +1226,29 @@
                 "sources_used": {
                     "perplexity": perplexity_data is not None,
                     "coresignal": coresignal_data is not None,
-                    "youtube": youtube_media is not None
-                }
+                    "youtube": youtube_media is not None,
+                },
             }
-            
+
             print(f"‚úÖ Company enrichment completed successfully in {execution_time_ms}ms")
-            
+
             # Step 5: Consolidate with LLM if enabled (now with LinkedIn field exclusion)
             consolidated_result = enriched_result
             if get_enable_llm_consolidation():
                 try:
                     print(f"ü§ñ Starting LLM consolidation with LinkedIn field exclusion...")
-                    self.logger.info("Starting LLM consolidation", extra={
-                        "run_id": run_id,
-                        "model": "gemini-2.0-flash",
-                        "company_name": company_data.get('name', 'Unknown')
-                    })
-                    consolidated_result = await self._consolidate_with_llm(enriched_result, shared_output_file, user_id, run_id, company_data.get('name', 'Unknown'))
+                    self.logger.info(
+                        "Starting LLM consolidation",
+                        extra={"run_id": run_id, "model": "gemini-2.0-flash", "company_name": company_data.get("name", "Unknown")},
+                    )
+                    consolidated_result = await self._consolidate_with_llm(
+                        enriched_result, shared_output_file, user_id, run_id, company_data.get("name", "Unknown")
+                    )
                     print(f"‚úÖ LLM consolidation completed successfully")
                     self.logger.info("LLM consolidation completed successfully", extra={"run_id": run_id})
                 except Exception as llm_error:
                     print(f"‚ö†Ô∏è LLM consolidation failed, using original enriched data: {llm_error}")
-                    self.logger.error("LLM consolidation failed", extra={
-                        "run_id": run_id,
-                        "error": str(llm_error)
-                    })
+                    self.logger.error("LLM consolidation failed", extra={"run_id": run_id, "error": str(llm_error)})
                     consolidated_result = enriched_result
                     # Normalize awards_events_press even when consolidation fails
                     self._normalize_string_array_field(consolidated_result, "activity_sentiment_signals.awards_events_press")
@@ -1188,7 +1257,7 @@
                 self.logger.info("LLM consolidation disabled by environment variable", extra={"run_id": run_id})
                 # Normalize awards_events_press when consolidation is disabled
                 self._normalize_string_array_field(consolidated_result, "activity_sentiment_signals.awards_events_press")
-            
+
             # Step 6: Generate narrative if enabled
             narrative_result = None
             if get_enable_narrative_generation():
@@ -1196,7 +1265,7 @@
                     print(f"üìù Starting narrative generation...")
                     narrative_result = await self._generate_narrative(consolidated_result, user_id, shared_output_file, db, run_id)
                     print(f"‚úÖ Narrative generation completed successfully")
-                    
+
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
@@ -1204,34 +1273,31 @@
                             f"‚úÖ Narrative generated successfully\n"
                             f"**Model:** GPT-4o-mini\n"
                             f"**Cost:** ${narrative_result.get('cost', 0):.6f}\n"
-                            f"**Tokens:** {narrative_result.get('total_tokens', 0):,}")
-                        
+                            f"**Tokens:** {narrative_result.get('total_tokens', 0):,}",
+                        )
+
                 except Exception as narrative_error:
                     print(f"‚ö†Ô∏è Narrative generation failed, continuing without it: {narrative_error}")
                     if shared_output_file:
                         await self.append_markdown(
-                            shared_output_file,
-                            "Step 6: Narrative Generation - Error",
-                            f"‚ùå Narrative generation failed: {str(narrative_error)}")
+                            shared_output_file, "Step 6: Narrative Generation - Error", f"‚ùå Narrative generation failed: {str(narrative_error)}"
+                        )
             else:
                 print(f"‚ÑπÔ∏è Narrative generation disabled by ENABLE_NARRATIVE_GENERATION=false")
-            
+
             # Step 7: Store results in PostgreSQL if enabled
             if postgres_enabled and db and company_id:
                 try:
-                    self.logger.info("Starting PostgreSQL storage of company enrichment results", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "has_narrative": narrative_result is not None
-                    })
-                    
+                    self.logger.info(
+                        "Starting PostgreSQL storage of company enrichment results",
+                        extra={"run_id": run_id, "company_id": company_id, "has_narrative": narrative_result is not None},
+                    )
+
                     # Prepare agent data with narrative if available
-                    agent_data = {
-                        'company_enrichment_result': consolidated_result
-                    }
+                    agent_data = {"company_enrichment_result": consolidated_result}
                     if narrative_result:
-                        agent_data['narrative_result'] = narrative_result
-                    
+                        agent_data["narrative_result"] = narrative_result
+
                     await db.store_agent_result(
                         run_id=run_id,
                         user_id=user_id,
@@ -1239,35 +1305,35 @@
                         agent_name=self.agent_name,
                         result_data=agent_data,
                         company_id=company_id,
-                        execution_time_ms=execution_time_ms
+                        execution_time_ms=execution_time_ms,
                     )
                     print(f"‚úÖ Company Enrichment results stored in PostgreSQL for company_id: {company_id}")
-                    self.logger.info("Company enrichment results stored in PostgreSQL", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "execution_time_ms": execution_time_ms
-                    })
-                    
+                    self.logger.info(
+                        "Company enrichment results stored in PostgreSQL",
+                        extra={"run_id": run_id, "company_id": company_id, "execution_time_ms": execution_time_ms},
+                    )
+
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
                             "Step 7: Company Enrichment - PostgreSQL Storage",
                             f"‚úÖ Stored enriched data in PostgreSQL - Company ID: {company_id}\n"
-                            f"**Includes Narrative:** {'‚úÖ Yes' if narrative_result else '‚ùå No'}")
-                        
+                            f"**Includes Narrative:** {'‚úÖ Yes' if narrative_result else '‚ùå No'}",
+                        )
+
                 except Exception as storage_error:
                     print(f"‚ö†Ô∏è Failed to store Company Enrichment results in PostgreSQL: {storage_error}")
-                    self.logger.error("Failed to store company enrichment results in PostgreSQL", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "error": str(storage_error)
-                    })
+                    self.logger.error(
+                        "Failed to store company enrichment results in PostgreSQL",
+                        extra={"run_id": run_id, "company_id": company_id, "error": str(storage_error)},
+                    )
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
                             "Step 7: Company Enrichment - PostgreSQL Storage Error",
-                            f"‚ùå Failed to store in PostgreSQL: {str(storage_error)}")
-            
+                            f"‚ùå Failed to store in PostgreSQL: {str(storage_error)}",
+                        )
+
             # Step 8: Write results to JSON file (dev-only; skip when disabled)
             try:
                 if os.getenv("ENABLE_FILE_DEBUG_OUTPUT", "false").lower() != "true":
@@ -1283,56 +1349,60 @@
                             "coresignal": coresignal_data is not None,
                             "youtube": youtube_media is not None,
                             "linkedin_rag": linkedin_insights is not None,
-                            "narrative": narrative_result is not None
-                        }
+                            "narrative": narrative_result is not None,
+                        },
                     }
                 from pathlib import Path
-                
+
                 # Create output directory if it doesn't exist
                 output_dir = Path(self.output_dir)
                 output_dir.mkdir(parents=True, exist_ok=True)
-                
+
                 # Generate filename with timestamp and company_name
                 timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-                company_name_safe = company_data.get('name', 'unknown').replace(' ', '_').replace('/', '_').replace(',', '').replace('(', '').replace(')', '')
+                company_name_safe = (
+                    company_data.get("name", "unknown").replace(" ", "_").replace("/", "_").replace(",", "").replace("(", "").replace(")", "")
+                )
                 filename = f"company_enrichment_{company_name_safe}_{timestamp}.json"
                 filepath = output_dir / filename
-                
+
                 print(f"DEBUG: Company name: '{company_data.get('name', 'unknown')}'")
                 print(f"DEBUG: Company name safe: '{company_name_safe}'")
                 print(f"DEBUG: Generated filename: '{filename}'")
                 print(f"DEBUG: Full filepath: '{filepath}'")
-                
+
                 # Write consolidated results to JSON file
                 def json_serializer(obj):
                     """Custom JSON serializer for datetime and other objects."""
-                    if hasattr(obj, 'isoformat'):
+                    if hasattr(obj, "isoformat"):
                         return obj.isoformat()
                     return str(obj)
-                
-                with open(filepath, 'w', encoding='utf-8') as f:
+
+                with open(filepath, "w", encoding="utf-8") as f:
                     json.dump(consolidated_result, f, indent=2, ensure_ascii=False, default=json_serializer)
-                
+
                 print(f"üìÑ Company enrichment results written to: {filepath}")
-                
+
                 if narrative_result:
                     narrative_filename = f"narrative_{company_name_safe}_{timestamp}.json"
                     narrative_filepath = output_dir / narrative_filename
                     # Only write the narrative_data JSON, not the full result with metadata
-                    with open(narrative_filepath, 'w', encoding='utf-8') as nf:
-                        json.dump(narrative_result['narrative_data'], nf, indent=2, ensure_ascii=False)
+                    with open(narrative_filepath, "w", encoding="utf-8") as nf:
+                        json.dump(narrative_result["narrative_data"], nf, indent=2, ensure_ascii=False)
                     print(f"üìÑ Narrative result written to: {narrative_filepath}")
-                
+
                 if shared_output_file:
                     await self.append_markdown(
                         shared_output_file,
                         "Step 8: Company Enrichment - JSON File Output",
-                        f"üìÑ Results written to: `{filepath}`\n"
-                        f"üìÑ Narrative written to: `{narrative_filepath}`" if narrative_result else f"üìÑ Narrative not generated.")
-                    
+                        f"üìÑ Results written to: `{filepath}`\nüìÑ Narrative written to: `{narrative_filepath}`"
+                        if narrative_result
+                        else f"üìÑ Narrative not generated.",
+                    )
+
             except Exception as json_error:
                 print(f"‚ö†Ô∏è Failed to write company enrichment results to JSON file: {json_error}")
-            
+
             return {
                 "success": True,
                 "status": "success",
@@ -1344,24 +1414,28 @@
                     "coresignal": coresignal_data is not None,
                     "youtube": youtube_media is not None,
                     "linkedin_rag": linkedin_insights is not None,
-                    "narrative": narrative_result is not None
-                }
+                    "narrative": narrative_result is not None,
+                },
             }
 
         except Exception as e:
             import traceback
+
             print(f"\n‚ùå [CompanyEnrichAgent ERROR] Exception in execute():\n{traceback.format_exc()}")
-            self.logger.exception("CompanyEnrichAgent execution failed", extra={
-                "run_id": run_id,
-                "user_id": user_id,
-                "session_id": session_id,
-                "company_name": company_data.get('name', ''),
-                "company_id": company_id,
-                "error": str(e)
-            })
-            
+            self.logger.exception(
+                "CompanyEnrichAgent execution failed",
+                extra={
+                    "run_id": run_id,
+                    "user_id": user_id,
+                    "session_id": session_id,
+                    "company_name": company_data.get("name", ""),
+                    "company_id": company_id,
+                    "error": str(e),
+                },
+            )
+
             execution_time_ms = int((time.time() - start_time) * 1000)
-            
+
             # Store error in PostgreSQL if enabled
             if postgres_enabled and db and company_id:
                 try:
@@ -1370,35 +1444,33 @@
                         user_id=user_id,
                         session_id=session_id,
                         agent_name=self.agent_name,
-                        result_data={'error': str(e)},
+                        result_data={"error": str(e)},
                         company_id=company_id,
-                        execution_time_ms=execution_time_ms
+                        execution_time_ms=execution_time_ms,
                     )
                     self.logger.info("Error stored in PostgreSQL", extra={"run_id": run_id, "company_id": company_id})
                 except Exception as db_error:
                     print(f"‚ö†Ô∏è PostgreSQL error storage failed: {db_error}")
-                    self.logger.error("Failed to store error in PostgreSQL", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "error": str(db_error)
-                    })
+                    self.logger.error(
+                        "Failed to store error in PostgreSQL", extra={"run_id": run_id, "company_id": company_id, "error": str(db_error)}
+                    )
             elif postgres_enabled and db and not company_id:
                 print(f"‚ö†Ô∏è Cannot store error in PostgreSQL - company_id is required")
                 self.logger.warning("Cannot store error in PostgreSQL - company_id is required", extra={"run_id": run_id})
-            
+
             return {
-                'success': False,
-                'error': str(e),
-                'agent_name': self.agent_name,
-                'company_name': company_data.get('name', ''),
-                'execution_time_ms': execution_time_ms
+                "success": False,
+                "error": str(e),
+                "agent_name": self.agent_name,
+                "company_name": company_data.get("name", ""),
+                "execution_time_ms": execution_time_ms,
             }
         finally:
             # Note: Database connection is managed globally, no need to close here
             # Progress: company enrichment completed
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'company_enrichment_progress', 1.00)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "company_enrichment_progress", 1.00)
             except Exception:
                 pass
 
@@ -1406,77 +1478,98 @@
         """Initialize the enrichment result with the schema structure in SOURCE_MAPPINGS order."""
         # Initialize with ordered dict to preserve SOURCE_MAPPINGS order
         from collections import OrderedDict
+
         result = OrderedDict()
-        
+
         # Initialize all categories in SOURCE_MAPPINGS order
         for category in self.SOURCE_MAPPINGS.keys():
             result[category] = {}
-            
+
         return result
 
-    async def _process_field_mappings(self, enriched_result: Dict, perplexity_data: Optional[Dict], 
-                                     coresignal_data: Optional[Dict], field_stats: Dict, linkedin_insights: Optional[Dict] = None):
+    async def _process_field_mappings(
+        self,
+        enriched_result: Dict,
+        perplexity_data: Optional[Dict],
+        coresignal_data: Optional[Dict],
+        field_stats: Dict,
+        linkedin_insights: Optional[Dict] = None,
+    ):
         """Process fields using the new field-based mapping with LinkedIn RAG support."""
         if not perplexity_data and not coresignal_data and not linkedin_insights:
             print(f"‚ö†Ô∏è No source data available for field mappings")
             return
-        
+
         for category, fields in self.SOURCE_MAPPINGS.items():
             await self._process_category_fields(category, fields, enriched_result, perplexity_data, coresignal_data, field_stats, linkedin_insights)
-    
-    async def _process_category_fields(self, category: str, fields: Dict, enriched_result: Dict, 
-                                     perplexity_data: Optional[Dict], coresignal_data: Optional[Dict], field_stats: Dict, linkedin_insights: Optional[Dict] = None):
+
+    async def _process_category_fields(
+        self,
+        category: str,
+        fields: Dict,
+        enriched_result: Dict,
+        perplexity_data: Optional[Dict],
+        coresignal_data: Optional[Dict],
+        field_stats: Dict,
+        linkedin_insights: Optional[Dict] = None,
+    ):
         """Process fields within a category."""
         for field_name, field_config in fields.items():
             # Construct the proper nested field path: category.field_name
             # All fields should be nested under their category, including core_profile
             field_path = f"{category}.{field_name}"
-            
+
             # Handle nested fields (like social_media_presence)
             if isinstance(field_config, dict) and not any(key in field_config for key in ["perplexity_path", "coresignal_path", "linkedin_rag_path"]):
                 # This is a nested structure, process its children
                 for sub_field_name, sub_field_config in field_config.items():
                     sub_field_path = f"{field_path}.{sub_field_name}"
-                    await self._process_single_field(sub_field_path, sub_field_config, enriched_result, perplexity_data, coresignal_data, field_stats, linkedin_insights)
+                    await self._process_single_field(
+                        sub_field_path, sub_field_config, enriched_result, perplexity_data, coresignal_data, field_stats, linkedin_insights
+                    )
             else:
                 # This is a regular field with source paths
-                await self._process_single_field(field_path, field_config, enriched_result, perplexity_data, coresignal_data, field_stats, linkedin_insights)
-    
-    async def _process_single_field(self, field_path: str, field_config: Dict, enriched_result: Dict,
-                                  perplexity_data: Optional[Dict], coresignal_data: Optional[Dict], field_stats: Dict, linkedin_insights: Optional[Dict] = None):
+                await self._process_single_field(
+                    field_path, field_config, enriched_result, perplexity_data, coresignal_data, field_stats, linkedin_insights
+                )
+
+    async def _process_single_field(
+        self,
+        field_path: str,
+        field_config: Dict,
+        enriched_result: Dict,
+        perplexity_data: Optional[Dict],
+        coresignal_data: Optional[Dict],
+        field_stats: Dict,
+        linkedin_insights: Optional[Dict] = None,
+    ):
         """Process a single field with its configuration including LinkedIn RAG support."""
         try:
-
-                
             perp_path = field_config.get("perplexity_path")
             core_path = field_config.get("coresignal_path")
             linkedin_rag_path = field_config.get("linkedin_rag_path")
-            
+
             perp_value = None
             core_value = None
             linkedin_value = None
-            
 
             # Get values from each source if available
             if perp_path and perplexity_data:
                 perp_value = self._get_nested_value(perplexity_data, perp_path)
 
-            
             if core_path and coresignal_data:
                 # Handle multi-field paths for Coresignal (e.g., multiple field names to check)
                 core_value = self._get_coresignal_multi_field_value(coresignal_data, core_path, field_path)
-                
 
-            
             if linkedin_rag_path and linkedin_insights:
                 # Get LinkedIn RAG value
                 linkedin_value = self._get_linkedin_rag_value(linkedin_insights, linkedin_rag_path, field_path)
-            
+
             # Get Coresignal citation (linkedin_url)
             coresignal_citation = None
             if coresignal_data:
                 coresignal_citation = coresignal_data.get("professional_network_url") or coresignal_data.get("linkedin_url")
-            
+
             # Determine how to handle the field based on available data
             available_sources = []
             if perp_value is not None:
@@ -1485,12 +1578,10 @@
                 available_sources.append("coresignal")
             if linkedin_value is not None:
                 available_sources.append("linkedin_rag")
-            
 
             if len(available_sources) == 0:
                 # No data available from any source
                 self._set_nested_value(enriched_result, field_path, None)
-                
 
             elif len(available_sources) == 1:
                 # Single source available
@@ -1503,18 +1594,16 @@
                     field_stats["coresignal_only"]["processed"] += 1
                 elif source == "linkedin_rag":
                     formatted_value = self._format_single_source_value(linkedin_value, coresignal_citation)
-                
+
                 self._set_nested_value(enriched_result, field_path, formatted_value)
             else:
                 # Multiple sources available - combine them
-                combined_value = self._combine_multiple_sources(
-                    perp_value, core_value, linkedin_value, coresignal_citation
-                )
+                combined_value = self._combine_multiple_sources(perp_value, core_value, linkedin_value, coresignal_citation)
                 self._set_nested_value(enriched_result, field_path, combined_value)
-                
+
                 if "perplexity" in available_sources and "coresignal" in available_sources:
                     field_stats["both_sources"]["processed"] += 1
-            
+
             # Update total field counts
             if perp_path:
                 field_stats["perplexity_only"]["total"] += 1
@@ -1523,7 +1612,7 @@
             if perp_path and core_path:
                 field_stats["both_sources"]["total"] += 1
             # Note: LinkedIn RAG fields are counted as processed when they provide data
-                
+
         except Exception as e:
             print(f"‚ùå Error processing field {field_path}: {e}")
 
@@ -1536,7 +1625,7 @@
         else:
             actual_value = value
             existing_citation = None
-        
+
         # Combine citations if both exist
         citations = []
         if existing_citation:
@@ -1546,20 +1635,18 @@
                 citations.append(existing_citation)
         if citation:
             citations.append(citation)
-        
+
         # Format the result
-        result = {
-            "value": actual_value
-        }
-        
+        result = {"value": actual_value}
+
         if citations:
             result["citation"] = citations
-            
+
         return result
 
     def _join_data_with_citations(self, perp_value: Any, core_value: Any, coresignal_citation: str = None) -> Dict[str, Any]:
         """Join data from both sources with combined values separated by semicolons and merged citations."""
-        
+
         # Extract Perplexity value and citation
         if isinstance(perp_value, dict) and "value" in perp_value:
             perp_data = perp_value["value"]
@@ -1567,11 +1654,11 @@
         else:
             perp_data = perp_value
             perp_citation = None
-        
+
         # Convert values to strings for joining
         perp_str = str(perp_data) if perp_data is not None else ""
         core_str = str(core_value) if core_value is not None else ""
-        
+
         # Combine values with semicolon
         if perp_str and core_str:
             combined_value = f"{perp_str}; {core_str}"
@@ -1581,7 +1668,7 @@
             combined_value = core_str
         else:
             combined_value = ""
-        
+
         # Collect all citations
         all_citations = []
         if perp_citation:
@@ -1591,82 +1678,82 @@
                 all_citations.append(perp_citation)
         if coresignal_citation:
             all_citations.append(coresignal_citation)
-        
+
         # Format the joined result
-        result = {
-            "value": combined_value
-        }
-        
+        result = {"value": combined_value}
+
         if all_citations:
             result["citation"] = all_citations
-            
+
         return result
 
     def _get_nested_value(self, data: Dict, field_path) -> Any:
         """Get a nested value from a dictionary using either a list of keys or dot notation."""
         if not data or not field_path:
             return None
-        
+
         # Handle both list of keys and dot notation string
         if isinstance(field_path, list):
             keys = field_path
         elif isinstance(field_path, str):
-            keys = field_path.split('.')
+            keys = field_path.split(".")
         else:
             return None
-            
+
         current = data
-        
+
         for key in keys:
             if isinstance(current, dict) and key in current:
                 current = current[key]
             else:
                 return None
-            
+
         return current
 
     def _set_nested_value(self, data: Dict, field_path: str, value: Any):
         """Set a nested value in a dictionary using dot notation."""
-        keys = field_path.split('.')
+        keys = field_path.split(".")
         current = data
-        
+
         # Navigate to the parent of the final key
         for key in keys[:-1]:
             if key not in current:
                 current[key] = {}
             current = current[key]
-        
+
         # Set the final value
-        current[keys[-1]] = value 
+        current[keys[-1]] = value
 
     def _clean_coresignal_value(self, value: Any, field_path: str = "") -> Any:
         """
         Clean Coresignal values by removing empty values and parent_id fields from leadership-related fields.
-        
+
         Args:
             value: The value to clean
             field_path: The field path context to determine if parent_id should be removed
-            
+
         Returns:
             Cleaned value or None if value is empty
         """
         if value is None:
             return None
-            
+
         # Handle different types of values
         if isinstance(value, dict):
             cleaned_dict = {}
             for k, v in value.items():
                 # Remove parent_id from leadership-related fields
-                if k == "parent_id" and any(field in field_path for field in ["key_decision_makers", "key_executive_arrivals", "key_executive_departures"]):
+                if k == "parent_id" and any(
+                    field in field_path for field in ["key_decision_makers", "key_executive_arrivals", "key_executive_departures"]
+                ):
                     continue  # Skip parent_id fields for leadership changes
-                    
+
                 cleaned_v = self._clean_coresignal_value(v, field_path)
                 if cleaned_v is not None:  # Only include non-empty values
                     cleaned_dict[k] = cleaned_v
-                    
+
             return cleaned_dict if cleaned_dict else None
-            
+
         elif isinstance(value, list):
             # Clean each item in the list and filter out empty ones
             cleaned_list = []
@@ -1674,13 +1761,13 @@
                 cleaned_item = self._clean_coresignal_value(item, field_path)
                 if cleaned_item is not None:
                     cleaned_list.append(cleaned_item)
-                    
+
             return cleaned_list if cleaned_list else None
-            
+
         elif isinstance(value, str):
             # Return string only if it's not empty or just whitespace
             return value.strip() if value.strip() else None
-            
+
         else:
             # For other types (int, float, bool), return as-is unless it's 0 or False
             if isinstance(value, (int, float)) and value == 0:
@@ -1692,15 +1779,15 @@
     def _get_coresignal_multi_field_value(self, coresignal_data: Dict, core_path, field_path: str = "") -> Any:
         """
         Get value from Coresignal data, handling both single paths and multi-field paths.
-        
+
         For multi-field paths like ["key_executive_arrivals", "key_employee_change_events", "employees_count_change"],
         this method checks each field individually and combines the non-null values.
-        
+
         Filters out empty values and removes parent_id fields from key_decision_makers only.
         """
         if not core_path or not coresignal_data:
             return None
-            
+
         # If it's a simple nested path (not multi-field), use standard method
         if isinstance(core_path, list) and len(core_path) == 1:
             raw_value = self._get_nested_value(coresignal_data, core_path)
@@ -1708,27 +1795,24 @@
         elif isinstance(core_path, str):
             raw_value = self._get_nested_value(coresignal_data, core_path)
             return self._clean_coresignal_value(raw_value, field_path)
-        
+
         # Handle multi-field paths - check each field individually and combine results
         collected_values = []
-        
+
         if isinstance(core_path, list):
             for field_name in core_path:
                 # Each field_name should be a top-level field in coresignal_data
                 field_value = coresignal_data.get(field_name)
-                
+
                 # Clean the field value (remove empty values and parent_id from key_decision_makers only)
                 cleaned_value = self._clean_coresignal_value(field_value, field_path)
-                
+
                 if cleaned_value is not None:
-                    collected_values.append({
-                        "field": field_name,
-                        "value": cleaned_value
-                    })
+                    collected_values.append({"field": field_name, "value": cleaned_value})
                     print(f"üîç Found non-empty Coresignal data for {field_name}: {type(cleaned_value)} - {str(cleaned_value)[:100]}...")
                 else:
                     print(f"‚ö†Ô∏è Skipped empty Coresignal data for {field_name}")
-        
+
         # Return combined results if any were found
         if collected_values:
             if len(collected_values) == 1:
@@ -1736,38 +1820,35 @@
                 return collected_values[0]["value"]
             else:
                 # Multiple fields found, return as combined structure
-                return {
-                    "combined_fields": collected_values,
-                    "source": "coresignal_multi_field"
-                }
-        
-        return None 
+                return {"combined_fields": collected_values, "source": "coresignal_multi_field"}
+
+        return None
 
     def _get_linkedin_rag_value(self, linkedin_insights: Dict, rag_path, field_path: str = "") -> Any:
         """
         Extract values from LinkedIn RAG analysis results.
-        
+
         Args:
             linkedin_insights: Results from LinkedIn RAG analysis
             rag_path: Path specification for extracting data
             field_path: Current field path for context
-            
+
         Returns:
             Extracted value or None if not found
         """
         if not linkedin_insights or not rag_path:
             return None
-        
+
         try:
             # Handle different types of LinkedIn RAG paths
             if field_path.endswith("social_media_activity_summarised"):
                 # For summarized insights, return structured dictionary with each category as a key
                 summarized_insights = linkedin_insights.get("summarized_insights")
-                
+
                 if not summarized_insights:
                     print(f"‚ö†Ô∏è No summarized_insights found in LinkedIn RAG data for {field_path}")
                     return None
-                
+
                 # Build structured dictionary with each category as a separate key
                 result_dict = {}
                 for category in rag_path:
@@ -1780,14 +1861,14 @@
                                 result_dict[category] = value
                         else:
                             print(f"‚ö†Ô∏è Category {category} data is not in expected format: {type(category_data)}")
-                
+
                 if result_dict:
                     print(f"‚úÖ Structured LinkedIn insights: {len(result_dict)} categories")
                     return result_dict
                 else:
                     print(f"‚ö†Ô∏è No valid insights found in categories: {rag_path}")
                     return None
-                    
+
             elif field_path.endswith("company_updates"):
                 # For company updates, return only the top 12 posts that were analyzed
                 if "top_posts_analyzed" in rag_path:
@@ -1796,24 +1877,19 @@
                         # Simplify posts to just content with LinkedIn citation
                         simplified_posts = []
                         for post in top_posts:
-                            content = post.get('content', '')
+                            content = post.get("content", "")
                             # Get LinkedIn URL from metadata if available
-                            metadata = post.get('metadata', {})
-                            linkedin_url = (metadata.get('citation') or 
-                                          metadata.get('source') or 
-                                          'https://linkedin.com/company/unknown')
-                            
-                            simplified_posts.append({
-                                'content': content,
-                                'citation': linkedin_url
-                            })
-                        
+                            metadata = post.get("metadata", {})
+                            linkedin_url = metadata.get("citation") or metadata.get("source") or "https://linkedin.com/company/unknown"
+
+                            simplified_posts.append({"content": content, "citation": linkedin_url})
+
                         print(f"‚úÖ Found {len(simplified_posts)} simplified posts for company_updates")
                         return simplified_posts
                     else:
                         print(f"‚ö†Ô∏è No analyzed posts found for company_updates")
                         return None
-                    
+
             else:
                 # Generic path handling for future extensions
                 current = linkedin_insights
@@ -1823,13 +1899,12 @@
                     else:
                         return None
                 return current
-                
+
         except Exception as e:
             print(f"‚ùå Error extracting LinkedIn RAG value for {field_path}: {e}")
             return None
-        
-        return None
 
+        return None
 
     def _merge_youtube_media(self, enriched_result: Dict[str, Any], youtube_media: Dict[str, Any]) -> None:
         """
@@ -1854,16 +1929,18 @@
         for item in summaries:
             vid = item.get("video_id")
             summary_text = item.get("summary")
-            simplified_summaries.append({
-                "video_id": vid,
-                "title": item.get("title"),
-                "url": f"https://www.youtube.com/watch?v={vid}" if vid else None,
-                "summary": summary_text,
-                "bullets": item.get("bullets"),
-                "key_moments": item.get("key_moments"),
-                "published_at": item.get("published_at"),
-                "channel_title": item.get("channel_title"),
-            })
+            simplified_summaries.append(
+                {
+                    "video_id": vid,
+                    "title": item.get("title"),
+                    "url": f"https://www.youtube.com/watch?v={vid}" if vid else None,
+                    "summary": summary_text,
+                    "bullets": item.get("bullets"),
+                    "key_moments": item.get("key_moments"),
+                    "published_at": item.get("published_at"),
+                    "channel_title": item.get("channel_title"),
+                }
+            )
 
         payload = {
             "url": youtube_url,
@@ -1911,24 +1988,23 @@
             core_profile["social_media_presence"] = sm_presence
             enriched_result["core_profile"] = core_profile
 
-    def _combine_multiple_sources(self, perp_value: Any, core_value: Any, linkedin_value: Any, 
-                                 coresignal_citation: str = None) -> Dict[str, Any]:
+    def _combine_multiple_sources(self, perp_value: Any, core_value: Any, linkedin_value: Any, coresignal_citation: str = None) -> Dict[str, Any]:
         """
         Combine values from multiple sources (Perplexity, Coresignal, LinkedIn RAG).
-        
+
         Args:
             perp_value: Value from Perplexity
-            core_value: Value from Coresignal  
+            core_value: Value from Coresignal
             linkedin_value: Value from LinkedIn RAG
             coresignal_citation: Citation URL from Coresignal
-            
+
         Returns:
             Combined value with citations
         """
         # Collect all non-null values
         values = []
         citations = []
-        
+
         # Process Perplexity value
         if perp_value is not None:
             if isinstance(perp_value, dict) and "value" in perp_value:
@@ -1937,7 +2013,7 @@
             else:
                 perp_data = perp_value
                 perp_citation = None
-                
+
             if perp_data:
                 values.append(str(perp_data))
                 if perp_citation:
@@ -1945,13 +2021,13 @@
                         citations.extend(perp_citation)
                     else:
                         citations.append(perp_citation)
-        
+
         # Process Coresignal value
         if core_value is not None:
             values.append(str(core_value))
             if coresignal_citation:
                 citations.append(coresignal_citation)
-        
+
         # Process LinkedIn RAG value
         if linkedin_value is not None:
             # Handle dictionary structure for social_media_activity_summarised
@@ -1961,7 +2037,7 @@
                 combined_value = linkedin_value
                 if coresignal_citation:
                     citations.append(f"{coresignal_citation} (analyzed)")
-                
+
                 # Create result with dictionary value
                 result = {"value": combined_value}
                 if citations:
@@ -1971,7 +2047,7 @@
                         if citation not in unique_citations:
                             unique_citations.append(citation)
                     result["citation"] = unique_citations
-                
+
                 return result
             else:
                 # Regular value, convert to string
@@ -1979,10 +2055,10 @@
                 # LinkedIn RAG insights are derived from Coresignal data, so use same citation
                 if coresignal_citation:
                     citations.append(f"{coresignal_citation} (analyzed)")
-        
+
         # Combine values (for non-dictionary LinkedIn values)
         combined_value = "; ".join(values) if values else ""
-        
+
         # Create result
         result = {"value": combined_value}
         if citations:
@@ -1992,34 +2068,34 @@
                 if citation not in unique_citations:
                     unique_citations.append(citation)
             result["citation"] = unique_citations
-            
+
         return result
 
     def _extract_fields_for_exclusion(self, data: Dict[str, Any], field_paths_to_exclude: List[str]) -> tuple[Dict[str, Any], Dict[str, Any]]:
         """
         Extract specified fields from nested data structure for exclusion from LLM processing.
-        
+
         Args:
             data: The data dictionary to process
             field_paths_to_exclude: List of dot-notation field paths to extract (e.g., ["activity_sentiment_signals.social_media_activity_summarised"])
-            
+
         Returns:
             Tuple of (data_without_excluded_fields, extracted_fields_with_paths)
         """
         import copy
-        
+
         # Deep copy the data to avoid modifying the original
         data_copy = copy.deepcopy(data)
         extracted_fields = {}
-        
+
         for field_path in field_paths_to_exclude:
             try:
                 # Navigate to the field and extract it
-                keys = field_path.split('.')
+                keys = field_path.split(".")
                 current = data_copy
                 parent = None
                 last_key = None
-                
+
                 # Navigate to the parent of the target field
                 for i, key in enumerate(keys[:-1]):
                     if isinstance(current, dict) and key in current:
@@ -2030,7 +2106,7 @@
                         # Field doesn't exist, skip
                         current = None
                         break
-                
+
                 # Extract the field if it exists
                 if current is not None and isinstance(current, dict) and last_key in current:
                     extracted_value = current[last_key]
@@ -2040,63 +2116,64 @@
                     print(f"‚úÖ Extracted field for exclusion: {field_path}")
                 else:
                     print(f"‚ö†Ô∏è Field not found for exclusion: {field_path}")
-                    
+
             except Exception as e:
                 print(f"‚ùå Error extracting field {field_path}: {e}")
                 continue
-        
+
         return data_copy, extracted_fields
 
     def _reinsert_excluded_fields(self, processed_data: Dict[str, Any], extracted_fields: Dict[str, Any]) -> Dict[str, Any]:
         """
         Reinsert previously extracted fields back into the processed data structure.
-        
+
         Args:
             processed_data: Data that has been processed by LLM
             extracted_fields: Fields that were extracted before LLM processing
-            
+
         Returns:
             Combined data with extracted fields reinserted
         """
         import copy
-        
+
         # Deep copy to avoid modifying the original
         result = copy.deepcopy(processed_data)
-        
+
         for field_path, field_value in extracted_fields.items():
             try:
-                keys = field_path.split('.')
+                keys = field_path.split(".")
                 current = result
-                
+
                 # Navigate/create the path to the target field
                 for key in keys[:-1]:
                     if key not in current:
                         current[key] = {}
                     current = current[key]
-                
+
                 # Set the final field value
                 final_key = keys[-1]
                 current[final_key] = field_value
                 print(f"‚úÖ Reinserted excluded field: {field_path}")
-                
+
             except Exception as e:
                 print(f"‚ùå Error reinserting field {field_path}: {e}")
                 continue
-        
+
         return result
 
     def _strip_citations_and_collect(self, data: Dict[str, Any]) -> tuple[Dict[str, Any], Dict[str, Any]]:
         """Deep-copy data, remove all 'citation' keys, and collect them by dot-path for later reattachment."""
         import copy
+
         stripped = copy.deepcopy(data)
         citations: Dict[str, Any] = {}
 
         def recurse(node: Any, path: list[str]):
             if isinstance(node, dict):
                 # If this node is a field dict (commonly has 'value') and has citation, strip and collect
-                if 'citation' in node:
-                    path_key = '.'.join(path)
-                    citations[path_key] = node.pop('citation')
+                if "citation" in node:
+                    path_key = ".".join(path)
+                    citations[path_key] = node.pop("citation")
                 for key, value in list(node.items()):
                     recurse(value, path + [key])
             elif isinstance(node, list):
@@ -2111,14 +2188,15 @@
         """Return subtree containing only Perplexity-mapped leaf fields whose text shows a merge (semicolon).
         Only fields whose dot-path exists in allowed_perplexity_paths AND whose value contains ';' are kept.
         """
+
         def filter_node(node: Any, path: list[str]) -> Any:
             if isinstance(node, dict):
                 # Leaf field shape { value: ..., (citation stripped) }
-                if 'value' in node and isinstance(node['value'], str):
-                    val = node['value'].strip()
-                    dot_path = '.'.join(path)
-                    if (';' in val) and (dot_path in allowed_perplexity_paths):
-                        return {'value': node['value']}
+                if "value" in node and isinstance(node["value"], str):
+                    val = node["value"].strip()
+                    dot_path = ".".join(path)
+                    if (";" in val) and (dot_path in allowed_perplexity_paths):
+                        return {"value": node["value"]}
                     return None
                 # Recurse into children
                 out: Dict[str, Any] = {}
@@ -2180,15 +2258,16 @@
         All other fields remain and will be sent to LLM (unless globally excluded earlier).
         """
         import copy
+
         result = copy.deepcopy(data)
 
         def recurse(node: Any, path: list[str]) -> Any:
             if isinstance(node, dict):
                 # Leaf field
-                if 'value' in node and isinstance(node['value'], str):
-                    dot_path = '.'.join(path)
-                    val = node['value'].strip()
-                    if (dot_path in allowed_perplexity_paths) and (';' not in val):
+                if "value" in node and isinstance(node["value"], str):
+                    dot_path = ".".join(path)
+                    val = node["value"].strip()
+                    if (dot_path in allowed_perplexity_paths) and (";" not in val):
                         # remove this leaf by returning None signal to parent
                         return None
                     return node
@@ -2214,6 +2293,7 @@
     def _subtract_payload(self, full: Dict[str, Any], subset: Dict[str, Any]) -> Dict[str, Any]:
         """Deep-diff: remove subset keys from full, returning the remainder."""
         import copy
+
         result = copy.deepcopy(full)
 
         def recurse(a: Any, b: Any) -> Any:
@@ -2233,7 +2313,9 @@
     def _overlay_dicts(self, base: Dict[str, Any], top: Dict[str, Any]) -> Dict[str, Any]:
         """Deep-merge: values in top override base; dicts merged recursively."""
         import copy
+
         result = copy.deepcopy(base)
+
         def merge(a: Any, b: Any) -> Any:
             if isinstance(a, dict) and isinstance(b, dict):
                 out = copy.deepcopy(a)
@@ -2241,11 +2323,13 @@
                     out[k] = merge(a.get(k), v)
                 return out
             return copy.deepcopy(b) if b is not None else a
+
         return merge(result, top)
 
     def _reattach_citations(self, data: Dict[str, Any], citations_by_path: Dict[str, Any]) -> Dict[str, Any]:
         """Reattach previously stripped citations by dot-path."""
         import copy
+
         result = copy.deepcopy(data)
 
         for path_key, citation in citations_by_path.items():
@@ -2254,14 +2338,14 @@
                 current = result
                 if not path_key:
                     continue
-                for key in path_key.split('.'):    
+                for key in path_key.split("."):
                     if isinstance(current, dict) and key in current:
                         current = current[key]
                     else:
                         current = None
                         break
                 if isinstance(current, dict):
-                    current['citation'] = citation
+                    current["citation"] = citation
             except Exception:
                 continue
         return result
@@ -2318,47 +2402,47 @@
     def _normalize_dot_notation_keys(self, data: Dict[str, Any]) -> Dict[str, Any]:
         """
         Normalize dot-notation keys that appear as top-level keys.
-        
+
         The LLM may return keys like "organization_decision_making.recent_leadership_changes"
         as literal top-level keys. This function:
         1. Detects such keys
         2. Nests them under the correct parent object
         3. Removes the flat keys after nesting
         4. Always uses LLM processed data (from flat keys) over raw pre-LLM nested values
-        
+
         Args:
             data: Dictionary that may contain dot-notation keys
-            
+
         Returns:
             Dictionary with dot-notation keys properly nested
         """
         import copy
-        
+
         if not isinstance(data, dict):
             return data
-        
+
         result = copy.deepcopy(data)
         keys_to_remove = []
         keys_to_nest = {}
-        
+
         # Find all top-level keys that contain dots (but not in enrichment_metadata)
         for key in list(result.keys()):
             if key == "enrichment_metadata":
                 continue
-                
-            if '.' in key:
+
+            if "." in key:
                 # This is a dot-notation key that should be nested
                 keys_to_remove.append(key)
                 keys_to_nest[key] = result[key]
-        
+
         # Process each dot-notation key
         for dot_key, value in keys_to_nest.items():
             # Split the key into path components
-            path_parts = dot_key.split('.')
-            
+            path_parts = dot_key.split(".")
+
             if len(path_parts) < 2:
                 continue  # Skip if not a valid nested path
-            
+
             # Navigate/create the nested structure
             current = result
             for i, part in enumerate(path_parts[:-1]):
@@ -2372,11 +2456,11 @@
             else:
                 # Successfully navigated to parent, now set the nested value
                 final_key = path_parts[-1]
-                
+
                 # Always prefer LLM processed data over raw pre-LLM nested value
                 # The LLM is called specifically to transform data into the correct format
                 existing_value = current.get(final_key)
-                
+
                 # Preserve citations from existing nested structure if present
                 if isinstance(existing_value, dict) and "citation" in existing_value:
                     existing_citation = existing_value.get("citation")
@@ -2396,58 +2480,58 @@
                                     value["citation"] = [value["citation"], existing_citation]
                         else:
                             value["citation"] = existing_citation
-                
+
                 # Always replace with LLM processed data
                 current[final_key] = value
                 if existing_value is not None:
                     print(f"‚úÖ Nested dot-notation key '{dot_key}' ‚Üí {'.'.join(path_parts)} (replaced with LLM processed data)")
                 else:
                     print(f"‚úÖ Nested dot-notation key '{dot_key}' ‚Üí {'.'.join(path_parts)}")
-        
+
         # Remove the flat dot-notation keys
         for key in keys_to_remove:
             if key in result:
                 del result[key]
                 print(f"üóëÔ∏è Removed flat dot-notation key: '{key}'")
-        
+
         return result
 
     def _normalize_string_array_field(self, data: Dict[str, Any], field_path: str) -> None:
         """
         Ensure a field that should be a string array contains only strings.
         Converts any objects in the array to formatted strings.
-        
+
         Args:
             data: The data dictionary to normalize
             field_path: Dot-notation path to the field (e.g., "activity_sentiment_signals.awards_events_press")
         """
         # Navigate to the field using dot notation
-        keys = field_path.split('.')
+        keys = field_path.split(".")
         current = data
-        
+
         # Navigate to the parent of the target field
         for key in keys[:-1]:
             if isinstance(current, dict) and key in current:
                 current = current[key]
             else:
                 return  # Field path doesn't exist
-        
+
         # Get the final key
         final_key = keys[-1]
-        
+
         if final_key not in current:
             return  # Field doesn't exist
-        
+
         field_data = current[final_key]
-        
+
         # Check if it's in the expected format: {"value": [...], "citation": [...]}
         if isinstance(field_data, dict) and "value" in field_data:
             value_array = field_data["value"]
-            
+
             if isinstance(value_array, list):
                 normalized_array = []
                 objects_converted = 0
-                
+
                 for item in value_array:
                     if isinstance(item, str):
                         # Already a string, keep it
@@ -2456,7 +2540,7 @@
                         # Convert object to string
                         objects_converted += 1
                         parts = []
-                        
+
                         # Common fields in awards/events/press objects
                         if "recognition" in item:
                             parts.append(f"Recognition: {item['recognition']}")
@@ -2470,19 +2554,19 @@
                             parts.append(f"Recipient: {item['recipient']}")
                         if "significance" in item:
                             parts.append(f"Significance: {item['significance']}")
-                        
+
                         # If we have parts, join them; otherwise convert the whole dict to a readable string
                         if parts:
                             normalized_string = " | ".join(parts)
                         else:
                             # Fallback: convert all key-value pairs
                             normalized_string = ", ".join([f"{k}: {v}" for k, v in item.items() if v])
-                        
+
                         normalized_array.append(normalized_string)
                     else:
                         # Other types (int, float, etc.) - convert to string
                         normalized_array.append(str(item))
-                
+
                 # Update the value array
                 field_data["value"] = normalized_array
                 if objects_converted > 0:
@@ -2504,24 +2588,26 @@
                     field_data["value"] = [str(value_array)]
                 print(f"‚úÖ Normalized {field_path}: converted single object to string array")
 
-    async def _consolidate_with_llm(self, enriched_data: Dict[str, Any], shared_output_file: str = None, user_id: str = None, run_id: str = None, company_name: str = None) -> Dict[str, Any]:
+    async def _consolidate_with_llm(
+        self, enriched_data: Dict[str, Any], shared_output_file: str = None, user_id: str = None, run_id: str = None, company_name: str = None
+    ) -> Dict[str, Any]:
         """Use Gemini to consolidate and clean the enriched data while preserving structure and citations."""
-        
+
         # Check if model is initialized before proceeding
         if not self.model:
             print(f"‚ö†Ô∏è Gemini model not initialized - skipping LLM consolidation")
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: LLM Consolidation - Skipped", 
-                                         "‚ö†Ô∏è Gemini model not initialized - using original enriched data")
+                await self.append_markdown(
+                    shared_output_file, "Step 6: LLM Consolidation - Skipped", "‚ö†Ô∏è Gemini model not initialized - using original enriched data"
+                )
             # Normalize awards_events_press even when skipping LLM consolidation
             self._normalize_string_array_field(enriched_data, "activity_sentiment_signals.awards_events_press")
             return enriched_data
-        
+
         print(f"ü§ñ Starting LLM consolidation with Gemini model: {self.model.model_name if hasattr(self.model, 'model_name') else 'unknown'}")
-        
+
         # Remove metadata before sending to LLM
         data_without_metadata = {k: v for k, v in enriched_data.items() if k != "enrichment_metadata"}
-        
 
         # Define fields to exclude from LLM processing (they don't need consolidation)
         # Note: Use explicit field paths; top-level category names are not supported by the extractor
@@ -2530,7 +2616,6 @@
             "activity_sentiment_signals.social_media_activity_summarised",
             "activity_sentiment_signals.company_updates",
             "activity_sentiment_signals.recent_news",
-
             # CoreSignal-only fields that should bypass LLM consolidation
             "core_profile.firm_name",
             "core_profile.legal_entity_name",
@@ -2538,7 +2623,6 @@
             "core_profile.year_founded",
             "core_profile.employee_count",
             "core_profile.employee_distribution",
-
             # Social media URL fields (treat as immutable identifiers/links)
             "social_media_presence.twitter_url",
             "social_media_presence.youtube_url",
@@ -2546,37 +2630,32 @@
             "social_media_presence.instagram_url",
             "social_media_presence.linkedin_url",
         ]
-        
+
         # Extract fields before LLM processing
-        data_for_llm, extracted_fields = self._extract_fields_for_exclusion(
-            data_without_metadata, fields_to_exclude
-        )
+        data_for_llm, extracted_fields = self._extract_fields_for_exclusion(data_without_metadata, fields_to_exclude)
 
         # Strip citations and filter payload to only conflicted/long free-text fields
         data_for_llm_stripped, citations_by_path = self._strip_citations_and_collect(data_for_llm)
         # Only exclude Perplexity-only, unmerged fields; keep everything else for LLM
         # Remove ONLY unmerged Perplexity-only fields; keep mixed (Perplexity+CoreSignal) fields for LLM
         perplexity_only_paths = self._collect_perplexity_only_field_paths()
-        cleaned_llm_payload = self._remove_unmerged_perplexity_fields(
-            data_for_llm_stripped, perplexity_only_paths
-        )
+        cleaned_llm_payload = self._remove_unmerged_perplexity_fields(data_for_llm_stripped, perplexity_only_paths)
         # What we removed are the unmerged Perplexity-only leaves; the rest will be processed by LLM
         unprocessed_payload = self._subtract_payload(data_for_llm_stripped, cleaned_llm_payload)
 
         print(f"ü§ñ Data prepared for LLM (cleaned): {len(json.dumps(cleaned_llm_payload))} characters")
         print(f"ü§ñ Excluded {len(extracted_fields)} fields and stripped {len(citations_by_path)} citations")
-        
+
         # Get prompts from the prompt module, include JSON Schemas for complex fields
         system_prompt = get_llm_data_consolidation_system_prompt()
         try:
             from app.models.enrichment_models import get_enrichment_json_schemas
+
             schemas = get_enrichment_json_schemas()
         except Exception:
             schemas = {}
-        user_prompt = get_llm_data_consolidation_user_prompt(
-            json.dumps(cleaned_llm_payload, indent=2, ensure_ascii=False), schemas=schemas
-        )
-        
+        user_prompt = get_llm_data_consolidation_user_prompt(json.dumps(cleaned_llm_payload, indent=2, ensure_ascii=False), schemas=schemas)
+
         # Log the prompts to shared output file
         if shared_output_file:
             llm_content = f"""
@@ -2593,7 +2672,7 @@
 ```
 """
             await self.append_markdown(shared_output_file, "Step 6: LLM Consolidation - Input", llm_content)
-        
+
         # If nothing requires consolidation, skip LLM and reassemble
         if not cleaned_llm_payload:
             processed_data = unprocessed_payload
@@ -2601,112 +2680,123 @@
             final_result = self._reinsert_excluded_fields(processed_data, extracted_fields)
             if "enrichment_metadata" in enriched_data:
                 final_result["enrichment_metadata"] = enriched_data["enrichment_metadata"]
-            
+
             # Normalize awards_events_press to ensure all items are strings (not objects)
             self._normalize_string_array_field(final_result, "activity_sentiment_signals.awards_events_press")
-            
+
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: LLM Consolidation - Skipped (No Conflicts)", "No conflicted/long free-text fields detected; returned deterministically merged data.")
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 6: LLM Consolidation - Skipped (No Conflicts)",
+                    "No conflicted/long free-text fields detected; returned deterministically merged data.",
+                )
             return final_result
 
         # Call Gemini with retry logic and LangSmith tracing
         max_retries = 2
         response = None
-        
+
         for attempt in range(max_retries + 1):
             try:
                 print(f"ü§ñ Calling Gemini API (attempt {attempt + 1}/{max_retries + 1})...")
-                
+
                 # Check if model is properly initialized
                 if not self.model:
                     raise Exception("Gemini model not initialized")
-                
+
                 # API monitoring: Track Gemini API call
                 api_call_start_time = time.time()
-                
-                with trace_operation("llm_consolidation", {
-                    "company_name": company_name or "Unknown",
-                    "model": "gemini-2.0-flash",
-                    "fields_count": len(data_for_llm),
-                    "attempt": attempt + 1,
-                    "user_id": user_id,
-                    "run_id": run_id,
-                    "provider": "google"
-                }):
+
+                with trace_operation(
+                    "llm_consolidation",
+                    {
+                        "company_name": company_name or "Unknown",
+                        "model": "gemini-2.0-flash",
+                        "fields_count": len(data_for_llm),
+                        "attempt": attempt + 1,
+                        "user_id": user_id,
+                        "run_id": run_id,
+                        "provider": "google",
+                    },
+                ):
                     # Use LangChain async call with tuple format
-                    response = await self.model.ainvoke([
-                        ("system", system_prompt),
-                        ("user", user_prompt)
-                    ])
-                    
+                    response = await self.model.ainvoke([("system", system_prompt), ("user", user_prompt)])
+
                     # API monitoring: Track successful Gemini API call
                     api_response_time = time.time() - api_call_start_time
-                    self.logger.info("Gemini API call successful", extra={
-                        "model": "gemini-2.0-flash",
-                        "response_time": api_response_time,
-                        "api_call_type": "gemini_llm_consolidation",
-                        "attempt": attempt + 1
-                    })
-                    
+                    self.logger.info(
+                        "Gemini API call successful",
+                        extra={
+                            "model": "gemini-2.0-flash",
+                            "response_time": api_response_time,
+                            "api_call_type": "gemini_llm_consolidation",
+                            "attempt": attempt + 1,
+                        },
+                    )
+
                     print(f"ü§ñ API call completed, checking response...")
-                
+
                 if not response:
                     raise Exception("No response object from Gemini API")
-                
-                if not hasattr(response, 'content') or not response.content:
+
+                if not hasattr(response, "content") or not response.content:
                     raise Exception(f"Empty or invalid response from Gemini API. Response type: {type(response)}")
-                
+
                 # If we get here, the API call was successful
                 break
-                
+
             except Exception as api_error:
-                api_response_time = time.time() - api_call_start_time if 'api_call_start_time' in locals() else 0
-                
+                api_response_time = time.time() - api_call_start_time if "api_call_start_time" in locals() else 0
+
                 # API monitoring: Track failed Gemini API call
-                self.logger.error("Gemini API call failed", extra={
-                    "model": "gemini-2.0-flash",
-                    "error": str(api_error),
-                    "response_time": api_response_time,
-                    "api_call_type": "gemini_llm_consolidation",
-                    "attempt": attempt + 1
-                })
-                
+                self.logger.error(
+                    "Gemini API call failed",
+                    extra={
+                        "model": "gemini-2.0-flash",
+                        "error": str(api_error),
+                        "response_time": api_response_time,
+                        "api_call_type": "gemini_llm_consolidation",
+                        "attempt": attempt + 1,
+                    },
+                )
+
                 print(f"‚ùå API attempt {attempt + 1} failed: {api_error}")
                 if attempt == max_retries:
                     raise api_error
                 print(f"üîÑ Retrying in 2 seconds...")
                 import asyncio
+
                 await asyncio.sleep(2)
-        
+
         try:
             llm_output = response.content.strip()
             print(f"ü§ñ Received response from Gemini ({len(llm_output)} characters)")
-            
+
             # Check if response is empty or contains non-JSON content
             if not llm_output:
                 raise Exception("Empty response text from Gemini API")
-            
+
             # Try to parse JSON with retry logic
             max_retries = 3
             parsed_subset = None
-            
+
             for attempt in range(1, max_retries + 1):
                 # Find the first complete JSON object
                 brace_count = 0
                 json_start = -1
                 json_end = -1
-                
+
                 for i, char in enumerate(llm_output):
-                    if char == '{':
+                    if char == "{":
                         if json_start == -1:
                             json_start = i
                         brace_count += 1
-                    elif char == '}':
+                    elif char == "}":
                         brace_count -= 1
                         if brace_count == 0 and json_start != -1:
                             json_end = i + 1
                             break
-                
+
                 if json_start != -1 and json_end != -1:
                     json_content = llm_output[json_start:json_end]
                     try:
@@ -2725,7 +2815,7 @@
                         print(f"‚ùå No valid JSON found after {max_retries} attempts, using deterministic data only")
                         parsed_subset = {}
                     continue
-            
+
             # Overlay processed subset onto unprocessed payload and reattach citations
             processed_data = self._overlay_dicts(unprocessed_payload, parsed_subset or {})
             processed_data = self._reattach_citations(processed_data, citations_by_path)
@@ -2736,35 +2826,39 @@
             # Reorder fields to match SOURCE_MAPPINGS for stable output
             final_result = self._reorder_by_source_mappings(final_result)
             print(f"ü§ñ Reinserted {len(extracted_fields)} excluded fields after LLM processing")
-            
 
             # Add back the metadata (already handled by reordering if present)
             if "enrichment_metadata" in enriched_data and "enrichment_metadata" not in final_result:
                 final_result["enrichment_metadata"] = enriched_data["enrichment_metadata"]
-            
+
             # Normalize awards_events_press to ensure all items are strings (not objects)
             self._normalize_string_array_field(final_result, "activity_sentiment_signals.awards_events_press")
-            
+
             # Log the final consolidated result
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: LLM Consolidation - Final Result",
-                                         f"```json\n{json.dumps(final_result, indent=2, ensure_ascii=False)}\n```")
-            
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 6: LLM Consolidation - Final Result",
+                    f"```json\n{json.dumps(final_result, indent=2, ensure_ascii=False)}\n```",
+                )
+
             return final_result
-            
+
         except Exception as e:
             print(f"‚ùå Gemini API error: {e}")
             raise Exception(f"Gemini API error: {e}")
 
-    async def _generate_narrative(self, company_data: Dict[str, Any], user_id: str, shared_output_file: str = None, db: ProspectingDB = None, run_id: str = None) -> Dict[str, Any]:
+    async def _generate_narrative(
+        self, company_data: Dict[str, Any], user_id: str, shared_output_file: str = None, db: ProspectingDB = None, run_id: str = None
+    ) -> Dict[str, Any]:
         """
         Generate a narrative for the enriched company data using GPT-4.1-mini.
-        
+
         Args:
             company_data: Enriched company data
             user_id: User identifier for getting user bio
             shared_output_file: Optional shared markdown file path
-            
+
         Returns:
             Dictionary with narrative data and metadata
         """
@@ -2772,17 +2866,18 @@
             # Use provided database or fall back to self.db
             if db is None:
                 db = self.db
-            
+
             # Get user bio from database
             if not db:
                 from app.utils.global_db import get_global_db
+
                 db = get_global_db()
-            
+
             user_bio = await db.get_user_profile(user_id)
-            
+
             if not user_bio:
                 raise Exception(f"No user bio found for user_id: {user_id}")
-            
+
             # Extract investor_profile JSONB fields
             investor_profile_raw = user_bio.get("investor_profile")
             if investor_profile_raw is None:
@@ -2792,29 +2887,30 @@
             elif isinstance(investor_profile_raw, str):
                 try:
                     import json
+
                     investor_profile = json.loads(investor_profile_raw)
                 except (json.JSONDecodeError, TypeError):
                     investor_profile = {}
             else:
                 investor_profile = {}
-            
+
             # Extract fund details
             fund_type = investor_profile.get("fund_type") or ""
             fundraising_status = investor_profile.get("fundraising_status") or investor_profile.get("current_fundraising_status") or ""
-            
+
             # Extract LP search criteria and format lists
             user_geographic_focus = investor_profile.get("geographic_focus") or ""
             if isinstance(user_geographic_focus, list):
                 user_geographic_focus = ", ".join(str(v) for v in user_geographic_focus if v)
             else:
                 user_geographic_focus = str(user_geographic_focus) if user_geographic_focus else ""
-            
+
             user_sector_focus = investor_profile.get("sector_focus") or ""
             if isinstance(user_sector_focus, list):
                 user_sector_focus = ", ".join(str(v) for v in user_sector_focus if v)
             else:
                 user_sector_focus = str(user_sector_focus) if user_sector_focus else ""
-            
+
             user_lp_target_types = investor_profile.get("lp_target_types") or ""
             if isinstance(user_lp_target_types, list):
                 if len(user_lp_target_types) == 1:
@@ -2825,29 +2921,31 @@
                     user_lp_target_types = f"{', '.join(user_lp_target_types[:-1])}, and {user_lp_target_types[-1]}"
             else:
                 user_lp_target_types = str(user_lp_target_types) if user_lp_target_types else ""
-            
+
             # Filter and format company data for narrative generation
             filtered_company_data = self._filter_company_data_for_narrative(company_data)
             formatted_company_data = self._format_company_data_for_narrative(filtered_company_data)
-            
+
             # Retrieve search context if this is a company selection from a general search
             search_context = ""
             if run_id and "_sel_" in run_id:
                 # Get search parameters from the current company selection run
                 # The original prompt is stored in search_params.original_prompt for company selections
                 search_params = await db.get_search_parameters(run_id)
-                
-                if search_params and search_params.get('original_prompt'):
-                    original_prompt = search_params.get('original_prompt')
+
+                if search_params and search_params.get("original_prompt"):
+                    original_prompt = search_params.get("original_prompt")
                     search_context = f"SEARCH CONTEXT:\n"
-                    search_context += f"Original Search Query: \"{original_prompt}\"\n"
-                    
+                    search_context += f'Original Search Query: "{original_prompt}"\n'
+
                     search_context += f"\nThis firm was selected from the search results above. "
-                    search_context += f"Analyze how this firm relates to the original search criteria and why it was relevant to the user's search.\n\n"
-            
+                    search_context += (
+                        f"Analyze how this firm relates to the original search criteria and why it was relevant to the user's search.\n\n"
+                    )
+
             # Prepare prompts
             system_prompt = get_narrative_generation_system_prompt()
-            
+
             # Debug logging for narrative prompt fields
             print(f"üìù Narrative prompt fields:")
             print(f"   fund_type: {fund_type}")
@@ -2855,119 +2953,121 @@
             print(f"   user_geographic_focus: {user_geographic_focus}")
             print(f"   user_sector_focus: {user_sector_focus}")
             print(f"   user_lp_target_types: {user_lp_target_types}")
-            
+
             user_prompt = get_narrative_generation_user_prompt(
                 narrative_instructions=system_prompt,
-                firm_description=user_bio.get('firm_description', ''),
-                key_differentiators=user_bio.get('key_differentiators', ''),
-                key_objectives=user_bio.get('key_objectives', ''),
+                firm_description=user_bio.get("firm_description", ""),
+                key_differentiators=user_bio.get("key_differentiators", ""),
+                key_objectives=user_bio.get("key_objectives", ""),
                 fund_type=fund_type,
                 fundraising_status=fundraising_status,
                 user_geographic_focus=user_geographic_focus,
                 user_sector_focus=user_sector_focus,
                 user_lp_target_types=user_lp_target_types,
                 company_data=formatted_company_data,
-                search_context=search_context
+                search_context=search_context,
             )
-            
+
             # Generate narrative using OpenAI with LangSmith tracing
             # API monitoring: Track OpenAI API call
             api_call_start_time = time.time()
-            
+
             try:
-                with trace_operation("narrative_generation", {
-                    "company_name": company_data.get('firm_name', 'Unknown'),
-                    "model": "gpt-4.1-mini",
-                    "user_id": user_id,
-                    "run_id": run_id,
-                    "provider": "openai"
-                }):
-                    chat = ChatOpenAI(
-                        model="gpt-4.1-mini",
-                        max_tokens=3000,
-                        temperature=0.7,
-                        api_key=get_openai_api_key()
+                with trace_operation(
+                    "narrative_generation",
+                    {
+                        "company_name": company_data.get("firm_name", "Unknown"),
+                        "model": "gpt-4.1-mini",
+                        "user_id": user_id,
+                        "run_id": run_id,
+                        "provider": "openai",
+                    },
+                ):
+                    chat = ChatOpenAI(model="gpt-4.1-mini", max_tokens=3000, temperature=0.7, api_key=get_openai_api_key())
+
+                    response = await chat.ainvoke(
+                        [
+                            ("system", "You are a professional fundraising narrative writer. Always respond with valid JSON only."),
+                            ("user", user_prompt),
+                        ]
                     )
-                    
-                    response = await chat.ainvoke([
-                        ("system", "You are a professional fundraising narrative writer. Always respond with valid JSON only."),
-                        ("user", user_prompt)
-                    ])
-                    
+
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track successful OpenAI API call
-                    self.logger.info("OpenAI API call successful", extra={
-                        "model": "gpt-4o-mini",
-                        "response_time": api_response_time,
-                        "company_name": company_data.get('firm_name', 'Unknown'),
-                        "run_id": run_id,
-                        "user_id": user_id,
-                        "analysis_type": "narrative_generation"
-                    })
-                    
+                    self.logger.info(
+                        "OpenAI API call successful",
+                        extra={
+                            "model": "gpt-4o-mini",
+                            "response_time": api_response_time,
+                            "company_name": company_data.get("firm_name", "Unknown"),
+                            "run_id": run_id,
+                            "user_id": user_id,
+                            "analysis_type": "narrative_generation",
+                        },
+                    )
+
                     narrative_response = response.content
-                    
+
             except Exception as api_error:
                 api_response_time = time.time() - api_call_start_time
-                
+
                 # API monitoring: Track failed OpenAI API call
-                self.logger.error("OpenAI API call failed", extra={
-                    "model": "gpt-4o-mini",
-                    "response_time": api_response_time,
-                    "company_name": company_data.get('firm_name', 'Unknown'),
-                    "run_id": run_id,
-                    "user_id": user_id,
-                    "analysis_type": "narrative_generation",
-                    "error": str(api_error)
-                })
-                
+                self.logger.error(
+                    "OpenAI API call failed",
+                    extra={
+                        "model": "gpt-4o-mini",
+                        "response_time": api_response_time,
+                        "company_name": company_data.get("firm_name", "Unknown"),
+                        "run_id": run_id,
+                        "user_id": user_id,
+                        "analysis_type": "narrative_generation",
+                        "error": str(api_error),
+                    },
+                )
+
                 print(f"‚ùå OpenAI API call failed for narrative generation: {api_error}")
                 raise Exception(f"Narrative generation API error: {api_error}")
-            
+
             # Parse JSON response
             try:
                 # Clean the response to extract JSON
                 narrative_response = narrative_response.strip()
-                if narrative_response.startswith('```json'):
+                if narrative_response.startswith("```json"):
                     narrative_response = narrative_response[7:]
-                if narrative_response.endswith('```'):
+                if narrative_response.endswith("```"):
                     narrative_response = narrative_response[:-3]
                 narrative_response = narrative_response.strip()
-                
+
                 narrative_data = json.loads(narrative_response)
-                
+
                 # Validate required fields
-                required_fields = ['introduction', 'investment_focus', 'recent_news', 'synergies', 'suggested_approach', 'key_talking_points']
+                required_fields = ["introduction", "investment_focus", "recent_news", "synergies", "suggested_approach", "key_talking_points"]
                 for field in required_fields:
                     if field not in narrative_data:
                         raise ValueError(f"Missing required field: {field}")
-                
-                if not isinstance(narrative_data['key_talking_points'], list):
+
+                if not isinstance(narrative_data["key_talking_points"], list):
                     raise ValueError("key_talking_points must be a list")
-                
+
                 print(f"‚úÖ Successfully parsed narrative JSON response with {len(narrative_data)} sections")
-                
+
             except (json.JSONDecodeError, ValueError) as e:
                 print(f"‚ùå Error parsing narrative JSON response: {e}")
                 print(f"Raw response: {narrative_response[:200]}...")
                 # Fallback to error narrative
                 narrative_data = {
                     "introduction": "Error parsing narrative response",
-                    "investment_focus": "Error parsing narrative response", 
+                    "investment_focus": "Error parsing narrative response",
                     "recent_news": "Error parsing narrative response",
                     "synergies": "Error parsing narrative response",
                     "suggested_approach": "Error parsing narrative response",
-                    "key_talking_points": ["Error parsing narrative response"]
+                    "key_talking_points": ["Error parsing narrative response"],
                 }
-            
+
             # Prepare result (simplified without cost/token calculations)
-            result = {
-                'narrative_data': narrative_data,
-                'model': 'gpt-4o-mini',
-                'timestamp': datetime.now().isoformat()
-            }
-            
+            result = {"narrative_data": narrative_data, "model": "gpt-4o-mini", "timestamp": datetime.now().isoformat()}
+
             # Log narrative generation details (simplified)
             if shared_output_file:
                 await self.append_markdown(
@@ -2975,10 +3075,11 @@
                     "Step 6: Narrative Generation - Details",
                     f"**Model:** GPT-4o-mini\n"
                     f"**Narrative Sections:** {', '.join(narrative_data.keys())}\n"
-                    f"**Talking Points:** {len(narrative_data.get('key_talking_points', []))}")
-            
+                    f"**Talking Points:** {len(narrative_data.get('key_talking_points', []))}",
+                )
+
             return result
-            
+
         except Exception as e:
             print(f"‚ùå Narrative generation error: {e}")
             raise Exception(f"Narrative generation error: {e}")
@@ -2987,31 +3088,31 @@
         """Filter company data to only include fields needed for narrative generation."""
         # Fields to include based on the narrative generation requirements
         field_mappings = {
-            'firm_name': 'core_profile.firm_name',
-            'headquarters_location': 'core_profile.headquarters_location',
-            'primary_investment_strategy': 'investment_strategy_mandate.primary_investment_strategy',
-            'primary_investment_asset_class': 'investment_strategy_mandate.primary_investment_asset_class',
-            'public_fund_strategy_commentary': 'investment_strategy_mandate.public_fund_strategy_commentary',
-            'distribution_coverage_relevance': 'distribution_coverage_relevance',
-            'social_media_activity_summarised': 'social_media_presence.social_media_activity_summarised',
-            'sector_focus': 'investment_strategy_mandate.sector_focus',
-            'geographic_focus': 'investment_strategy_mandate.geographic_focus',
-            'aum': 'assets_under_management.aum',
-            'current_funds_active': 'fund_information.current_funds_active',
-            'year_founded': 'core_profile.year_founded',
-            'employee_count': 'core_profile.employee_count',
-            'key_decision_makers': 'organization_decision_making.key_decision_makers',
-            'linkedin_url': 'social_media_presence.linkedin_url',
-            'typical_check_size': 'investment_strategy_mandate.typical_check_size',
-            'awards_events_press': 'activity_sentiment_signals.awards_events_press',
-            'recent_fund_launches': 'fund_information.recent_fund_launches',
-            'recent_fund_closures': 'fund_information.recent_fund_closures',
-            'recent_fund_activities': 'fund_information.recent_fund_activities',
-            'recent_news': 'activity_sentiment_signals.recent_news',
-            'esg_dei_strategy': 'assets_under_management.esg_dei_strategy',
-            'growth_decline_trajectory': 'assets_under_management.growth_decline_trajectory'
+            "firm_name": "core_profile.firm_name",
+            "headquarters_location": "core_profile.headquarters_location",
+            "primary_investment_strategy": "investment_strategy_mandate.primary_investment_strategy",
+            "primary_investment_asset_class": "investment_strategy_mandate.primary_investment_asset_class",
+            "public_fund_strategy_commentary": "investment_strategy_mandate.public_fund_strategy_commentary",
+            "distribution_coverage_relevance": "distribution_coverage_relevance",
+            "social_media_activity_summarised": "social_media_presence.social_media_activity_summarised",
+            "sector_focus": "investment_strategy_mandate.sector_focus",
+            "geographic_focus": "investment_strategy_mandate.geographic_focus",
+            "aum": "assets_under_management.aum",
+            "current_funds_active": "fund_information.current_funds_active",
+            "year_founded": "core_profile.year_founded",
+            "employee_count": "core_profile.employee_count",
+            "key_decision_makers": "organization_decision_making.key_decision_makers",
+            "linkedin_url": "social_media_presence.linkedin_url",
+            "typical_check_size": "investment_strategy_mandate.typical_check_size",
+            "awards_events_press": "activity_sentiment_signals.awards_events_press",
+            "recent_fund_launches": "fund_information.recent_fund_launches",
+            "recent_fund_closures": "fund_information.recent_fund_closures",
+            "recent_fund_activities": "fund_information.recent_fund_activities",
+            "recent_news": "activity_sentiment_signals.recent_news",
+            "esg_dei_strategy": "assets_under_management.esg_dei_strategy",
+            "growth_decline_trajectory": "assets_under_management.growth_decline_trajectory",
         }
-        
+
         filtered_data = {}
         for target_field, source_path in field_mappings.items():
             value = self._get_nested_value(company_data, source_path)
@@ -3019,16 +3120,16 @@
                 clean_value = self._extract_clean_value_for_narrative(value, field=target_field)
                 if clean_value and clean_value != "" and clean_value != "None" and clean_value != "null":
                     filtered_data[target_field] = clean_value
-        
+
         return filtered_data
 
     def _format_company_data_for_narrative(self, company_data: Dict[str, Any]) -> str:
         """Format company data in a token-efficient way for narrative generation."""
         formatted_lines = []
-        
+
         for field, value in company_data.items():
             formatted_lines.append(f"{field}: {value}")
-        
+
         return "\n".join(formatted_lines)
 
     def _extract_clean_value_for_narrative(self, value: Any, field: str = None) -> Any:
@@ -3036,98 +3137,90 @@
         # For dicts with 'value', use only the value
         if isinstance(value, dict):
             # Special handling for key_decision_makers (list of dicts)
-            if field == 'key_decision_makers' and 'value' in value and isinstance(value['value'], list):
-                people = value['value']
-                return ', '.join(
-                    f"{p.get('member_full_name', '')} ({p.get('member_position_title', '')})"
-                    for p in people if isinstance(p, dict)
-                )
+            if field == "key_decision_makers" and "value" in value and isinstance(value["value"], list):
+                people = value["value"]
+                return ", ".join(f"{p.get('member_full_name', '')} ({p.get('member_position_title', '')})" for p in people if isinstance(p, dict))
             # Special handling for distribution_coverage_relevance (nested dict)
-            if field == 'distribution_coverage_relevance':
+            if field == "distribution_coverage_relevance":
                 # Join all string values found under 'value' keys in subfields
                 parts = []
                 for subfield in value.values():
-                    if isinstance(subfield, dict) and 'value' in subfield:
-                        v = subfield['value']
+                    if isinstance(subfield, dict) and "value" in subfield:
+                        v = subfield["value"]
                         if isinstance(v, list):
                             parts.extend([str(x) for x in v if x])
                         elif v:
                             parts.append(str(v))
-                return ', '.join(parts)
+                return ", ".join(parts)
             # General case: just use the 'value' key if present
-            if 'value' in value:
-                value = value['value']
-            elif 'text' in value:
-                value = value['text']
+            if "value" in value:
+                value = value["value"]
+            elif "text" in value:
+                value = value["text"]
             else:
                 # Fallback: join all string values in the dict
-                value = ', '.join(str(v) for v in value.values() if isinstance(v, str))
+                value = ", ".join(str(v) for v in value.values() if isinstance(v, str))
         # For lists, join as comma-separated
         if isinstance(value, list):
             if len(value) == 1:
                 value = value[0]
             else:
-                value = ', '.join(str(v) for v in value if v)
+                value = ", ".join(str(v) for v in value if v)
         # Remove citation patterns and extra whitespace
         if isinstance(value, str):
             import re
-            value = re.sub(r'\[\d+\]', '', value)
-            value = re.sub(r'Source:.*?(?=\n|$)', '', value, flags=re.IGNORECASE)
-            value = re.sub(r'\s+', ' ', value).strip()
+
+            value = re.sub(r"\[\d+\]", "", value)
+            value = re.sub(r"Source:.*?(?=\n|$)", "", value, flags=re.IGNORECASE)
+            value = re.sub(r"\s+", " ", value).strip()
         return value
 
     def _clean_executive_list(self, executives: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
         """
         Remove parent_id from executive records while preserving other fields.
-        
+
         Args:
             executives: List of executive records (arrivals or departures)
-            
+
         Returns:
             List of executive records with parent_id removed
         """
         if not executives or not isinstance(executives, list):
             return []
-            
+
         cleaned_executives = []
         for exec_record in executives:
             if isinstance(exec_record, dict):
                 # Create new dict excluding parent_id
-                cleaned_record = {k: v for k, v in exec_record.items() if k != 'parent_id'}
+                cleaned_record = {k: v for k, v in exec_record.items() if k != "parent_id"}
                 cleaned_executives.append(cleaned_record)
-                
+
         return cleaned_executives
 
     def _get_coresignal_value(self, data: Dict[str, Any], field_path: str) -> Any:
         """Get value from CoreSignal data using dot notation path."""
         try:
             # Split path into parts
-            parts = field_path.split('.')
-            
+            parts = field_path.split(".")
+
             # Handle special case for recent_leadership_changes
-            if parts[-1] == 'recent_leadership_changes':
-                arrivals = self._get_nested_value(data, 'key_executive_arrivals', [])
-                departures = self._get_nested_value(data, 'key_executive_departures', [])
-                
+            if parts[-1] == "recent_leadership_changes":
+                arrivals = self._get_nested_value(data, "key_executive_arrivals", [])
+                departures = self._get_nested_value(data, "key_executive_departures", [])
+
                 # Clean executive lists by removing parent_id
                 cleaned_arrivals = self._clean_executive_list(arrivals)
                 cleaned_departures = self._clean_executive_list(departures)
-                
+
                 return {
-                    'value': {
-                        'combined_fields': [
-                            {
-                                'field': 'key_executive_arrivals',
-                                'value': cleaned_arrivals
-                            },
-                            {
-                                'field': 'key_executive_departures',
-                                'value': cleaned_departures
-                            }
+                    "value": {
+                        "combined_fields": [
+                            {"field": "key_executive_arrivals", "value": cleaned_arrivals},
+                            {"field": "key_executive_departures", "value": cleaned_departures},
                         ]
                     }
                 }
-                
+
             # Handle other fields normally
             current = data
             for part in parts:

--- app/agents/sub_agents/company_search_agent.py
+++ app/agents/sub_agents/company_search_agent.py
@@ -29,7 +29,7 @@
     def __init__(self, output_dir: str = "app/data", db: Optional[ProspectingDB] = None):
         # Initialize LangSmith
         initialize_langsmith()
-        
+
         super().__init__(output_dir, db)
         logger.info("CompanySearchAgent initialized", extra={"output_dir": output_dir})
 
@@ -37,10 +37,19 @@
     def agent_name(self) -> str:
         return "Company Search Agent"
 
-    async def execute(self, company_search_params: Dict[str, Any], run_id: str, user_id: str, session_id: str, shared_output_file: str = None, db: ProspectingDB = None, postgres_enabled: bool = None) -> Dict[str, Any]:
+    async def execute(
+        self,
+        company_search_params: Dict[str, Any],
+        run_id: str,
+        user_id: str,
+        session_id: str,
+        shared_output_file: str = None,
+        db: ProspectingDB = None,
+        postgres_enabled: bool = None,
+    ) -> Dict[str, Any]:
         """
         Execute Company Search Agent using Vector Search (Qdrant). Perplexity is only used to resolve missing domains.
-        
+
         Args:
             company_search_params: Dictionary containing search parameters:
                 - investor_type: VC, Pension Funds, endowments, insurance companies, family offices, sovereign wealth funds etc.
@@ -54,45 +63,48 @@
             db: Optional database connection (will be created if None)
             postgres_enabled: Whether to store results in PostgreSQL (auto-detected if None)
             session_id: Session identifier
-        
+
         Returns:
             Dictionary with search results containing 10 companies
         """
         start_time = time.time()
-        
-        logger.info("Company search execution started", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id, "search_params": company_search_params})
+
+        logger.info(
+            "Company search execution started",
+            extra={"run_id": run_id, "user_id": user_id, "session_id": session_id, "search_params": company_search_params},
+        )
         # Progress: search phase started
         try:
             if db and run_id:
                 await ProgressStore.instance().set_progress(db, run_id, 20)
         except Exception:
             pass
-        
+
         # Validate required parameters
         if not run_id:
             logger.error("Company search failed: missing run_id", extra={"user_id": user_id, "session_id": session_id})
             return {
                 "success": False,
                 "error": "run_id is required for CompanySearchAgent execution",
-                "execution_time_ms": int((time.time() - start_time) * 1000)
+                "execution_time_ms": int((time.time() - start_time) * 1000),
             }
-        
+
         if not user_id:
             logger.error("Company search failed: missing user_id", extra={"run_id": run_id, "session_id": session_id})
             return {
                 "success": False,
                 "error": "user_id is required for CompanySearchAgent execution",
-                "execution_time_ms": int((time.time() - start_time) * 1000)
+                "execution_time_ms": int((time.time() - start_time) * 1000),
             }
-        
+
         # Use provided database or fall back to self.db
         if db is None:
             db = self.db
-        
+
         # Initialize PostgreSQL connection if not provided
         if postgres_enabled is None:
             postgres_enabled = get_enable_postgres_storage()
-            
+
         if not db and postgres_enabled:
             try:
                 db = await get_global_db()
@@ -103,32 +115,35 @@
                 logger.warning("PostgreSQL disabled due to connection error", extra={"run_id": run_id, "error": str(e)})
                 postgres_enabled = False
                 db = None
-        
+
         try:
             # Extract search parameters
-            investor_type = company_search_params.get('investor_type', '')
-            investor_focus = company_search_params.get('investor_focus', '')
-            investment_stage = company_search_params.get('investment_stage', '')
-            location = company_search_params.get('location', '')
-            additional_company_info = company_search_params.get('additional_company_info', '')
+            investor_type = company_search_params.get("investor_type", "")
+            investor_focus = company_search_params.get("investor_focus", "")
+            investment_stage = company_search_params.get("investment_stage", "")
+            location = company_search_params.get("location", "")
+            additional_company_info = company_search_params.get("additional_company_info", "")
             # Normalize for safe logging/query building while preserving original for metadata storage
             normalized_additional_info = self._normalize_additional_info(additional_company_info)
-            
+
             print(f"üîç CompanySearchAgent: Searching for {investor_type} firms in {location}")
             print(f"   Focus: {investor_focus}, Stage: {investment_stage}")
             if normalized_additional_info:
                 print(f"   Additional Criteria: {normalized_additional_info}")
             print(f"   Run ID: {run_id}, User ID: {user_id}")
-            
-            logger.info("Search parameters extracted", extra={
-                "run_id": run_id, 
-                "investor_type": investor_type, 
-                "investor_focus": investor_focus, 
-                "investment_stage": investment_stage, 
-                "location": location,
-                "has_additional_info": bool(normalized_additional_info)
-            })
-            
+
+            logger.info(
+                "Search parameters extracted",
+                extra={
+                    "run_id": run_id,
+                    "investor_type": investor_type,
+                    "investor_focus": investor_focus,
+                    "investment_stage": investment_stage,
+                    "location": location,
+                    "has_additional_info": bool(normalized_additional_info),
+                },
+            )
+
             if shared_output_file:
                 search_description = f"Searching for {investor_type} firms in {location} with focus on {investor_focus} at {investment_stage} stage"
                 if normalized_additional_info:
@@ -137,9 +152,9 @@
 
             # Build search query (use normalized info)
             search_query = self._build_search_query(investor_type, investor_focus, investment_stage, location, normalized_additional_info)
-            
+
             logger.info("Search query built", extra={"run_id": run_id, "query_length": len(search_query)})
-            
+
             # Execute Vector Search (top 50)
             search_result = await self._vector_company_search(search_query, limit=50, user_id=user_id, db=db)
             # Progress: initial search completed
@@ -148,21 +163,21 @@
                     await ProgressStore.instance().set_progress(db, run_id, 40)
             except Exception:
                 pass
-            
+
             if search_result["success"]:
-                logger.info("Vector search successful", extra={"run_id": run_id, "results": len(search_result.get('companies', []))})
-                
-                companies = search_result.get('companies', [])
-                qdrant_filters = search_result.get('qdrant_filters', {})  # Extract qdrant_filters from search_result
+                logger.info("Vector search successful", extra={"run_id": run_id, "results": len(search_result.get("companies", []))})
+
+                companies = search_result.get("companies", [])
+                qdrant_filters = search_result.get("qdrant_filters", {})  # Extract qdrant_filters from search_result
                 # Progress: results aggregation started/mid
                 try:
                     if db and run_id:
                         await ProgressStore.instance().set_progress(db, run_id, 60)
                 except Exception:
                     pass
-                
+
                 logger.info("Company results parsed", extra={"run_id": run_id, "companies_found": len(companies)})
-                
+
                 # Create structured output
                 output_data = {
                     "search_metadata": {
@@ -175,11 +190,11 @@
                         "search_timestamp": datetime.now().isoformat(),
                         "run_id": run_id,
                         "user_id": user_id,
-                        "qdrant_filters": qdrant_filters  # Store Qdrant filters for feedback collection
+                        "qdrant_filters": qdrant_filters,  # Store Qdrant filters for feedback collection
                     },
-                    "companies": companies
+                    "companies": companies,
                 }
-                
+
                 # Store in PostgreSQL if enabled
                 if postgres_enabled and db:
                     try:
@@ -190,10 +205,12 @@
                             session_id=session_id,
                             search_query=json.dumps(company_search_params),
                             search_results=output_data,
-                            execution_time_ms=execution_time_ms
+                            execution_time_ms=execution_time_ms,
                         )
                         print(f"‚úÖ Company search results stored in PostgreSQL for user_id: {user_id} (session: {session_id})")
-                        logger.info("Company search results stored in PostgreSQL", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id})
+                        logger.info(
+                            "Company search results stored in PostgreSQL", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id}
+                        )
                         # Progress: results persisted (cap below 100; 100 is reserved for final completion)
                         try:
                             await ProgressStore.instance().set_progress(db, run_id, 90)
@@ -202,14 +219,17 @@
                     except Exception as db_error:
                         print(f"‚ùå Error storing company search results in PostgreSQL: {db_error}")
                         logger.error("Failed to store company search results in PostgreSQL", extra={"run_id": run_id, "error": str(db_error)})
-                
+
                 output_file = None
-                
+
                 execution_time = int((time.time() - start_time) * 1000)
-                
-                logger.info("Company search completed successfully", extra={"run_id": run_id, "companies_found": len(companies), "execution_time_ms": execution_time})
+
+                logger.info(
+                    "Company search completed successfully",
+                    extra={"run_id": run_id, "companies_found": len(companies), "execution_time_ms": execution_time},
+                )
                 # Do NOT set 100% here; reserve 100% for final workflow completion
-                
+
                 return {
                     "success": True,
                     "output_file": output_file,
@@ -217,13 +237,13 @@
                     "execution_time_ms": execution_time,
                     "companies_found": len(companies),
                     "run_id": run_id,
-                    "user_id": user_id
+                    "user_id": user_id,
                 }
             else:
                 error_msg = f"Vector search failed: {search_result.get('error', 'Unknown error')}"
                 print(f"‚ùå {error_msg}")
-                logger.error("Perplexity search failed", extra={"run_id": run_id, "error": search_result.get('error', 'Unknown error')})
-                
+                logger.error("Perplexity search failed", extra={"run_id": run_id, "error": search_result.get("error", "Unknown error")})
+
                 # Store error in PostgreSQL if enabled
                 if postgres_enabled and db:
                     try:
@@ -233,31 +253,33 @@
                             user_id=user_id,
                             session_id=session_id,
                             search_query=json.dumps(company_search_params),
-                            search_results={'error': error_msg},
-                            execution_time_ms=execution_time_ms
+                            search_results={"error": error_msg},
+                            execution_time_ms=execution_time_ms,
                         )
                         print(f"‚úÖ Company search error stored in PostgreSQL for user_id: {user_id} (session: {session_id})")
-                        logger.info("Company search error stored in PostgreSQL", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id})
+                        logger.info(
+                            "Company search error stored in PostgreSQL", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id}
+                        )
                     except Exception as db_error:
                         print(f"‚ùå Error storing company search error in PostgreSQL: {db_error}")
                         logger.error("Failed to store company search error in PostgreSQL", extra={"run_id": run_id, "error": str(db_error)})
-                
+
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Company Search Error", error_msg)
-                
+
                 return {
                     "success": False,
                     "error": error_msg,
                     "execution_time_ms": int((time.time() - start_time) * 1000),
                     "run_id": run_id,
-                    "user_id": user_id
+                    "user_id": user_id,
                 }
-                
+
         except Exception as e:
             error_msg = f"CompanySearchAgent execution failed: {str(e)}"
             print(f"‚ùå {error_msg}")
             logger.exception("CompanySearchAgent execution failed", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id})
-            
+
             # Store error in PostgreSQL if enabled
             if postgres_enabled and db:
                 try:
@@ -267,24 +289,24 @@
                         user_id=user_id,
                         session_id=session_id,
                         search_query=json.dumps(company_search_params),
-                        search_results={'error': error_msg},
-                        execution_time_ms=execution_time_ms
+                        search_results={"error": error_msg},
+                        execution_time_ms=execution_time_ms,
                     )
                     print(f"‚úÖ Company search error stored in PostgreSQL for user_id: {user_id} (session: {session_id})")
                     logger.info("Company search error stored in PostgreSQL", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id})
                 except Exception as db_error:
                     print(f"‚ùå Error storing company search error in PostgreSQL: {db_error}")
                     logger.error("Failed to store company search error in PostgreSQL", extra={"run_id": run_id, "error": str(db_error)})
-            
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Company Search Error", error_msg)
-            
+
             return {
                 "success": False,
                 "error": error_msg,
                 "execution_time_ms": int((time.time() - start_time) * 1000),
                 "run_id": run_id,
-                "user_id": user_id
+                "user_id": user_id,
             }
         finally:
             # Note: Database connection is managed globally, no need to close here
@@ -297,12 +319,12 @@
         """
         # Build the base query
         base_query = f"Find {investor_type} firms in {location} that focus on {investor_focus} and invest in {investment_stage} companies"
-        
+
         # Add additional criteria if provided (safe for non-strings)
         safe_info = additional_company_info if isinstance(additional_company_info, str) else self._normalize_additional_info(additional_company_info)
         if safe_info and isinstance(safe_info, str) and safe_info.strip():
             base_query += f" with the following additional criteria: {safe_info}"
-        
+
         # Return simple query for vector search (NOT Perplexity prompt)
         # The vector search engine's extract_filters_with_llm() expects a simple natural language query
         return base_query
@@ -329,11 +351,13 @@
         except Exception:
             return ""
 
-    async def _vector_company_search(self, query: str, limit: int = 50, user_id: Optional[str] = None, db: Optional[ProspectingDB] = None) -> Dict[str, Any]:
+    async def _vector_company_search(
+        self, query: str, limit: int = 50, user_id: Optional[str] = None, db: Optional[ProspectingDB] = None
+    ) -> Dict[str, Any]:
         """Run hybrid vector search in Qdrant via FundLPMatchingEngine and return structured companies.
 
         Vector search replaces Perplexity list retrieval. Perplexity is used only to resolve missing domains.
-        
+
         Args:
             query: Search query string
             limit: Maximum number of results to return
@@ -346,22 +370,23 @@
             # Try to get user profile (optional, for fallback and embedding storage)
             fund_profile = {"firm_name": "Agent Search", "investor_profile": {}}
             profile_row = None  # Full profile row for embedding storage
-            
+
             # Use provided database or fall back to self.db (following pattern used throughout codebase)
             db_for_profile = db
             if db_for_profile is None:
                 db_for_profile = self.db
-            
+
             # Retrieve database instance if still not available
             if not db_for_profile and user_id:
                 try:
                     from app.utils.global_db import get_global_db
+
                     db_for_profile = await get_global_db()
                     print(f"üîó CompanySearchAgent: Retrieved global database instance for profile retrieval: {id(db_for_profile)}")
                 except Exception as e:
                     print(f"‚ö†Ô∏è Failed to retrieve global database instance: {e}")
                     db_for_profile = None
-            
+
             if user_id and db_for_profile:
                 try:
                     print(f"üîç DEBUG: Attempting to retrieve user profile for user_id: {user_id}")
@@ -376,19 +401,19 @@
                         elif isinstance(investor_profile_raw, str):
                             try:
                                 import json
+
                                 investor_profile = json.loads(investor_profile_raw)
                             except (json.JSONDecodeError, TypeError):
                                 investor_profile = {}
                         else:
                             investor_profile = {}
-                        
-                        fund_profile = {
-                            "firm_name": profile.get("firm_name") or "Agent Search",
-                            "investor_profile": investor_profile
-                        }
+
+                        fund_profile = {"firm_name": profile.get("firm_name") or "Agent Search", "investor_profile": investor_profile}
                         # Store full profile row for embedding storage
                         profile_row = profile
-                        print(f"‚úÖ DEBUG: Retrieved user profile for user_id: {user_id}, profile_row keys: {list(profile.keys()) if isinstance(profile, dict) else 'not a dict'}")
+                        print(
+                            f"‚úÖ DEBUG: Retrieved user profile for user_id: {user_id}, profile_row keys: {list(profile.keys()) if isinstance(profile, dict) else 'not a dict'}"
+                        )
                         logger.debug(f"Retrieved user profile for user_id: {user_id}")
                     else:
                         print(f"‚ö†Ô∏è DEBUG: Profile query returned None/empty for user_id: {user_id}")
@@ -405,9 +430,9 @@
                 limit=limit,
                 use_cached_profile_embedding=True if user_id else False,
                 user_id=user_id,
-                profile_row=profile_row
+                profile_row=profile_row,
             )
-            
+
             # Handle both old format (list) and new format (dict with results and filters)
             if isinstance(search_response, dict):
                 results = search_response.get("results", [])
@@ -460,22 +485,24 @@
                 if isinstance(investor_type, list):
                     investor_type = ", ".join(str(t) for t in investor_type) if investor_type else ""
 
-                companies.append({
-                    "company_name": metadata.get("company_name", ""),
-                    "company_domain": domain,
-                    "investor_type": investor_type,
-                    "investor_focus": metadata.get("preferred_industry", ""),
-                    "location": location,
-                    "hq_city": filters.get("hq_city"),
-                    "hq_country_region": filters.get("hq_country_region"),
-                    "hq_global_region": filters.get("hq_global_region"),
-                    "aum": aum,
-                    "preferred_investment_types": preferred_investment_types,
-                    "other_investor_types": other_investor_types,
-                    "preferred_industry": metadata.get("preferred_industry"),
-                    "preferred_geography": metadata.get("preferred_geography"),
-                    "website": website
-                })
+                companies.append(
+                    {
+                        "company_name": metadata.get("company_name", ""),
+                        "company_domain": domain,
+                        "investor_type": investor_type,
+                        "investor_focus": metadata.get("preferred_industry", ""),
+                        "location": location,
+                        "hq_city": filters.get("hq_city"),
+                        "hq_country_region": filters.get("hq_country_region"),
+                        "hq_global_region": filters.get("hq_global_region"),
+                        "aum": aum,
+                        "preferred_investment_types": preferred_investment_types,
+                        "other_investor_types": other_investor_types,
+                        "preferred_industry": metadata.get("preferred_industry"),
+                        "preferred_geography": metadata.get("preferred_geography"),
+                        "website": website,
+                    }
+                )
 
             return {"success": True, "companies": companies, "qdrant_filters": qdrant_filters}
 
@@ -505,79 +532,59 @@
         try:
             if not get_perplexity_api_key():
                 logger.error("PERPLEXITY_API_KEY not found in environment variables")
-                return {
-                    "success": False,
-                    "error": "PERPLEXITY_API_KEY not found in environment variables"
-                }
-            
-            chat = ChatPerplexity(
-                temperature=0, 
-                model="sonar-pro",
-                api_key=get_perplexity_api_key()
-            )
-            
+                return {"success": False, "error": "PERPLEXITY_API_KEY not found in environment variables"}
+
+            chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=get_perplexity_api_key())
+
             logger.info("Perplexity API search initiated", extra={"model": "sonar-pro", "query_length": len(query)})
-            
+
             # API monitoring: Track Perplexity API call
             api_call_start_time = time.time()
-            
-            with trace_operation("company_search", {
-                "model": "sonar-pro",
-                "search_type": "company_discovery",
-                "query_length": len(query)
-            }):
+
+            with trace_operation("company_search", {"model": "sonar-pro", "search_type": "company_discovery", "query_length": len(query)}):
                 try:
-                    response = await chat.ainvoke(
-                        [
-                            ("user", query)
-                        ],
-                        extra_body={
-                            "web_search_options": {
-                                "search_context_size": "high"
-                            }
-                        }
-                    )
-                    
+                    response = await chat.ainvoke([("user", query)], extra_body={"web_search_options": {"search_context_size": "high"}})
+
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track successful Perplexity API call
-                    logger.info("Perplexity API call successful", extra={
-                        "model": "sonar-pro",
-                        "response_time": api_response_time,
-                        "api_call_type": "perplexity_company_search",
-                        "query_length": len(query),
-                        "content_length": len(response.content)
-                    })
-                    
+                    logger.info(
+                        "Perplexity API call successful",
+                        extra={
+                            "model": "sonar-pro",
+                            "response_time": api_response_time,
+                            "api_call_type": "perplexity_company_search",
+                            "query_length": len(query),
+                            "content_length": len(response.content),
+                        },
+                    )
+
                     content = response.content
-                    
+
                     logger.info("Perplexity API search completed", extra={"content_length": len(content)})
-                    
-                    return {
-                        "success": True,
-                        "content": content
-                    }
-                    
+
+                    return {"success": True, "content": content}
+
                 except Exception as api_error:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track failed Perplexity API call
-                    logger.error("Perplexity API call failed", extra={
-                        "model": "sonar-pro",
-                        "error": str(api_error),
-                        "response_time": api_response_time,
-                        "api_call_type": "perplexity_company_search",
-                        "query_length": len(query)
-                    })
-                    
+                    logger.error(
+                        "Perplexity API call failed",
+                        extra={
+                            "model": "sonar-pro",
+                            "error": str(api_error),
+                            "response_time": api_response_time,
+                            "api_call_type": "perplexity_company_search",
+                            "query_length": len(query),
+                        },
+                    )
+
                     raise api_error
-                        
+
         except Exception as e:
             logger.exception("Perplexity API search failed")
-            return {
-                "success": False,
-                "error": f"Perplexity search failed: {str(e)}"
-            }
+            return {"success": False, "error": f"Perplexity search failed: {str(e)}"}
 
     def _parse_company_results(self, content: str) -> List[Dict[str, Any]]:
         """
@@ -586,22 +593,23 @@
         try:
             # Try to extract JSON from the response
             parsed_content = safe_parse_perplexity_content(content)
-            
+
             if isinstance(parsed_content, list):
                 companies = parsed_content
-            elif isinstance(parsed_content, dict) and 'companies' in parsed_content:
-                companies = parsed_content['companies']
+            elif isinstance(parsed_content, dict) and "companies" in parsed_content:
+                companies = parsed_content["companies"]
             else:
                 # Try to find JSON array in the text
                 import re
-                json_match = re.search(r'\[.*\]', content, re.DOTALL)
+
+                json_match = re.search(r"\[.*\]", content, re.DOTALL)
                 if json_match:
                     companies = json.loads(json_match.group())
                 else:
                     print(f"‚ö†Ô∏è Could not parse company results from content")
                     logger.warning("Could not parse company results from content", extra={"content_length": len(content)})
                     return []
-            
+
             # Validate and clean each company entry
             validated_companies = []
             for company in companies:
@@ -613,18 +621,20 @@
                         "investor_focus": company.get("investor_focus", "").strip(),
                         "investment_stage": company.get("investment_stage", "").strip(),
                         "location": company.get("location", "").strip(),
-                        "additional_company_info": company.get("additional_company_info", "").strip()
+                        "additional_company_info": company.get("additional_company_info", "").strip(),
                     }
-                    
+
                     # Only include companies with at least a name
                     if validated_company["company_name"]:
                         validated_companies.append(validated_company)
-            
+
             print(f"‚úÖ Found {len(validated_companies)} valid companies")
-            logger.info("Company results validation completed", extra={"total_companies": len(companies), "valid_companies": len(validated_companies)})
+            logger.info(
+                "Company results validation completed", extra={"total_companies": len(companies), "valid_companies": len(validated_companies)}
+            )
             return validated_companies[:10]  # Limit to 10 companies
-            
+
         except Exception as e:
             print(f"‚ö†Ô∏è Error parsing company results: {e}")
             logger.exception("Error parsing company results")
-            return [] 
\ No newline at end of file
+            return []

--- app/agents/sub_agents/coresignal_agent.py
+++ app/agents/sub_agents/coresignal_agent.py
@@ -20,14 +20,11 @@
 from app.utils.config import get_coresignal_api_key, get_enable_postgres_storage, get_perplexity_api_key, get_openai_api_key
 from app.utils.progress_store import ProgressStore
 from app.prompts.data_retrieval_prompts import get_coresignal_fallback_extract_prompt
-from app.prompts.data_process_prompts import (
-    get_coresignal_llm_company_selection_system_prompt,
-    get_coresignal_llm_company_selection_user_prompt
-)
+from app.prompts.data_process_prompts import get_coresignal_llm_company_selection_system_prompt, get_coresignal_llm_company_selection_user_prompt
 from app.utils.langsmith_config import trace_operation
 
 # Load environment variables from .env file
-dotenv_path = Path(__file__).parent.parent.parent / '.env'
+dotenv_path = Path(__file__).parent.parent.parent / ".env"
 load_dotenv(dotenv_path)
 
 logger = get_logger(__name__)
@@ -43,16 +40,25 @@
         super().__init__(output_dir, db)
         self.mcp_tools = mcp_tools
         logger.info("CoreSignalSubAgent initialized", extra={"output_dir": output_dir, "mcp_tools_count": len(mcp_tools)})
-
 
     @property
     def agent_name(self) -> str:
         return "Coresignal Agent"
 
-    async def execute(self, company_data: Dict[str, Any], run_id: str, company_id: str, found_url: str, found_urls: List[str], user_id: str, session_id: str, db: Optional[ProspectingDB] = None) -> Dict[str, Any]:
+    async def execute(
+        self,
+        company_data: Dict[str, Any],
+        run_id: str,
+        company_id: str,
+        found_url: str,
+        found_urls: List[str],
+        user_id: str,
+        session_id: str,
+        db: Optional[ProspectingDB] = None,
+    ) -> Dict[str, Any]:
         """
         Execute CoreSignal Sub-Agent to retrieve comprehensive company data.
-        
+
         Args:
             company_data: Dictionary containing company information
             run_id: Optional run ID for tracking
@@ -61,26 +67,29 @@
             found_urls: List of pre-discovered company URLs (optional)
             user_id: User identifier for data isolation
             session_id: Session identifier for data isolation
-        
+
         Returns:
             Dictionary with CoreSignal data and metadata
         """
         start_time = time.time()
-        
-        logger.info("CoreSignal execution started", extra={
-            "run_id": run_id, 
-            "user_id": user_id, 
-            "session_id": session_id, 
-            "company_name": company_data.get('name', ''),
-            "company_id": company_id,
-            "has_found_url": bool(found_url),
-            "has_found_urls": bool(found_urls)
-        })
-        
+
+        logger.info(
+            "CoreSignal execution started",
+            extra={
+                "run_id": run_id,
+                "user_id": user_id,
+                "session_id": session_id,
+                "company_name": company_data.get("name", ""),
+                "company_id": company_id,
+                "has_found_url": bool(found_url),
+                "has_found_urls": bool(found_urls),
+            },
+        )
+
         # Use provided database or fall back to self.db
         if db is None:
             db = self.db
-        
+
         # Initialize PostgreSQL connection if enabled and not provided
         postgres_enabled = get_enable_postgres_storage()
         if not db and postgres_enabled:
@@ -92,119 +101,139 @@
                 print(f"‚ö†Ô∏è PostgreSQL disabled due to connection error: {e}")
                 logger.warning("PostgreSQL disabled due to connection error", extra={"run_id": run_id, "error": str(e)})
                 postgres_enabled = False
-        
+
         try:
             # Progress: coresignal started
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'coresignal_progress', 0.10)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "coresignal_progress", 0.10)
             except Exception:
                 pass
-            company_name = company_data.get('name', '')
-            location = company_data.get('location', '')  # Keep for backward compatibility
-            hq_city = company_data.get('hq_city')
-            hq_country = company_data.get('hq_country')
-            focus_area = company_data.get('focus_area', '')
-            investor_type = company_data.get('investor_type', '')
-            shared_output_file = company_data.get('output_file')
+            company_name = company_data.get("name", "")
+            location = company_data.get("location", "")  # Keep for backward compatibility
+            hq_city = company_data.get("hq_city")
+            hq_country = company_data.get("hq_country")
+            focus_area = company_data.get("focus_area", "")
+            investor_type = company_data.get("investor_type", "")
+            shared_output_file = company_data.get("output_file")
 
-            logger.info("Company data extracted", extra={
-                "run_id": run_id,
-                "company_name": company_name,
-                "location": location,
-                "hq_city": hq_city,
-                "hq_country": hq_country,
-                "focus_area": focus_area
-            })
+            logger.info(
+                "Company data extracted",
+                extra={
+                    "run_id": run_id,
+                    "company_name": company_name,
+                    "location": location,
+                    "hq_city": hq_city,
+                    "hq_country": hq_country,
+                    "focus_area": focus_area,
+                },
+            )
 
             # Use all discovered URLs, with fallback to primary URL, then to company_data
             all_urls = found_urls if found_urls else ([found_url] if found_url else [])
-            
+
             # Additional fallback to company_data.website_url
             if not all_urls:
-                website_url = company_data.get('website_url', '')
+                website_url = company_data.get("website_url", "")
                 if website_url:
                     all_urls = [website_url]
-            
-            logger.info("URLs prepared for CoreSignal search", extra={
-                "run_id": run_id,
-                "urls_count": len(all_urls),
-                "urls": all_urls
-            })
-            
+
+            logger.info("URLs prepared for CoreSignal search", extra={"run_id": run_id, "urls_count": len(all_urls), "urls": all_urls})
+
             if not all_urls:
                 error_msg = f"No website URL(s) provided for {company_name}. URLs should have been discovered in orchestrator."
                 logger.error("No website URLs provided", extra={"run_id": run_id, "company_name": company_name})
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 5: Coresignal - Error", error_msg)
                 return {
-                    'success': False,
-                    'error': error_msg,
-                    'output_file': shared_output_file,
-                    'agent_name': self.agent_name,
-                    'company_name': company_name
+                    "success": False,
+                    "error": error_msg,
+                    "output_file": shared_output_file,
+                    "agent_name": self.agent_name,
+                    "company_name": company_name,
                 }
 
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 5: Coresignal - Using Pre-discovered URLs", 
-                    f"Searching all discovered domains ({len(all_urls)} URLs):\n" + "\n".join([f"- {url}" for url in all_urls]))
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 5: Coresignal - Using Pre-discovered URLs",
+                    f"Searching all discovered domains ({len(all_urls)} URLs):\n" + "\n".join([f"- {url}" for url in all_urls]),
+                )
 
             # Query Coresignal API with all discovered URLs
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 5: Coresignal - Multi-Domain Search", 
-                    f"Searching Coresignal database for all {len(all_urls)} domain(s)...")
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 5: Coresignal - Multi-Domain Search",
+                    f"Searching Coresignal database for all {len(all_urls)} domain(s)...",
+                )
 
             logger.info("Starting CoreSignal multi-domain search", extra={"run_id": run_id, "domains_count": len(all_urls)})
             coresignal_result = await self._query_coresignal_api_multi_domain(
-                all_urls, company_name, shared_output_file, run_id, company_id, user_id, session_id,
-                hq_city=hq_city, hq_country=hq_country, investor_type=investor_type
+                all_urls,
+                company_name,
+                shared_output_file,
+                run_id,
+                company_id,
+                user_id,
+                session_id,
+                hq_city=hq_city,
+                hq_country=hq_country,
+                investor_type=investor_type,
             )
 
             # Store in PostgreSQL if enabled and successful
-            if postgres_enabled and db and coresignal_result.get('success'):
+            if postgres_enabled and db and coresignal_result.get("success"):
                 try:
                     # Use provided company_id (required)
                     if not company_id:
                         raise ValueError("company_id is required for CoreSignal agent")
-                    
+
                     print(f"‚úÖ Using provided domain-based company_id: {company_id}")
                     logger.info("Using provided company_id", extra={"run_id": run_id, "company_id": company_id})
-                    
+
                     # Extract structured fields only (no raw data storage)
                     try:
-                        raw_data = coresignal_result.get('data', {})
+                        raw_data = coresignal_result.get("data", {})
                         logger.info("Starting CoreSignal field extraction", extra={"run_id": run_id})
                         print(f"üîç DEBUG: Raw data passed to extraction:")
                         print(f"   Type: {type(raw_data)}")
                         print(f"   Has 'key_executives': {'key_executives' in raw_data if isinstance(raw_data, dict) else 'N/A'}")
                         if isinstance(raw_data, dict):
                             print(f"   Available keys (first 20): {list(raw_data.keys())[:20]}")
-                            if 'key_executives' not in raw_data:
+                            if "key_executives" not in raw_data:
                                 print(f"   ‚ö†Ô∏è key_executives missing! Checking if it's nested...")
                                 # Check if it's in a nested structure
-                                if 'data' in raw_data and isinstance(raw_data['data'], dict):
+                                if "data" in raw_data and isinstance(raw_data["data"], dict):
                                     print(f"   Has nested 'data' key with 'key_executives': {'key_executives' in raw_data['data']}")
-                        if isinstance(raw_data, dict) and 'key_executives' in raw_data:
-                            ke_value = raw_data['key_executives']
+                        if isinstance(raw_data, dict) and "key_executives" in raw_data:
+                            ke_value = raw_data["key_executives"]
                             print(f"   key_executives type: {type(ke_value)}")
                             print(f"   key_executives is None: {ke_value is None}")
                             print(f"   key_executives length: {len(ke_value) if isinstance(ke_value, list) else 'N/A'}")
                         extracted_fields = await extract_coresignal_fields_with_filtering(raw_data, company_name)
                         print(f"üîç DEBUG: After extraction:")
-                        print(f"   Has 'key_executives' in extracted_fields: {'key_executives' in extracted_fields if isinstance(extracted_fields, dict) else 'N/A'}")
-                        if isinstance(extracted_fields, dict) and 'key_executives' in extracted_fields:
-                            print(f"   Extracted key_executives length: {len(extracted_fields['key_executives']) if isinstance(extracted_fields['key_executives'], list) else 'N/A'}")
-                        logger.info("CoreSignal field extraction completed", extra={"run_id": run_id, "fields_count": len(extracted_fields) if extracted_fields else 0})
+                        print(
+                            f"   Has 'key_executives' in extracted_fields: {'key_executives' in extracted_fields if isinstance(extracted_fields, dict) else 'N/A'}"
+                        )
+                        if isinstance(extracted_fields, dict) and "key_executives" in extracted_fields:
+                            print(
+                                f"   Extracted key_executives length: {len(extracted_fields['key_executives']) if isinstance(extracted_fields['key_executives'], list) else 'N/A'}"
+                            )
+                        logger.info(
+                            "CoreSignal field extraction completed",
+                            extra={"run_id": run_id, "fields_count": len(extracted_fields) if extracted_fields else 0},
+                        )
                     except Exception as extract_error:
                         print(f"‚ö†Ô∏è CoreSignal field extraction failed: {extract_error}")
                         logger.error("CoreSignal field extraction failed", extra={"run_id": run_id, "error": str(extract_error)})
                         extracted_fields = {}  # Fallback to empty dict
-                    
+
                     # Log extracted fields to shared output file
                     if shared_output_file:
                         try:
                             # Log multi-domain results summary first
-                            multi_domain = coresignal_result.get('multi_domain_results', {})
+                            multi_domain = coresignal_result.get("multi_domain_results", {})
                             if multi_domain:
                                 await self.append_markdown(
                                     shared_output_file,
@@ -213,17 +242,17 @@
                                     f"**Successful domains:** {multi_domain.get('successful_domains', 0)}\n"
                                     f"**Found domains:** {', '.join(multi_domain.get('domains_found', []))}\n"
                                     f"**Primary domain used:** {multi_domain.get('primary_domain', 'Unknown')}\n"
-                                    f"**Total LinkedIn posts:** {coresignal_result.get('chunks_processed', 0)}"
+                                    f"**Total LinkedIn posts:** {coresignal_result.get('chunks_processed', 0)}",
                                 )
-                            
+
                             await self.append_markdown(
                                 shared_output_file,
                                 "Step 5: Coresignal - Extracted Fields",
-                                f"**Structured data extracted from Coresignal:**\n\n```json\n{json.dumps(extracted_fields, indent=2, ensure_ascii=False)}\n```"
+                                f"**Structured data extracted from Coresignal:**\n\n```json\n{json.dumps(extracted_fields, indent=2, ensure_ascii=False)}\n```",
                             )
                         except Exception as markdown_error:
                             print(f"‚ö†Ô∏è Markdown serialization failed: {markdown_error}")
-                    
+
                     # Store agent result using new agent_data structure
                     execution_time_ms = int((time.time() - start_time) * 1000)
                     await db.store_agent_result(
@@ -232,63 +261,64 @@
                         session_id=session_id,
                         agent_name=self.agent_name,
                         result_data={
-                            'coresignal_extracted_fields': extracted_fields,
-                            'multi_domain_results': coresignal_result.get('multi_domain_results', {})
+                            "coresignal_extracted_fields": extracted_fields,
+                            "multi_domain_results": coresignal_result.get("multi_domain_results", {}),
                         },
                         company_id=company_id,
-                        execution_time_ms=execution_time_ms
+                        execution_time_ms=execution_time_ms,
+                    )
+
+                    logger.info(
+                        "CoreSignal data stored in PostgreSQL",
+                        extra={
+                            "run_id": run_id,
+                            "company_id": company_id,
+                            "session_id": session_id,
+                            "fields_count": len(extracted_fields) if extracted_fields else 0,
+                            "domains_found": coresignal_result.get("multi_domain_results", {}).get("successful_domains", 1),
+                        },
                     )
-                    
-                    logger.info("CoreSignal data stored in PostgreSQL", extra={
-                        "run_id": run_id, 
-                        "company_id": company_id, 
-                        "session_id": session_id,
-                        "fields_count": len(extracted_fields) if extracted_fields else 0,
-                        "domains_found": coresignal_result.get('multi_domain_results', {}).get('successful_domains', 1)
-                    })
-                    
+
                     if shared_output_file:
                         try:
                             field_count = len(extracted_fields) if extracted_fields is not None else 0
-                            domains_found = coresignal_result.get('multi_domain_results', {}).get('successful_domains', 1)
+                            domains_found = coresignal_result.get("multi_domain_results", {}).get("successful_domains", 1)
                             await self.append_markdown(
                                 shared_output_file,
                                 "Step 5: Coresignal - PostgreSQL Storage",
                                 f"‚úÖ Stored extracted fields in PostgreSQL - Company ID: {company_id}, "
-                                f"Extracted {field_count} fields from {domains_found} domain(s)"
+                                f"Extracted {field_count} fields from {domains_found} domain(s)",
                             )
                         except Exception as final_error:
                             print(f"‚ö†Ô∏è Final markdown update failed: {final_error}")
-                        
+
                 except Exception as e:
                     print(f"‚ö†Ô∏è PostgreSQL storage failed: {e}")
                     logger.error("Failed to store CoreSignal data in PostgreSQL", extra={"run_id": run_id, "error": str(e)})
                     if shared_output_file:
                         await self.append_markdown(
-                            shared_output_file,
-                            "Step 5: Coresignal - PostgreSQL Error",
-                            f"‚ùå PostgreSQL storage failed: {str(e)}"
+                            shared_output_file, "Step 5: Coresignal - PostgreSQL Error", f"‚ùå PostgreSQL storage failed: {str(e)}"
                         )
 
             return {
-                'success': coresignal_result['success'],
-                'website_url': all_urls[0] if all_urls else None,
-                'all_website_urls': all_urls,
-                'coresignal_data': coresignal_result.get('data'),
-                'company_id': coresignal_result.get('company_id'),
-                'multi_domain_results': coresignal_result.get('multi_domain_results', {}),
-                'error': coresignal_result.get('error'),
-                'output_file': shared_output_file,
-                'agent_name': self.agent_name,
-                'company_name': company_name,
-                'postgres_stored': postgres_enabled and coresignal_result.get('success'),
-                'fallback_source': coresignal_result.get('fallback_source')  # Include fallback source if present
+                "success": coresignal_result["success"],
+                "website_url": all_urls[0] if all_urls else None,
+                "all_website_urls": all_urls,
+                "coresignal_data": coresignal_result.get("data"),
+                "company_id": coresignal_result.get("company_id"),
+                "multi_domain_results": coresignal_result.get("multi_domain_results", {}),
+                "error": coresignal_result.get("error"),
+                "output_file": shared_output_file,
+                "agent_name": self.agent_name,
+                "company_name": company_name,
+                "postgres_stored": postgres_enabled and coresignal_result.get("success"),
+                "fallback_source": coresignal_result.get("fallback_source"),  # Include fallback source if present
             }
 
         except Exception as e:
             print(f"\n[CoreSignalSubAgent ERROR] Exception in execute():\n{traceback.format_exc()}")
             logger.exception("CoreSignalSubAgent execution failed", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id})
-            
+
             # Store error in PostgreSQL if enabled
             if postgres_enabled and db and company_id:
                 try:
@@ -298,37 +328,48 @@
                         user_id=user_id,
                         session_id=session_id,
                         agent_name=self.agent_name,
-                        result_data={'error': str(e)},
+                        result_data={"error": str(e)},
                         company_id=company_id,
-                        execution_time_ms=execution_time_ms
+                        execution_time_ms=execution_time_ms,
                     )
                     logger.info("CoreSignalSubAgent error stored in PostgreSQL", extra={"run_id": run_id, "company_id": company_id})
                 except Exception as db_error:
                     print(f"‚ö†Ô∏è PostgreSQL error storage failed: {db_error}")
                     logger.error("Failed to store CoreSignalSubAgent error in PostgreSQL", extra={"run_id": run_id, "error": str(db_error)})
-            
+
             return {
-                'success': False,
-                'error': str(e),
-                'output_file': company_data.get('output_file'),
-                'agent_name': self.agent_name,
-                'company_name': company_data.get('name', ''),
-                'postgres_stored': False
+                "success": False,
+                "error": str(e),
+                "output_file": company_data.get("output_file"),
+                "agent_name": self.agent_name,
+                "company_name": company_data.get("name", ""),
+                "postgres_stored": False,
             }
         finally:
             # Note: Database connection is managed globally, no need to close here
             # Progress: coresignal completed if success path already persisted, otherwise leave as-is
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'coresignal_progress', 1.00)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "coresignal_progress", 1.00)
             except Exception:
                 pass
-
 
-    async def _query_coresignal_api_multi_domain(self, urls: List[str], company_name: str, shared_output_file: str = None, run_id: str = None, company_id: str = None, user_id: str = None, session_id: str = None, hq_city: str = None, hq_country: str = None, investor_type: str = None) -> Dict[str, Any]:
+    async def _query_coresignal_api_multi_domain(
+        self,
+        urls: List[str],
+        company_name: str,
+        shared_output_file: str = None,
+        run_id: str = None,
+        company_id: str = None,
+        user_id: str = None,
+        session_id: str = None,
+        hq_city: str = None,
+        hq_country: str = None,
+        investor_type: str = None,
+    ) -> Dict[str, Any]:
         """
         Query the Coresignal company multi-source API for multiple domains and merge results.
-        
+
         Args:
             urls: List of company website URLs to search for
             company_name: Company name for logging
@@ -337,7 +378,7 @@
             company_id: Domain-based company ID for LinkedIn posts filtering
             user_id: User identifier for data isolation
             session_id: Session identifier for data isolation
-            
+
         Returns:
             Dict with merged API results or error
         """
@@ -346,7 +387,7 @@
         api_calls_successful = 0
         api_calls_failed = 0
         api_errors = []
-        
+
         try:
             api_key = get_coresignal_api_key()
             if not api_key:
@@ -354,185 +395,160 @@
                 logger.error("CORESIG_API_KEY not found in environment variables", extra={"run_id": run_id})
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 5: Coresignal - API Key Error", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
+                return {"success": False, "error": error_msg}
 
             # API endpoint and headers for search (use preview endpoint for richer data)
             search_url = "https://api.coresignal.com/cdapi/v2/company_multi_source/search/es_dsl/preview"
-            headers = {
-                "apikey": api_key,
-                "Content-Type": "application/json"
-            }
+            headers = {"apikey": api_key, "Content-Type": "application/json"}
 
             logger.info("CoreSignal API configuration prepared", extra={"run_id": run_id, "search_url": search_url})
 
             all_results = []
             total_linkedin_posts = 0
             best_result = None
-            
+
             if shared_output_file:
                 await self.append_markdown(
                     shared_output_file,
                     "Step 5: Coresignal - Multi-Domain Search Strategy",
-                    f"Searching for {company_name} across {len(urls)} domains:\n" + "\n".join([f"- {url}" for url in urls])
+                    f"Searching for {company_name} across {len(urls)} domains:\n" + "\n".join([f"- {url}" for url in urls]),
                 )
 
             # Search each domain
             for i, url in enumerate(urls, 1):
                 try:
                     domain_url = url
-                    if url.startswith('http://'):
+                    if url.startswith("http://"):
                         domain_url = url[7:]
-                    elif url.startswith('https://'):
+                    elif url.startswith("https://"):
                         domain_url = url[8:]
-                    
+
                     # Remove www. prefix if present
-                    if domain_url.startswith('www.'):
+                    if domain_url.startswith("www."):
                         domain_url = domain_url[4:]
-                    
+
                     # Extract only the domain part (before first slash or query parameter)
-                    if '/' in domain_url:
-                        domain_url = domain_url.split('/')[0]
-                    if '?' in domain_url:
-                        domain_url = domain_url.split('?')[0]
-                    
+                    if "/" in domain_url:
+                        domain_url = domain_url.split("/")[0]
+                    if "?" in domain_url:
+                        domain_url = domain_url.split("?")[0]
+
                     logger.info("Processing domain", extra={"run_id": run_id, "domain_index": i, "original_url": url, "processed_domain": domain_url})
-                    
+
                     # Build Elasticsearch query with domain (must) and fuzzy matching for company name/location (should)
                     must_clauses = [
                         {
                             "query_string": {
                                 "default_field": "website.domain_only",
-                                "query": f'"{domain_url}"'  # Exact match for domain
+                                "query": f'"{domain_url}"',  # Exact match for domain
                             }
                         }
                     ]
-                    
+
                     should_clauses = []
-                    
+
                     # Add company name fuzzy matching
                     if company_name:
-                        should_clauses.append({
-                            "match": {
-                                "company_name": {
-                                    "query": company_name,
-                                    "fuzziness": "AUTO",
-                                    "boost": 2.0
-                                }
-                            }
-                        })
-                    
+                        should_clauses.append({"match": {"company_name": {"query": company_name, "fuzziness": "AUTO", "boost": 2.0}}})
+
                     # Add location fuzzy matching using hq_city and hq_country
                     if hq_city:
-                        should_clauses.append({
-                            "match": {
-                                "hq_city": {
-                                    "query": hq_city,
-                                    "fuzziness": "AUTO",
-                                    "boost": 1.0
-                                }
-                            }
-                        })
-                    
+                        should_clauses.append({"match": {"hq_city": {"query": hq_city, "fuzziness": "AUTO", "boost": 1.0}}})
+
                     if hq_country:
-                        should_clauses.append({
-                            "match": {
-                                "hq_country": {
-                                    "query": hq_country,
-                                    "fuzziness": "AUTO",
-                                    "boost": 1.5
-                                }
-                            }
-                        })
-                    
+                        should_clauses.append({"match": {"hq_country": {"query": hq_country, "fuzziness": "AUTO", "boost": 1.5}}})
+
                     # Build complete query
-                    search_query = {
-                        "bool": {
-                            "must": must_clauses
-                        }
-                    }
-                    
+                    search_query = {"bool": {"must": must_clauses}}
+
                     # Add should clauses only if we have them
                     if should_clauses:
                         search_query["bool"]["should"] = should_clauses
                         search_query["bool"]["minimum_should_match"] = 0
-                    
+
                     # Sort by relevance score (API expects list, but only accepts single sort field)
                     # Note: API validation requires list type, but only one field is allowed
                     search_payload = {
                         "query": search_query,
-                        "sort": ["_score"]  # Single-item list (API expects list format)
+                        "sort": ["_score"],  # Single-item list (API expects list format)
                     }
-                    
+
                     if shared_output_file:
                         await self.append_markdown(
-                            shared_output_file,
-                            f"Step 5: Coresignal - Domain {i}/{len(urls)}",
-                            f"Searching for: {url} ‚Üí {domain_url}"
+                            shared_output_file, f"Step 5: Coresignal - Domain {i}/{len(urls)}", f"Searching for: {url} ‚Üí {domain_url}"
                         )
-                    
+
                     async with aiohttp.ClientSession() as session:
                         # Step 1: Search for company ID
                         logger.info("Searching for company ID", extra={"run_id": run_id, "domain": domain_url})
-                        
+
                         # API monitoring: Track search API call
                         api_calls_total += 1
                         search_start_time = time.time()
-                        
+
                         async with session.post(search_url, headers=headers, json=search_payload, timeout=30) as response:
                             search_response_time = time.time() - search_start_time
-                            
+
                             if response.status != 200:
                                 error_text = await response.text()
                                 error_msg = f"Search API error for {url}: {response.status} - {error_text}"
                                 print(f"‚ö†Ô∏è {error_msg}")
-                                
+
                                 # API monitoring: Track failed search API call
                                 api_calls_failed += 1
-                                api_errors.append({
-                                    "api": "coresignal_search",
-                                    "domain": domain_url,
-                                    "status": response.status,
-                                    "error": error_text,
-                                    "response_time": search_response_time
-                                })
-                                
-                                logger.error("CoreSignal search API error", extra={
-                                    "run_id": run_id, 
-                                    "domain": domain_url, 
-                                    "status": response.status, 
-                                    "error": error_text,
-                                    "response_time": search_response_time,
-                                    "api_call_type": "search"
-                                })
-                                
+                                api_errors.append(
+                                    {
+                                        "api": "coresignal_search",
+                                        "domain": domain_url,
+                                        "status": response.status,
+                                        "error": error_text,
+                                        "response_time": search_response_time,
+                                    }
+                                )
+
+                                logger.error(
+                                    "CoreSignal search API error",
+                                    extra={
+                                        "run_id": run_id,
+                                        "domain": domain_url,
+                                        "status": response.status,
+                                        "error": error_text,
+                                        "response_time": search_response_time,
+                                        "api_call_type": "search",
+                                    },
+                                )
+
                                 if shared_output_file:
                                     await self.append_markdown(shared_output_file, f"Step 5: Coresignal - Domain {i} Error", error_msg)
                                 continue
-                            
+
                             # API monitoring: Track successful search API call
                             api_calls_successful += 1
-                            
+
                             search_data = await response.json()
-                            logger.info("CoreSignal search API response received", extra={
-                                "run_id": run_id, 
-                                "domain": domain_url, 
-                                "response_type": type(search_data).__name__,
-                                "response_time": search_response_time,
-                                "api_call_type": "search"
-                            })
-                        
+                            logger.info(
+                                "CoreSignal search API response received",
+                                extra={
+                                    "run_id": run_id,
+                                    "domain": domain_url,
+                                    "response_type": type(search_data).__name__,
+                                    "response_time": search_response_time,
+                                    "api_call_type": "search",
+                                },
+                            )
+
                         # Extract company ID - handle both list and dict response formats
                         companies = []
                         if isinstance(search_data, list):
                             companies = search_data
                         elif isinstance(search_data, dict):
-                            companies = search_data.get('companies', [])
-                        
-                        logger.info("Companies extracted from search response", extra={"run_id": run_id, "domain": domain_url, "companies_count": len(companies)})
-                        
+                            companies = search_data.get("companies", [])
+
+                        logger.info(
+                            "Companies extracted from search response",
+                            extra={"run_id": run_id, "domain": domain_url, "companies_count": len(companies)},
+                        )
+
                         if not companies:
                             error_msg = f"No company found for domain: {url}"
                             print(f"‚ö†Ô∏è {error_msg}")
@@ -540,83 +556,88 @@
                             if shared_output_file:
                                 await self.append_markdown(shared_output_file, f"Step 5: Coresignal - Domain {i} No Results", error_msg)
                             continue
-                        
+
                         # Check for direct name match and determine if LLM selection is needed
                         first_company = companies[0]
-                        first_company_name = first_company.get('company_name', '') if isinstance(first_company, dict) else ''
+                        first_company_name = first_company.get("company_name", "") if isinstance(first_company, dict) else ""
                         has_direct_name_match = self._check_direct_name_match(company_name, first_company_name)
-                        
+
                         # Determine selected company ID
                         coresignal_company_id = None
-                        
+
                         if has_direct_name_match:
                             # Use first company (existing logic)
                             if isinstance(first_company, dict):
-                                coresignal_company_id = first_company.get('id')
+                                coresignal_company_id = first_company.get("id")
                             elif isinstance(first_company, int):
                                 coresignal_company_id = first_company
                             print(f"‚úÖ Direct name match found, using first result: {coresignal_company_id}")
                         elif len(companies) > 1:
                             # Check if any results are within score threshold of 10 from #1
-                            top_score = first_company.get('_score', 0) if isinstance(first_company, dict) else 0
+                            top_score = first_company.get("_score", 0) if isinstance(first_company, dict) else 0
                             threshold = top_score - 10
-                            
+
                             # Filter candidates within threshold
                             candidates_within_threshold = []
                             for company in companies:
                                 if isinstance(company, dict):
-                                    score = company.get('_score', 0)
+                                    score = company.get("_score", 0)
                                     if score >= threshold:
                                         candidates_within_threshold.append(company)
                                 elif isinstance(company, int):
                                     # If company is just an ID, skip LLM selection
                                     break
-                            
+
                             if len(candidates_within_threshold) > 1:
                                 # Trigger LLM selection
-                                print(f"ü§ñ No direct name match, {len(candidates_within_threshold)} candidates within score threshold. Using LLM to select best match...")
-                                logger.info("Triggering LLM company selection", extra={
-                                    "run_id": run_id,
-                                    "domain": domain_url,
-                                    "candidates_count": len(candidates_within_threshold),
-                                    "top_score": top_score,
-                                    "threshold": threshold
-                                })
-                                
+                                print(
+                                    f"ü§ñ No direct name match, {len(candidates_within_threshold)} candidates within score threshold. Using LLM to select best match..."
+                                )
+                                logger.info(
+                                    "Triggering LLM company selection",
+                                    extra={
+                                        "run_id": run_id,
+                                        "domain": domain_url,
+                                        "candidates_count": len(candidates_within_threshold),
+                                        "top_score": top_score,
+                                        "threshold": threshold,
+                                    },
+                                )
+
                                 selected_id = await self._llm_select_best_company_match(
                                     target_company_name=company_name,
                                     target_hq_city=hq_city,
                                     target_hq_country=hq_country,
-                                    target_investor_type=investor_type or '',
+                                    target_investor_type=investor_type or "",
                                     candidates=candidates_within_threshold,
                                     run_id=run_id,
                                     user_id=user_id,
-                                    shared_output_file=shared_output_file
+                                    shared_output_file=shared_output_file,
                                 )
-                                
+
                                 if selected_id:
                                     coresignal_company_id = selected_id
                                     print(f"‚úÖ LLM selected Company ID: {coresignal_company_id}")
                                 else:
                                     # Fallback to first result
                                     if isinstance(first_company, dict):
-                                        coresignal_company_id = first_company.get('id')
+                                        coresignal_company_id = first_company.get("id")
                                     elif isinstance(first_company, int):
                                         coresignal_company_id = first_company
                                     print(f"‚ö†Ô∏è LLM selection failed, falling back to first result: {coresignal_company_id}")
                             else:
                                 # Only one candidate within threshold, use first result
                                 if isinstance(first_company, dict):
-                                    coresignal_company_id = first_company.get('id')
+                                    coresignal_company_id = first_company.get("id")
                                 elif isinstance(first_company, int):
                                     coresignal_company_id = first_company
                         else:
                             # Only one result, use it
                             if isinstance(first_company, dict):
-                                coresignal_company_id = first_company.get('id')
+                                coresignal_company_id = first_company.get("id")
                             elif isinstance(first_company, int):
                                 coresignal_company_id = first_company
-                        
+
                         if not coresignal_company_id:
                             error_msg = f"Company ID not found for domain: {url}"
                             print(f"‚ö†Ô∏è {error_msg}")
@@ -624,297 +645,318 @@
                             if shared_output_file:
                                 await self.append_markdown(shared_output_file, f"Step 5: Coresignal - Domain {i} No ID", error_msg)
                             continue
-                        
+
                         print(f"‚úÖ Found CoreSignal ID {coresignal_company_id} for domain: {url}")
-                        logger.info("CoreSignal company ID found", extra={"run_id": run_id, "domain": domain_url, "coresignal_company_id": coresignal_company_id, "has_direct_name_match": has_direct_name_match})
+                        logger.info(
+                            "CoreSignal company ID found",
+                            extra={
+                                "run_id": run_id,
+                                "domain": domain_url,
+                                "coresignal_company_id": coresignal_company_id,
+                                "has_direct_name_match": has_direct_name_match,
+                            },
+                        )
                         if shared_output_file:
                             await self.append_markdown(
                                 shared_output_file,
                                 f"Step 5: Coresignal - Domain {i} Found",
-                                f"‚úÖ CoreSignal Company ID: {coresignal_company_id} for {url}"
+                                f"‚úÖ CoreSignal Company ID: {coresignal_company_id} for {url}",
                             )
-                        
+
                         # Step 2: Collect full company data
                         collect_url = f"https://api.coresignal.com/cdapi/v2/company_multi_source/collect/{coresignal_company_id}"
-                        
+
                         logger.info("Collecting full company data", extra={"run_id": run_id, "domain": domain_url, "collect_url": collect_url})
-                        
+
                         # API monitoring: Track collect API call
                         api_calls_total += 1
                         collect_start_time = time.time()
-                        
+
                         async with session.get(collect_url, headers=headers, timeout=30) as response:
                             collect_response_time = time.time() - collect_start_time
-                            
+
                             if response.status != 200:
                                 error_text = await response.text()
                                 error_msg = f"Collect API error for {url}: {response.status} - {error_text}"
                                 print(f"‚ö†Ô∏è {error_msg}")
-                                
+
                                 # API monitoring: Track failed collect API call
                                 api_calls_failed += 1
-                                api_errors.append({
-                                    "api": "coresignal_collect",
-                                    "domain": domain_url,
-                                    "company_id": coresignal_company_id,
-                                    "status": response.status,
-                                    "error": error_text,
-                                    "response_time": collect_response_time
-                                })
-                                
-                                logger.error("CoreSignal collect API error", extra={
-                                    "run_id": run_id, 
-                                    "domain": domain_url, 
-                                    "status": response.status, 
-                                    "error": error_text,
-                                    "response_time": collect_response_time,
-                                    "api_call_type": "collect"
-                                })
-                                
+                                api_errors.append(
+                                    {
+                                        "api": "coresignal_collect",
+                                        "domain": domain_url,
+                                        "company_id": coresignal_company_id,
+                                        "status": response.status,
+                                        "error": error_text,
+                                        "response_time": collect_response_time,
+                                    }
+                                )
+
+                                logger.error(
+                                    "CoreSignal collect API error",
+                                    extra={
+                                        "run_id": run_id,
+                                        "domain": domain_url,
+                                        "status": response.status,
+                                        "error": error_text,
+                                        "response_time": collect_response_time,
+                                        "api_call_type": "collect",
+                                    },
+                                )
+
                                 if shared_output_file:
                                     await self.append_markdown(shared_output_file, f"Step 5: Coresignal - Domain {i} Collect Error", error_msg)
                                 continue
-                            
+
                             # API monitoring: Track successful collect API call
                             api_calls_successful += 1
-                            
+
                             collect_data = await response.json()
-                            logger.info("CoreSignal collect API response received", extra={
-                                "run_id": run_id, 
-                                "domain": domain_url, 
-                                "data_keys": list(collect_data.keys()) if isinstance(collect_data, dict) else [],
-                                "response_time": collect_response_time,
-                                "api_call_type": "collect"
-                            })
-                            
+                            logger.info(
+                                "CoreSignal collect API response received",
+                                extra={
+                                    "run_id": run_id,
+                                    "domain": domain_url,
+                                    "data_keys": list(collect_data.keys()) if isinstance(collect_data, dict) else [],
+                                    "response_time": collect_response_time,
+                                    "api_call_type": "collect",
+                                },
+                            )
+
                             # Process LinkedIn posts to Qdrant
                             linkedin_posts_count = 0
                             if company_id:
-                                logger.info("Starting LinkedIn posts processing to Qdrant", extra={"run_id": run_id, "domain": domain_url, "company_id": company_id})
+                                logger.info(
+                                    "Starting LinkedIn posts processing to Qdrant",
+                                    extra={"run_id": run_id, "domain": domain_url, "company_id": company_id},
+                                )
                                 linkedin_posts_count = chunk_linkedin_posts_to_qdrant(
                                     coresignal_data=collect_data,
                                     company_name=company_name,
                                     company_id=company_id,
                                     run_id=run_id,
                                     user_id=user_id,
-                                    collection_name="linkedin_posts"
+                                    collection_name="linkedin_posts",
                                 )
                                 total_linkedin_posts += linkedin_posts_count
                                 print(f"‚úÖ Processed {linkedin_posts_count} LinkedIn posts for domain {url}")
-                                logger.info("LinkedIn posts processed to Qdrant", extra={"run_id": run_id, "domain": domain_url, "posts_count": linkedin_posts_count})
-                            
+                                logger.info(
+                                    "LinkedIn posts processed to Qdrant",
+                                    extra={"run_id": run_id, "domain": domain_url, "posts_count": linkedin_posts_count},
+                                )
+
                             # Debug: Check if key_executives is in collect_data
                             if isinstance(collect_data, dict):
                                 print(f"üîç DEBUG: collect_data has 'key_executives': {'key_executives' in collect_data}")
-                                if 'key_executives' in collect_data:
-                                    ke = collect_data['key_executives']
+                                if "key_executives" in collect_data:
+                                    ke = collect_data["key_executives"]
                                     print(f"   key_executives type: {type(ke)}, length: {len(ke) if isinstance(ke, list) else 'N/A'}")
-                            
+
                             domain_result = {
-                                'domain': url,
-                                'coresignal_company_id': coresignal_company_id,
-                                'data': collect_data,
-                                'linkedin_posts_count': linkedin_posts_count
+                                "domain": url,
+                                "coresignal_company_id": coresignal_company_id,
+                                "data": collect_data,
+                                "linkedin_posts_count": linkedin_posts_count,
                             }
-                            
+
                             all_results.append(domain_result)
-                            logger.info("Domain result added to results", extra={"run_id": run_id, "domain": domain_url, "is_best_result": best_result is None})
-                            
+                            logger.info(
+                                "Domain result added to results",
+                                extra={"run_id": run_id, "domain": domain_url, "is_best_result": best_result is None},
+                            )
+
                             # Use the first successful result as the "best" result for backward compatibility
                             if best_result is None:
                                 best_result = domain_result
-                                
+
                             if shared_output_file:
                                 await self.append_markdown(
                                     shared_output_file,
                                     f"Step 5: Coresignal - Domain {i} Success",
-                                    f"‚úÖ Retrieved data for {url} (LinkedIn posts: {linkedin_posts_count})"
+                                    f"‚úÖ Retrieved data for {url} (LinkedIn posts: {linkedin_posts_count})",
                                 )
-                            
+
                             # Early exit: Stop after first successful match to avoid mixing data from different companies
                             print(f"‚úÖ First successful match found for domain: {url}, stopping domain search")
-                            logger.info("First successful match found, stopping domain search", extra={
-                                "run_id": run_id,
-                                "domain": domain_url,
-                                "coresignal_company_id": coresignal_company_id,
-                                "total_domains_processed": i,
-                                "total_domains_available": len(urls)
-                            })
+                            logger.info(
+                                "First successful match found, stopping domain search",
+                                extra={
+                                    "run_id": run_id,
+                                    "domain": domain_url,
+                                    "coresignal_company_id": coresignal_company_id,
+                                    "total_domains_processed": i,
+                                    "total_domains_available": len(urls),
+                                },
+                            )
                             break  # Exit the domain loop after first successful match
-                
+
                 except Exception as domain_error:
                     error_msg = f"Error processing domain {url}: {str(domain_error)}"
                     print(f"‚ùå {error_msg}")
-                    
+
                     # API monitoring: Track domain processing error
                     api_calls_failed += 1
-                    api_errors.append({
-                        "api": "coresignal_domain_processing",
-                        "domain": url,
-                        "error": str(domain_error)
-                    })
-                    
+                    api_errors.append({"api": "coresignal_domain_processing", "domain": url, "error": str(domain_error)})
+
                     logger.exception("Error processing domain", extra={"run_id": run_id, "domain": url, "error": str(domain_error)})
                     if shared_output_file:
                         await self.append_markdown(shared_output_file, f"Step 5: Coresignal - Domain {i} Exception", error_msg)
                     continue
 
             # Log API monitoring summary
-            logger.info("CoreSignal API monitoring summary", extra={
-                "run_id": run_id,
-                "api_calls_total": api_calls_total,
-                "api_calls_successful": api_calls_successful,
-                "api_calls_failed": api_calls_failed,
-                "success_rate": (api_calls_successful / api_calls_total * 100) if api_calls_total > 0 else 0,
-                "api_errors": api_errors
-            })
+            logger.info(
+                "CoreSignal API monitoring summary",
+                extra={
+                    "run_id": run_id,
+                    "api_calls_total": api_calls_total,
+                    "api_calls_successful": api_calls_successful,
+                    "api_calls_failed": api_calls_failed,
+                    "success_rate": (api_calls_successful / api_calls_total * 100) if api_calls_total > 0 else 0,
+                    "api_errors": api_errors,
+                },
+            )
 
             # Check if we found any results
             if not all_results:
                 # Trigger Perplexity fallback
-                logger.info("No CoreSignal matches found, triggering Perplexity fallback", 
-                            extra={"run_id": run_id, "domains_count": len(urls)})
-                
+                logger.info("No CoreSignal matches found, triggering Perplexity fallback", extra={"run_id": run_id, "domains_count": len(urls)})
+
                 if shared_output_file:
                     await self.append_markdown(
                         shared_output_file,
                         "Step 5: Coresignal - Perplexity Fallback",
-                        f"No CoreSignal matches found for {len(urls)} domain(s). Attempting Perplexity fallback extraction..."
+                        f"No CoreSignal matches found for {len(urls)} domain(s). Attempting Perplexity fallback extraction...",
                     )
-                
+
                 # Use first domain for fallback
                 primary_url = urls[0] if urls else None
-                
+
                 # Call Perplexity fallback
                 fallback_data = await self._perplexity_fallback_extraction(
                     company_name=company_name,
                     website_url=primary_url,
                     location=f"{hq_city}, {hq_country}" if hq_city and hq_country else (hq_city or hq_country or ""),
                     run_id=run_id,
-                    shared_output_file=shared_output_file
+                    shared_output_file=shared_output_file,
                 )
-                
+
                 if fallback_data:
                     # Map to CoreSignal format
-                    mapped_data = self._map_perplexity_to_coresignal_format(
-                        fallback_data,
-                        company_name,
-                        primary_url
-                    )
-                    
+                    mapped_data = self._map_perplexity_to_coresignal_format(fallback_data, company_name, primary_url)
+
                     # Return in CoreSignal-compatible format
                     return {
-                        'success': True,
-                        'data': mapped_data.get('data', {}),
-                        'company_id': company_id,
-                        'chunks_processed': 0,
-                        'multi_domain_results': {
-                            'total_domains_searched': len(urls),
-                            'successful_domains': 1,
-                            'domains_found': [primary_url] if primary_url else [],
-                            'primary_domain': primary_url,
-                            'all_domain_data': [{
-                                'domain': primary_url,
-                                'coresignal_company_id': None,
-                                'data': mapped_data.get('data', {}),
-                                'linkedin_posts_count': 0
-                            }] if primary_url else []
+                        "success": True,
+                        "data": mapped_data.get("data", {}),
+                        "company_id": company_id,
+                        "chunks_processed": 0,
+                        "multi_domain_results": {
+                            "total_domains_searched": len(urls),
+                            "successful_domains": 1,
+                            "domains_found": [primary_url] if primary_url else [],
+                            "primary_domain": primary_url,
+                            "all_domain_data": [
+                                {"domain": primary_url, "coresignal_company_id": None, "data": mapped_data.get("data", {}), "linkedin_posts_count": 0}
+                            ]
+                            if primary_url
+                            else [],
                         },
-                        'fallback_source': 'perplexity'
+                        "fallback_source": "perplexity",
                     }
-                
+
                 # Fallback also failed
                 error_msg = f"No company data found for any of the {len(urls)} domains (CoreSignal and Perplexity fallback both failed)"
                 print(f"‚ùå {error_msg}")
-                logger.error("No company data found (CoreSignal and Perplexity fallback both failed)", 
-                            extra={"run_id": run_id, "domains_count": len(urls)})
+                logger.error(
+                    "No company data found (CoreSignal and Perplexity fallback both failed)", extra={"run_id": run_id, "domains_count": len(urls)}
+                )
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 5: Coresignal - All Sources Failed", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
+                return {"success": False, "error": error_msg}
 
             # Log final results summary
             if shared_output_file:
-                domain_summary = "\n".join([f"- {result['domain']}: ID {result['coresignal_company_id']} ({result['linkedin_posts_count']} posts)" for result in all_results])
+                domain_summary = "\n".join(
+                    [f"- {result['domain']}: ID {result['coresignal_company_id']} ({result['linkedin_posts_count']} posts)" for result in all_results]
+                )
                 await self.append_markdown(
                     shared_output_file,
                     "Step 5: Coresignal - Multi-Domain Results Summary",
                     f"**Successful domains ({len(all_results)}/{len(urls)}):**\n{domain_summary}\n\n"
                     f"**Total LinkedIn posts processed:** {total_linkedin_posts}\n"
-                    f"**Using primary result from:** {best_result['domain']}"
+                    f"**Using primary result from:** {best_result['domain']}",
                 )
-            
+
             print(f"‚úÖ Multi-domain search completed: {len(all_results)}/{len(urls)} domains found")
             print(f"üì± Total LinkedIn posts processed: {total_linkedin_posts}")
-            
-            logger.info("Multi-domain search completed successfully", extra={
-                "run_id": run_id,
-                "successful_domains": len(all_results),
-                "total_domains": len(urls),
-                "total_linkedin_posts": total_linkedin_posts,
-                "primary_domain": best_result['domain']
-            })
 
+            logger.info(
+                "Multi-domain search completed successfully",
+                extra={
+                    "run_id": run_id,
+                    "successful_domains": len(all_results),
+                    "total_domains": len(urls),
+                    "total_linkedin_posts": total_linkedin_posts,
+                    "primary_domain": best_result["domain"],
+                },
+            )
+
             # Return the best result with additional metadata about all domains
             return {
-                'success': True,
-                'data': best_result['data'],  # Primary data for backward compatibility
-                'company_id': company_id,
-                'chunks_processed': total_linkedin_posts,
-                'multi_domain_results': {
-                    'total_domains_searched': len(urls),
-                    'successful_domains': len(all_results),
-                    'domains_found': [result['domain'] for result in all_results],
-                    'primary_domain': best_result['domain'],
-                    'all_domain_data': all_results  # Full data from all domains
-                }
+                "success": True,
+                "data": best_result["data"],  # Primary data for backward compatibility
+                "company_id": company_id,
+                "chunks_processed": total_linkedin_posts,
+                "multi_domain_results": {
+                    "total_domains_searched": len(urls),
+                    "successful_domains": len(all_results),
+                    "domains_found": [result["domain"] for result in all_results],
+                    "primary_domain": best_result["domain"],
+                    "all_domain_data": all_results,  # Full data from all domains
+                },
             }
-                         
+
         except Exception as e:
             error_msg = f"Multi-domain Coresignal API query failed: {str(e)}"
             print(f"‚ùå {error_msg}")
-            
+
             # API monitoring: Track overall failure
             api_calls_failed += 1
-            api_errors.append({
-                "api": "coresignal_overall",
-                "error": str(e)
-            })
-            
-            logger.exception("Multi-domain CoreSignal API query failed", extra={
-                "run_id": run_id, 
-                "error": str(e),
-                "api_calls_total": api_calls_total,
-                "api_calls_successful": api_calls_successful,
-                "api_calls_failed": api_calls_failed,
-                "api_errors": api_errors
-            })
-            
+            api_errors.append({"api": "coresignal_overall", "error": str(e)})
+
+            logger.exception(
+                "Multi-domain CoreSignal API query failed",
+                extra={
+                    "run_id": run_id,
+                    "error": str(e),
+                    "api_calls_total": api_calls_total,
+                    "api_calls_successful": api_calls_successful,
+                    "api_calls_failed": api_calls_failed,
+                    "api_errors": api_errors,
+                },
+            )
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Step 5: Coresignal - Multi-Domain Exception", error_msg)
-            
-            return {
-                'success': False,
-                'error': error_msg
-            }
-    
+
+            return {"success": False, "error": error_msg}
+
     def _check_direct_name_match(self, target_name: str, candidate_name: str) -> bool:
         """
         Check if there's a direct name match between target and candidate.
         Simple rule: if normalized names don't match exactly, send to LLM to decide.
-        
+
         Args:
             target_name: Target company name
             candidate_name: Candidate company name from CoreSignal
-            
+
         Returns:
             True if exact match found (after normalization), False otherwise
         """
         if not target_name or not candidate_name:
             return False
-        
+
         # Normalize names for comparison
         def normalize_name(name: str) -> str:
             """Normalize company name for matching."""
@@ -922,41 +964,41 @@
                 return ""
             # Lowercase and remove common punctuation
             normalized = name.lower().strip()
-            normalized = re.sub(r'[.,\-\(\)]', ' ', normalized)
-            normalized = re.sub(r'\s+', ' ', normalized)
+            normalized = re.sub(r"[.,\-\(\)]", " ", normalized)
+            normalized = re.sub(r"\s+", " ", normalized)
             return normalized
-        
+
         target_normalized = normalize_name(target_name)
         candidate_normalized = normalize_name(candidate_name)
-        
+
         # Exact match after normalization
         if target_normalized == candidate_normalized:
             return True
-        
+
         # Remove legal suffixes and location suffixes, then check again
-        legal_suffixes = ['ltd', 'limited', 'llc', 'inc', 'corp', 'corporation', 'sa', 'gmbh', 'plc', 'llp']
-        location_suffixes = ['uk', 'usa', 'us', 'europe', 'emea', 'asia', 'apac', 'americas']
-        
+        legal_suffixes = ["ltd", "limited", "llc", "inc", "corp", "corporation", "sa", "gmbh", "plc", "llp"]
+        location_suffixes = ["uk", "usa", "us", "europe", "emea", "asia", "apac", "americas"]
+
         target_core = target_normalized
         candidate_core = candidate_normalized
-        
+
         # Remove legal suffixes
         for suffix in legal_suffixes:
-            target_core = re.sub(rf'\b{suffix}\b', '', target_core).strip()
-            candidate_core = re.sub(rf'\b{suffix}\b', '', candidate_core).strip()
-        
+            target_core = re.sub(rf"\b{suffix}\b", "", target_core).strip()
+            candidate_core = re.sub(rf"\b{suffix}\b", "", candidate_core).strip()
+
         # Remove location suffixes (common at end of company names)
         for suffix in location_suffixes:
             # Only remove if at the end
-            target_core = re.sub(rf'\b{suffix}\s*$', '', target_core).strip()
-            candidate_core = re.sub(rf'\b{suffix}\s*$', '', candidate_core).strip()
-        
+            target_core = re.sub(rf"\b{suffix}\s*$", "", target_core).strip()
+            candidate_core = re.sub(rf"\b{suffix}\s*$", "", candidate_core).strip()
+
         # Exact match after removing suffixes
         # If not exact match, return False (send to LLM)
         return target_core == candidate_core
-    
+
     async def _llm_select_best_company_match(
-        self, 
+        self,
         target_company_name: str,
         target_hq_city: str,
         target_hq_country: str,
@@ -964,11 +1006,11 @@
         candidates: List[Dict[str, Any]],
         run_id: str = None,
         user_id: str = None,
-        shared_output_file: str = None
+        shared_output_file: str = None,
     ) -> Optional[int]:
         """
         Use LLM to select the best matching company from candidates.
-        
+
         Args:
             target_company_name: Target company name
             target_hq_city: Target HQ city
@@ -978,59 +1020,71 @@
             run_id: Run ID for logging
             user_id: User ID for logging
             shared_output_file: Optional output file
-            
+
         Returns:
             Selected company ID (integer) or None if selection fails
         """
         if not candidates or len(candidates) == 0:
             return None
-        
+
         try:
             # Extract only relevant fields from candidates (preview endpoint may not have hq_city, fetch if needed)
             candidate_list = []
             for idx, candidate in enumerate(candidates, 1):
                 if isinstance(candidate, dict):
-                    candidate_id = candidate.get('id')
-                    candidate_name = candidate.get('company_name', 'N/A')
+                    candidate_id = candidate.get("id")
+                    candidate_name = candidate.get("company_name", "N/A")
                     # Preview endpoint may not include hq_city, use what's available
-                    candidate_city = candidate.get('hq_city') or candidate.get('city') or 'N/A'
-                    candidate_country = candidate.get('hq_country') or candidate.get('country') or 'N/A'
-                    candidate_industry = candidate.get('industry', 'N/A')
-                    candidate_score = candidate.get('_score', 0)
-                    
+                    candidate_city = candidate.get("hq_city") or candidate.get("city") or "N/A"
+                    candidate_country = candidate.get("hq_country") or candidate.get("country") or "N/A"
+                    candidate_industry = candidate.get("industry", "N/A")
+                    candidate_score = candidate.get("_score", 0)
+
                     if candidate_id:
-                        candidate_list.append({
-                            'id': candidate_id,
-                            'name': candidate_name,
-                            'city': candidate_city,
-                            'country': candidate_country,
-                            'industry': candidate_industry,
-                            'score': candidate_score
-                        })
-            
+                        candidate_list.append(
+                            {
+                                "id": candidate_id,
+                                "name": candidate_name,
+                                "city": candidate_city,
+                                "country": candidate_country,
+                                "industry": candidate_industry,
+                                "score": candidate_score,
+                            }
+                        )
+
             if not candidate_list:
                 return None
-            
+
             # Build prompt
-            candidates_text = "\n".join([
-                f"{idx}. ID: {c['id']}, Name: \"{c['name']}\", City: {c['city']}, Country: {c['country']}, Industry: {c['industry']}, Score: {c['score']:.2f}"
-                for idx, c in enumerate(candidate_list, 1)
-            ])
-            
+            candidates_text = "\n".join(
+                [
+                    f'{idx}. ID: {c["id"]}, Name: "{c["name"]}", City: {c["city"]}, Country: {c["country"]}, Industry: {c["industry"]}, Score: {c["score"]:.2f}'
+                    for idx, c in enumerate(candidate_list, 1)
+                ]
+            )
+
             # Log candidates for debugging
             print(f"üîç LLM Selection - Target: '{target_company_name}' ({target_hq_city}, {target_hq_country}, {target_investor_type})")
             print(f"üîç LLM Selection - Candidates sent to LLM:")
             for c in candidate_list:
-                print(f"   - ID: {c['id']}, Name: \"{c['name']}\", City: {c['city']}, Country: {c['country']}, Industry: {c['industry']}, Score: {c['score']:.2f}")
-            logger.info("LLM company selection candidates", extra={
-                "run_id": run_id,
-                "target_company_name": target_company_name,
-                "target_hq_city": target_hq_city,
-                "target_hq_country": target_hq_country,
-                "target_investor_type": target_investor_type,
-                "candidates": [{"id": c['id'], "name": c['name'], "city": c['city'], "country": c['country'], "industry": c['industry'], "score": c['score']} for c in candidate_list]
-            })
-            
+                print(
+                    f'   - ID: {c["id"]}, Name: "{c["name"]}", City: {c["city"]}, Country: {c["country"]}, Industry: {c["industry"]}, Score: {c["score"]:.2f}'
+                )
+            logger.info(
+                "LLM company selection candidates",
+                extra={
+                    "run_id": run_id,
+                    "target_company_name": target_company_name,
+                    "target_hq_city": target_hq_city,
+                    "target_hq_country": target_hq_country,
+                    "target_investor_type": target_investor_type,
+                    "candidates": [
+                        {"id": c["id"], "name": c["name"], "city": c["city"], "country": c["country"], "industry": c["industry"], "score": c["score"]}
+                        for c in candidate_list
+                    ],
+                },
+            )
+
             # Get prompts from centralized prompts module
             system_prompt = get_coresignal_llm_company_selection_system_prompt()
             user_prompt = get_coresignal_llm_company_selection_user_prompt(
@@ -1038,354 +1092,303 @@
                 target_hq_city=target_hq_city,
                 target_hq_country=target_hq_country,
                 target_investor_type=target_investor_type,
-                candidates_text=candidates_text
+                candidates_text=candidates_text,
             )
-            
+
             # Call LLM
             api_call_start_time = time.time()
-            
-            with trace_operation("coresignal_llm_company_selection", {
-                "company_name": target_company_name,
-                "model": "gpt-4.1-mini",
-                "candidates_count": len(candidate_list),
-                "run_id": run_id,
-                "user_id": user_id
-            }):
+
+            with trace_operation(
+                "coresignal_llm_company_selection",
+                {
+                    "company_name": target_company_name,
+                    "model": "gpt-4.1-mini",
+                    "candidates_count": len(candidate_list),
+                    "run_id": run_id,
+                    "user_id": user_id,
+                },
+            ):
                 chat = ChatOpenAI(
                     model="gpt-4.1-mini",
                     max_tokens=200,  # Increased to allow for JSON response with reason
                     temperature=0,
-                    api_key=get_openai_api_key()
+                    api_key=get_openai_api_key(),
                 )
-                
-                response = await chat.ainvoke([
-                    ("system", system_prompt),
-                    ("user", user_prompt)
-                ])
-                
+
+                response = await chat.ainvoke([("system", system_prompt), ("user", user_prompt)])
+
                 api_response_time = time.time() - api_call_start_time
-                
-                logger.info("LLM company selection successful", extra={
-                    "model": "gpt-4.1-mini",
-                    "response_time": api_response_time,
-                    "company_name": target_company_name,
-                    "candidates_count": len(candidate_list),
-                    "run_id": run_id,
-                    "user_id": user_id
-                })
-                
+
+                logger.info(
+                    "LLM company selection successful",
+                    extra={
+                        "model": "gpt-4.1-mini",
+                        "response_time": api_response_time,
+                        "company_name": target_company_name,
+                        "candidates_count": len(candidate_list),
+                        "run_id": run_id,
+                        "user_id": user_id,
+                    },
+                )
+
                 if not response or not response.content:
                     logger.warning("Empty response from LLM company selection", extra={"run_id": run_id})
                     return None
-                
+
                 # Parse JSON response (should contain selected_company_id and reason)
                 response_text = response.content.strip()
                 selected_id = None
                 reason = None
-                
+
                 try:
                     import json
+
                     # Remove markdown code blocks if present
                     if "```json" in response_text:
                         response_text = response_text.split("```json")[1].split("```")[0].strip()
                     elif "```" in response_text:
                         response_text = response_text.split("```")[1].split("```")[0].strip()
-                    
+
                     result = json.loads(response_text)
                     selected_id = result.get("selected_company_id")
                     reason = result.get("reason", "No reason provided")
                 except (json.JSONDecodeError, TypeError, AttributeError):
                     # Fallback: try to extract just the ID from text
-                    match = re.search(r'\d+', response_text)
+                    match = re.search(r"\d+", response_text)
                     if match:
                         selected_id = int(match.group())
                         reason = "Extracted from text response (JSON parsing failed)"
                     else:
                         logger.warning(f"Could not parse LLM response as JSON or extract ID: {response_text}", extra={"run_id": run_id})
-                
+
                 if selected_id:
                     # Validate selected ID exists in candidates
-                    candidate_ids = [c['id'] for c in candidate_list]
+                    candidate_ids = [c["id"] for c in candidate_list]
                     if selected_id in candidate_ids:
                         print(f"‚úÖ LLM selected company ID: {selected_id}")
                         if reason:
                             print(f"   Reason: {reason}")
-                        logger.info("LLM company selection completed", extra={
-                            "selected_company_id": selected_id,
-                            "reason": reason,
-                            "run_id": run_id
-                        })
+                        logger.info("LLM company selection completed", extra={"selected_company_id": selected_id, "reason": reason, "run_id": run_id})
                         if shared_output_file:
                             reason_text = f"\n   Reason: {reason}" if reason else ""
                             await self.append_markdown(
                                 shared_output_file,
                                 "Step 5: Coresignal - LLM Company Selection",
-                                f"ü§ñ LLM selected Company ID: {selected_id} from {len(candidate_list)} candidates{reason_text}"
+                                f"ü§ñ LLM selected Company ID: {selected_id} from {len(candidate_list)} candidates{reason_text}",
                             )
                         return selected_id
                     else:
-                        logger.warning(f"LLM selected invalid company ID: {selected_id} (not in candidates)", extra={
-                            "selected_id": selected_id,
-                            "candidate_ids": candidate_ids,
-                            "reason": reason,
-                            "run_id": run_id
-                        })
+                        logger.warning(
+                            f"LLM selected invalid company ID: {selected_id} (not in candidates)",
+                            extra={"selected_id": selected_id, "candidate_ids": candidate_ids, "reason": reason, "run_id": run_id},
+                        )
                 else:
                     logger.warning(f"Could not extract company ID from LLM response: {response_text}", extra={"run_id": run_id})
-                
+
         except Exception as e:
-            logger.exception("LLM company selection failed", extra={
-                "run_id": run_id,
-                "user_id": user_id,
-                "error": str(e)
-            })
+            logger.exception("LLM company selection failed", extra={"run_id": run_id, "user_id": user_id, "error": str(e)})
             print(f"‚ö†Ô∏è LLM company selection failed: {e}")
-        
+
         return None
-    
+
     async def _perplexity_fallback_extraction(
-        self,
-        company_name: str,
-        website_url: str,
-        location: str,
-        run_id: str = None,
-        shared_output_file: str = None
+        self, company_name: str, website_url: str, location: str, run_id: str = None, shared_output_file: str = None
     ) -> Optional[Dict[str, Any]]:
         """
         Extract company data using Perplexity when CoreSignal returns 0 matches.
-        
+
         Args:
             company_name: Company name to search for
             website_url: Company website URL
             location: Company location (city, country)
             run_id: Run ID for logging
             shared_output_file: Output file for markdown updates
-        
+
         Returns:
             Parsed JSON data from Perplexity, or None if extraction fails
         """
         if not get_perplexity_api_key():
             logger.error("PERPLEXITY_API_KEY not found for fallback extraction", extra={"run_id": run_id})
             return None
-        
+
         api_call_start_time = time.time()
-        
+
         try:
-            chat = ChatPerplexity(
-                temperature=0,
-                model="sonar-pro",
-                api_key=get_perplexity_api_key()
-            )
-            
+            chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=get_perplexity_api_key())
+
             # Get the formatted prompt (single prompt, not system/user split)
-            prompt = get_coresignal_fallback_extract_prompt(
-                company_name=company_name,
-                website_url=website_url or "",
-                location=location
+            prompt = get_coresignal_fallback_extract_prompt(company_name=company_name, website_url=website_url or "", location=location)
+
+            logger.info(
+                "Perplexity fallback extraction initiated", extra={"run_id": run_id, "company_name": company_name, "website_url": website_url}
             )
-            
-            logger.info("Perplexity fallback extraction initiated", 
-                       extra={"run_id": run_id, "company_name": company_name, "website_url": website_url})
-            
+
             if shared_output_file:
                 await self.append_markdown(
                     shared_output_file,
                     "Step 5: Coresignal - Perplexity Fallback",
-                    f"Extracting company data from Perplexity (CoreSignal returned 0 matches)..."
+                    f"Extracting company data from Perplexity (CoreSignal returned 0 matches)...",
                 )
-            
-            with trace_operation("coresignal_perplexity_fallback", {
-                "model": "sonar-pro",
-                "company_name": company_name,
-                "website_url": website_url,
-                "run_id": run_id
-            }):
+
+            with trace_operation(
+                "coresignal_perplexity_fallback", {"model": "sonar-pro", "company_name": company_name, "website_url": website_url, "run_id": run_id}
+            ):
                 # Call Perplexity with single prompt
-                response = await chat.ainvoke(
-                    prompt,
-                    extra_body={
-                        "web_search_options": {"search_context_size": "high"}
-                    }
-                )
-                
+                response = await chat.ainvoke(prompt, extra_body={"web_search_options": {"search_context_size": "high"}})
+
                 api_response_time = time.time() - api_call_start_time
-                logger.info("Perplexity fallback API call successful", extra={
-                    "model": "sonar-pro",
-                    "response_time": api_response_time,
-                    "run_id": run_id,
-                    "api_call_type": "coresignal_fallback"
-                })
-            
+                logger.info(
+                    "Perplexity fallback API call successful",
+                    extra={"model": "sonar-pro", "response_time": api_response_time, "run_id": run_id, "api_call_type": "coresignal_fallback"},
+                )
+
             content = response.content if response else None
-            
+
             if not content:
                 logger.warning("Perplexity fallback returned empty content", extra={"run_id": run_id})
                 return None
-            
+
             # Parse JSON from response
             # Handle markdown code blocks if present
-            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
+            json_match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", content, re.DOTALL)
             if json_match:
                 json_str = json_match.group(1)
             else:
                 # Try to find JSON object directly
-                json_match = re.search(r'\{.*\}', content, re.DOTALL)
+                json_match = re.search(r"\{.*\}", content, re.DOTALL)
                 if json_match:
                     json_str = json_match.group(0)
                 else:
                     json_str = content
-            
+
             parsed_data = json.loads(json_str)
-            
+
             # Insert known fields (company_name and website) that were removed from prompt
             # These are already known from the agent run, so we add them here instead of asking LLM
-            parsed_data['company_name'] = company_name
-            parsed_data['website'] = website_url or ""
-            
-            logger.info("Perplexity fallback extraction successful", 
-                       extra={"run_id": run_id, "fields_extracted": len(parsed_data)})
-            
+            parsed_data["company_name"] = company_name
+            parsed_data["website"] = website_url or ""
+
+            logger.info("Perplexity fallback extraction successful", extra={"run_id": run_id, "fields_extracted": len(parsed_data)})
+
             if shared_output_file:
                 await self.append_markdown(
-                    shared_output_file,
-                    "Step 5: Coresignal - Perplexity Fallback Success",
-                    f"‚úÖ Extracted {len(parsed_data)} fields from Perplexity"
+                    shared_output_file, "Step 5: Coresignal - Perplexity Fallback Success", f"‚úÖ Extracted {len(parsed_data)} fields from Perplexity"
                 )
-            
+
             return parsed_data
-            
+
         except json.JSONDecodeError as e:
-            logger.exception("Perplexity fallback JSON parsing failed", 
-                            extra={"run_id": run_id, "error": str(e), "content_preview": content[:200] if content else None})
+            logger.exception(
+                "Perplexity fallback JSON parsing failed",
+                extra={"run_id": run_id, "error": str(e), "content_preview": content[:200] if content else None},
+            )
             if shared_output_file:
                 await self.append_markdown(
-                    shared_output_file,
-                    "Step 5: Coresignal - Perplexity Fallback Error",
-                    f"‚ùå Failed to parse JSON response: {str(e)}"
+                    shared_output_file, "Step 5: Coresignal - Perplexity Fallback Error", f"‚ùå Failed to parse JSON response: {str(e)}"
                 )
             return None
         except Exception as e:
             api_response_time = time.time() - api_call_start_time
-            logger.exception("Perplexity fallback extraction failed", 
-                            extra={"run_id": run_id, "error": str(e), "response_time": api_response_time})
+            logger.exception("Perplexity fallback extraction failed", extra={"run_id": run_id, "error": str(e), "response_time": api_response_time})
             if shared_output_file:
                 await self.append_markdown(
-                    shared_output_file,
-                    "Step 5: Coresignal - Perplexity Fallback Error",
-                    f"‚ùå Fallback extraction failed: {str(e)}"
+                    shared_output_file, "Step 5: Coresignal - Perplexity Fallback Error", f"‚ùå Fallback extraction failed: {str(e)}"
                 )
             return None
-    
-    def _map_perplexity_to_coresignal_format(
-        self,
-        perplexity_data: Dict[str, Any],
-        company_name: str,
-        website_url: str
-    ) -> Dict[str, Any]:
+
+    def _map_perplexity_to_coresignal_format(self, perplexity_data: Dict[str, Any], company_name: str, website_url: str) -> Dict[str, Any]:
         """
         Map Perplexity fallback extraction to CoreSignal data structure format.
-        
+
         Args:
             perplexity_data: JSON response from Perplexity with extracted fields
             company_name: Company name (for fallback/validation)
             website_url: Company website URL (for fallback/validation)
-        
+
         Returns:
             Dict matching CoreSignal API response structure (wrapped in 'data' key)
         """
         coresignal_data = {}
-        
+
         # 1. Direct mappings (use known values from agent run)
-        coresignal_data['company_name'] = company_name
-        coresignal_data['company_legal_name'] = perplexity_data.get('company_legal_name')
-        coresignal_data['website'] = website_url or perplexity_data.get('website')
-        coresignal_data['hq_full_address'] = perplexity_data.get('hq_full_address')
-        coresignal_data['linkedin_url'] = perplexity_data.get('linkedin_url')
-        
+        coresignal_data["company_name"] = company_name
+        coresignal_data["company_legal_name"] = perplexity_data.get("company_legal_name")
+        coresignal_data["website"] = website_url or perplexity_data.get("website")
+        coresignal_data["hq_full_address"] = perplexity_data.get("hq_full_address")
+        coresignal_data["linkedin_url"] = perplexity_data.get("linkedin_url")
+
         # 2. Type conversions
-        founded_year = perplexity_data.get('founded_year')
-        coresignal_data['founded_year'] = str(founded_year) if founded_year else None
-        
-        employees_count = perplexity_data.get('employees_count', 0)
+        founded_year = perplexity_data.get("founded_year")
+        coresignal_data["founded_year"] = str(founded_year) if founded_year else None
+
+        employees_count = perplexity_data.get("employees_count", 0)
         if isinstance(employees_count, str):
             # Extract numbers from string (e.g., "5,000 employees" -> 5000)
-            numbers = re.sub(r'[^\d]', '', employees_count)
-            coresignal_data['employees_count'] = int(numbers) if numbers else 0
+            numbers = re.sub(r"[^\d]", "", employees_count)
+            coresignal_data["employees_count"] = int(numbers) if numbers else 0
         elif isinstance(employees_count, int):
-            coresignal_data['employees_count'] = employees_count
+            coresignal_data["employees_count"] = employees_count
         else:
-            coresignal_data['employees_count'] = 0
-        
+            coresignal_data["employees_count"] = 0
+
         # 3. Complex structures
         # company_locations_full
-        locations = perplexity_data.get('company_locations_full', [])
+        locations = perplexity_data.get("company_locations_full", [])
         if not locations or not isinstance(locations, list):
             # Fallback: create from hq_full_address
-            hq_address = perplexity_data.get('hq_full_address')
+            hq_address = perplexity_data.get("hq_full_address")
             if hq_address:
-                locations = [{
-                    "location_address": hq_address,
-                    "is_primary": 1
-                }]
+                locations = [{"location_address": hq_address, "is_primary": 1}]
             else:
                 locations = []
-        
+
         # Ensure proper structure
         normalized_locations = []
         for i, loc in enumerate(locations):
             if isinstance(loc, dict):
-                normalized_locations.append({
-                    "location_address": loc.get('location_address', ''),
-                    "is_primary": 1 if i == 0 else 0
-                })
+                normalized_locations.append({"location_address": loc.get("location_address", ""), "is_primary": 1 if i == 0 else 0})
             elif isinstance(loc, str):
                 # Convert string to dict
-                normalized_locations.append({
-                    "location_address": loc,
-                    "is_primary": 1 if i == 0 else 0
-                })
-        
+                normalized_locations.append({"location_address": loc, "is_primary": 1 if i == 0 else 0})
+
         # Ensure first is primary
         if normalized_locations:
             normalized_locations[0]["is_primary"] = 1
             for loc in normalized_locations[1:]:
                 loc["is_primary"] = 0
-        
-        coresignal_data['company_locations_full'] = normalized_locations
-        
+
+        coresignal_data["company_locations_full"] = normalized_locations
+
         # revenue_annual
-        revenue_data = perplexity_data.get('revenue_annual')
+        revenue_data = perplexity_data.get("revenue_annual")
         if revenue_data and isinstance(revenue_data, dict):
-            source_5 = revenue_data.get('source_5_annual_revenue', {})
-            if source_5 and source_5.get('annual_revenue'):
-                coresignal_data['revenue_annual'] = {
+            source_5 = revenue_data.get("source_5_annual_revenue", {})
+            if source_5 and source_5.get("annual_revenue"):
+                coresignal_data["revenue_annual"] = {
                     "source_5_annual_revenue": {
-                        "annual_revenue": source_5.get('annual_revenue'),
-                        "annual_revenue_currency": source_5.get('annual_revenue_currency', '$')
+                        "annual_revenue": source_5.get("annual_revenue"),
+                        "annual_revenue_currency": source_5.get("annual_revenue_currency", "$"),
                     },
-                    "source_1_annual_revenue": None
+                    "source_1_annual_revenue": None,
                 }
             else:
-                coresignal_data['revenue_annual'] = None
+                coresignal_data["revenue_annual"] = None
         else:
-            coresignal_data['revenue_annual'] = None
-        
+            coresignal_data["revenue_annual"] = None
+
         # parent_company_information
-        parent_company = perplexity_data.get('parent_company_information')
+        parent_company = perplexity_data.get("parent_company_information")
         if parent_company and isinstance(parent_company, dict):
-            coresignal_data['parent_company_information'] = parent_company
+            coresignal_data["parent_company_information"] = parent_company
         else:
-            coresignal_data['parent_company_information'] = None
-        
+            coresignal_data["parent_company_information"] = None
+
         # competitors
-        competitors = perplexity_data.get('competitors', [])
+        competitors = perplexity_data.get("competitors", [])
         if not isinstance(competitors, list):
             competitors = []
         # Ensure all are strings, limit to 10
-        coresignal_data['competitors'] = [
-            str(c) if not isinstance(c, str) else c 
-            for c in competitors[:10]
-        ]
-        
+        coresignal_data["competitors"] = [str(c) if not isinstance(c, str) else c for c in competitors[:10]]
+
         # Wrap in 'data' key to match CoreSignal API response structure
-        return {'data': coresignal_data}
+        return {"data": coresignal_data}

--- app/agents/sub_agents/person_enrich_agent.py
+++ app/agents/sub_agents/person_enrich_agent.py
@@ -13,16 +13,27 @@
 from app.utils.db_utils import ProspectingDB
 from app.utils.qdrant_utils import search_chunks_in_qdrant
 from app.prompts.data_retrieval_prompts import get_person_enrichment_prompt, get_osint_person_system_prompt, get_osint_person_user_prompt
-from app.prompts.data_process_prompts import get_linkedin_mentions_summary_system_prompt, get_linkedin_mentions_summary_user_prompt, get_person_company_verification_system_prompt, get_person_company_verification_user_prompt
+from app.prompts.data_process_prompts import (
+    get_linkedin_mentions_summary_system_prompt,
+    get_linkedin_mentions_summary_user_prompt,
+    get_person_company_verification_system_prompt,
+    get_person_company_verification_user_prompt,
+)
 from app.utils.global_db import get_global_db
 from app.utils.langsmith_config import trace_operation, initialize_langsmith
 from app.utils.logging_config import get_logger
 from app.utils.progress_store import ProgressStore
-from app.utils.config import get_coresignal_api_key, get_perplexity_api_key, get_apollo_api_key, get_enable_postgres_storage, get_openai_api_key, get_enable_youtube_ingest
+from app.utils.config import (
+    get_coresignal_api_key,
+    get_perplexity_api_key,
+    get_apollo_api_key,
+    get_enable_postgres_storage,
+    get_openai_api_key,
+    get_enable_youtube_ingest,
+)
 from app.services.youtube_media_service import YouTubeMediaService
 
 
-
 class PersonEnrichAgent(BaseSubAgent):
     """
     Sub-agent that extracts the most relevant person for investment outreach
@@ -39,66 +50,64 @@
             "email_address": {"apollo_path": ["email", "email_status", "extrapolated_email_confidence"]},
             "linkedin_profile_url": {"coresignal_path": ["websites_linkedin"]},
             "time_zone": {"coresignal_path": ["location_raw_address"]},  # Updated to correct field name
-            "office_location": {"coresignal_path": ["location_raw_address"]}  # Updated to correct field name
+            "office_location": {"coresignal_path": ["location_raw_address"]},  # Updated to correct field name
         },
         "professional_background": {
             "employment_history": {"coresignal_path": ["experience"]},  # Last 4 experiences
             "education": {"coresignal_path": ["education"]},  # Last 2 education entries
             "years_of_experience": {"coresignal_path": ["total_experience_duration_months"]},
             "languages_spoken": {"coresignal_path": ["languages"]},
-            "conferences_public_speaking": {"perplexity_path": ["conferences", "public_speaking"]}
+            "conferences_public_speaking": {"perplexity_path": ["conferences", "public_speaking"]},
         },
         "strategic_notes_personalization": {
             "recent_news": {"perplexity_path": ["recent_events", "achievements", "investment_news"]},
             "recent_social_posts": {"coresignal_path": ["activity"]},
-            "company_linkedin_mentions_summary": {"company_linkedin_mentions_path": ["summary"]}, 
-            "company_linkedin_mentions_raw": {"company_linkedin_mentions_path": ["raw_posts"]} 
-        }
+            "company_linkedin_mentions_summary": {"company_linkedin_mentions_path": ["summary"]},
+            "company_linkedin_mentions_raw": {"company_linkedin_mentions_path": ["raw_posts"]},
+        },
     }
 
     def __init__(self, output_dir: str = "app/data", num_people: int = 2, db: Optional[ProspectingDB] = None):
         # Initialize LangSmith
         initialize_langsmith()
-        
+
         super().__init__(output_dir, db)
         self.num_people = num_people
         self.youtube_service: Optional[YouTubeMediaService] = None
-        
+
         # Enhanced search configuration with simple early termination
         self.search_config = {
-            'enable_exact_search': True,
-            'enable_surname_search': True,
-            'enable_early_termination': True,
-            'max_search_attempts': 4,  # Reduced from 6 to 4
-            'max_search_time_seconds': 20,  # New: time limit
-            'high_confidence_threshold': 8,  # New: stop immediately if found
-            'low_confidence_threshold': 6,   # New: minimum acceptable confidence
+            "enable_exact_search": True,
+            "enable_surname_search": True,
+            "enable_early_termination": True,
+            "max_search_attempts": 4,  # Reduced from 6 to 4
+            "max_search_time_seconds": 20,  # New: time limit
+            "high_confidence_threshold": 8,  # New: stop immediately if found
+            "low_confidence_threshold": 6,  # New: minimum acceptable confidence
         }
-        
+
         # Initialize LLM for person analysis and verification
         self.llm = ChatOpenAI(
             model="gpt-4o-mini",
             temperature=0.1,
             max_tokens=500,  # Reduced for faster responses
-            api_key=get_openai_api_key()
+            api_key=get_openai_api_key(),
         )
-        
+
         # Initialize GPT-5-mini LLM specifically for deduplication task
         self.llm_deduplication = ChatOpenAI(
             model="gpt-5-mini",
             max_tokens=4000,  # Increased from 1000 to handle larger responses
-            api_key=get_openai_api_key()
+            api_key=get_openai_api_key(),
         )
-        
+
         # Initialize logger for PersonEnrichAgent
         self.logger = get_logger(__name__)
-        self.logger.info("PersonEnrichAgent initialized", extra={
-            "output_dir": output_dir, 
-            "num_people": num_people,
-            "search_config": self.search_config,
-            "models": ["gpt-4o-mini", "gpt-5"]
-        })
-        
+        self.logger.info(
+            "PersonEnrichAgent initialized",
+            extra={"output_dir": output_dir, "num_people": num_people, "search_config": self.search_config, "models": ["gpt-4o-mini", "gpt-5"]},
+        )
+
         print(f"‚úÖ PersonEnrichAgent initialized (enriching {num_people} people)")
         print(f"üîß Search config: {self.search_config}")
         print(f"ü§ñ LLM Models: GPT-4o-mini (default), GPT-5 (deduplication)")
@@ -110,34 +119,34 @@
     def generate_unique_person_id(self, linkedin_url: str = None) -> Optional[str]:
         """
         Generate a unique person ID using LinkedIn profile identifier extraction.
-        
+
         Extracts the full profile identifier from LinkedIn URLs like:
         - https://www.linkedin.com/in/jane-smith-12345/ -> jane-smith-12345
-        - https://linkedin.com/in/emilychang -> emilychang  
+        - https://linkedin.com/in/emilychang -> emilychang
         - https://www.linkedin.com/in/john-doe-92384a17b?trk=someparam -> john-doe-92384a17b
-        
+
         Args:
-            linkedin_url: LinkedIn profile URL 
-            
+            linkedin_url: LinkedIn profile URL
+
         Returns:
             LinkedIn profile identifier (e.g., "jane-smith-12345"), or None if not extractable
         """
         if not linkedin_url:
             print(f"‚ö†Ô∏è No LinkedIn URL provided for person ID generation")
             return None
-            
+
         try:
             # Extract LinkedIn profile identifier from URL
-            if '/in/' in linkedin_url:
+            if "/in/" in linkedin_url:
                 # Get the part after /in/
-                profile_part = linkedin_url.split('/in/')[-1]
-                
+                profile_part = linkedin_url.split("/in/")[-1]
+
                 # Remove query parameters (everything after ?)
-                profile_part = profile_part.split('?')[0]
-                
+                profile_part = profile_part.split("?")[0]
+
                 # Remove trailing slash if present
-                profile_part = profile_part.rstrip('/')
-                
+                profile_part = profile_part.rstrip("/")
+
                 # Validate that we have a reasonable profile identifier
                 if profile_part and len(profile_part) >= 3:
                     person_id = profile_part
@@ -149,7 +158,7 @@
             else:
                 print(f"‚ö†Ô∏è Invalid LinkedIn URL format: {linkedin_url}")
                 return None
-                
+
         except Exception as e:
             print(f"‚ùå Failed to extract LinkedIn profile ID: {e}")
             return None
@@ -157,91 +166,101 @@
     def extract_intelligent_company_search_term(self, full_company_name: str) -> str:
         """
         Extract intelligent company search term by removing common suffixes and legal entities.
-        
+
         Args:
             full_company_name: Full company name (e.g., "Example Capital Partners Limited")
-            
+
         Returns:
             Intelligent search term (e.g., "Example Capital Partners")
         """
         if not full_company_name:
             return ""
-        
+
         # Remove common legal entity suffixes
         legal_suffixes = [
-            " Limited", " Ltd", " LLC", " Inc", " Corporation", " Corp", " Company", " Co",
-            " Limited.", " Ltd.", " LLC.", " Inc.", " Corporation.", " Corp.", " Company.", " Co.",
-            " LP", " LLP", " L.P.", " L.L.P.", " Limited Partnership", " Limited Liability Partnership"
+            " Limited",
+            " Ltd",
+            " LLC",
+            " Inc",
+            " Corporation",
+            " Corp",
+            " Company",
+            " Co",
+            " Limited.",
+            " Ltd.",
+            " LLC.",
+            " Inc.",
+            " Corporation.",
+            " Corp.",
+            " Company.",
+            " Co.",
+            " LP",
+            " LLP",
+            " L.P.",
+            " L.L.P.",
+            " Limited Partnership",
+            " Limited Liability Partnership",
         ]
-        
+
         search_term = full_company_name
         for suffix in legal_suffixes:
             if search_term.endswith(suffix):
-                search_term = search_term[:-len(suffix)]
+                search_term = search_term[: -len(suffix)]
                 break
-        
+
         return search_term.strip()
 
     def parse_name_components(self, full_name: str) -> Dict[str, str]:
         """
         Parse full name into components for flexible matching.
-        
+
         Args:
             full_name: Full name (e.g., "Alastair J. Rae", "Richard Lockwood Chilton, Jr.")
-            
+
         Returns:
             Dict with first_name, surname, full_name_clean
         """
         if not full_name or not full_name.strip():
-            return {
-                'first_name': '',
-                'surname': '',
-                'full_name_clean': ''
-            }
-        
+            return {"first_name": "", "surname": "", "full_name_clean": ""}
+
         # Clean the name
         name = full_name.strip()
-        
+
         # Handle suffixes like Jr., Sr., III, etc.
-        suffixes = ['Jr.', 'Sr.', 'Jr', 'Sr', 'I', 'II', 'III', 'IV', 'V']
-        suffix = ''
-        
+        suffixes = ["Jr.", "Sr.", "Jr", "Sr", "I", "II", "III", "IV", "V"]
+        suffix = ""
+
         for s in suffixes:
-            if name.endswith(f', {s}') or name.endswith(f' {s}'):
+            if name.endswith(f", {s}") or name.endswith(f" {s}"):
                 suffix = s
-                name = name.replace(f', {s}', '').replace(f' {s}', '')
+                name = name.replace(f", {s}", "").replace(f" {s}", "")
                 break
-        
+
         # Split into parts
         parts = name.split()
-        
+
         if len(parts) == 1:
             # Single name - treat as surname
-            return {
-                'first_name': '',
-                'surname': parts[0],
-                'full_name_clean': name
-            }
+            return {"first_name": "", "surname": parts[0], "full_name_clean": name}
         elif len(parts) == 2:
             # First Last
             first_name, surname = parts
-            return {
-                'first_name': first_name,
-                'surname': surname,
-                'full_name_clean': f"{first_name} {surname}{' ' + suffix if suffix else ''}"
-            }
+            return {"first_name": first_name, "surname": surname, "full_name_clean": f"{first_name} {surname}{' ' + suffix if suffix else ''}"}
         else:
             # Multiple parts - first is first name, last is surname
             first_name = parts[0]
             surname = parts[-1]
-            return {
-                'first_name': first_name,
-                'surname': surname,
-                'full_name_clean': f"{first_name} {surname}{' ' + suffix if suffix else ''}"
-            }
+            return {"first_name": first_name, "surname": surname, "full_name_clean": f"{first_name} {surname}{' ' + suffix if suffix else ''}"}
 
-    async def verify_person_company_match(self, employee_data: dict, target_person_name: str, target_company_name: str, 
-                                        sector_type: str = "Financial Sector", person_title: str = None, search_type: str = "exact") -> dict:
+    async def verify_person_company_match(
+        self,
+        employee_data: dict,
+        target_person_name: str,
+        target_company_name: str,
+        sector_type: str = "Financial Sector",
+        person_title: str = None,
+        search_type: str = "exact",
+    ) -> dict:
         """
         Use LLM to verify if the found person matches the target search criteria.
         Args:
@@ -257,34 +276,38 @@
         try:
             # Extract company references from employee data - Updated for clean API
             company_context = []
-            
+
             # Get current role from first experience entry (order_in_profile: 1)
-            experience_list = employee_data.get('experience', [])
+            experience_list = employee_data.get("experience", [])
             if experience_list:
                 current_experience = experience_list[0]  # First entry is current role
-                if current_experience.get('company_name'):
-                    company_context.append({
-                        "company": current_experience['company_name'],
-                        "title": current_experience.get('title', '')  # Changed from 'position_title'
-                    })
-            
+                if current_experience.get("company_name"):
+                    company_context.append(
+                        {
+                            "company": current_experience["company_name"],
+                            "title": current_experience.get("title", ""),  # Changed from 'position_title'
+                        }
+                    )
+
             # Work history - only top 3 experiences
             for exp in experience_list[:3]:
-                if exp.get('company_name'):
-                    company_context.append({
-                        "company": exp['company_name'],
-                        "title": exp.get('title', '')  # Changed from 'position_title'
-                    })
-            
+                if exp.get("company_name"):
+                    company_context.append(
+                        {
+                            "company": exp["company_name"],
+                            "title": exp.get("title", ""),  # Changed from 'position_title'
+                        }
+                    )
+
             # Prepare context for LLM
-            person_name = employee_data.get('full_name', 'Unknown')
-            location = employee_data.get('location_raw_address', 'Unknown')  # Changed from 'location_full'
-            
+            person_name = employee_data.get("full_name", "Unknown")
+            location = employee_data.get("location_raw_address", "Unknown")  # Changed from 'location_full'
+
             # Simplified company context - only essential info
             company_context_text = ""
             for i, context in enumerate(company_context[:3], 1):  # Only top 3 experiences
                 company_context_text += f"{i}. {context['company']} - {context['title']}\n"
-            
+
             # Use prompt functions from data_process_prompts.py
             system_prompt = get_person_company_verification_system_prompt(search_type)
             user_prompt = get_person_company_verification_user_prompt(
@@ -294,98 +317,96 @@
                 search_type=search_type,
                 person_name=person_name,
                 location=location,
-                company_context_text=company_context_text
+                company_context_text=company_context_text,
             )
             # Call LLM with LangSmith tracing
-            with trace_operation("person_company_verification", {
-                "target_person_name": target_person_name,
-                "target_company_name": target_company_name,
-                "search_type": search_type,
-                "model": "gpt-4o-mini"
-            }):
+            with trace_operation(
+                "person_company_verification",
+                {
+                    "target_person_name": target_person_name,
+                    "target_company_name": target_company_name,
+                    "search_type": search_type,
+                    "model": "gpt-4o-mini",
+                },
+            ):
                 system_message = SystemMessage(content=system_prompt)
                 human_message = HumanMessage(content=user_prompt)
-                
+
                 # API monitoring: Track OpenAI API call
                 api_call_start_time = time.time()
-                
+
                 try:
                     response = await self.llm.ainvoke([system_message, human_message])
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track successful OpenAI API call
-                    self.logger.info("OpenAI API call successful", extra={
-                        "model": "gpt-4o-mini",
-                        "response_time": api_response_time,
-                        "api_call_type": "openai_person_verification"
-                    })
-                    
+                    self.logger.info(
+                        "OpenAI API call successful",
+                        extra={"model": "gpt-4o-mini", "response_time": api_response_time, "api_call_type": "openai_person_verification"},
+                    )
+
                 except Exception as api_error:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track failed OpenAI API call
-                    self.logger.error("OpenAI API call failed", extra={
-                        "model": "gpt-4o-mini",
-                        "error": str(api_error),
-                        "response_time": api_response_time,
-                        "api_call_type": "openai_person_verification"
-                    })
-                    
+                    self.logger.error(
+                        "OpenAI API call failed",
+                        extra={
+                            "model": "gpt-4o-mini",
+                            "error": str(api_error),
+                            "response_time": api_response_time,
+                            "api_call_type": "openai_person_verification",
+                        },
+                    )
+
                     raise api_error
-                    
+
             response_content = response.content.strip()
             # Parse JSON response
             try:
                 # Clean the response to extract JSON
-                if '```json' in response_content:
-                    json_start = response_content.find('```json') + 7
-                    json_end = response_content.find('```', json_start)
+                if "```json" in response_content:
+                    json_start = response_content.find("```json") + 7
+                    json_end = response_content.find("```", json_start)
                     json_content = response_content[json_start:json_end].strip()
-                elif '{' in response_content:
-                    json_start = response_content.find('{')
-                    json_end = response_content.rfind('}') + 1
+                elif "{" in response_content:
+                    json_start = response_content.find("{")
+                    json_end = response_content.rfind("}") + 1
                     json_content = response_content[json_start:json_end]
                 else:
                     json_content = response_content
                 verification_result = json.loads(json_content)
                 # Validate required fields
-                required_fields = ['is_match', 'confidence_score', 'reasoning']
+                required_fields = ["is_match", "confidence_score", "reasoning"]
                 for field in required_fields:
                     if field not in verification_result:
                         verification_result[field] = None
                 return {
-                    'success': True,
-                    'verification': verification_result,
-                    'person_name': person_name,
-                    'target_name': target_person_name,
-                    'target_company': target_company_name,
-                    'search_type': search_type
+                    "success": True,
+                    "verification": verification_result,
+                    "person_name": person_name,
+                    "target_name": target_person_name,
+                    "target_company": target_company_name,
+                    "search_type": search_type,
                 }
             except json.JSONDecodeError as e:
                 print(f"‚ùå Failed to parse LLM verification response: {e}")
-                return {
-                    'success': False,
-                    'error': f'JSON parse error: {e}',
-                    'raw_response': response_content
-                }
+                return {"success": False, "error": f"JSON parse error: {e}", "raw_response": response_content}
         except Exception as e:
             print(f"‚ùå Person-company verification failed: {e}")
-            return {
-                'success': False,
-                'error': str(e)
-            }
+            return {"success": False, "error": str(e)}
 
     async def search_employee_by_name_and_company(self, api_key: str, full_name: str, company_name: str) -> Dict[str, Any]:
         """
         Search for an employee using name and company filters in Coresignal Employee API.
         Enhanced with multi-tier name matching and intelligent company name extraction.
         Simple early termination to prevent long searches.
-        
+
         Args:
             api_key: Coresignal API key
             full_name: Employee full name (e.g., "John Smith")
             company_name: Company name (e.g., "Example Capital")
-        
+
         Returns:
             Dict with search results or error
         """
@@ -393,78 +414,70 @@
             search_url = "https://api.coresignal.com/cdapi/v2/employee_clean/search/es_dsl"
             collect_url = "https://api.coresignal.com/cdapi/v2/employee_clean/collect"
             headers = {"apikey": api_key, "Content-Type": "application/json"}
-            
+
             # Parse name components for multi-tier matching
             name_components = self.parse_name_components(full_name)
-            first_name = name_components['first_name']
-            surname = name_components['surname']
-            full_name_clean = name_components['full_name_clean']
-            
+            first_name = name_components["first_name"]
+            surname = name_components["surname"]
+            full_name_clean = name_components["full_name_clean"]
+
             print(f"üîç Name components: first_name='{first_name}', surname='{surname}', full_name_clean='{full_name_clean}'")
-            
+
             # üöÄ SIMPLE OPTIMIZATION: Focus on most effective strategies only
             company_search_strategies = [
                 ("First word only", company_name.split()[0] if company_name else ""),
-                ("Intelligent extraction", self.extract_intelligent_company_search_term(company_name))
+                ("Intelligent extraction", self.extract_intelligent_company_search_term(company_name)),
             ]
-            
+
             # Multi-tier name matching strategies - simplified
             name_search_strategies = []
-            
-            if self.search_config['enable_exact_search']:
+
+            if self.search_config["enable_exact_search"]:
                 name_search_strategies.append(("Exact full name", full_name_clean, "exact"))
-            if self.search_config['enable_surname_search']:
+            if self.search_config["enable_surname_search"]:
                 name_search_strategies.append(("Surname only", surname, "surname"))
-            
+
             verified_candidates = []
             total_attempts = 0
             start_time = time.time()
-            
+
             # Try each name strategy
             for name_strategy_name, search_name, search_type in name_search_strategies:
                 if not search_name.strip():
                     continue
-                    
+
                 print(f"\nüîç Name Strategy: {name_strategy_name} - Searching for '{search_name}'")
-                
+
                 # Try each company strategy for this name strategy
                 for company_strategy_name, search_company_name in company_search_strategies:
                     if not search_company_name.strip():
                         continue
-                    
+
                     # üöÄ SIMPLE EARLY TERMINATION: Check time and attempt limits
                     current_time = time.time()
                     elapsed_time = current_time - start_time
-                    
-                    if elapsed_time > self.search_config['max_search_time_seconds']:
+
+                    if elapsed_time > self.search_config["max_search_time_seconds"]:
                         print(f"      ‚è∞ Time limit reached ({elapsed_time:.1f}s) - stopping search")
                         break
-                    
+
                     total_attempts += 1
-                    if total_attempts > self.search_config['max_search_attempts']:
+                    if total_attempts > self.search_config["max_search_attempts"]:
                         print(f"      ‚ö†Ô∏è Reached max search attempts ({self.search_config['max_search_attempts']}) - stopping search")
                         break
-                        
-                    print(f"   üìç Company Strategy: {company_strategy_name} - Company: '{search_company_name}' (Attempt {total_attempts}, {elapsed_time:.1f}s)")
-                    
+
+                    print(
+                        f"   üìç Company Strategy: {company_strategy_name} - Company: '{search_company_name}' (Attempt {total_attempts}, {elapsed_time:.1f}s)"
+                    )
+
                     # Build search query based on search type
                     if search_type == "exact":
                         # Exact name match
-                        name_query = {
-                            "query_string": {
-                                "default_field": "full_name",
-                                "query": f'"{search_name}"'
-                            }
-                        }
+                        name_query = {"query_string": {"default_field": "full_name", "query": f'"{search_name}"'}}
                     else:  # surname
                         # Surname match (broader)
-                        name_query = {
-                            "query_string": {
-                                "default_field": "full_name",
-                                "query": f'"{search_name}"'
-                            }
-                        }
-                    
+                        name_query = {"query_string": {"default_field": "full_name", "query": f'"{search_name}"'}}
+
                     search_query = {
                         "bool": {
                             "must": [
@@ -472,231 +485,254 @@
                                 {
                                     "nested": {
                                         "path": "experience",
-                                        "query": {
-                                            "query_string": {
-                                                "default_field": "experience.company_name",
-                                                "query": f'"{search_company_name}"'
-                                            }
-                                        }
+                                        "query": {"query_string": {"default_field": "experience.company_name", "query": f'"{search_company_name}"'}},
                                     }
-                                }
+                                },
                             ]
                         }
                     }
-                    
+
                     # API monitoring: Track CoreSignal search API call
                     search_api_call_start_time = time.time()
-                    
+
                     try:
                         async with aiohttp.ClientSession() as session:
                             async with session.post(search_url, headers=headers, json=search_query) as response:
                                 search_api_response_time = time.time() - search_api_call_start_time
-                                
+
                                 if response.status == 200:
                                     # API monitoring: Track successful CoreSignal search API call
-                                    self.logger.info("CoreSignal search API call successful", extra={
-                                        "api_call_type": "coresignal_search",
-                                        "response_time": search_api_response_time,
-                                        "person_name": full_name,
-                                        "company_name": company_name,
-                                        "search_strategy": f"{name_strategy_name} + {company_strategy_name}"
-                                    })
-                                    
+                                    self.logger.info(
+                                        "CoreSignal search API call successful",
+                                        extra={
+                                            "api_call_type": "coresignal_search",
+                                            "response_time": search_api_response_time,
+                                            "person_name": full_name,
+                                            "company_name": company_name,
+                                            "search_strategy": f"{name_strategy_name} + {company_strategy_name}",
+                                        },
+                                    )
+
                                     search_data = await response.json()
-                                    hits = search_data.get('hits', {}).get('hits', [])
-                                    
+                                    hits = search_data.get("hits", {}).get("hits", [])
+
                                     if hits:
                                         print(f"      ‚úÖ Found {len(hits)} potential matches")
-                                        
+
                                         for hit in hits:
-                                            employee_id = hit.get('_id')
+                                            employee_id = hit.get("_id")
                                             if not employee_id:
                                                 continue
-                                            
+
                                             # Get detailed employee data
                                             collect_api_call_start_time = time.time()
-                                            
+
                                             try:
                                                 async with session.post(collect_url, headers=headers, json={"ids": [employee_id]}) as detail_response:
                                                     collect_api_response_time = time.time() - collect_api_call_start_time
-                                                    
+
                                                     if detail_response.status == 200:
                                                         # API monitoring: Track successful CoreSignal collect API call
-                                                        self.logger.info("CoreSignal collect API call successful", extra={
-                                                            "api_call_type": "coresignal_collect",
-                                                            "response_time": collect_api_response_time,
-                                                            "employee_id": employee_id,
-                                                            "person_name": full_name
-                                                        })
-                                                        
+                                                        self.logger.info(
+                                                            "CoreSignal collect API call successful",
+                                                            extra={
+                                                                "api_call_type": "coresignal_collect",
+                                                                "response_time": collect_api_response_time,
+                                                                "employee_id": employee_id,
+                                                                "person_name": full_name,
+                                                            },
+                                                        )
+
                                                         employee_data = await detail_response.json()
-                                                        
+
                                                         # Use LLM to verify if this is the correct person
                                                         verification = await self.verify_person_company_match(
                                                             employee_data=employee_data,
                                                             target_person_name=full_name,
                                                             target_company_name=company_name,
                                                             sector_type="Financial Sector",
-                                                            search_type=search_type
+                                                            search_type=search_type,
                                                         )
-                                                        
-                                                        if verification.get('success') and verification.get('verification', {}).get('is_match'):
-                                                            confidence = verification.get('verification', {}).get('confidence_score', 0)
-                                                            reasoning = verification.get('verification', {}).get('reasoning', '')
-                                                            
+
+                                                        if verification.get("success") and verification.get("verification", {}).get("is_match"):
+                                                            confidence = verification.get("verification", {}).get("confidence_score", 0)
+                                                            reasoning = verification.get("verification", {}).get("reasoning", "")
+
                                                             # üöÄ SIMPLE CONFIDENCE-BASED TERMINATION
-                                                            if confidence >= self.search_config['high_confidence_threshold']:
-                                                                verified_candidates.append({
-                                                                    'employee_id': employee_id,
-                                                                    'employee_data': employee_data,
-                                                                    'confidence_score': confidence,
-                                                                    'reasoning': reasoning,
-                                                                    'strategy_used': f"{name_strategy_name} + {company_strategy_name}",
-                                                                    'search_company_name': search_company_name,
-                                                                    'search_name': search_name,
-                                                                    'search_type': search_type,
-                                                                    'verification': verification
-                                                                })
-                                                                
-                                                                print(f"      ‚úÖ HIGH CONFIDENCE MATCH: {employee_data.get('full_name', 'Unknown')} (confidence: {confidence}/10)")
+                                                            if confidence >= self.search_config["high_confidence_threshold"]:
+                                                                verified_candidates.append(
+                                                                    {
+                                                                        "employee_id": employee_id,
+                                                                        "employee_data": employee_data,
+                                                                        "confidence_score": confidence,
+                                                                        "reasoning": reasoning,
+                                                                        "strategy_used": f"{name_strategy_name} + {company_strategy_name}",
+                                                                        "search_company_name": search_company_name,
+                                                                        "search_name": search_name,
+                                                                        "search_type": search_type,
+                                                                        "verification": verification,
+                                                                    }
+                                                                )
+
+                                                                print(
+                                                                    f"      ‚úÖ HIGH CONFIDENCE MATCH: {employee_data.get('full_name', 'Unknown')} (confidence: {confidence}/10)"
+                                                                )
                                                                 print(f"         Reasoning: {reasoning}")
                                                                 print(f"         Search type: {search_type}")
-                                                                
+
                                                                 # üöÄ SIMPLE EARLY TERMINATION: Stop immediately for high confidence
                                                                 print(f"      üéØ HIGH CONFIDENCE MATCH FOUND - STOPPING SEARCH")
-                                                                verified_candidates.sort(key=lambda x: x['confidence_score'], reverse=True)
+                                                                verified_candidates.sort(key=lambda x: x["confidence_score"], reverse=True)
                                                                 best_candidate = verified_candidates[0]
-                                                                
+
                                                                 print(f"\nüéØ BEST VERIFIED MATCH (EARLY TERMINATION):")
                                                                 print(f"   Name: {best_candidate['employee_data'].get('full_name', 'Unknown')}")
                                                                 print(f"   Strategy: {best_candidate['strategy_used']}")
                                                                 print(f"   Search Type: {best_candidate['search_type']}")
                                                                 print(f"   Confidence: {best_candidate['confidence_score']}/10")
                                                                 print(f"   Company used in search: {best_candidate['search_company_name']}")
-                                                                
+
                                                                 return {
-                                                                    'success': True,
-                                                                    'employee_ids': [best_candidate['employee_id']],
-                                                                    'count': 1,
-                                                                    'verification_results': verified_candidates,
-                                                                    'best_match': best_candidate,
-                                                                    'strategy_used': best_candidate['strategy_used'],
-                                                                    'early_termination': True,
-                                                                    'total_attempts': total_attempts,
-                                                                    'search_time_seconds': elapsed_time
+                                                                    "success": True,
+                                                                    "employee_ids": [best_candidate["employee_id"]],
+                                                                    "count": 1,
+                                                                    "verification_results": verified_candidates,
+                                                                    "best_match": best_candidate,
+                                                                    "strategy_used": best_candidate["strategy_used"],
+                                                                    "early_termination": True,
+                                                                    "total_attempts": total_attempts,
+                                                                    "search_time_seconds": elapsed_time,
                                                                 }
-                                                            
+
                                                             else:
-                                                                print(f"      ‚ö†Ô∏è Low confidence match: {employee_data.get('full_name', 'Unknown')} (confidence: {confidence}/10)")
+                                                                print(
+                                                                    f"      ‚ö†Ô∏è Low confidence match: {employee_data.get('full_name', 'Unknown')} (confidence: {confidence}/10)"
+                                                                )
                                                     else:
                                                         # API monitoring: Track failed CoreSignal collect API call
-                                                        self.logger.error("CoreSignal collect API call failed", extra={
-                                                            "api_call_type": "coresignal_collect",
-                                                            "response_time": collect_api_response_time,
-                                                            "employee_id": employee_id,
-                                                            "person_name": full_name,
-                                                            "status_code": detail_response.status
-                                                        })
-                                                        
+                                                        self.logger.error(
+                                                            "CoreSignal collect API call failed",
+                                                            extra={
+                                                                "api_call_type": "coresignal_collect",
+                                                                "response_time": collect_api_response_time,
+                                                                "employee_id": employee_id,
+                                                                "person_name": full_name,
+                                                                "status_code": detail_response.status,
+                                                            },
+                                                        )
+
                                                         print(f"      ‚ùå Failed to get employee details for {employee_id}: {detail_response.status}")
-                                                        
+
                                             except Exception as collect_error:
                                                 # API monitoring: Track failed CoreSignal collect API call
                                                 collect_api_response_time = time.time() - collect_api_call_start_time
-                                                self.logger.error("CoreSignal collect API call failed", extra={
-                                                    "api_call_type": "coresignal_collect",
-                                                    "response_time": collect_api_response_time,
-                                                    "employee_id": employee_id,
-                                                    "person_name": full_name,
-                                                    "error": str(collect_error)
-                                                })
-                                                
+                                                self.logger.error(
+                                                    "CoreSignal collect API call failed",
+                                                    extra={
+                                                        "api_call_type": "coresignal_collect",
+                                                        "response_time": collect_api_response_time,
+                                                        "employee_id": employee_id,
+                                                        "person_name": full_name,
+                                                        "error": str(collect_error),
+                                                    },
+                                                )
+
                                                 print(f"      ‚ùå Error getting employee details for {employee_id}: {collect_error}")
                                     else:
                                         print(f"      ‚ö†Ô∏è No hits found for this strategy")
-                                        
+
                                 else:
                                     # API monitoring: Track failed CoreSignal search API call
-                                    self.logger.error("CoreSignal search API call failed", extra={
-                                        "api_call_type": "coresignal_search",
-                                        "response_time": search_api_response_time,
-                                        "person_name": full_name,
-                                        "company_name": company_name,
-                                        "search_strategy": f"{name_strategy_name} + {company_strategy_name}",
-                                        "status_code": response.status
-                                    })
-                                    
+                                    self.logger.error(
+                                        "CoreSignal search API call failed",
+                                        extra={
+                                            "api_call_type": "coresignal_search",
+                                            "response_time": search_api_response_time,
+                                            "person_name": full_name,
+                                            "company_name": company_name,
+                                            "search_strategy": f"{name_strategy_name} + {company_strategy_name}",
+                                            "status_code": response.status,
+                                        },
+                                    )
+
                                     print(f"      ‚ùå Search failed: {response.status}")
-                                    
+
                     except Exception as search_error:
                         # API monitoring: Track failed CoreSignal search API call
                         search_api_response_time = time.time() - search_api_call_start_time
-                        self.logger.error("CoreSignal search API call failed", extra={
-                            "api_call_type": "coresignal_search",
-                            "response_time": search_api_response_time,
-                            "person_name": full_name,
-                            "company_name": company_name,
-                            "search_strategy": f"{name_strategy_name} + {company_strategy_name}",
-                            "error": str(search_error)
-                        })
-                        
+                        self.logger.error(
+                            "CoreSignal search API call failed",
+                            extra={
+                                "api_call_type": "coresignal_search",
+                                "response_time": search_api_response_time,
+                                "person_name": full_name,
+                                "company_name": company_name,
+                                "search_strategy": f"{name_strategy_name} + {company_strategy_name}",
+                                "error": str(search_error),
+                            },
+                        )
+
                         print(f"      ‚ùå Search error: {search_error}")
-                
+
                 # üöÄ SIMPLE EARLY TERMINATION: Stop if we found any acceptable candidates
                 if verified_candidates:
                     print(f"      ‚úÖ Found acceptable candidates - stopping search")
                     break
-            
+
             # Sort verified candidates by confidence score
-            verified_candidates.sort(key=lambda x: x['confidence_score'], reverse=True)
-            
+            verified_candidates.sort(key=lambda x: x["confidence_score"], reverse=True)
+
             if verified_candidates:
                 # Return best verified candidate
                 best_candidate = verified_candidates[0]
-                
+
                 print(f"\nüéØ BEST VERIFIED MATCH:")
                 print(f"   Name: {best_candidate['employee_data'].get('full_name', 'Unknown')}")
                 print(f"   Strategy: {best_candidate['strategy_used']}")
                 print(f"   Search Type: {best_candidate['search_type']}")
                 print(f"   Confidence: {best_candidate['confidence_score']}/10")
                 print(f"   Company used in search: {best_candidate['search_company_name']}")
-                
+
                 return {
-                    'success': True,
-                    'employee_ids': [best_candidate['employee_id']],
-                    'count': 1,
-                    'verification_results': verified_candidates,
-                    'best_match': best_candidate,
-                    'strategy_used': best_candidate['strategy_used'],
-                    'total_attempts': total_attempts,
-                    'search_time_seconds': time.time() - start_time
+                    "success": True,
+                    "employee_ids": [best_candidate["employee_id"]],
+                    "count": 1,
+                    "verification_results": verified_candidates,
+                    "best_match": best_candidate,
+                    "strategy_used": best_candidate["strategy_used"],
+                    "total_attempts": total_attempts,
+                    "search_time_seconds": time.time() - start_time,
                 }
             else:
                 print(f"\n‚ùå No verified matches found for {full_name} at {company_name} using any search strategy")
                 return {
-                    'success': False,
-                    'employee_ids': [],
-                    'count': 0,
-                    'verification_results': [],
-                    'total_attempts': total_attempts,
-                    'search_time_seconds': time.time() - start_time,
-                    'error': f"No verified matches found for {full_name} at {company_name}"
+                    "success": False,
+                    "employee_ids": [],
+                    "count": 0,
+                    "verification_results": [],
+                    "total_attempts": total_attempts,
+                    "search_time_seconds": time.time() - start_time,
+                    "error": f"No verified matches found for {full_name} at {company_name}",
                 }
-                
+
         except Exception as e:
             print(f"‚ùå Error in employee search: {e}")
-            return {
-                'success': False,
-                'employee_ids': [],
-                'count': 0,
-                'verification_results': [],
-                'error': str(e)
-            }
+            return {"success": False, "employee_ids": [], "count": 0, "verification_results": [], "error": str(e)}
 
-    async def execute(self, company_data: Dict[str, Any], run_id: str = None, company_id: str = None, shared_output_file: str = None, db: ProspectingDB = None, postgres_enabled: bool = None, user_id: str = None, session_id: str = None) -> Dict[str, Any]:
+    async def execute(
+        self,
+        company_data: Dict[str, Any],
+        run_id: str = None,
+        company_id: str = None,
+        shared_output_file: str = None,
+        db: ProspectingDB = None,
+        postgres_enabled: bool = None,
+        user_id: str = None,
+        session_id: str = None,
+    ) -> Dict[str, Any]:
         """
         Execute Person Enrich Agent to extract and enrich person data for investment outreach.
-        
+
         Args:
             company_data: Dictionary containing company information
             run_id: Optional run ID for tracking
@@ -706,24 +742,27 @@
             postgres_enabled: Whether to store results in PostgreSQL (auto-detected if None)
             user_id: User identifier for data isolation
             session_id: Session identifier for tracking
-        
+
         Returns:
             Dictionary with enriched person data
         """
         start_time = time.time()
-        
-        self.logger.info("Person enrichment execution started", extra={
-            "run_id": run_id,
-            "user_id": user_id,
-            "session_id": session_id,
-            "company_name": company_data.get('name', ''),
-            "company_id": company_id
-        })
-        
+
+        self.logger.info(
+            "Person enrichment execution started",
+            extra={
+                "run_id": run_id,
+                "user_id": user_id,
+                "session_id": session_id,
+                "company_name": company_data.get("name", ""),
+                "company_id": company_id,
+            },
+        )
+
         # Use provided database or fall back to self.db
         if db is None:
             db = self.db
-        
+
         # Initialize PostgreSQL connection if not provided (only for reading data)
         if not db:
             postgres_enabled = get_enable_postgres_storage()
@@ -737,7 +776,6 @@
                     postgres_enabled = False
                     db = None
 
-
         if get_enable_youtube_ingest() and db and not self.youtube_service:
             try:
                 self.youtube_service = YouTubeMediaService(db)
@@ -745,18 +783,17 @@
                 self.logger.warning("Failed to initialize YouTube media service", extra={"error": str(youtube_error)})
 
         try:
-            company_name = company_data.get('name', '')
+            company_name = company_data.get("name", "")
             print(f"üîÑ Starting Person Enrich Agent for {company_name}")
             # Progress: person enrichment start (seed)
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'person_enrichment_progress', 0.05)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "person_enrichment_progress", 0.05)
             except Exception:
                 pass
 
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: Person Enrichment", 
-                                         f"Starting person enrichment for {company_name}")
+                await self.append_markdown(shared_output_file, "Step 6: Person Enrichment", f"Starting person enrichment for {company_name}")
 
             if not db:
                 raise ValueError("Database connection required for person enrichment - PostgreSQL must be enabled or database connection provided")
@@ -764,36 +801,32 @@
             # Step 1: Retrieve CoreSignal data from PostgreSQL
             print(f"üìä Retrieving CoreSignal data from PostgreSQL...")
             self.logger.info("Retrieving CoreSignal data from PostgreSQL", extra={"run_id": run_id, "company_id": company_id})
-            
+
             if not company_id:
                 raise ValueError("company_id is required for Person Enrich Agent - cannot proceed without it")
-            
+
             print(f"üîç Looking up CoreSignal data by company_id: {company_id}")
-            coresignal_data = await db.get_agent_data_by_company_id(company_id, 'Coresignal Agent', run_id, 'coresignal_extracted_fields', user_id)
+            coresignal_data = await db.get_agent_data_by_company_id(company_id, "Coresignal Agent", run_id, "coresignal_extracted_fields", user_id)
             youtube_media = None
             try:
                 youtube_media = await db.get_youtube_media_by_run(run_id, user_id)
             except Exception as youtube_error:
-                self.logger.warning("Failed to fetch YouTube media result", extra={"run_id": run_id, "company_id": company_id, "error": str(youtube_error)})
-            
+                self.logger.warning(
+                    "Failed to fetch YouTube media result", extra={"run_id": run_id, "company_id": company_id, "error": str(youtube_error)}
+                )
+
             if not coresignal_data:
                 error_msg = f"No CoreSignal data found for {company_name}"
                 print(f"‚ùå {error_msg}")
                 self.logger.error("No CoreSignal data found", extra={"run_id": run_id, "company_name": company_name, "company_id": company_id})
                 if shared_output_file:
-                    await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Error",
-                                             f"‚ùå {error_msg}")
-                return {
-                    'success': False,
-                    'error': error_msg,
-                    'agent_name': self.agent_name,
-                    'company_name': company_name
-                }
+                    await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Error", f"‚ùå {error_msg}")
+                return {"success": False, "error": error_msg, "agent_name": self.agent_name, "company_name": company_name}
 
             # Step 2: Retrieve Perplexity data from PostgreSQL
             print(f"üìä Retrieving Perplexity data from PostgreSQL...")
             self.logger.info("Retrieving Perplexity data from PostgreSQL", extra={"run_id": run_id, "company_id": company_id})
-            perplexity_data = await db.get_agent_data_by_company_id(company_id, 'Web Research Agent', run_id, 'perplexity_merged_data', user_id)
+            perplexity_data = await db.get_agent_data_by_company_id(company_id, "Web Research Agent", run_id, "perplexity_merged_data", user_id)
             # Pull YouTube candidate people (if any) to augment exec list
             youtube_candidates = []
             try:
@@ -802,14 +835,14 @@
                     youtube_candidates = youtube_media_payload.get("candidate_people") or []
             except Exception as yt_cand_err:
                 self.logger.warning("Failed to fetch YouTube candidate people", extra={"run_id": run_id, "error": str(yt_cand_err)})
-            
+
             # Step 3: Extract executives from both sources
-            key_executives = coresignal_data.get('key_executives', [])
+            key_executives = coresignal_data.get("key_executives", [])
             c_suite_members = []
-            
+
             if perplexity_data:
-                organization_data = perplexity_data.get('organization_decision_making', {})
-                c_suite_members = organization_data.get('c_suite_members', [])
+                organization_data = perplexity_data.get("organization_decision_making", {})
+                c_suite_members = organization_data.get("c_suite_members", [])
                 print(f"‚úÖ Found {len(c_suite_members)} c-suite members in Perplexity data")
                 self.logger.info("Found c-suite members in Perplexity data", extra={"run_id": run_id, "c_suite_count": len(c_suite_members)})
             else:
@@ -821,10 +854,7 @@
                 for cand in youtube_candidates:
                     if not cand:
                         continue
-                    exists = any(
-                        isinstance(member, dict) and member.get("member_full_name") == cand
-                        for member in c_suite_members
-                    )
+                    exists = any(isinstance(member, dict) and member.get("member_full_name") == cand for member in c_suite_members)
                     if exists:
                         continue
                     c_suite_members.append(
@@ -834,183 +864,193 @@
                             "search_required": True,
                         }
                     )
-            
-            self.logger.info("Executive data extracted", extra={
-                "run_id": run_id,
-                "key_executives_count": len(key_executives),
-                "c_suite_members_count": len(c_suite_members),
-                "youtube_candidate_count": len(youtube_candidates)
-            })
+
+            self.logger.info(
+                "Executive data extracted",
+                extra={
+                    "run_id": run_id,
+                    "key_executives_count": len(key_executives),
+                    "c_suite_members_count": len(c_suite_members),
+                    "youtube_candidate_count": len(youtube_candidates),
+                },
+            )
             # Progress: if we know counts, update a coarse fraction
             try:
                 if db and run_id:
                     total_people = max(1, (len(key_executives) or 0) + (len(c_suite_members) or 0))
                     # Placeholder: treat extraction as halfway if any found
                     frac = 0.5 if total_people > 0 else 0.1
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'person_enrichment_progress', frac)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "person_enrichment_progress", frac)
             except Exception:
                 pass
-            
+
             if not key_executives and not c_suite_members:
                 error_msg = f"No executives found in either CoreSignal or Perplexity data for {company_name}"
                 print(f"‚ùå {error_msg}")
                 self.logger.error("No executives found in any data source", extra={"run_id": run_id, "company_name": company_name})
                 if shared_output_file:
-                    await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Error",
-                                             f"‚ùå {error_msg}")
-                return {
-                    'success': False,
-                    'error': error_msg,
-                    'agent_name': self.agent_name,
-                    'company_name': company_name
-                }
+                    await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Error", f"‚ùå {error_msg}")
+                return {"success": False, "error": error_msg, "agent_name": self.agent_name, "company_name": company_name}
 
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Executives Found",
-                    f"‚úÖ Found {len(key_executives)} CoreSignal executives + {len(c_suite_members)} Perplexity c-suite members")
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 6: Person Enrichment - Executives Found",
+                    f"‚úÖ Found {len(key_executives)} CoreSignal executives + {len(c_suite_members)} Perplexity c-suite members",
+                )
 
             # Step 4: Combine executives for LLM analysis
             print(f"ü§ñ Analyzing {len(key_executives)} CoreSignal executives + {len(c_suite_members)} Perplexity c-suite members...")
-            self.logger.info("Starting LLM analysis of executives", extra={
-                "run_id": run_id,
-                "key_executives_count": len(key_executives),
-                "c_suite_members_count": len(c_suite_members),
-                "total_executives": len(key_executives) + len(c_suite_members)
-            })
-            
+            self.logger.info(
+                "Starting LLM analysis of executives",
+                extra={
+                    "run_id": run_id,
+                    "key_executives_count": len(key_executives),
+                    "c_suite_members_count": len(c_suite_members),
+                    "total_executives": len(key_executives) + len(c_suite_members),
+                },
+            )
+
             # Prepare combined input for the LLM
-            combined_executives = {
-                "key_executives": key_executives,
-                "c_suite_members": c_suite_members
-            }
+            combined_executives = {"key_executives": key_executives, "c_suite_members": c_suite_members}
             executives_json = json.dumps(combined_executives, indent=2, ensure_ascii=False)
-            
+
             # Create messages for the LLM
             system_message = SystemMessage(content=get_person_enrichment_prompt(num_people=self.num_people))
             human_message = HumanMessage(content=f"Key executives data:\n\n{executives_json}")
-            
+
             # Get LLM response using GPT-5-mini for better deduplication
             print(f"ü§ñ Using GPT-5-mini for executive deduplication and ranking...")
-            self.logger.info("Starting LLM executive deduplication", extra={
-                "run_id": run_id,
-                "model": "gpt-5-mini",
-                "executives_count": len(key_executives),
-                "c_suite_count": len(c_suite_members),
-                "input_length": len(executives_json)
-            })
-            
+            self.logger.info(
+                "Starting LLM executive deduplication",
+                extra={
+                    "run_id": run_id,
+                    "model": "gpt-5-mini",
+                    "executives_count": len(key_executives),
+                    "c_suite_count": len(c_suite_members),
+                    "input_length": len(executives_json),
+                },
+            )
+
             response = None
             response_content = None
-            
+
             try:
-                with trace_operation("executive_deduplication", {
-                    "company_name": company_name,
-                    "executives_count": len(key_executives),
-                    "model": "gpt-5-mini"
-                }):
+                with trace_operation(
+                    "executive_deduplication", {"company_name": company_name, "executives_count": len(key_executives), "model": "gpt-5-mini"}
+                ):
                     print(f"ü§ñ Calling GPT-5-mini API for executive deduplication...")
                     response = await self.llm_deduplication.ainvoke([system_message, human_message])
                     print(f"‚úÖ GPT-5-mini API call completed")
-                    
+
                     # Validate response before processing
                     if not response:
                         raise Exception("No response object from GPT-5-mini API")
-                    
-                    if not hasattr(response, 'content'):
+
+                    if not hasattr(response, "content"):
                         raise Exception(f"Response missing 'content' attribute. Response type: {type(response)}")
-                    
+
                     if not response.content:
                         raise Exception("Empty response content from GPT-5-mini API")
-                    
+
                     response_content = response.content.strip()
-                    
-                    self.logger.info("LLM executive deduplication completed", extra={
-                        "run_id": run_id,
-                        "model": "gpt-5-mini",
-                        "response_length": len(response_content),
-                        "response_type": type(response).__name__
-                    })
-                    
+
+                    self.logger.info(
+                        "LLM executive deduplication completed",
+                        extra={
+                            "run_id": run_id,
+                            "model": "gpt-5-mini",
+                            "response_length": len(response_content),
+                            "response_type": type(response).__name__,
+                        },
+                    )
+
             except Exception as api_error:
                 error_msg = f"GPT-5-mini API call failed: {api_error}"
                 print(f"‚ùå {error_msg}")
-                self.logger.error("LLM executive deduplication API call failed", extra={
-                    "run_id": run_id,
-                    "model": "gpt-5-mini",
-                    "error": str(api_error),
-                    "error_type": type(api_error).__name__,
-                    "has_response": response is not None,
-                    "response_type": type(response).__name__ if response else None,
-                    "has_content": hasattr(response, 'content') if response else False,
-                    "content_length": len(response.content) if response and hasattr(response, 'content') and response.content else 0
-                })
+                self.logger.error(
+                    "LLM executive deduplication API call failed",
+                    extra={
+                        "run_id": run_id,
+                        "model": "gpt-5-mini",
+                        "error": str(api_error),
+                        "error_type": type(api_error).__name__,
+                        "has_response": response is not None,
+                        "response_type": type(response).__name__ if response else None,
+                        "has_content": hasattr(response, "content") if response else False,
+                        "content_length": len(response.content) if response and hasattr(response, "content") and response.content else 0,
+                    },
+                )
                 raise
-            
+
             # Parse the JSON response
             try:
                 # First attempt: try to parse the response directly
                 selection_result = json.loads(response_content)
-                
+
                 # Validate the response structure
                 if "ranked_people" not in selection_result or not isinstance(selection_result["ranked_people"], list):
                     raise ValueError("Response must contain 'ranked_people' array")
-                
+
                 ranked_people = selection_result["ranked_people"]
-                
+
                 if not ranked_people:
                     raise ValueError("No people were ranked by the LLM")
-                
+
                 # Validate each ranked person
                 required_fields = ["rank", "data_source", "member_full_name", "member_position_title", "search_required"]
                 for person in ranked_people:
                     if not all(field in person for field in required_fields):
                         raise ValueError(f"Person missing required fields: {required_fields}")
-                
+
                 print(f"‚úÖ LLM ranked {len(ranked_people)} people for enrichment consideration")
                 self.logger.info("LLM executive ranking successful", extra={"run_id": run_id, "ranked_people_count": len(ranked_people)})
-                
+
             except json.JSONDecodeError as e:
                 print(f"‚ö†Ô∏è JSON parsing failed: {e}")
                 print(f"‚ö†Ô∏è Response length: {len(response_content) if response_content else 0} characters")
                 print(f"‚ö†Ô∏è Response preview: {response_content[:500] if response_content else '(empty)'}...")
-                self.logger.error("LLM response JSON parsing failed", extra={
-                    "run_id": run_id,
-                    "model": "gpt-5-mini",
-                    "error": str(e),
-                    "error_type": type(e).__name__,
-                    "response_length": len(response_content) if response_content else 0,
-                    "response_preview": response_content[:500] if response_content else None,
-                    "has_response_content": response_content is not None
-                })
-                
+                self.logger.error(
+                    "LLM response JSON parsing failed",
+                    extra={
+                        "run_id": run_id,
+                        "model": "gpt-5-mini",
+                        "error": str(e),
+                        "error_type": type(e).__name__,
+                        "response_length": len(response_content) if response_content else 0,
+                        "response_preview": response_content[:500] if response_content else None,
+                        "has_response_content": response_content is not None,
+                    },
+                )
+
                 # Try to extract JSON from markdown code blocks if present
                 try:
-                    if '```json' in response_content:
-                        json_start = response_content.find('```json') + 7
-                        json_end = response_content.find('```', json_start)
+                    if "```json" in response_content:
+                        json_start = response_content.find("```json") + 7
+                        json_end = response_content.find("```", json_start)
                         if json_end > json_start:
                             json_content = response_content[json_start:json_end].strip()
                             selection_result = json.loads(json_content)
                             print("‚úÖ Successfully extracted JSON from markdown code blocks")
                         else:
                             raise ValueError("Incomplete markdown code block")
-                    elif '{' in response_content:
+                    elif "{" in response_content:
                         # Try to find the first complete JSON object
                         brace_count = 0
                         json_start = -1
                         json_end = -1
-                        
+
                         for i, char in enumerate(response_content):
-                            if char == '{':
+                            if char == "{":
                                 if json_start == -1:
                                     json_start = i
                                 brace_count += 1
-                            elif char == '}':
+                            elif char == "}":
                                 brace_count -= 1
                                 if brace_count == 0 and json_start != -1:
                                     json_end = i + 1
                                     break
-                        
+
                         if json_start != -1 and json_end != -1:
                             json_content = response_content[json_start:json_end]
                             selection_result = json.loads(json_content)
@@ -1019,232 +1059,222 @@
                             raise ValueError("Could not find complete JSON object")
                     else:
                         raise ValueError("No JSON structure found in response")
-                    
+
                     # Validate the extracted response
                     if "ranked_people" not in selection_result or not isinstance(selection_result["ranked_people"], list):
                         raise ValueError("Extracted response must contain 'ranked_people' array")
-                    
+
                     ranked_people = selection_result["ranked_people"]
-                    
+
                     if not ranked_people:
                         raise ValueError("No people were ranked in extracted response")
-                    
+
                     # Validate each ranked person
                     required_fields = ["rank", "data_source", "member_full_name", "member_position_title", "search_required"]
                     for person in ranked_people:
                         if not all(field in person for field in required_fields):
                             raise ValueError(f"Person missing required fields: {required_fields}")
-                    
+
                     print(f"‚úÖ LLM ranked {len(ranked_people)} people for enrichment consideration (extracted)")
-                
+
                 except Exception as extraction_error:
                     print(f"‚ùå JSON extraction failed: {extraction_error}")
                     print(f"‚ùå Falling back to simple ranking based on CoreSignal data only")
-                    
+
                     # Fallback: create simple ranking from CoreSignal data only
                     ranked_people = []
                     for i, person in enumerate(key_executives[:10], 1):  # Limit to top 10
                         name = person.get("member_full_name") or person.get("name") or "Unknown"
                         title = person.get("member_position_title") or person.get("title") or "Unknown"
-                        ranked_people.append({
-                            "rank": i,
-                            "data_source": "coresignal",
-                            "parent_id": person.get("parent_id"),
-                            "member_full_name": name,
-                            "member_position_title": title,
-                            "search_required": False
-                        })
-                    
+                        ranked_people.append(
+                            {
+                                "rank": i,
+                                "data_source": "coresignal",
+                                "parent_id": person.get("parent_id"),
+                                "member_full_name": name,
+                                "member_position_title": title,
+                                "search_required": False,
+                            }
+                        )
+
                     print(f"‚úÖ Created fallback ranking with {len(ranked_people)} people from CoreSignal data")
-                
+
             except Exception as validation_error:
                 print(f"‚ùå Response validation failed: {validation_error}")
                 print(f"‚ùå Falling back to simple ranking based on CoreSignal data only")
-                
+
                 # Fallback: create simple ranking from CoreSignal data only
                 ranked_people = []
                 for i, person in enumerate(key_executives[:10], 1):  # Limit to top 10
                     name = person.get("member_full_name") or person.get("name") or "Unknown"
                     title = person.get("member_position_title") or person.get("title") or "Unknown"
-                    ranked_people.append({
-                        "rank": i,
-                        "data_source": "coresignal",
-                        "parent_id": person.get("parent_id"),
-                        "member_full_name": name,
-                        "member_position_title": title,
-                        "search_required": False
-                    })
-                
+                    ranked_people.append(
+                        {
+                            "rank": i,
+                            "data_source": "coresignal",
+                            "parent_id": person.get("parent_id"),
+                            "member_full_name": name,
+                            "member_position_title": title,
+                            "search_required": False,
+                        }
+                    )
+
                 print(f"‚úÖ Created fallback ranking with {len(ranked_people)} people from CoreSignal data")
 
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Ranked People",
-                    f"**All Ranked People for Investment Outreach:**\n\n"
-                    f"```json\n{json.dumps(ranked_people, indent=2, ensure_ascii=False)}\n```")
-            
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 6: Person Enrichment - Ranked People",
+                    f"**All Ranked People for Investment Outreach:**\n\n```json\n{json.dumps(ranked_people, indent=2, ensure_ascii=False)}\n```",
+                )
+
             # Step 5: Sequential enrichment with fallback until we get target number of people
             print(f"üîÑ Starting sequential enrichment to get {self.num_people} successful enrichments...")
-            
+
             final_results = []
             attempts = 0
             max_attempts = min(len(ranked_people), 15)  # Limit attempts to prevent excessive API calls
-            
+
             for person in ranked_people:
                 if len(final_results) >= self.num_people:
                     print(f"‚úÖ Target of {self.num_people} successful enrichments reached")
                     break
-                
+
                 attempts += 1
                 if attempts > max_attempts:
                     print(f"‚ö†Ô∏è Maximum attempts ({max_attempts}) reached, stopping enrichment")
                     break
-                
+
                 rank = person.get("rank", attempts)
                 person_name = person.get("member_full_name", "Unknown")
                 data_source = person.get("data_source", "unknown")
                 search_required = person.get("search_required", False)
-                
+
                 print(f"üîÑ Attempting enrichment for rank {rank}: {person_name} ({data_source})")
-                
+
                 # Handle CoreSignal search if required (for Perplexity sources)
                 if search_required and data_source == "perplexity":
                     print(f"üîç Searching CoreSignal for {person_name}...")
-                    
+
                     api_key = get_coresignal_api_key()
                     if not api_key:
                         print(f"‚ùå No CoreSignal API key available for search, skipping {person_name}")
                         continue
-                    
-                    search_result = await self.search_employee_by_name_and_company(
-                        api_key=api_key,
-                        full_name=person_name,
-                        company_name=company_name
-                    )
-                    
-                    if not search_result.get('success') or not search_result.get('employee_ids'):
+
+                    search_result = await self.search_employee_by_name_and_company(api_key=api_key, full_name=person_name, company_name=company_name)
+
+                    if not search_result.get("success") or not search_result.get("employee_ids"):
                         print(f"‚ùå No CoreSignal data found for {person_name}, skipping to next rank")
                         continue
-                    
+
                     # Use first employee ID found
-                    employee_ids = search_result['employee_ids']
-                    person['parent_id'] = str(employee_ids[0])  # Convert to string for consistency
+                    employee_ids = search_result["employee_ids"]
+                    person["parent_id"] = str(employee_ids[0])  # Convert to string for consistency
                     print(f"‚úÖ Found CoreSignal parent_id {person['parent_id']} for {person_name}")
-                
+
                 # Proceed with enrichment
                 try:
-                    self.logger.info("Starting person enrichment", extra={
-                        "run_id": run_id,
-                        "rank": rank,
-                        "person_name": person_name,
-                        "data_source": person.get('data_source', 'unknown')
-                    })
-                    
+                    self.logger.info(
+                        "Starting person enrichment",
+                        extra={"run_id": run_id, "rank": rank, "person_name": person_name, "data_source": person.get("data_source", "unknown")},
+                    )
+
                     result = await self._enrich_single_person(
                         person=person,
                         company_name=company_name,
-                        company_website=coresignal_data.get('website'),
-                        run_id=run_id or 'default',
+                        company_website=coresignal_data.get("website"),
+                        run_id=run_id or "default",
                         company_id=company_id,
                         shared_output_file=shared_output_file,
                         user_id=user_id,
-                        youtube_media_shared=youtube_media
+                        youtube_media_shared=youtube_media,
                     )
-                    
+
                     # Attach YouTube media payload for downstream consumers
-                    if 'youtube_media' not in result:
-                        result['youtube_media'] = youtube_media
-                    
-                    if result.get('enrichment_success', False):
+                    if "youtube_media" not in result:
+                        result["youtube_media"] = youtube_media
+
+                    if result.get("enrichment_success", False):
                         final_results.append(result)
                         print(f"‚úÖ Successfully enriched rank {rank}: {person_name}")
-                        self.logger.info("Person enrichment completed successfully", extra={
-                            "run_id": run_id,
-                            "rank": rank,
-                            "person_name": person_name
-                        })
+                        self.logger.info(
+                            "Person enrichment completed successfully", extra={"run_id": run_id, "rank": rank, "person_name": person_name}
+                        )
                     else:
                         print(f"‚ùå Enrichment failed for rank {rank}: {person_name} - {result.get('error', 'Unknown error')}")
-                        self.logger.error("Person enrichment failed", extra={
-                            "run_id": run_id,
-                            "rank": rank,
-                            "person_name": person_name,
-                            "error": result.get('error', 'Unknown error')
-                        })
-            
+                        self.logger.error(
+                            "Person enrichment failed",
+                            extra={"run_id": run_id, "rank": rank, "person_name": person_name, "error": result.get("error", "Unknown error")},
+                        )
+
                 except Exception as e:
                     print(f"‚ùå Exception during enrichment for rank {rank}: {person_name} - {str(e)}")
-                    self.logger.exception("Exception during person enrichment", extra={
-                        "run_id": run_id,
-                        "rank": rank,
-                        "person_name": person_name,
-                        "error": str(e)
-                    })
+                    self.logger.exception(
+                        "Exception during person enrichment", extra={"run_id": run_id, "rank": rank, "person_name": person_name, "error": str(e)}
+                    )
                     continue
-            
+
             print(f"‚úÖ Sequential enrichment completed: {len(final_results)} successful out of {attempts} attempts")
-            self.logger.info("Sequential enrichment completed", extra={
-                "run_id": run_id,
-                "successful_enrichments": len(final_results),
-                "total_attempts": attempts
-            })
-            
+            self.logger.info(
+                "Sequential enrichment completed", extra={"run_id": run_id, "successful_enrichments": len(final_results), "total_attempts": attempts}
+            )
+
             # Step 6: Store results in PostgreSQL with provided company_id
             postgres_enabled = get_enable_postgres_storage()
             if postgres_enabled and db:
                 print(f"üíæ Storing enrichment results in PostgreSQL...")
-                self.logger.info("Starting PostgreSQL storage of person enrichment results", extra={
-                    "run_id": run_id,
-                    "company_id": company_id,
-                    "results_count": len(final_results)
-                })
-                
+                self.logger.info(
+                    "Starting PostgreSQL storage of person enrichment results",
+                    extra={"run_id": run_id, "company_id": company_id, "results_count": len(final_results)},
+                )
+
                 try:
                     # Use provided company_id
                     if company_id:
                         # Store person enrichment results with provided company_id
                         stored_ids = await db.store_person_enrichment_batch(
-                            run_id=run_id,
-                            user_id=user_id,
-                            session_id=session_id,
-                            company_id=company_id,
-                            person_enrichment_results=final_results
+                            run_id=run_id, user_id=user_id, session_id=session_id, company_id=company_id, person_enrichment_results=final_results
                         )
                         print(f"‚úÖ Stored {len(stored_ids)} person enrichment results in PostgreSQL with company_id: {company_id}")
-                        self.logger.info("Person enrichment results stored in PostgreSQL", extra={
-                            "run_id": run_id,
-                            "company_id": company_id,
-                            "stored_ids_count": len(stored_ids),
-                            "stored_ids": stored_ids
-                        })
-                        
+                        self.logger.info(
+                            "Person enrichment results stored in PostgreSQL",
+                            extra={"run_id": run_id, "company_id": company_id, "stored_ids_count": len(stored_ids), "stored_ids": stored_ids},
+                        )
+
                         if shared_output_file:
-                            await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Database Storage",
+                            await self.append_markdown(
+                                shared_output_file,
+                                "Step 6: Person Enrichment - Database Storage",
                                 f"‚úÖ Stored {len(stored_ids)} person enrichment results in PostgreSQL\n"
                                 f"- Company ID: {company_id}\n"
-                                f"- Result IDs: {stored_ids}")
+                                f"- Result IDs: {stored_ids}",
+                            )
                     else:
                         print(f"‚ö†Ô∏è No company_id provided for {company_name}, skipping database storage")
-                        self.logger.warning("No company_id provided, skipping database storage", extra={
-                            "run_id": run_id,
-                            "company_name": company_name
-                        })
+                        self.logger.warning(
+                            "No company_id provided, skipping database storage", extra={"run_id": run_id, "company_name": company_name}
+                        )
                         if shared_output_file:
-                            await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Storage Warning",
-                                f"‚ö†Ô∏è No company_id provided, skipping database storage")
+                            await self.append_markdown(
+                                shared_output_file,
+                                "Step 6: Person Enrichment - Storage Warning",
+                                f"‚ö†Ô∏è No company_id provided, skipping database storage",
+                            )
                 except Exception as e:
                     print(f"‚ùå Failed to store person enrichment results: {e}")
-                    self.logger.error("Failed to store person enrichment results in PostgreSQL", extra={
-                        "run_id": run_id,
-                        "company_id": company_id,
-                        "error": str(e)
-                    })
+                    self.logger.error(
+                        "Failed to store person enrichment results in PostgreSQL", extra={"run_id": run_id, "company_id": company_id, "error": str(e)}
+                    )
                     if shared_output_file:
-                        await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Storage Error",
-                            f"‚ùå Failed to store results in database: {e}")
-            
+                        await self.append_markdown(
+                            shared_output_file, "Step 6: Person Enrichment - Storage Error", f"‚ùå Failed to store results in database: {e}"
+                        )
+
             # Step 7: Print final results to agent_test.md for evaluation
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Final Results for Evaluation",
+                await self.append_markdown(
+                    shared_output_file,
+                    "Step 6: Person Enrichment - Final Results for Evaluation",
                     f"**EXACT JSON OUTPUT SENT TO POSTGRESQL:**\n\n"
                     f"```json\n{json.dumps(final_results, indent=2, ensure_ascii=False)}\n```\n\n"
                     f"**Processing Summary:**\n"
@@ -1252,160 +1282,160 @@
                     f"- Enrichment attempts: {attempts}\n"
                     f"- Successfully enriched: {len(final_results)}\n"
                     f"- Target people: {self.num_people}\n"
-                    f"- Database storage: {'‚úÖ Enabled' if postgres_enabled else '‚ùå Disabled'}")
-            
+                    f"- Database storage: {'‚úÖ Enabled' if postgres_enabled else '‚ùå Disabled'}",
+                )
+
             execution_time_ms = int((time.time() - start_time) * 1000)
-            
+
             # Step 8: Write results to JSON file (temporary feature for data retrieval)
             try:
                 from pathlib import Path
-                
+
                 # Create output directory if it doesn't exist
                 output_dir = Path(self.output_dir)
                 output_dir.mkdir(parents=True, exist_ok=True)
-                
+
                 # Generate filename with timestamp and company_name
                 timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-                company_name_safe = company_name.replace(' ', '_').replace('/', '_').replace(',', '').replace('(', '').replace(')', '')
+                company_name_safe = company_name.replace(" ", "_").replace("/", "_").replace(",", "").replace("(", "").replace(")", "")
                 filename = f"person_enrichment_{company_name_safe}_{timestamp}.json"
                 filepath = output_dir / filename
-                
+
                 print(f"DEBUG: Company name: '{company_name}'")
                 print(f"DEBUG: Company name safe: '{company_name_safe}'")
                 print(f"DEBUG: Generated filename: '{filename}'")
                 print(f"DEBUG: Full filepath: '{filepath}'")
-                
+
                 # Write final results to JSON file (dev-only; skip when disabled)
                 import os
+
                 if os.getenv("ENABLE_FILE_DEBUG_OUTPUT", "false").lower() == "true":
+
                     def json_serializer(obj):
                         """Custom JSON serializer for datetime and other objects."""
-                        if hasattr(obj, 'isoformat'):
+                        if hasattr(obj, "isoformat"):
                             return obj.isoformat()
                         return str(obj)
-                    with open(filepath, 'w', encoding='utf-8') as f:
+
+                    with open(filepath, "w", encoding="utf-8") as f:
                         json.dump(final_results, f, indent=2, ensure_ascii=False, default=json_serializer)
                     print(f"üìÑ Person enrichment results written to: {filepath}")
-                
+
                 if shared_output_file:
                     await self.append_markdown(
-                        shared_output_file,
-                        "Step 6: Person Enrichment - JSON File Output",
-                        f"üìÑ Results written to: `{filepath}`"
+                        shared_output_file, "Step 6: Person Enrichment - JSON File Output", f"üìÑ Results written to: `{filepath}`"
                     )
-                    
+
             except Exception as json_error:
                 print(f"‚ö†Ô∏è Failed to write person enrichment results to JSON file: {json_error}")
-            
+
             return {
-                'success': True,
-                'enriched_people': final_results,
-                'people_ranked': len(ranked_people),
-                'enrichment_attempts': attempts,
-                'people_enriched': len(final_results),
-                'executives_analyzed': len(key_executives),
-                'output_file': shared_output_file,
-                'agent_name': self.agent_name,
-                'company_name': company_name,
-                'execution_time_ms': execution_time_ms
+                "success": True,
+                "enriched_people": final_results,
+                "people_ranked": len(ranked_people),
+                "enrichment_attempts": attempts,
+                "people_enriched": len(final_results),
+                "executives_analyzed": len(key_executives),
+                "output_file": shared_output_file,
+                "agent_name": self.agent_name,
+                "company_name": company_name,
+                "execution_time_ms": execution_time_ms,
             }
-            
+
         except json.JSONDecodeError as e:
             error_msg = f"Failed to parse LLM response as JSON: {e}\nResponse: {response_content}"
             print(f"‚ùå {error_msg}")
-            self.logger.error("Failed to parse LLM response as JSON", extra={
-                "run_id": run_id,
-                "error": str(e),
-                "response_length": len(response_content)
-            })
+            self.logger.error(
+                "Failed to parse LLM response as JSON", extra={"run_id": run_id, "error": str(e), "response_length": len(response_content)}
+            )
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - JSON Parse Error",
-                                         f"‚ùå {error_msg}")
-            return {
-                'success': False,
-                'error': error_msg,
-                'agent_name': self.agent_name,
-                'company_name': company_name
-            }
+                await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - JSON Parse Error", f"‚ùå {error_msg}")
+            return {"success": False, "error": error_msg, "agent_name": self.agent_name, "company_name": company_name}
 
         except Exception as e:
             import traceback
+
             print(f"\n‚ùå [PersonEnrichAgent ERROR] Exception in execute():\n{traceback.format_exc()}")
-            self.logger.exception("PersonEnrichAgent execution failed", extra={
-                "run_id": run_id,
-                "user_id": user_id,
-                "session_id": session_id,
-                "company_name": company_data.get('name', ''),
-                "error": str(e)
-            })
-            
+            self.logger.exception(
+                "PersonEnrichAgent execution failed",
+                extra={"run_id": run_id, "user_id": user_id, "session_id": session_id, "company_name": company_data.get("name", ""), "error": str(e)},
+            )
+
             execution_time_ms = int((time.time() - start_time) * 1000)
-            
+
             return {
-                'success': False,
-                'error': str(e),
-                'agent_name': self.agent_name,
-                'company_name': company_data.get('name', ''),
-                'execution_time_ms': execution_time_ms
+                "success": False,
+                "error": str(e),
+                "agent_name": self.agent_name,
+                "company_name": company_data.get("name", ""),
+                "execution_time_ms": execution_time_ms,
             }
         finally:
             # Note: Database connection is managed globally, no need to close here
             # Progress: mark completion best-effort
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'person_enrichment_progress', 1.00)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "person_enrichment_progress", 1.00)
             except Exception:
                 pass
 
-    async def _enrich_single_person(self, person: Dict[str, Any], company_name: str, 
-                                   company_website: str, run_id: str, company_id: str = None, shared_output_file: str = None, user_id: str = None,
-                                   youtube_media_shared: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+    async def _enrich_single_person(
+        self,
+        person: Dict[str, Any],
+        company_name: str,
+        company_website: str,
+        run_id: str,
+        company_id: str = None,
+        shared_output_file: str = None,
+        user_id: str = None,
+        youtube_media_shared: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
         """
         Enrich a single person with all available data sources.
-        
+
         Args:
             person: Dictionary with rank, parent_id, member_full_name, member_position_title
             company_name: Name of the company
             company_website: Company website URL
             shared_output_file: Optional output file for logging
-            
+
         Returns:
             Dictionary with enrichment results including success status and data
         """
         person_start_time = time.time()
-        person_name = person.get('member_full_name', 'Unknown')
-        parent_id = person.get('parent_id')
-        rank = person.get('rank', 0)
-        
+        person_name = person.get("member_full_name", "Unknown")
+        parent_id = person.get("parent_id")
+        rank = person.get("rank", 0)
+
         print(f"üîÑ Enriching person rank {rank}: {person_name} (parent_id: {parent_id})")
-        
+
         # Initialize result structure
         result = {
             "rank": rank,
             "parent_id": parent_id,
             "member_full_name": person_name,
-            "member_position_title": person.get('member_position_title'),
+            "member_position_title": person.get("member_position_title"),
             "enrichment_success": False,
             "processing_time_ms": 0,
-            "person_id": None  # Will be generated after data fusion
+            "person_id": None,  # Will be generated after data fusion
         }
-        
+
         try:
             # Step 1: Query CoreSignal Employee API for detailed person data
             employee_data = None
             if parent_id:
                 print(f"üìä Querying CoreSignal Employee API for {person_name}...")
                 employee_data = await self._query_coresignal_employee_api(parent_id, shared_output_file)
-                
-                if employee_data.get('success'):
+
+                if employee_data.get("success"):
                     print(f"‚úÖ Retrieved detailed employee data for {person_name}")
                 else:
                     print(f"‚ö†Ô∏è Failed to retrieve employee data for {person_name}: {employee_data.get('error', 'Unknown error')}")
             else:
                 # For Perplexity sources without parent_id, search CoreSignal by name and company
                 print(f"‚ö†Ô∏è No parent_id for {person_name}, searching CoreSignal by name...")
-                search_required = person.get('search_required', False)
-                
+                search_required = person.get("search_required", False)
+
                 if search_required:
                     api_key = get_coresignal_api_key()
                     if api_key:
@@ -1413,17 +1443,14 @@
                         search_result = await self.search_employee_by_name_and_company(
                             api_key=api_key,
                             full_name=person_name,
-                            company_name=company_name  # Use full company name for intelligent extraction
+                            company_name=company_name,  # Use full company name for intelligent extraction
                         )
-                        
-                        if search_result.get('success') and search_result.get('employee_ids'):
+
+                        if search_result.get("success") and search_result.get("employee_ids"):
                             # Enhanced search already includes verification and returns best match
-                            best_match = search_result.get('best_match', {})
+                            best_match = search_result.get("best_match", {})
                             if best_match:
-                                employee_data = {
-                                    'success': True,
-                                    'data': best_match['employee_data']
-                                }
+                                employee_data = {"success": True, "data": best_match["employee_data"]}
                                 print(f"‚úÖ Enhanced search found verified match for {person_name}")
                                 print(f"   Strategy used: {best_match['strategy_used']}")
                                 print(f"   Confidence: {best_match['confidence_score']}/10")
@@ -1431,68 +1458,58 @@
                             else:
                                 print(f"‚ùå Enhanced search succeeded but no best match found for {person_name}")
                         else:
-                            error_msg = search_result.get('error', 'No verified matches found')
-                            strategies = search_result.get('strategies_attempted', [])
+                            error_msg = search_result.get("error", "No verified matches found")
+                            strategies = search_result.get("strategies_attempted", [])
                             print(f"‚ùå Enhanced search failed for {person_name}: {error_msg}")
                             print(f"   Strategies attempted: {strategies}")
                     else:
                         print(f"‚ùå No CoreSignal API key available for enhanced search of {person_name}")
                 else:
                     print(f"‚ö†Ô∏è Search not required for {person_name}, skipping CoreSignal search")
-            
+
             # Step 2: Run OSINT research on the person
             osint_data = None
             if person_name and company_website:
                 print(f"üîç Running OSINT research on {person_name}...")
                 osint_data = await self._run_person_osint(
-                    person_name=person_name,
-                    company_name=company_name,
-                    company_website=company_website,
-                    shared_output_file=shared_output_file
+                    person_name=person_name, company_name=company_name, company_website=company_website, shared_output_file=shared_output_file
                 )
-                
-                if osint_data.get('success'):
+
+                if osint_data.get("success"):
                     print(f"‚úÖ OSINT research completed for {person_name}")
                 else:
                     print(f"‚ö†Ô∏è OSINT research failed for {person_name}: {osint_data.get('error', 'Unknown error')}")
             else:
                 print(f"‚ö†Ô∏è Missing person name or company website for {person_name}, skipping OSINT research")
-            
+
             # Step 3: Apollo.io Email Enrichment
             apollo_data = None
             # Use the same LinkedIn URL extraction logic that works correctly
             linkedin_url = self._extract_linkedin_url_from_sources(
                 fused_person_data=None,  # No fused data yet
                 employee_data=employee_data,
-                person_name=person_name
+                person_name=person_name,
             )
-            
+
             if linkedin_url:
                 print(f"üìß Enriching email data with Apollo.io for {person_name}...")
-                apollo_data = await self._enrich_with_apollo_io(
-                    linkedin_url=linkedin_url,
-                    shared_output_file=shared_output_file
-                )
-                
-                if apollo_data.get('success'):
+                apollo_data = await self._enrich_with_apollo_io(linkedin_url=linkedin_url, shared_output_file=shared_output_file)
+
+                if apollo_data.get("success"):
                     print(f"‚úÖ Apollo.io email enrichment completed for {person_name}")
                 else:
                     print(f"‚ö†Ô∏è Apollo.io enrichment failed for {person_name}: {apollo_data.get('error', 'Unknown error')}")
             else:
                 print(f"‚ö†Ô∏è No LinkedIn URL for {person_name}, skipping Apollo.io enrichment")
-            
+
             # Step 4: LinkedIn Mentions Search
             linkedin_mentions_data = None
             print(f"üîç Searching LinkedIn posts for mentions of {person_name}...")
-            
+
             try:
                 if company_id:
                     linkedin_mentions_data = await self._search_linkedin_mentions(
-                        person_name=person_name,
-                        company_name=company_name,
-                        run_id=run_id,
-                        company_id=company_id,
-                        user_id=user_id
+                        person_name=person_name, company_name=company_name, run_id=run_id, company_id=company_id, user_id=user_id
                     )
                     print(f"‚úÖ LinkedIn mentions search completed for {person_name}")
                 else:
@@ -1513,12 +1530,14 @@
                         session_id=None,
                         exec_names=[person_name],
                         youtube_url=None,
-                        person_name=person_name
+                        person_name=person_name,
                     )
                     videos_found = person_youtube_media.get("videos_found") if person_youtube_media else None
                     videos_ingested = person_youtube_media.get("videos_ingested") if person_youtube_media else None
                     skipped_no_transcript = person_youtube_media.get("skipped_no_transcript") if person_youtube_media else None
-                    print(f"‚úÖ YouTube media search completed for {person_name} | found={videos_found} ingested={videos_ingested} skipped_no_transcript={skipped_no_transcript}")
+                    print(
+                        f"‚úÖ YouTube media search completed for {person_name} | found={videos_found} ingested={videos_ingested} skipped_no_transcript={skipped_no_transcript}"
+                    )
                     self.logger.info(
                         f"YouTube media search completed for person {person_name} | found={videos_found} ingested={videos_ingested} skipped_no_transcript={skipped_no_transcript} run_id={run_id} company_id={company_id}"
                     )
@@ -1532,99 +1551,94 @@
 
             # Step 5: Person Data Fusion
             print(f"üîÄ Starting person data fusion for {person_name}...")
-            
+
             # Prepare data sources for fusion
             fusion_sources = {
-                'coresignal': employee_data.get('data') if employee_data and employee_data.get('success') else None,
-                'perplexity': osint_data.get('data') if osint_data and osint_data.get('success') else None,
-                'apollo': apollo_data.get('data') if apollo_data and apollo_data.get('success') else None,
-                'linkedin_mentions': linkedin_mentions_data
+                "coresignal": employee_data.get("data") if employee_data and employee_data.get("success") else None,
+                "perplexity": osint_data.get("data") if osint_data and osint_data.get("success") else None,
+                "apollo": apollo_data.get("data") if apollo_data and apollo_data.get("success") else None,
+                "linkedin_mentions": linkedin_mentions_data,
             }
-            
+
             fused_person_data = await self._fuse_person_data(
-                fusion_sources=fusion_sources,
-                person_name=person_name,
-                run_id=run_id,
-                shared_output_file=shared_output_file
+                fusion_sources=fusion_sources, person_name=person_name, run_id=run_id, shared_output_file=shared_output_file
             )
-            
-            if fused_person_data.get('success'):
-                result['fused_person_profile'] = fused_person_data.get('data')
-                result['enrichment_success'] = True
+
+            if fused_person_data.get("success"):
+                result["fused_person_profile"] = fused_person_data.get("data")
+                result["enrichment_success"] = True
                 print(f"‚úÖ Person data fusion completed successfully for {person_name}")
-                
+
                 # Step 5: Generate unique person ID after successful data fusion
                 print(f"üîë Generating unique person ID for {person_name}...")
-                
+
                 # Extract LinkedIn URL with multiple fallback strategies
                 linkedin_url = self._extract_linkedin_url_from_sources(
-                    fused_person_data=fused_person_data,
-                    employee_data=employee_data,
-                    person_name=person_name
+                    fused_person_data=fused_person_data, employee_data=employee_data, person_name=person_name
                 )
-                
+
                 # Generate unique person ID using LinkedIn URL
                 unique_person_id = self.generate_unique_person_id(linkedin_url=linkedin_url)
-                
+
                 if unique_person_id:
-                    result['person_id'] = unique_person_id
+                    result["person_id"] = unique_person_id
                     print(f"‚úÖ Generated unique person ID: {unique_person_id}")
                 else:
                     print(f"‚ùå Failed to generate person ID for {person_name} - no valid LinkedIn URL")
-                    result['person_id'] = None
-                    result['person_id_error'] = "No valid LinkedIn URL found for ID generation"
-                
+                    result["person_id"] = None
+                    result["person_id_error"] = "No valid LinkedIn URL found for ID generation"
+
             else:
-                result['error'] = f"Person data fusion failed: {fused_person_data.get('error', 'Unknown error')}"
+                result["error"] = f"Person data fusion failed: {fused_person_data.get('error', 'Unknown error')}"
                 print(f"‚ö†Ô∏è Person data fusion failed for {person_name}: {result['error']}")
-                
+
                 # Even if fusion fails, try to generate person ID if LinkedIn URL is available
                 print(f"üîë Attempting to generate person ID for failed enrichment...")
-                
+
                 # Try to get LinkedIn URL using the same extraction method
                 linkedin_url = self._extract_linkedin_url_from_sources(
                     fused_person_data=None,  # No fused data since fusion failed
                     employee_data=employee_data,
-                    person_name=person_name
+                    person_name=person_name,
                 )
-                
+
                 fallback_person_id = self.generate_unique_person_id(linkedin_url=linkedin_url)
-                
+
                 if fallback_person_id:
-                    result['person_id'] = fallback_person_id
+                    result["person_id"] = fallback_person_id
                     print(f"‚úÖ Generated fallback person ID: {fallback_person_id}")
                 else:
                     print(f"‚ùå Could not generate person ID - no LinkedIn URL available")
-                    result['person_id'] = None
-                    result['person_id_error'] = "No LinkedIn URL available for ID generation"
-            
+                    result["person_id"] = None
+                    result["person_id_error"] = "No LinkedIn URL available for ID generation"
+
             # Calculate processing time
-            result['processing_time_ms'] = int((time.time() - person_start_time) * 1000)
-            
+            result["processing_time_ms"] = int((time.time() - person_start_time) * 1000)
+
             print(f"‚úÖ Enrichment completed for {person_name} (rank {rank}) in {result['processing_time_ms']}ms")
-            
+
             return result
-            
+
         except Exception as e:
-            result['error'] = str(e)
-            result['processing_time_ms'] = int((time.time() - person_start_time) * 1000)
+            result["error"] = str(e)
+            result["processing_time_ms"] = int((time.time() - person_start_time) * 1000)
             print(f"‚ùå Enrichment failed for {person_name} (rank {rank}): {e}")
             return result
 
     async def _query_coresignal_employee_api(self, parent_id: str, shared_output_file: str = None) -> Dict[str, Any]:
         """
         Query the CoreSignal employee multi-source API to get detailed employee information.
-        
+
         Args:
             parent_id: The parent_id from the key executives data
             shared_output_file: Optional output file for logging
-            
+
         Returns:
             Dict with API results or error
         """
         # API monitoring metrics
         api_call_start_time = time.time()
-        
+
         try:
             api_key = get_coresignal_api_key()
             if not api_key:
@@ -1632,194 +1646,168 @@
                 self.logger.error("CORESIG_API_KEY not found in environment variables", extra={"parent_id": parent_id})
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - API Key Error", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
+                return {"success": False, "error": error_msg}
 
             # CoreSignal Employee API endpoint
             collect_url = f"https://api.coresignal.com/cdapi/v2/employee_clean/collect/{parent_id}"
-            
+
             # Use correct CoreSignal authentication format
-            headers = {
-                "apikey": api_key,
-                "Content-Type": "application/json"
-            }
-            
-            self.logger.info("CoreSignal Employee API request prepared", extra={
-                "parent_id": parent_id,
-                "collect_url": collect_url
-            })
-            
+            headers = {"apikey": api_key, "Content-Type": "application/json"}
+
+            self.logger.info("CoreSignal Employee API request prepared", extra={"parent_id": parent_id, "collect_url": collect_url})
+
             if shared_output_file:
                 await self.append_markdown(
-                    shared_output_file,
-                    "Step 6: Person Enrichment - CoreSignal Employee API Request",
-                    f"Requesting employee data from: {collect_url}"
+                    shared_output_file, "Step 6: Person Enrichment - CoreSignal Employee API Request", f"Requesting employee data from: {collect_url}"
                 )
-            
+
             async with aiohttp.ClientSession() as session:
                 async with session.get(collect_url, headers=headers, timeout=30) as response:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     if response.status != 200:
                         error_text = await response.text()
                         error_msg = f"CoreSignal Employee API error: {response.status} - {error_text}"
-                        
+
                         # API monitoring: Track failed API call
-                        self.logger.error("CoreSignal Employee API error", extra={
-                            "parent_id": parent_id,
-                            "status": response.status,
-                            "error": error_text,
-                            "response_time": api_response_time,
-                            "api_call_type": "coresignal_employee"
-                        })
-                        
+                        self.logger.error(
+                            "CoreSignal Employee API error",
+                            extra={
+                                "parent_id": parent_id,
+                                "status": response.status,
+                                "error": error_text,
+                                "response_time": api_response_time,
+                                "api_call_type": "coresignal_employee",
+                            },
+                        )
+
                         if shared_output_file:
                             await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Employee API Error", error_msg)
-                        
-                        return {
-                            'success': False,
-                            'error': error_msg,
-                            'parent_id': parent_id
-                        }
-                    
+
+                        return {"success": False, "error": error_msg, "parent_id": parent_id}
+
                     # API monitoring: Track successful API call
-                    self.logger.info("CoreSignal Employee API response received", extra={
-                        "parent_id": parent_id,
-                        "response_time": api_response_time,
-                        "api_call_type": "coresignal_employee"
-                    })
-                    
+                    self.logger.info(
+                        "CoreSignal Employee API response received",
+                        extra={"parent_id": parent_id, "response_time": api_response_time, "api_call_type": "coresignal_employee"},
+                    )
+
                     employee_data = await response.json()
-                    
+
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
                             "Step 6: Person Enrichment - Employee API Success",
-                            f"‚úÖ Successfully retrieved employee data for parent_id: {parent_id}"
+                            f"‚úÖ Successfully retrieved employee data for parent_id: {parent_id}",
                         )
-                    
-                    return {
-                        'success': True,
-                        'data': employee_data,
-                        'parent_id': parent_id
-                    }
-                        
+
+                    return {"success": True, "data": employee_data, "parent_id": parent_id}
+
         except Exception as e:
             api_response_time = time.time() - api_call_start_time
             error_msg = f"CoreSignal Employee API query failed: {str(e)}"
-            
+
             # API monitoring: Track API call exception
-            self.logger.exception("CoreSignal Employee API query failed", extra={
-                "parent_id": parent_id,
-                "error": str(e),
-                "response_time": api_response_time,
-                "api_call_type": "coresignal_employee"
-            })
-            
+            self.logger.exception(
+                "CoreSignal Employee API query failed",
+                extra={"parent_id": parent_id, "error": str(e), "response_time": api_response_time, "api_call_type": "coresignal_employee"},
+            )
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - Employee API Exception", error_msg)
-            
-            return {
-                'success': False,
-                'error': error_msg,
-                'parent_id': parent_id
-            }
 
+            return {"success": False, "error": error_msg, "parent_id": parent_id}
+
     async def _run_person_osint(self, person_name: str, company_name: str, company_website: str, shared_output_file: str = None) -> Dict[str, Any]:
         """
         Run OSINT research on a specific person using Perplexity API.
-        
+
         Args:
             person_name: Full name of the person to research
             company_name: Name of the company the person works for
             company_website: Company website URL for focused research
             shared_output_file: Optional output file for logging
-            
+
         Returns:
             Dict with OSINT results or error
         """
         try:
             from app.utils.config import get_perplexity_api_key
+
             if not get_perplexity_api_key():
                 error_msg = "PERPLEXITY_API_KEY not found in environment variables"
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - OSINT API Key Error", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
+                return {"success": False, "error": error_msg}
 
             # Get the OSINT system prompt
             system_prompt = get_osint_person_system_prompt()
-            
+
             # Get the user prompt with formatted parameters
             user_prompt = get_osint_person_user_prompt(person_name, company_name, company_website)
-            
-            chat = ChatPerplexity(
-                temperature=0, 
-                model="sonar-pro",
-                api_key=get_perplexity_api_key()
-            )
-            
+
+            chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=get_perplexity_api_key())
+
             # API monitoring: Track Perplexity API call
             api_call_start_time = time.time()
-            
+
             if shared_output_file:
                 await self.append_markdown(
                     shared_output_file,
                     "Step 6: Person Enrichment - OSINT API Request",
-                    f"Requesting OSINT research for {person_name} at {company_name}"
+                    f"Requesting OSINT research for {person_name} at {company_name}",
                 )
-            
-            with trace_operation("person_osint", {
-                "person_name": person_name,
-                "company_name": company_name,
-                "model": "sonar-pro",
-                "search_type": "person_osint",
-                "provider": "perplexity"
-            }):
+
+            with trace_operation(
+                "person_osint",
+                {
+                    "person_name": person_name,
+                    "company_name": company_name,
+                    "model": "sonar-pro",
+                    "search_type": "person_osint",
+                    "provider": "perplexity",
+                },
+            ):
                 try:
                     response = await chat.ainvoke(
-                        [
-                            ("system", system_prompt),
-                            ("user", user_prompt)
-                        ],
-                        extra_body={
-                            "web_search_options": {"search_context_size": "high"}
-                        }
+                        [("system", system_prompt), ("user", user_prompt)], extra_body={"web_search_options": {"search_context_size": "high"}}
                     )
-                    
+
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track successful Perplexity API call
-                    self.logger.info("Perplexity API call successful", extra={
-                        "model": "sonar-pro",
-                        "response_time": api_response_time,
-                        "person_name": person_name,
-                        "company_name": company_name,
-                        "api_call_type": "person_osint"
-                    })
-                
+                    self.logger.info(
+                        "Perplexity API call successful",
+                        extra={
+                            "model": "sonar-pro",
+                            "response_time": api_response_time,
+                            "person_name": person_name,
+                            "company_name": company_name,
+                            "api_call_type": "person_osint",
+                        },
+                    )
+
                 except Exception as api_error:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track failed Perplexity API call
-                    self.logger.error("Perplexity API call failed", extra={
-                        "model": "sonar-pro",
-                        "error": str(api_error),
-                        "response_time": api_response_time,
-                        "person_name": person_name,
-                        "company_name": company_name,
-                        "api_call_type": "person_osint"
-                    })
-                    
+                    self.logger.error(
+                        "Perplexity API call failed",
+                        extra={
+                            "model": "sonar-pro",
+                            "error": str(api_error),
+                            "response_time": api_response_time,
+                            "person_name": person_name,
+                            "company_name": company_name,
+                            "api_call_type": "person_osint",
+                        },
+                    )
+
                     raise api_error
 
             content = response.content if response else None
-            citations = getattr(response, 'additional_kwargs', {}).get('citations') if response else None
-            
+            citations = getattr(response, "additional_kwargs", {}).get("citations") if response else None
+
             # Debug logging for response
             print(f"üîç Perplexity OSINT response check:")
             print(f"   response exists: {response is not None}")
@@ -1829,49 +1817,45 @@
                 print(f"   response type: {type(response)}")
                 print(f"   response attributes: {dir(response)}")
                 # Check for status code if available
-                if hasattr(response, 'response_metadata'):
+                if hasattr(response, "response_metadata"):
                     print(f"   response_metadata: {response.response_metadata}")
-                if hasattr(response, 'additional_kwargs'):
+                if hasattr(response, "additional_kwargs"):
                     print(f"   additional_kwargs: {response.additional_kwargs}")
-                
+
             if not content:
                 error_msg = "No content returned from Perplexity OSINT API (204 No Content or empty response)"
                 print(f"‚ùå {error_msg}")
-                self.logger.warning("Perplexity OSINT returned no content", extra={
-                    "person_name": person_name,
-                    "company_name": company_name,
-                    "response_type": str(type(response)) if response else "None"
-                })
+                self.logger.warning(
+                    "Perplexity OSINT returned no content",
+                    extra={"person_name": person_name, "company_name": company_name, "response_type": str(type(response)) if response else "None"},
+                )
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - OSINT No Content", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
-            
+                return {"success": False, "error": error_msg}
+
             # Parse the JSON response
             try:
                 # Extract JSON from markdown code blocks if present
-                if '```json' in content:
-                    json_start = content.find('```json') + 7
-                    json_end = content.find('```', json_start)
+                if "```json" in content:
+                    json_start = content.find("```json") + 7
+                    json_end = content.find("```", json_start)
                     json_content = content[json_start:json_end].strip()
-                elif '{' in content:
-                    json_start = content.find('{')
-                    json_end = content.rfind('}') + 1
+                elif "{" in content:
+                    json_start = content.find("{")
+                    json_end = content.rfind("}") + 1
                     json_content = content[json_start:json_end]
                 else:
                     json_content = content
-                
+
                 osint_data = json.loads(json_content)
-                
+
                 if shared_output_file:
                     await self.append_markdown(
                         shared_output_file,
                         "Step 6: Person Enrichment - OSINT API Success",
-                        f"‚úÖ Successfully retrieved OSINT data for {person_name}\n\n**Citations:** {citations if citations else 'None'}"
+                        f"‚úÖ Successfully retrieved OSINT data for {person_name}\n\n**Citations:** {citations if citations else 'None'}",
                     )
-                
+
                 # Write detailed output to agent_test.md
                 try:
                     debug_file = "test_output/agent_test.md"
@@ -1882,84 +1866,73 @@
                         f.write(f"**Company:** {company_name}\n")
                         f.write(f"**Company Website:** {company_website}\n")
                         f.write(f"**Timestamp:** {datetime.now().isoformat()}\n\n")
-                        
+
                         # Raw API response
                         f.write(f"### Raw Perplexity Response\n\n")
                         f.write(f"**Status:** Success\n")
                         f.write(f"**Content Length:** {len(content)} characters\n")
                         f.write(f"**Raw Content:**\n```json\n{content}\n```\n\n")
                         f.write(f"**Citations:** {citations if citations else 'None'}\n\n")
-                        
+
                         # Parsed OSINT data
                         f.write(f"### Parsed OSINT Data\n\n")
                         f.write(f"**Parsed Content:**\n```json\n{json.dumps(osint_data, indent=2, ensure_ascii=False)}\n```\n\n")
                         f.write("---\n")
                     print(f"‚úÖ OSINT detailed output written to {debug_file}")
                 except Exception as debug_error:
-                        print(f"‚ö†Ô∏è Failed to write debug output: {debug_error}")
-                
-                return {
-                    'success': True,
-                    'data': osint_data,
-                    'citations': citations,
-                    'person_name': person_name
-                }
-                
+                    print(f"‚ö†Ô∏è Failed to write debug output: {debug_error}")
+
+                return {"success": True, "data": osint_data, "citations": citations, "person_name": person_name}
+
             except json.JSONDecodeError as e:
                 error_msg = f"Failed to parse OSINT response as JSON: {e}\nResponse: {content}"
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - OSINT JSON Parse Error", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                    }
-                    
+                return {"success": False, "error": error_msg}
+
             except Exception as api_error:
                 api_response_time = time.time() - api_call_start_time
-                
+
                 # API monitoring: Track failed Perplexity API call
-                self.logger.error("Perplexity API call failed", extra={
-                    "model": "sonar-pro",
-                    "response_time": api_response_time,
-                    "person_name": person_name,
-                    "company_name": company_name,
-                    "api_call_type": "person_osint",
-                    "error": str(api_error)
-                })
-                
+                self.logger.error(
+                    "Perplexity API call failed",
+                    extra={
+                        "model": "sonar-pro",
+                        "response_time": api_response_time,
+                        "person_name": person_name,
+                        "company_name": company_name,
+                        "api_call_type": "person_osint",
+                        "error": str(api_error),
+                    },
+                )
+
                 error_msg = f"Perplexity OSINT API error: {str(api_error)}"
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - OSINT API Error", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
+                return {"success": False, "error": error_msg}
 
         except Exception as e:
             error_msg = f"Person OSINT research failed: {str(e)}"
-            
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Step 6: Person Enrichment - OSINT Exception", error_msg)
-            
-            return {
-                'success': False,
-                'error': error_msg
-            }
 
+            return {"success": False, "error": error_msg}
+
     async def _enrich_with_apollo_io(self, linkedin_url: str, shared_output_file: str = None) -> Dict[str, Any]:
         """
         Enrich person data with email information using Apollo.io People Enrichment API.
-        
+
         Args:
             linkedin_url: LinkedIn profile URL of the person to enrich
             shared_output_file: Optional output file for logging
-            
+
         Returns:
             Dict with Apollo.io enrichment results or error
         """
         # API monitoring metrics
         api_call_start_time = time.time()
-        
+
         try:
             api_key = get_apollo_api_key()
             if not api_key:
@@ -1967,102 +1940,83 @@
                 self.logger.error("APOLLO_IO_API_KEY not found in environment variables", extra={"linkedin_url": linkedin_url})
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 7: Apollo.io Email Enrichment - API Key Error", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
+                return {"success": False, "error": error_msg}
 
             # Apollo.io People Enrichment API endpoint
             url = "https://api.apollo.io/api/v1/people/match"
-            
+
             # Headers for Apollo.io API
-            headers = {
-                "Cache-Control": "no-cache",
-                "Content-Type": "application/json",
-                "X-Api-Key": api_key
-            }
-            
+            headers = {"Cache-Control": "no-cache", "Content-Type": "application/json", "X-Api-Key": api_key}
+
             # Apollo.io API payload
-            payload = {
-                "linkedin_url": linkedin_url,
-                "reveal_personal_emails": True
-            }
-            
-            self.logger.info("Apollo.io API request prepared", extra={
-                "linkedin_url": linkedin_url,
-                "url": url
-            })
-            
+            payload = {"linkedin_url": linkedin_url, "reveal_personal_emails": True}
+
+            self.logger.info("Apollo.io API request prepared", extra={"linkedin_url": linkedin_url, "url": url})
+
             if shared_output_file:
                 await self.append_markdown(
                     shared_output_file,
                     "Step 7: Apollo.io Email Enrichment - API Request",
-                    f"Requesting email enrichment for LinkedIn URL: {linkedin_url}"
+                    f"Requesting email enrichment for LinkedIn URL: {linkedin_url}",
                 )
-            
+
             async with aiohttp.ClientSession() as session:
                 async with session.post(url, headers=headers, json=payload, timeout=30) as response:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     if response.status != 200:
                         error_text = await response.text()
                         error_msg = f"Apollo.io API error: {response.status} - {error_text}"
-                        
+
                         # API monitoring: Track failed Apollo.io API call
-                        self.logger.error("Apollo.io API error", extra={
-                            "linkedin_url": linkedin_url,
-                            "status": response.status,
-                            "error": error_text,
-                            "response_time": api_response_time,
-                            "api_call_type": "apollo_io_email_enrichment"
-                        })
-                        
+                        self.logger.error(
+                            "Apollo.io API error",
+                            extra={
+                                "linkedin_url": linkedin_url,
+                                "status": response.status,
+                                "error": error_text,
+                                "response_time": api_response_time,
+                                "api_call_type": "apollo_io_email_enrichment",
+                            },
+                        )
+
                         if shared_output_file:
                             await self.append_markdown(shared_output_file, "Step 7: Apollo.io Email Enrichment - API Error", error_msg)
-                        
-                        return {
-                            'success': False,
-                            'error': error_msg,
-                            'linkedin_url': linkedin_url
-                        }
-                    
+
+                        return {"success": False, "error": error_msg, "linkedin_url": linkedin_url}
+
                     # API monitoring: Track successful Apollo.io API call
-                    self.logger.info("Apollo.io API response received", extra={
-                        "linkedin_url": linkedin_url,
-                        "response_time": api_response_time,
-                        "api_call_type": "apollo_io_email_enrichment"
-                    })
-                    
+                    self.logger.info(
+                        "Apollo.io API response received",
+                        extra={"linkedin_url": linkedin_url, "response_time": api_response_time, "api_call_type": "apollo_io_email_enrichment"},
+                    )
+
                     response_data = await response.json()
                     person_data = response_data.get("person")
-                    
+
                     if not person_data:
                         error_msg = "No person data returned from Apollo.io API"
-                        self.logger.warning("No person data returned from Apollo.io API", extra={
-                            "linkedin_url": linkedin_url,
-                            "response_data_keys": list(response_data.keys()) if response_data else []
-                        })
+                        self.logger.warning(
+                            "No person data returned from Apollo.io API",
+                            extra={"linkedin_url": linkedin_url, "response_data_keys": list(response_data.keys()) if response_data else []},
+                        )
                         if shared_output_file:
                             await self.append_markdown(shared_output_file, "Step 7: Apollo.io Email Enrichment - No Person Data", error_msg)
-                        return {
-                            'success': False,
-                            'error': error_msg,
-                            'linkedin_url': linkedin_url
-                        }
-                    
+                        return {"success": False, "error": error_msg, "linkedin_url": linkedin_url}
+
                     # Extract the required fields
                     email = person_data.get("email")
                     email_status = person_data.get("email_status")
                     extrapolated_email_confidence = person_data.get("extrapolated_email_confidence")
-                    
+
                     enrichment_result = {
-                        'email': email,
-                        'email_status': email_status,
-                        'extrapolated_email_confidence': extrapolated_email_confidence,
-                        'linkedin_url_used': linkedin_url,
-                        'apollo_enrichment_timestamp': datetime.now().isoformat()
+                        "email": email,
+                        "email_status": email_status,
+                        "extrapolated_email_confidence": extrapolated_email_confidence,
+                        "linkedin_url_used": linkedin_url,
+                        "apollo_enrichment_timestamp": datetime.now().isoformat(),
                     }
-                    
+
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
@@ -2070,9 +2024,9 @@
                             f"‚úÖ Successfully retrieved email data from Apollo.io\n\n"
                             f"**Email:** {email if email else 'Not found'}\n"
                             f"**Email Status:** {email_status if email_status else 'Not available'}\n"
-                            f"**Confidence:** {extrapolated_email_confidence if extrapolated_email_confidence else 'Not available'}"
+                            f"**Confidence:** {extrapolated_email_confidence if extrapolated_email_confidence else 'Not available'}",
                         )
-                    
+
                     # Write detailed output to agent_test.md
                     try:
                         debug_file = "test_output/agent_test.md"
@@ -2081,12 +2035,12 @@
                             f.write(f"\n\n## Person Enrichment Agent - Apollo.io Email Enrichment Output\n\n")
                             f.write(f"**LinkedIn URL:** {linkedin_url}\n")
                             f.write(f"**Timestamp:** {datetime.now().isoformat()}\n\n")
-                            
+
                             # Raw API response
                             f.write(f"### Raw Apollo.io Response\n\n")
                             f.write(f"**Status:** Success\n")
                             f.write(f"**Raw Response:**\n```json\n{json.dumps(response_data, indent=2, ensure_ascii=False)}\n```\n\n")
-                            
+
                             # Extracted email data
                             f.write(f"### Extracted Email Data\n\n")
                             f.write(f"**Extracted Data:**\n```json\n{json.dumps(enrichment_result, indent=2, ensure_ascii=False)}\n```\n\n")
@@ -2094,86 +2048,78 @@
                         print(f"‚úÖ Apollo.io detailed output written to {debug_file}")
                     except Exception as debug_error:
                         print(f"‚ùå Error writing Apollo.io output to debug file: {debug_error}")
-                    
-                    return {
-                        'success': True,
-                        'data': enrichment_result,
-                        'linkedin_url': linkedin_url
-                    }
-                        
+
+                    return {"success": True, "data": enrichment_result, "linkedin_url": linkedin_url}
+
         except Exception as e:
             error_msg = f"Apollo.io email enrichment failed: {str(e)}"
-            
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Step 7: Apollo.io Email Enrichment - Exception", error_msg)
-            
-            return {
-                'success': False,
-                'error': error_msg,
-                'linkedin_url': linkedin_url
-            }
 
-    async def _fuse_person_data(self, fusion_sources: Dict[str, Any], person_name: str, run_id: str = None, shared_output_file: str = None) -> Dict[str, Any]:
+            return {"success": False, "error": error_msg, "linkedin_url": linkedin_url}
+
+    async def _fuse_person_data(
+        self, fusion_sources: Dict[str, Any], person_name: str, run_id: str = None, shared_output_file: str = None
+    ) -> Dict[str, Any]:
         """
         Fuse person data from CoreSignal, Perplexity, and Apollo.io sources using PERSON_SOURCE_MAPPINGS.
-        
+
         Args:
             fusion_sources: Dict with 'coresignal', 'perplexity', 'apollo' data
             person_name: Name of the person for logging
             run_id: Optional run ID for tracking and indexing
             shared_output_file: Optional output file for logging
-            
+
         Returns:
             Dict with fused person data or error
         """
         try:
             print(f"üîÄ Starting person data fusion for {person_name}")
-            
+
             if shared_output_file:
                 await self.append_markdown(
                     shared_output_file,
                     "Step 8: Person Data Fusion - Processing",
-                    f"Processing person data fusion using PERSON_SOURCE_MAPPINGS configuration"
+                    f"Processing person data fusion using PERSON_SOURCE_MAPPINGS configuration",
                 )
-            
+
             # Initialize the fusion result with schema structure
             fused_result = self._initialize_person_fusion_result()
-            
+
             # Track sources used
             sources_used = []
-            
+
             # Track which sources have data
-            if fusion_sources.get('coresignal'):
+            if fusion_sources.get("coresignal"):
                 sources_used.append("CoreSignal")
-            if fusion_sources.get('perplexity'):
+            if fusion_sources.get("perplexity"):
                 sources_used.append("Perplexity")
-            if fusion_sources.get('apollo'):
+            if fusion_sources.get("apollo"):
                 sources_used.append("Apollo.io")
-            if fusion_sources.get('linkedin_mentions'):
+            if fusion_sources.get("linkedin_mentions"):
                 sources_used.append("LinkedIn Mentions")
-            
+
             # Process each category of fields
             for category, fields in self.PERSON_SOURCE_MAPPINGS.items():
-                await self._process_person_category_fields(
-                    category, fields, fused_result, fusion_sources
-                )
-            
+                await self._process_person_category_fields(category, fields, fused_result, fusion_sources)
+
             # Add metadata
             fused_result["fusion_metadata"] = {
                 "person_name": person_name,
-                "run_id": run_id or 'default',
+                "run_id": run_id or "default",
                 "sources_used": sources_used,
                 "completeness_percentage": self._calculate_completeness_percentage(fused_result),
-                "timestamp": datetime.now().isoformat()
+                "timestamp": datetime.now().isoformat(),
             }
-            
+
             if shared_output_file:
                 await self.append_markdown(
                     shared_output_file,
                     "Step 8: Person Data Fusion - Completed",
-                    f"‚úÖ Successfully fused person data with {len([s for s, d in fusion_sources.items() if d])} sources"
+                    f"‚úÖ Successfully fused person data with {len([s for s, d in fusion_sources.items() if d])} sources",
                 )
-            
+
             # Write detailed output to agent_test.md
             try:
                 debug_file = "test_output/agent_test.md"
@@ -2182,16 +2128,18 @@
                     f.write(f"\n\n## Person Enrichment Agent - Data Fusion Output\n\n")
                     f.write(f"**Person:** {person_name}\n")
                     f.write(f"**Timestamp:** {datetime.now().isoformat()}\n\n")
-                    
+
                     # Source data
                     f.write(f"### Input Sources\n\n")
                     for source_name, source_data in fusion_sources.items():
-                        f.write(f"**{source_name.capitalize()} Data:**\n```json\n{json.dumps(source_data, indent=2, ensure_ascii=False) if source_data else 'null'}\n```\n\n")
-                    
+                        f.write(
+                            f"**{source_name.capitalize()} Data:**\n```json\n{json.dumps(source_data, indent=2, ensure_ascii=False) if source_data else 'null'}\n```\n\n"
+                        )
+
                     # PERSON_SOURCE_MAPPINGS configuration
                     f.write(f"### PERSON_SOURCE_MAPPINGS Configuration\n\n")
                     f.write(f"```json\n{json.dumps(self.PERSON_SOURCE_MAPPINGS, indent=2, ensure_ascii=False)}\n```\n\n")
-                    
+
                     # Fused result
                     f.write(f"### Fused Person Profile\n\n")
                     f.write(f"```json\n{json.dumps(fused_result, indent=2, ensure_ascii=False)}\n```\n\n")
@@ -2199,93 +2147,89 @@
                 print(f"‚úÖ Person fusion detailed output written to {debug_file}")
             except Exception as debug_error:
                 print(f"‚ùå Error writing person fusion output to debug file: {debug_error}")
-            
-            return {
-                'success': True,
-                'data': fused_result
-            }
-            
+
+            return {"success": True, "data": fused_result}
+
         except Exception as e:
             error_msg = f"Person data fusion failed: {str(e)}"
-            
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Step 8: Person Data Fusion - Exception", error_msg)
-            
-            return {
-                'success': False,
-                'error': error_msg
-            }
 
+            return {"success": False, "error": error_msg}
+
     def _initialize_person_fusion_result(self) -> Dict[str, Any]:
         """Initialize the person fusion result with the schema structure."""
-        return {
-            "contact_identity": {},
-            "professional_background": {},
-            "strategic_notes_personalization": {}
-        }
+        return {"contact_identity": {}, "professional_background": {}, "strategic_notes_personalization": {}}
 
-    async def _process_person_category_fields(self, category: str, fields: Dict, fused_result: Dict,
-                                            fusion_sources: Dict[str, Any]):
+    async def _process_person_category_fields(self, category: str, fields: Dict, fused_result: Dict, fusion_sources: Dict[str, Any]):
         """Process fields within a person category."""
         for field_name, field_config in fields.items():
             field_path = f"{category}.{field_name}"
             await self._process_person_single_field(field_path, field_config, fused_result, fusion_sources)
 
-    async def _process_person_single_field(self, field_path: str, field_config: Dict, fused_result: Dict,
-                                         fusion_sources: Dict[str, Any]):
+    async def _process_person_single_field(self, field_path: str, field_config: Dict, fused_result: Dict, fusion_sources: Dict[str, Any]):
         """Process a single person field with its configuration."""
         try:
             coresignal_path = field_config.get("coresignal_path")
             perplexity_path = field_config.get("perplexity_path")
             apollo_path = field_config.get("apollo_path")
             company_linkedin_mentions_path = field_config.get("company_linkedin_mentions_path")
-            
+
             # Get LinkedIn URL from coresignal data for citation
             linkedin_url = None
-            if fusion_sources.get('coresignal'):
-                linkedin_url = fusion_sources['coresignal'].get('linkedin_url')
-            
+            if fusion_sources.get("coresignal"):
+                linkedin_url = fusion_sources["coresignal"].get("linkedin_url")
+
             # Get values from each source
             coresignal_value = None
             perplexity_value = None
             apollo_value = None
             company_linkedin_mentions_value = None
-            
-            if coresignal_path and fusion_sources.get('coresignal'):
-                coresignal_value = self._get_person_nested_value(fusion_sources['coresignal'], coresignal_path)
+
+            if coresignal_path and fusion_sources.get("coresignal"):
+                coresignal_value = self._get_person_nested_value(fusion_sources["coresignal"], coresignal_path)
                 coresignal_value = await self._process_coresignal_person_field(coresignal_value, field_path)
-            
-            if perplexity_path and fusion_sources.get('perplexity'):
-                perplexity_value = self._get_perplexity_person_value(fusion_sources['perplexity'], perplexity_path, field_path)
-            
-            if apollo_path and fusion_sources.get('apollo'):
-                apollo_value = self._get_person_apollo_value(fusion_sources['apollo'], apollo_path, field_path)
-            
-            if company_linkedin_mentions_path and fusion_sources.get('linkedin_mentions'):
-                company_linkedin_mentions_value = await self._get_company_linkedin_mentions_value(fusion_sources['linkedin_mentions'], company_linkedin_mentions_path, field_path)
-            
+
+            if perplexity_path and fusion_sources.get("perplexity"):
+                perplexity_value = self._get_perplexity_person_value(fusion_sources["perplexity"], perplexity_path, field_path)
+
+            if apollo_path and fusion_sources.get("apollo"):
+                apollo_value = self._get_person_apollo_value(fusion_sources["apollo"], apollo_path, field_path)
+
+            if company_linkedin_mentions_path and fusion_sources.get("linkedin_mentions"):
+                company_linkedin_mentions_value = await self._get_company_linkedin_mentions_value(
+                    fusion_sources["linkedin_mentions"], company_linkedin_mentions_path, field_path
+                )
+
             # Count available sources
             available_sources = sum([1 for v in [coresignal_value, perplexity_value, apollo_value, company_linkedin_mentions_value] if v is not None])
-            
+
             # Determine fusion approach
             if available_sources == 0:
                 # No data available
                 self._set_person_nested_value(fused_result, field_path, None)
-                
+
             elif available_sources == 1:
                 # Single source
                 if coresignal_value is not None:
-                    self._set_person_nested_value(fused_result, field_path, self._format_person_single_source_value(coresignal_value, "CoreSignal", linkedin_url))
-                    
+                    self._set_person_nested_value(
+                        fused_result, field_path, self._format_person_single_source_value(coresignal_value, "CoreSignal", linkedin_url)
+                    )
+
                 elif perplexity_value is not None:
                     self._set_person_nested_value(fused_result, field_path, self._format_person_single_source_value(perplexity_value, "Perplexity"))
-                    
+
                 elif apollo_value is not None:
                     self._set_person_nested_value(fused_result, field_path, self._format_person_single_source_value(apollo_value, "Apollo.io"))
-                    
+
                 elif company_linkedin_mentions_value is not None:
-                    self._set_person_nested_value(fused_result, field_path, self._format_person_single_source_value(company_linkedin_mentions_value, "Company LinkedIn Mentions", linkedin_url))
-                    
+                    self._set_person_nested_value(
+                        fused_result,
+                        field_path,
+                        self._format_person_single_source_value(company_linkedin_mentions_value, "Company LinkedIn Mentions", linkedin_url),
+                    )
+
             else:
                 # Multiple sources - join them (for Company LinkedIn mentions, usually single source)
                 if company_linkedin_mentions_value is not None:
@@ -2294,7 +2238,7 @@
                 else:
                     value_to_set = self._join_person_data_with_citations(coresignal_value, perplexity_value, apollo_value, linkedin_url)
                 self._set_person_nested_value(fused_result, field_path, value_to_set)
-                
+
         except Exception as e:
             print(f"‚ùå Error processing person field {field_path}: {e}")
 
@@ -2302,36 +2246,36 @@
         """Get a nested value from person data using field path."""
         if not data or not field_path:
             return None
-        
+
         # Handle both list of keys and dot notation string
         if isinstance(field_path, list):
             keys = field_path
         elif isinstance(field_path, str):
-            keys = field_path.split('.')
+            keys = field_path.split(".")
         else:
             return None
-            
+
         current = data
-        
+
         for key in keys:
             if isinstance(current, dict) and key in current:
                 current = current[key]
             else:
                 return None
-            
+
         return current
 
     def _get_person_apollo_value(self, apollo_data: Dict, apollo_path: list, field_path: str) -> Any:
         """Get value from Apollo.io data, combining multiple fields if specified."""
         if not apollo_data or not apollo_path:
             return None
-        
+
         if field_path.endswith("email_address"):
             # Special handling for email address - combine email, status, and confidence
             email = apollo_data.get("email")
             status = apollo_data.get("email_status")
             confidence = apollo_data.get("extrapolated_email_confidence")
-            
+
             if email:
                 result = email
                 if status:
@@ -2353,53 +2297,41 @@
         """Get value from Perplexity OSINT data with special handling for different field types."""
         if not perplexity_data or not perplexity_path:
             return None
-        
+
         if field_path.endswith("conferences_public_speaking"):
             # Special handling: get both conferences and public_speaking arrays and combine them
             conferences = perplexity_data.get("conferences", [])
             public_speaking = perplexity_data.get("public_speaking", [])
-            
+
             combined_activities = []
-            
+
             # Add conferences
             if isinstance(conferences, list):
                 for conf in conferences:
                     if isinstance(conf, dict) and conf.get("value"):
-                        combined_activities.append({
-                            "type": "conference",
-                            "value": conf["value"],
-                            "citation": conf.get("citation")
-                        })
-            
+                        combined_activities.append({"type": "conference", "value": conf["value"], "citation": conf.get("citation")})
+
             # Add public speaking
             if isinstance(public_speaking, list):
                 for speaking in public_speaking:
                     if isinstance(speaking, dict) and speaking.get("value"):
-                        combined_activities.append({
-                            "type": "public_speaking", 
-                            "value": speaking["value"],
-                            "citation": speaking.get("citation")
-                        })
-            
+                        combined_activities.append({"type": "public_speaking", "value": speaking["value"], "citation": speaking.get("citation")})
+
             return combined_activities if combined_activities else None
-            
+
         elif field_path.endswith("recent_news"):
             # Special handling: combine recent_events and achievements
             combined_news = []
-            
+
             for field_name in perplexity_path:
                 field_data = perplexity_data.get(field_name, [])
                 if isinstance(field_data, list):
                     for item in field_data:
                         if isinstance(item, dict) and item.get("value"):
-                            combined_news.append({
-                                "category": field_name,
-                                "value": item["value"],
-                                "citation": item.get("citation")
-                            })
-            
+                            combined_news.append({"category": field_name, "value": item["value"], "citation": item.get("citation")})
+
             return combined_news if combined_news else None
-            
+
         elif field_path.endswith("investment_authority"):
             # Special handling: analyze investment_news to infer authority level
             investment_news = perplexity_data.get("investment_news", [])
@@ -2408,22 +2340,13 @@
                 authority_indicators = []
                 for item in investment_news:
                     if isinstance(item, dict) and item.get("value"):
-                        authority_indicators.append({
-                            "evidence": item["value"],
-                            "citation": item.get("citation")
-                        })
-                
+                        authority_indicators.append({"evidence": item["value"], "citation": item.get("citation")})
+
                 if authority_indicators:
-                    return {
-                        "inferred_authority": "Yes - based on investment activities",
-                        "evidence": authority_indicators
-                    }
-            
-            return {
-                "inferred_authority": "Unknown - no investment activity found",
-                "evidence": []
-            }
-        
+                    return {"inferred_authority": "Yes - based on investment activities", "evidence": authority_indicators}
+
+            return {"inferred_authority": "Unknown - no investment activity found", "evidence": []}
+
         else:
             # For single field paths, get the value directly
             if len(perplexity_path) == 1:
@@ -2442,12 +2365,12 @@
         """Process CoreSignal person field values with special handling for specific fields."""
         if value is None:
             return None
-        
+
         if field_path.endswith("time_zone"):
             # Infer timezone from location using GPT-4o-mini
             timezone_result = await self._infer_timezone_from_location(value)
             return timezone_result
-            
+
         elif field_path.endswith("employment_history"):
             # Extract last 4 experiences with required fields - Updated for clean API
             if isinstance(value, list):
@@ -2465,12 +2388,12 @@
                             "company_employees_count": exp.get("company_size_employees_count"),  # Changed from "company_employees_count"
                             "company_website": exp.get("company_website"),
                             "company_type": exp.get("company_type"),
-                            "company_industry": exp.get("company_industry")
+                            "company_industry": exp.get("company_industry"),
                         }
                         processed_experiences.append(processed_exp)
                 return processed_experiences
             return value
-            
+
         elif field_path.endswith("education"):
             # Extract last 2 education entries with required fields - Updated for clean API
             if isinstance(value, list):
@@ -2483,26 +2406,26 @@
                             "institution_name": edu.get("title"),  # Changed from "institution_name"
                             "date_from_year": edu.get("date_from"),
                             "date_to_year": edu.get("date_to"),
-                            "activities_and_societies": edu.get("activities_and_societies")
+                            "activities_and_societies": edu.get("activities_and_societies"),
                         }
                         processed_education.append(processed_edu)
                 return processed_education
             return value
-            
+
         elif field_path.endswith("years_of_experience"):
             # Convert months to years
             if isinstance(value, (int, float)) and value > 0:
                 years = round(value / 12, 1)
                 return f"{years} years"
             return value
-            
+
         return value
 
     async def _infer_timezone_from_location(self, location: str) -> str:
         """Infer timezone from location string using GPT-4o-mini for accurate timezone detection."""
         if not location:
             return "Unknown"
-        
+
         try:
             # Create a prompt for timezone inference
             timezone_prompt = f"""Given the location "{location}", determine the most likely timezone for business purposes.
@@ -2521,44 +2444,47 @@
 Location: {location}"""
 
             # Use the existing LLM instance to get timezone with LangSmith tracing
-            with trace_operation("timezone_inference", {
-                "location": location,
-                "model": "gpt-4o-mini"
-            }):
+            with trace_operation("timezone_inference", {"location": location, "model": "gpt-4o-mini"}):
                 system_message = SystemMessage(content="You are a timezone expert. Provide accurate timezone information for business locations.")
                 human_message = HumanMessage(content=timezone_prompt)
-                
+
                 # API monitoring: Track OpenAI API call
                 api_call_start_time = time.time()
-                
+
                 try:
                     response = await self.llm.ainvoke([system_message, human_message])
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track successful OpenAI API call
-                    self.logger.info("OpenAI API call successful", extra={
-                        "model": "gpt-4o-mini",
-                        "response_time": api_response_time,
-                        "api_call_type": "openai_timezone_inference",
-                        "location": location
-                    })
-                    
+                    self.logger.info(
+                        "OpenAI API call successful",
+                        extra={
+                            "model": "gpt-4o-mini",
+                            "response_time": api_response_time,
+                            "api_call_type": "openai_timezone_inference",
+                            "location": location,
+                        },
+                    )
+
                 except Exception as api_error:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track failed OpenAI API call
-                    self.logger.error("OpenAI API call failed", extra={
-                        "model": "gpt-4o-mini",
-                        "error": str(api_error),
-                        "response_time": api_response_time,
-                        "api_call_type": "openai_timezone_inference",
-                        "location": location
-                    })
-                    
+                    self.logger.error(
+                        "OpenAI API call failed",
+                        extra={
+                            "model": "gpt-4o-mini",
+                            "error": str(api_error),
+                            "response_time": api_response_time,
+                            "api_call_type": "openai_timezone_inference",
+                            "location": location,
+                        },
+                    )
+
                     raise api_error
-            
+
             timezone_result = response.content.strip()
-            
+
             # Validate the response format
             if timezone_result and not timezone_result.startswith("Unknown"):
                 print(f"‚úÖ GPT-4o-mini timezone inference: {location} -> {timezone_result}")
@@ -2566,19 +2492,15 @@
             else:
                 print(f"‚ö†Ô∏è GPT-4o-mini could not determine timezone for: {location}")
                 return f"Unknown (Location: {location})"
-                
+
         except Exception as e:
             print(f"‚ùå Error in GPT-4o-mini timezone inference: {e}")
             return f"Unknown (Location: {location})"
-
 
-
     def _format_person_single_source_value(self, value: Any, source_name: str, linkedin_url: str = None) -> Dict[str, Any]:
         """Format a single source value for person data with citation instead of source."""
-        result = {
-            "value": value
-        }
-        
+        result = {"value": value}
+
         # Add citation based on source type
         if source_name == "CoreSignal" and linkedin_url:
             result["citation"] = linkedin_url
@@ -2589,27 +2511,26 @@
         elif source_name == "Perplexity":
             # Perplexity data may already have citations in the value structure
             pass
-            
+
         return result
 
-    def _join_person_data_with_citations(self, coresignal_value: Any, perplexity_value: Any, apollo_value: Any, linkedin_url: str = None) -> Dict[str, Any]:
+    def _join_person_data_with_citations(
+        self, coresignal_value: Any, perplexity_value: Any, apollo_value: Any, linkedin_url: str = None
+    ) -> Dict[str, Any]:
         """Join person data from multiple sources with citation attribution."""
         combined_values = []
-        
+
         if coresignal_value is not None:
             combined_values.append(str(coresignal_value))
-            
+
         if perplexity_value is not None:
             combined_values.append(str(perplexity_value))
-            
+
         if apollo_value is not None:
             combined_values.append(str(apollo_value))
-        
-        result = {
-            "value": "; ".join(combined_values) if combined_values else None,
-            "timestamp": datetime.now().isoformat()
-        }
-        
+
+        result = {"value": "; ".join(combined_values) if combined_values else None, "timestamp": datetime.now().isoformat()}
+
         # Add citation if LinkedIn URL is available
         if linkedin_url:
             result["citation"] = linkedin_url
@@ -2617,15 +2538,15 @@
 
     def _set_person_nested_value(self, data: Dict, field_path: str, value: Any):
         """Set a nested value in person data using dot notation."""
-        keys = field_path.split('.')
+        keys = field_path.split(".")
         current = data
-        
+
         # Navigate to the parent of the final key
         for key in keys[:-1]:
             if key not in current:
                 current[key] = {}
             current = current[key]
-        
+
         # Set the final value
         current[keys[-1]] = value
 
@@ -2633,68 +2554,68 @@
         """Calculate what percentage of expected fields are populated"""
         total_fields = 0
         populated_fields = 0
-        
+
         for category_data in fused_result.values():
             if isinstance(category_data, dict):
                 for field_name, field_data in category_data.items():
                     total_fields += 1
-                    if isinstance(field_data, dict) and field_data.get('value'):
+                    if isinstance(field_data, dict) and field_data.get("value"):
                         populated_fields += 1
                     elif field_data:  # Non-dict values that are truthy
                         populated_fields += 1
-        
+
         return (populated_fields / total_fields * 100) if total_fields > 0 else 0.0
 
     async def _search_linkedin_mentions(self, person_name: str, company_name: str, run_id: str, company_id: str, user_id: str) -> Dict[str, Any]:
         """
         Search for LinkedIn posts that mention the person by name.
-        
+
         HOW POST MATCHING WORKS:
         1. Multiple Search Strategies: Uses 4 different search queries (full name, first+last, first only, last only)
-        2. Semantic Search: Searches Qdrant vector database in "linkedin_posts" collection 
+        2. Semantic Search: Searches Qdrant vector database in "linkedin_posts" collection
         3. Content Validation: Validates actual mentions using regex patterns for:
            - Full name matches
-           - First + last name combinations  
+           - First + last name combinations
            - Professional titles with last name (e.g., "CEO Smith", "Mr. Johnson")
            - Contextual patterns (e.g., "Smith joined", "appointed Johnson")
         4. Deduplication: Uses content hashing to remove duplicate posts
         5. Relevance Ranking: Sorts by semantic similarity score + engagement metrics
         6. GUARDRAIL: Limits to max 10 posts per person for cost control
-        
+
         Args:
             person_name: Full name of the person to search for
             company_name: Company name for context
             run_id: Run ID for filtering posts
             company_id: Company ID for filtering posts
             user_id: User ID for filtering posts
-            
+
         Returns:
             Dictionary with summary and raw posts mentioning the person
         """
         try:
             print(f"üîç Searching LinkedIn posts for mentions of {person_name}")
-            
+
             # Create search queries for the person
             first_name = person_name.split()[0] if person_name else ""
             last_name = person_name.split()[-1] if len(person_name.split()) > 1 else ""
-            
+
             # Multiple search strategies to catch variations
             search_queries = [
                 person_name,  # Full name
                 f"{first_name} {last_name}",  # First and last name
                 first_name,  # Just first name (less precise but catches informal mentions)
-                last_name   # Just last name (for surnames in professional context)
+                last_name,  # Just last name (for surnames in professional context)
             ]
-            
+
             all_mentions = []
             seen_content_hashes = set()
-            
+
             for query in search_queries:
                 if not query.strip():
                     continue
-                
+
                 print(f"   üîç Searching for: '{query}'")
-                
+
                 # Search in the linkedin_posts collection where LinkedIn posts are stored
                 search_results = search_chunks_in_qdrant(
                     query_text=query,
@@ -2703,149 +2624,146 @@
                     run_id=run_id,
                     user_id=user_id,  # Add user_id filtering for multi-tenant isolation
                     additional_filters={"section": "company_updates"},  # Only LinkedIn posts
-                    limit=20  # Get more results to filter through
+                    limit=20,  # Get more results to filter through
                 )
-                
+
                 for result in search_results:
-                    content = result.get('text', '')
-                    metadata = result.get('metadata', {})
-                    
+                    content = result.get("text", "")
+                    metadata = result.get("metadata", {})
+
                     # Check if the person is actually mentioned in the content
                     if self._is_person_mentioned_in_content(content, person_name, first_name, last_name):
                         # Create content hash for deduplication
                         import hashlib
+
                         content_hash = hashlib.md5(content.encode()).hexdigest()
-                        
+
                         if content_hash not in seen_content_hashes:
                             seen_content_hashes.add(content_hash)
-                            
+
                             # Extract LinkedIn URL from metadata
-                            linkedin_url = metadata.get('citation', '') or metadata.get('source', '')
-                            
-                            all_mentions.append({
-                                'content': content,
-                                'citation': linkedin_url,
-                                'date': metadata.get('post_date') or metadata.get('date', 'Unknown'),
-                                'engagement_score': metadata.get('engagement_score', 0),
-                                'reactions_count': metadata.get('reactions_count', 0),
-                                'comments_count': metadata.get('comments_count', 0),
-                                'score': result.get('score', 0.0),
-                                'metadata': metadata
-                            })
-            
+                            linkedin_url = metadata.get("citation", "") or metadata.get("source", "")
+
+                            all_mentions.append(
+                                {
+                                    "content": content,
+                                    "citation": linkedin_url,
+                                    "date": metadata.get("post_date") or metadata.get("date", "Unknown"),
+                                    "engagement_score": metadata.get("engagement_score", 0),
+                                    "reactions_count": metadata.get("reactions_count", 0),
+                                    "comments_count": metadata.get("comments_count", 0),
+                                    "score": result.get("score", 0.0),
+                                    "metadata": metadata,
+                                }
+                            )
+
             # Sort by relevance score and engagement
-            all_mentions.sort(key=lambda x: (x['score'], x['engagement_score']), reverse=True)
-            
+            all_mentions.sort(key=lambda x: (x["score"], x["engagement_score"]), reverse=True)
+
             # GUARDRAIL: Limit to max 10 most relevant mentions per person
             top_mentions = all_mentions[:10]
-            
+
             print(f"‚úÖ Found {len(top_mentions)} LinkedIn posts mentioning {person_name}")
-            
+
             if not top_mentions:
-                return {
-                    'summary': f"No LinkedIn posts found mentioning {person_name}",
-                    'raw_posts': []
-                }
-            
+                return {"summary": f"No LinkedIn posts found mentioning {person_name}", "raw_posts": []}
+
             # Generate summary using LLM
             summary = await self._generate_linkedin_mentions_summary(top_mentions, person_name, company_name)
-            
+
             # Format raw posts for output
             raw_posts = []
             for mention in top_mentions:
-                raw_posts.append({
-                    "content": mention['content'],
-                    "citation": mention.get('linkedin_url', 'https://linkedin.com/company/unknown'),  # Use LinkedIn URL as citation
-                    "date": mention.get('date'),
-                    "engagement_score": mention.get('engagement_score', 0)
-                })
-            
-            return {
-                'summary': summary,
-                'raw_posts': raw_posts
-            }
-            
+                raw_posts.append(
+                    {
+                        "content": mention["content"],
+                        "citation": mention.get("linkedin_url", "https://linkedin.com/company/unknown"),  # Use LinkedIn URL as citation
+                        "date": mention.get("date"),
+                        "engagement_score": mention.get("engagement_score", 0),
+                    }
+                )
+
+            return {"summary": summary, "raw_posts": raw_posts}
+
         except Exception as e:
             print(f"‚ùå Error searching LinkedIn mentions for {person_name}: {e}")
-            return {
-                'summary': f"Error searching LinkedIn mentions: {str(e)}",
-                'raw_posts': []
-            }
+            return {"summary": f"Error searching LinkedIn mentions: {str(e)}", "raw_posts": []}
 
     def _is_person_mentioned_in_content(self, content: str, full_name: str, first_name: str, last_name: str) -> bool:
         """
         Check if a person is mentioned in the LinkedIn post content.
-        
+
         Args:
             content: Post content to search in
             full_name: Full name of the person
             first_name: First name of the person
             last_name: Last name of the person
-            
+
         Returns:
             True if person is mentioned, False otherwise
         """
         if not content or not full_name:
             return False
-        
+
         content_lower = content.lower()
-        
+
         # Check for full name match
         if full_name.lower() in content_lower:
             return True
-        
+
         # Check for first name + last name combination
         if first_name and last_name:
             if first_name.lower() in content_lower and last_name.lower() in content_lower:
                 return True
-        
+
         # Check for professional titles with last name (e.g., "CEO Smith", "Mr. Johnson")
         if last_name:
             import re
+
             title_patterns = [
-                rf'\b(ceo|cfo|cto|coo|president|director|manager|partner|founder)\s+{re.escape(last_name.lower())}',
-                rf'\b(mr|ms|mrs|dr|prof)\.\s*{re.escape(last_name.lower())}',
-                rf'\b{re.escape(last_name.lower())}\s+(joined|appointed|promoted|announced)'
+                rf"\b(ceo|cfo|cto|coo|president|director|manager|partner|founder)\s+{re.escape(last_name.lower())}",
+                rf"\b(mr|ms|mrs|dr|prof)\.\s*{re.escape(last_name.lower())}",
+                rf"\b{re.escape(last_name.lower())}\s+(joined|appointed|promoted|announced)",
             ]
-            
+
             for pattern in title_patterns:
                 if re.search(pattern, content_lower):
                     return True
-        
+
         return False
 
     async def _generate_linkedin_mentions_summary(self, mentions: list, person_name: str, company_name: str) -> str:
         """
         Generate a summary of LinkedIn posts mentioning the person using LLM.
-        
+
         Args:
             mentions: List of LinkedIn post mentions (max 10 due to guardrail)
             person_name: Name of the person
             company_name: Company name for context
-            
+
         Returns:
             Summarized analysis of the mentions
         """
         if not mentions:
             return f"No LinkedIn posts found mentioning {person_name}"
-        
+
         try:
             # GUARDRAIL: Ensure max 10 posts for summarization
             mentions_to_analyze = mentions[:10]
-            
+
             # Prepare posts for LLM analysis
             posts_text = ""
             for i, mention in enumerate(mentions_to_analyze, 1):
-                content = mention['content'][:300]  # Limit content length
-                date = mention['date']
-                engagement = mention['reactions_count'] + mention['comments_count']
-                
+                content = mention["content"][:300]  # Limit content length
+                date = mention["date"]
+                engagement = mention["reactions_count"] + mention["comments_count"]
+
                 posts_text += f"""
 Post {i} (Date: {date}, Engagement: {engagement}):
 {content}
 ---
 """
-            
+
             # Use prompts from data_process_prompts.py
             system_prompt = get_linkedin_mentions_summary_system_prompt()
             user_prompt = get_linkedin_mentions_summary_user_prompt(person_name, company_name, posts_text)
@@ -2853,204 +2771,208 @@
             # Get LLM response with timeout
             from langchain_core.messages import SystemMessage, HumanMessage
             import asyncio
-            
+
             system_message = SystemMessage(content=system_prompt)
             human_message = HumanMessage(content=user_prompt)
 
             # Add timeout to prevent hanging
             try:
-                with trace_operation("linkedin_mentions_summary", {
-                    "person_name": person_name,
-                    "company_name": company_name,
-                    "mentions_count": len(mentions_to_analyze),
-                    "model": "gpt-4o-mini"
-                }):
+                with trace_operation(
+                    "linkedin_mentions_summary",
+                    {"person_name": person_name, "company_name": company_name, "mentions_count": len(mentions_to_analyze), "model": "gpt-4o-mini"},
+                ):
                     # API monitoring: Track OpenAI API call
                     api_call_start_time = time.time()
-                    
+
                     try:
                         response = await asyncio.wait_for(
-                            self.llm.ainvoke([system_message, human_message]), 
-                            timeout=30.0  # 30 second timeout
+                            self.llm.ainvoke([system_message, human_message]),
+                            timeout=30.0,  # 30 second timeout
                         )
                         api_response_time = time.time() - api_call_start_time
-                        
+
                         # API monitoring: Track successful OpenAI API call
-                        self.logger.info("OpenAI API call successful", extra={
-                            "model": "gpt-4o-mini",
-                            "response_time": api_response_time,
-                            "api_call_type": "openai_linkedin_mentions_summary",
-                            "person_name": person_name,
-                            "company_name": company_name,
-                            "mentions_count": len(mentions_to_analyze)
-                        })
-                        
+                        self.logger.info(
+                            "OpenAI API call successful",
+                            extra={
+                                "model": "gpt-4o-mini",
+                                "response_time": api_response_time,
+                                "api_call_type": "openai_linkedin_mentions_summary",
+                                "person_name": person_name,
+                                "company_name": company_name,
+                                "mentions_count": len(mentions_to_analyze),
+                            },
+                        )
+
                         summary = response.content.strip()
                         return summary
-                        
+
                     except Exception as api_error:
                         api_response_time = time.time() - api_call_start_time
-                        
+
                         # API monitoring: Track failed OpenAI API call
-                        self.logger.error("OpenAI API call failed", extra={
-                            "model": "gpt-4o-mini",
-                            "error": str(api_error),
-                            "response_time": api_response_time,
-                            "api_call_type": "openai_linkedin_mentions_summary",
-                            "person_name": person_name,
-                            "company_name": company_name,
-                            "mentions_count": len(mentions_to_analyze)
-                        })
-                        
+                        self.logger.error(
+                            "OpenAI API call failed",
+                            extra={
+                                "model": "gpt-4o-mini",
+                                "error": str(api_error),
+                                "response_time": api_response_time,
+                                "api_call_type": "openai_linkedin_mentions_summary",
+                                "person_name": person_name,
+                                "company_name": company_name,
+                                "mentions_count": len(mentions_to_analyze),
+                            },
+                        )
+
                         raise api_error
             except asyncio.TimeoutError:
                 print(f"‚ö†Ô∏è LLM timeout generating LinkedIn mentions summary for {person_name}")
                 # Fallback to simple summary
                 return self._generate_simple_mentions_summary(mentions, person_name)
-            
+
         except Exception as e:
             print(f"‚ùå Error generating LinkedIn mentions summary: {e}")
             # Fallback to simple summary
             return self._generate_simple_mentions_summary(mentions, person_name)
-    
+
     def _generate_simple_mentions_summary(self, mentions: list, person_name: str) -> str:
         """
         Generate a simple fallback summary without LLM.
-        
+
         Args:
             mentions: List of LinkedIn post mentions
             person_name: Name of the person
-            
+
         Returns:
             Simple summary of the mentions
         """
         if not mentions:
             return f"No LinkedIn posts found mentioning {person_name}"
-        
-        total_engagement = sum(m['reactions_count'] + m['comments_count'] for m in mentions)
-        recent_posts = len([m for m in mentions if m['date'] != 'Unknown'])
-        
+
+        total_engagement = sum(m["reactions_count"] + m["comments_count"] for m in mentions)
+        recent_posts = len([m for m in mentions if m["date"] != "Unknown"])
+
         summary = f"{person_name} mentioned in {len(mentions)} LinkedIn posts"
         if total_engagement > 0:
             summary += f" with {total_engagement} total engagement"
         if recent_posts > 0:
             summary += f" ({recent_posts} with dates)"
-        
+
         # Add context from first few posts
         contexts = []
         for mention in mentions[:3]:
-            content = mention['content'][:100].strip()
+            content = mention["content"][:100].strip()
             if content:
                 contexts.append(content)
-        
+
         if contexts:
             summary += f". Key contexts: {'; '.join(contexts)}"
-        
+
         return summary
 
     def _extract_linkedin_url_from_sources(self, fused_person_data: Dict = None, employee_data: Dict = None, person_name: str = "") -> Optional[str]:
         """
         Extract LinkedIn URL from multiple data sources with intelligent fallback strategy.
-        
+
         Args:
             fused_person_data: Fused person data from data fusion process
             employee_data: Raw CoreSignal employee data
             person_name: Person name for logging
-            
+
         Returns:
             LinkedIn URL string or None if not found
         """
         print(f"üîç Extracting LinkedIn URL for {person_name} from available sources...")
-        
+
         linkedin_url = None
         source_used = None
-        
+
         # Strategy 1: Try fused data structure (most comprehensive)
-        if fused_person_data and fused_person_data.get('success'):
+        if fused_person_data and fused_person_data.get("success"):
             print(f"   üìä Checking fused person data...")
-            fused_data = fused_person_data.get('data', {})
-            contact_identity = fused_data.get('contact_identity', {})
-            linkedin_field = contact_identity.get('linkedin_profile_url')
-            
+            fused_data = fused_person_data.get("data", {})
+            contact_identity = fused_data.get("contact_identity", {})
+            linkedin_field = contact_identity.get("linkedin_profile_url")
+
             if linkedin_field:
                 if isinstance(linkedin_field, dict):
                     # Structured format: {"value": "url", "citation": "..."}
-                    linkedin_url = linkedin_field.get('value')
+                    linkedin_url = linkedin_field.get("value")
                     if linkedin_url:
                         source_used = "fused_data_structured"
                 elif isinstance(linkedin_field, str):
                     # Direct string format
                     linkedin_url = linkedin_field
                     source_used = "fused_data_direct"
-        
+
         # Strategy 2: Try raw CoreSignal employee data (direct from API)
-        if not linkedin_url and employee_data and employee_data.get('success'):
+        if not linkedin_url and employee_data and employee_data.get("success"):
             print(f"   üîß Checking raw CoreSignal employee data...")
-            raw_data = employee_data.get('data', {})
-            
+            raw_data = employee_data.get("data", {})
+
             # Updated for clean API: use websites_linkedin field
-            if 'websites_linkedin' in raw_data and raw_data['websites_linkedin']:
-                linkedin_url = raw_data['websites_linkedin']
+            if "websites_linkedin" in raw_data and raw_data["websites_linkedin"]:
+                linkedin_url = raw_data["websites_linkedin"]
                 source_used = "coresignal_direct"
-        
+
         # Strategy 3: Validate and clean the URL
         if linkedin_url:
             # Clean and validate LinkedIn URL
             linkedin_url = linkedin_url.strip()
-            
+
             # Check if it's a valid LinkedIn URL
-            if '/in/' in linkedin_url and ('linkedin.com' in linkedin_url):
+            if "/in/" in linkedin_url and ("linkedin.com" in linkedin_url):
                 print(f"‚úÖ Found LinkedIn URL for {person_name} from {source_used}: {linkedin_url}")
                 return linkedin_url
             else:
                 print(f"‚ö†Ô∏è Invalid LinkedIn URL format found for {person_name}: {linkedin_url}")
                 return None
-        
+
         # Strategy 4: Debug logging to understand what data is available
         print(f"‚ùå No LinkedIn URL found for {person_name}. Available data:")
-        
+
         if fused_person_data:
-            contact_data = fused_person_data.get('data', {}).get('contact_identity', {})
+            contact_data = fused_person_data.get("data", {}).get("contact_identity", {})
             print(f"   - Fused contact_identity keys: {list(contact_data.keys())}")
-            
-            if 'linkedin_profile_url' in contact_data:
-                linkedin_value = contact_data['linkedin_profile_url']
+
+            if "linkedin_profile_url" in contact_data:
+                linkedin_value = contact_data["linkedin_profile_url"]
                 print(f"   - linkedin_profile_url type: {type(linkedin_value)}")
                 print(f"   - linkedin_profile_url value: {linkedin_value}")
-        
-        if employee_data and employee_data.get('success'):
-            raw_data = employee_data.get('data', {})
+
+        if employee_data and employee_data.get("success"):
+            raw_data = employee_data.get("data", {})
             print(f"   - CoreSignal employee data keys: {list(raw_data.keys())}")
-            if 'websites_linkedin' in raw_data:
+            if "websites_linkedin" in raw_data:
                 print(f"   - CoreSignal websites_linkedin: {raw_data.get('websites_linkedin')}")
-        
+
         return None
 
     async def _get_company_linkedin_mentions_value(self, linkedin_mentions_data: Dict, company_linkedin_mentions_path: list, field_path: str) -> Any:
         """
         Extract Company LinkedIn mentions data based on the specified path.
-        
+
         Args:
             linkedin_mentions_data: Company LinkedIn mentions data dictionary
             company_linkedin_mentions_path: Path to extract from (e.g., ['summary'] or ['raw_posts'])
             field_path: Field path for error reporting
-            
+
         Returns:
             Extracted value or None if not found
         """
         try:
             if not linkedin_mentions_data or not company_linkedin_mentions_path:
                 return None
-            
+
             value = linkedin_mentions_data
             for path_component in company_linkedin_mentions_path:
                 if isinstance(value, dict) and path_component in value:
                     value = value[path_component]
                 else:
                     return None
-            
+
             return value
-            
+
         except Exception as e:
             print(f"‚ùå Error extracting Company LinkedIn mentions value for {field_path}: {e}")
-            return None 
+            return None

--- app/agents/sub_agents/ria_detection_agent.py
+++ app/agents/sub_agents/ria_detection_agent.py
@@ -29,43 +29,42 @@
 class RIADetectionAgent(BaseSubAgent):
     """
     RIA Detection Agent that determines if a company is a Registered Investment Adviser.
-    
+
     Logic flow:
     1. Check if location is in US (using GPT-4o-mini)
     2. If US, try cache lookup by domain
     3. If cache miss, use Perplexity to search SEC IAPD
     """
-    
+
     def __init__(self, output_dir: str = "output", db=None):
         super().__init__(output_dir, db)
-        
+
         # Initialize LLM clients
-        self.openai_client = ChatOpenAI(
-            model="gpt-4o-mini",
-            temperature=0,
-            api_key=get_openai_api_key()
-        )
-        
-        self.perplexity_client = ChatPerplexity(
-            model="sonar-pro",
-            temperature=0.1,
-            api_key=get_perplexity_api_key()
-        )
-        
+        self.openai_client = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=get_openai_api_key())
+
+        self.perplexity_client = ChatPerplexity(model="sonar-pro", temperature=0.1, api_key=get_perplexity_api_key())
+
         # Cache for RIA index data
         self._ria_cache = None
         self._cache_loaded = False
-    
+
     @property
     def agent_name(self) -> str:
         return "RIA Detection Agent"
-    
-    async def execute(self, company_data: Dict[str, Any], run_id: str = None, 
-                     shared_output_file: str = None, db=None, company_id: str = None,
-                     user_id: str = None, session_id: str = None) -> Dict[str, Any]:
+
+    async def execute(
+        self,
+        company_data: Dict[str, Any],
+        run_id: str = None,
+        shared_output_file: str = None,
+        db=None,
+        company_id: str = None,
+        user_id: str = None,
+        session_id: str = None,
+    ) -> Dict[str, Any]:
         """
         Execute RIA detection for a company.
-        
+
         Args:
             company_data: Dictionary containing company information
             run_id: Run identifier
@@ -74,298 +73,281 @@
             company_id: Company identifier
             user_id: User identifier
             session_id: Session identifier
-            
+
         Returns:
             Dictionary with RIA detection results
         """
         start_time = datetime.now()
-        
+
         try:
-            company_name = company_data.get('name', 'Unknown Company')
-            location = company_data.get('location', 'Unknown')
-            domain = company_data.get('website_url', '')
-            
+            company_name = company_data.get("name", "Unknown Company")
+            location = company_data.get("location", "Unknown")
+            domain = company_data.get("website_url", "")
+
             print(f"üîç RIA Detection: {company_name} ({location})")
-            
+
             # Step 1: Check if location is in US (only if location is provided)
-            if location and location.lower() not in ['unknown', '']:
+            if location and location.lower() not in ["unknown", ""]:
                 is_us_location = await self._check_us_location(location)
-                
+
                 if not is_us_location:
                     result = {
-                        'is_ria': False,
-                        'crd_number': None,
-                        'method': 'location_rule',
-                        'reasoning': f'Location "{location}" is not in the United States',
-                        'domain_checked': domain,
-                        'evidence': {'location': location, 'is_us': False}
+                        "is_ria": False,
+                        "crd_number": None,
+                        "method": "location_rule",
+                        "reasoning": f'Location "{location}" is not in the United States',
+                        "domain_checked": domain,
+                        "evidence": {"location": location, "is_us": False},
                     }
-                    
+
                     print(f"   ‚ùå Non-US location: {location}")
                     await self._store_result(result, run_id, user_id, session_id, company_id, db)
                     return self._create_success_response(result, start_time)
-                
+
                 print(f"   ‚úÖ US location confirmed: {location}")
             else:
                 print(f"   ‚ö†Ô∏è No location provided, proceeding with domain check")
-            
+
             # Step 2: Try cache lookup by domain
             cache_result = await self._check_cache_lookup(domain)
-            
-            if cache_result and cache_result['is_ria']:
+
+            if cache_result and cache_result["is_ria"]:
                 print(f"   ‚úÖ Cache hit: CRD {cache_result['crd_number']}")
                 await self._store_result(cache_result, run_id, user_id, session_id, company_id, db)
                 return self._create_success_response(cache_result, start_time)
-            
+
             print(f"   ‚ö†Ô∏è Cache miss for domain: {domain}")
-            
+
             # Step 3: Perplexity fallback (only if cache didn't find RIA)
             perplexity_result = await self._check_perplexity_lookup(company_name)
-            
+
             print(f"   {'‚úÖ' if perplexity_result['is_ria'] else '‚ùå'} Perplexity result: {perplexity_result['method']}")
             await self._store_result(perplexity_result, run_id, user_id, session_id, company_id, db)
             return self._create_success_response(perplexity_result, start_time)
-            
+
         except Exception as e:
             error_msg = f"RIA detection failed: {str(e)}"
             print(f"   ‚ùå {error_msg}")
-            
+
             error_result = {
-                'is_ria': False,
-                'crd_number': None,
-                'method': 'error',
-                'reasoning': error_msg,
-                'domain_checked': company_data.get('website_url', ''),
-                'evidence': {'error': str(e)}
+                "is_ria": False,
+                "crd_number": None,
+                "method": "error",
+                "reasoning": error_msg,
+                "domain_checked": company_data.get("website_url", ""),
+                "evidence": {"error": str(e)},
             }
-            
+
             await self._store_result(error_result, run_id, user_id, session_id, company_id, db)
             return self._create_error_response(error_msg, start_time)
-    
+
     async def _check_us_location(self, location: str) -> bool:
         """Check if location is in the United States using GPT-4o-mini."""
         try:
             prompt = get_location_ria_detection_prompt(location)
-            
-            with trace_operation("location_ria_detection", {
-                "model": "gpt-4o-mini",
-                "location": location
-            }):
+
+            with trace_operation("location_ria_detection", {"model": "gpt-4o-mini", "location": location}):
                 response = await self.openai_client.ainvoke([HumanMessage(content=prompt)])
-            
+
             # Parse JSON response
             response_text = response.content.strip()
-            json_start = response_text.find('{')
-            json_end = response_text.rfind('}') + 1
-            
+            json_start = response_text.find("{")
+            json_end = response_text.rfind("}") + 1
+
             if json_start != -1 and json_end > json_start:
                 json_text = response_text[json_start:json_end]
                 result = json.loads(json_text)
-                return result.get('is_us_location', False)
-            
+                return result.get("is_us_location", False)
+
             # Fallback parsing
             response_lower = response_text.lower()
-            return 'true' in response_lower and 'false' not in response_lower
-            
+            return "true" in response_lower and "false" not in response_lower
+
         except Exception as e:
             print(f"   ‚ö†Ô∏è Location check failed: {e}")
             # Default to True to allow further processing
             return True
-    
+
     async def _check_cache_lookup(self, domain: str) -> Optional[Dict[str, Any]]:
         """Check RIA cache for domain match."""
         try:
             # Load cache if not already loaded
             if not self._cache_loaded:
                 await self._load_ria_cache()
-            
+
             if not self._ria_cache:
                 return None
-            
+
             # Normalize domain for comparison
             normalized_domain = self._normalize_domain(domain)
-            
+
             if not normalized_domain:
                 return None
-            
+
             # Search cache for exact domain match (using "domain" column)
             for row in self._ria_cache:
-                cache_domain = row.get('domain', '')
+                cache_domain = row.get("domain", "")
                 if not cache_domain:
                     continue
-                
+
                 if normalized_domain == cache_domain:
-                    crd_number = str(row.get('Organization CRD#', ''))
-                    website_address = row.get('Website Address', '')
-                    
+                    crd_number = str(row.get("Organization CRD#", ""))
+                    website_address = row.get("Website Address", "")
+
                     return {
-                        'is_ria': True,
-                        'crd_number': crd_number if crd_number else None,
-                        'method': 'cache',
-                        'reasoning': f'Found in RIA cache with exact domain match',
-                        'domain_checked': domain,
-                        'evidence': {
-                            'cache_domain': website_address,
-                            'normalized_match': normalized_domain,
-                            'crd': crd_number
-                        }
+                        "is_ria": True,
+                        "crd_number": crd_number if crd_number else None,
+                        "method": "cache",
+                        "reasoning": f"Found in RIA cache with exact domain match",
+                        "domain_checked": domain,
+                        "evidence": {"cache_domain": website_address, "normalized_match": normalized_domain, "crd": crd_number},
                     }
-            
+
             return None
-            
+
         except Exception as e:
             print(f"   ‚ö†Ô∏è Cache lookup failed: {e}")
             return None
-    
+
     async def _check_perplexity_lookup(self, company_name: str) -> Dict[str, Any]:
         """Use Perplexity to search SEC IAPD for RIA status."""
         try:
             prompt = get_ria_detection_user_prompt(company_name)
-            
-            with trace_operation("perplexity_ria_detection", {
-                "model": "sonar-pro",
-                "company_name": company_name
-            }):
+
+            with trace_operation("perplexity_ria_detection", {"model": "sonar-pro", "company_name": company_name}):
                 response = await self.perplexity_client.ainvoke([HumanMessage(content=prompt)])
-            
+
             # Parse response using the same logic as test_ria_manual.py
             result = self._parse_ria_response(response.content)
-            
+
             return {
-                'is_ria': result['is_ria'],
-                'crd_number': result['crd_number'],
-                'method': 'perplexity',
-                'reasoning': f'Perplexity SEC IAPD search result',
-                'domain_checked': '',
-                'evidence': {
-                    'raw_response': response.content,
-                    'parsed_result': result
-                }
+                "is_ria": result["is_ria"],
+                "crd_number": result["crd_number"],
+                "method": "perplexity",
+                "reasoning": f"Perplexity SEC IAPD search result",
+                "domain_checked": "",
+                "evidence": {"raw_response": response.content, "parsed_result": result},
             }
-            
+
         except Exception as e:
             print(f"   ‚ö†Ô∏è Perplexity lookup failed: {e}")
             return {
-                'is_ria': False,
-                'crd_number': None,
-                'method': 'perplexity_error',
-                'reasoning': f'Perplexity search failed: {str(e)}',
-                'domain_checked': '',
-                'evidence': {'error': str(e)}
+                "is_ria": False,
+                "crd_number": None,
+                "method": "perplexity_error",
+                "reasoning": f"Perplexity search failed: {str(e)}",
+                "domain_checked": "",
+                "evidence": {"error": str(e)},
             }
-    
+
     def _normalize_domain(self, url_or_domain: str) -> str:
         """Normalize domain/URL for comparison."""
         if not url_or_domain:
-            return ''
-        
+            return ""
+
         # Remove leading @ if present
-        domain = url_or_domain.lstrip('@')
-        
+        domain = url_or_domain.lstrip("@")
+
         # Remove protocol
-        domain = re.sub(r'^https?://', '', domain, flags=re.IGNORECASE)
-        
+        domain = re.sub(r"^https?://", "", domain, flags=re.IGNORECASE)
+
         # Remove www.
-        domain = re.sub(r'^www\.', '', domain, flags=re.IGNORECASE)
-        
+        domain = re.sub(r"^www\.", "", domain, flags=re.IGNORECASE)
+
         # Remove trailing slash and path
-        domain = domain.split('/')[0]
-        
+        domain = domain.split("/")[0]
+
         # Convert to lowercase
         domain = domain.lower()
-        
+
         return domain
-    
+
     def _parse_ria_response(self, response_text: str) -> Dict[str, Any]:
         """Parse Perplexity response using logic from test_ria_manual.py."""
-        result = {
-            'is_ria': False,
-            'crd_number': None,
-            'raw_response': response_text
-        }
-        
+        result = {"is_ria": False, "crd_number": None, "raw_response": response_text}
+
         try:
             # Clean up the response text
             cleaned_text = response_text.strip()
-            
+
             # Try to find JSON in the response
-            json_start = cleaned_text.find('{')
-            json_end = cleaned_text.rfind('}') + 1
-            
+            json_start = cleaned_text.find("{")
+            json_end = cleaned_text.rfind("}") + 1
+
             if json_start != -1 and json_end > json_start:
                 json_text = cleaned_text[json_start:json_end]
                 json_data = json.loads(json_text)
-                
-                result['is_ria'] = json_data.get('is_ria', False)
-                result['crd_number'] = json_data.get('crd_number')
-                
+
+                result["is_ria"] = json_data.get("is_ria", False)
+                result["crd_number"] = json_data.get("crd_number")
+
                 # Convert CRD to string if it's a number
-                if result['crd_number'] and isinstance(result['crd_number'], (int, float)):
-                    result['crd_number'] = str(int(result['crd_number']))
-                
+                if result["crd_number"] and isinstance(result["crd_number"], (int, float)):
+                    result["crd_number"] = str(int(result["crd_number"]))
+
                 # Handle null/None CRD
-                if result['crd_number'] in [None, 'null', '']:
-                    result['crd_number'] = None
+                if result["crd_number"] in [None, "null", ""]:
+                    result["crd_number"] = None
             else:
                 raise json.JSONDecodeError("No JSON found", cleaned_text, 0)
-                
+
         except json.JSONDecodeError:
             # Fallback parsing if JSON parsing fails
             response_lower = response_text.lower()
-            
+
             # Look for clear indicators
-            if any(phrase in response_lower for phrase in [
-                '"is_ria": true', '"is_ria":true', 'is_ria": true', 'is_ria":true',
-                '"is_ria": "true"', 'is_ria": "true"'
-            ]):
-                result['is_ria'] = True
-                
+            if any(
+                phrase in response_lower
+                for phrase in ['"is_ria": true', '"is_ria":true', 'is_ria": true', 'is_ria":true', '"is_ria": "true"', 'is_ria": "true"']
+            ):
+                result["is_ria"] = True
+
                 # Try to extract CRD from various patterns
                 crd_patterns = [
                     r'"crd_number":\s*"([^"]+)"',
                     r'"crd_number":\s*(\d+)',
                     r'crd_number":\s*"([^"]+)"',
                     r'crd_number":\s*(\d+)',
-                    r'crd[:\s]*(\d+)',
-                    r'(\d{6,7})'
+                    r"crd[:\s]*(\d+)",
+                    r"(\d{6,7})",
                 ]
-                
+
                 for pattern in crd_patterns:
                     crd_match = re.search(pattern, response_text, re.IGNORECASE)
                     if crd_match:
-                        result['crd_number'] = crd_match.group(1)
+                        result["crd_number"] = crd_match.group(1)
                         break
-                    
-            elif any(phrase in response_lower for phrase in [
-                '"is_ria": false', '"is_ria":false', 'is_ria": false', 'is_ria":false',
-                '"is_ria": "false"', 'is_ria": "false"'
-            ]):
-                result['is_ria'] = False
-                result['crd_number'] = None
-                
+
+            elif any(
+                phrase in response_lower
+                for phrase in ['"is_ria": false', '"is_ria":false', 'is_ria": false', 'is_ria":false', '"is_ria": "false"', 'is_ria": "false"']
+            ):
+                result["is_ria"] = False
+                result["crd_number"] = None
+
             else:
                 # Final fallback - look for any indicators
-                if any(word in response_lower for word in ['true', 'yes', 'registered', 'ria']):
-                    result['is_ria'] = True
+                if any(word in response_lower for word in ["true", "yes", "registered", "ria"]):
+                    result["is_ria"] = True
                     # Try to extract CRD
-                    crd_match = re.search(r'(\d{6,7})', response_text)
+                    crd_match = re.search(r"(\d{6,7})", response_text)
                     if crd_match:
-                        result['crd_number'] = crd_match.group(1)
+                        result["crd_number"] = crd_match.group(1)
                 else:
-                    result['is_ria'] = False
-                    result['crd_number'] = None
-        
+                    result["is_ria"] = False
+                    result["crd_number"] = None
+
         return result
-    
+
     async def _load_ria_cache(self):
         """Load RIA cache data using the existing _INDEX_CACHE system."""
         try:
             # Import the existing cache system
             from app.routers.prospecting_api import _INDEX_CACHE
-            
+
             # Load the index data
             df = _INDEX_CACHE.load()
-            
+
             # Check if "domain" column already exists
             if "domain" in df.columns:
                 print(f"   üìã Using existing 'domain' column from cache")
@@ -373,62 +355,53 @@
             else:
                 print(f"   üìã Creating 'domain' column from 'Website Address' (temporary fix)")
                 # Create domain column by normalizing Website Address values
-                df = df.with_columns([
-                    pl.col("Website Address").map_elements(
-                        lambda x: self._normalize_domain(x) if x else "", 
-                        return_dtype=pl.Utf8
-                    ).alias("domain")
-                ])
+                df = df.with_columns(
+                    [pl.col("Website Address").map_elements(lambda x: self._normalize_domain(x) if x else "", return_dtype=pl.Utf8).alias("domain")]
+                )
                 self._ria_cache = df.to_dicts()
-            
+
             self._cache_loaded = True
-            
+
             print(f"   üìã RIA cache loaded: {len(self._ria_cache)} entries")
-            
+
         except Exception as e:
             print(f"   ‚ö†Ô∏è Failed to load RIA cache: {e}")
             self._ria_cache = []
             self._cache_loaded = True
-    
-    async def _store_result(self, result: Dict[str, Any], run_id: str, user_id: str, 
-                           session_id: str, company_id: str, db) -> None:
+
+    async def _store_result(self, result: Dict[str, Any], run_id: str, user_id: str, session_id: str, company_id: str, db) -> None:
         """Store RIA detection result in database."""
         if not db or not run_id:
             return
-        
+
         try:
             await db.store_agent_result(
-                run_id=run_id,
-                user_id=user_id,
-                session_id=session_id,
-                agent_name="RIA Detection",
-                result_data=result,
-                company_id=company_id
+                run_id=run_id, user_id=user_id, session_id=session_id, agent_name="RIA Detection", result_data=result, company_id=company_id
             )
             print(f"   üíæ Stored RIA detection result in database")
         except Exception as e:
             print(f"   ‚ö†Ô∏è Failed to store RIA detection result: {e}")
-    
+
     def _create_success_response(self, result: Dict[str, Any], start_time: datetime) -> Dict[str, Any]:
         """Create success response."""
         execution_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
-        
+
         return {
-            'success': True,
-            'agent_name': self.agent_name,
-            'result': result,
-            'execution_time_ms': execution_time_ms,
-            'timestamp': datetime.now().isoformat()
+            "success": True,
+            "agent_name": self.agent_name,
+            "result": result,
+            "execution_time_ms": execution_time_ms,
+            "timestamp": datetime.now().isoformat(),
         }
-    
+
     def _create_error_response(self, error_msg: str, start_time: datetime) -> Dict[str, Any]:
         """Create error response."""
         execution_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
-        
+
         return {
-            'success': False,
-            'agent_name': self.agent_name,
-            'error': error_msg,
-            'execution_time_ms': execution_time_ms,
-            'timestamp': datetime.now().isoformat()
+            "success": False,
+            "agent_name": self.agent_name,
+            "error": error_msg,
+            "execution_time_ms": execution_time_ms,
+            "timestamp": datetime.now().isoformat(),
         }

--- app/agents/sub_agents/user_prompt_extractor.py
+++ app/agents/sub_agents/user_prompt_extractor.py
@@ -13,28 +13,29 @@
 from app.utils.config import get_openai_api_key
 import time
 
+
 class UserPromptExtractorAgent(BaseSubAgent):
     """
     Sub-agent that extracts structured information from a user prompt using LLM only.
     Extracts: company_name, location_or_industry, key_area_of_focus.
     If extraction fails or any field is missing, returns an error and writes a message to output.
     """
-    
+
     def __init__(self, llm=None, output_dir: str = "app/data"):
         # Initialize LangSmith
         initialize_langsmith()
-        
+
         super().__init__(output_dir)
         self.llm = llm or ChatOpenAI(model="gpt-4", temperature=0, api_key=get_openai_api_key())
         self.extract_prompt = get_company_info_prompt()
-        
+
         logger = get_logger(__name__)
         logger.info("UserPromptExtractorAgent initialized", extra={"output_dir": output_dir, "model": "gpt-4"})
-    
+
     @property
     def agent_name(self) -> str:
         return "User Prompt Extractor"
-    
+
     async def execute(self, user_prompt: str, output_file: Optional[str] = None) -> Dict[str, Any]:
         """
         Extract structured information from a user prompt using LLM only.
@@ -46,18 +47,13 @@
         """
         logger = get_logger(__name__)
         logger.info("User prompt extraction started", extra={"prompt_length": len(user_prompt) if user_prompt else 0})
-        
+
         if not user_prompt:
-            error_msg = 'No user prompt provided.'
+            error_msg = "No user prompt provided."
             logger.error("No user prompt provided")
             if output_file:
                 await self.append_markdown(output_file, "Step 1: Extracted Company Info", error_msg)
-            return {
-                'success': False,
-                'error': error_msg,
-                'output_file': output_file,
-                'agent_name': self.agent_name
-            }
+            return {"success": False, "error": error_msg, "output_file": output_file, "agent_name": self.agent_name}
         try:
             # If no output file, create one
             if not output_file:
@@ -73,43 +69,39 @@
                 logger.info("Created new output file", extra={"run_id": run_id, "output_file": output_file})
             # LLM extraction with LangSmith tracing
             logger.info("Starting LLM extraction", extra={"model": "gpt-4", "prompt_length": len(user_prompt)})
-            
+
             # API monitoring: Track OpenAI API call
             api_call_start_time = time.time()
-            
-            with trace_operation("user_prompt_extraction", {
-                "user_prompt_length": len(user_prompt),
-                "model": "gpt-4"
-            }):
-                messages = [
-                    SystemMessage(content=self.extract_prompt),
-                    HumanMessage(content=user_prompt)
-                ]
-                
+
+            with trace_operation("user_prompt_extraction", {"user_prompt_length": len(user_prompt), "model": "gpt-4"}):
+                messages = [SystemMessage(content=self.extract_prompt), HumanMessage(content=user_prompt)]
+
                 try:
                     response = await self.llm.ainvoke(messages)
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track successful OpenAI API call
-                    logger.info("OpenAI API call successful", extra={
-                        "model": "gpt-4",
-                        "response_time": api_response_time,
-                        "api_call_type": "openai_user_prompt_extraction"
-                    })
-                    
+                    logger.info(
+                        "OpenAI API call successful",
+                        extra={"model": "gpt-4", "response_time": api_response_time, "api_call_type": "openai_user_prompt_extraction"},
+                    )
+
                 except Exception as api_error:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track failed OpenAI API call
-                    logger.error("OpenAI API call failed", extra={
-                        "model": "gpt-4",
-                        "error": str(api_error),
-                        "response_time": api_response_time,
-                        "api_call_type": "openai_user_prompt_extraction"
-                    })
-                    
+                    logger.error(
+                        "OpenAI API call failed",
+                        extra={
+                            "model": "gpt-4",
+                            "error": str(api_error),
+                            "response_time": api_response_time,
+                            "api_call_type": "openai_user_prompt_extraction",
+                        },
+                    )
+
                     raise api_error
-                    
+
             company_info = getattr(response, "content", str(response))
             logger.info("LLM extraction completed", extra={"response_length": len(company_info)})
             # Try JSON parse only
@@ -119,42 +111,37 @@
             except Exception as parse_error:
                 info = None
                 logger.error("JSON parsing failed", extra={"error": str(parse_error), "response_content": company_info[:200]})
-            
+
             required_fields = ["company_name", "location_or_industry", "key_area_of_focus"]
             if not info or not all(info.get(f) not in (None, "", "null") for f in required_fields):
                 error_msg = "The necessary information is not present in the user prompt."
-                logger.error("Required fields missing", extra={"required_fields": required_fields, "extracted_fields": list(info.keys()) if info else []})
+                logger.error(
+                    "Required fields missing", extra={"required_fields": required_fields, "extracted_fields": list(info.keys()) if info else []}
+                )
                 await self.append_markdown(output_file, "Step 1: Extracted Company Info", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg,
-                    'output_file': output_file,
-                    'agent_name': self.agent_name
-                }
+                return {"success": False, "error": error_msg, "output_file": output_file, "agent_name": self.agent_name}
             # Success: all fields present
-            logger.info("User prompt extraction completed successfully", extra={
-                "company_name": info.get('company_name'),
-                "location_or_industry": info.get('location_or_industry'),
-                "key_area_of_focus": info.get('key_area_of_focus')
-            })
+            logger.info(
+                "User prompt extraction completed successfully",
+                extra={
+                    "company_name": info.get("company_name"),
+                    "location_or_industry": info.get("location_or_industry"),
+                    "key_area_of_focus": info.get("key_area_of_focus"),
+                },
+            )
             await self.append_markdown(output_file, "Step 1: Extracted Company Info", json.dumps(info, indent=2))
             return {
-                'success': True,
-                'extracted_info': info,
-                'output_file': output_file,
-                'agent_name': self.agent_name,
-                'company_name': info.get('company_name'),
-                'location_or_industry': info.get('location_or_industry'),
-                'key_area_of_focus': info.get('key_area_of_focus')
+                "success": True,
+                "extracted_info": info,
+                "output_file": output_file,
+                "agent_name": self.agent_name,
+                "company_name": info.get("company_name"),
+                "location_or_industry": info.get("location_or_industry"),
+                "key_area_of_focus": info.get("key_area_of_focus"),
             }
         except Exception as e:
             error_msg = f"Extraction failed: {str(e)}"
             logger.exception("User prompt extraction failed", extra={"error": str(e)})
             if output_file:
                 await self.append_markdown(output_file, "Step 1: Extracted Company Info", error_msg)
-            return {
-                'success': False,
-                'error': error_msg,
-                'output_file': output_file,
-                'agent_name': self.agent_name
-            } 
\ No newline at end of file
+            return {"success": False, "error": error_msg, "output_file": output_file, "agent_name": self.agent_name}

--- app/agents/sub_agents/web_research_agent.py
+++ app/agents/sub_agents/web_research_agent.py
@@ -10,7 +10,7 @@
     get_osint_user_prompt_v2,
     get_osint_system_prompt_website_v2,
     get_osint_system_prompt_external_v2,
-    get_c_suite_members_extract_prompt
+    get_c_suite_members_extract_prompt,
 )
 from app.tools.search_module import SearchModule
 from app.utils.perplexity_utils import merge_perplexity_outputs, safe_parse_perplexity_content
@@ -25,6 +25,7 @@
 
 logger = get_logger(__name__)
 
+
 class WebResearchAgent(BaseSubAgent):
     """
     Sub-agent that performs deep web research using OSINT methodology.
@@ -34,7 +35,7 @@
     def __init__(self, mcp_tools: List[Any], output_dir: str = "app/data", db: Optional[ProspectingDB] = None):
         # Initialize LangSmith
         initialize_langsmith()
-        
+
         super().__init__(output_dir, db)
         self.mcp_tools = mcp_tools
         logger.info("WebResearchAgent initialized", extra={"output_dir": output_dir, "mcp_tools_count": len(mcp_tools)})
@@ -42,13 +43,24 @@
     @property
     def agent_name(self) -> str:
         return "Web Research Agent"
-        
 
-
-    async def execute(self, company_data, run_id=None, shared_output_file=None, db=None, postgres_enabled=None, company_id=None, found_url=None, found_urls=None, found_by_perplexity=False, user_id=None, session_id=None):
+    async def execute(
+        self,
+        company_data,
+        run_id=None,
+        shared_output_file=None,
+        db=None,
+        postgres_enabled=None,
+        company_id=None,
+        found_url=None,
+        found_urls=None,
+        found_by_perplexity=False,
+        user_id=None,
+        session_id=None,
+    ):
         """
         Execute Web Research Agent using V2 prompts only.
-        
+
         Args:
             company_data: Dictionary containing company information
             run_id: Optional run ID for tracking
@@ -60,32 +72,36 @@
             found_by_perplexity: Whether URL was found by Perplexity (determines OSINT strategy)
             user_id: User identifier for data isolation
             session_id: Session identifier
-        
+
         Returns:
             Dictionary with website_result, external_result, and merged_data from V2 OSINT
         """
         start_time = time.time()
-        
-        logger.info("Web research execution started", extra={
-            "run_id": run_id, 
-            "user_id": user_id, 
-            "session_id": session_id, 
-            "company_name": company_data.get('name', ''),
-            "found_by_perplexity": found_by_perplexity,
-            "has_found_url": bool(found_url),
-            "has_found_urls": bool(found_urls)
-        })
-        
+
+        logger.info(
+            "Web research execution started",
+            extra={
+                "run_id": run_id,
+                "user_id": user_id,
+                "session_id": session_id,
+                "company_name": company_data.get("name", ""),
+                "found_by_perplexity": found_by_perplexity,
+                "has_found_url": bool(found_url),
+                "has_found_urls": bool(found_urls),
+            },
+        )
+
         # Initialize c_suite_members to None to prevent UnboundLocalError
         c_suite_members = None
-        
+
         # Initialize PostgreSQL connection if not provided
         if postgres_enabled is None:
             postgres_enabled = get_enable_postgres_storage()
-            
+
         if not db and postgres_enabled:
             try:
                 from app.utils.global_db import get_global_db
+
                 db = await get_global_db()
                 print(f"üîó WebResearchAgent: Using global database instance: {id(db)}")
                 logger.info("Using global database instance", extra={"run_id": run_id, "db_instance_id": id(db)})
@@ -94,30 +110,27 @@
                 logger.warning("PostgreSQL disabled due to connection error", extra={"run_id": run_id, "error": str(e)})
                 postgres_enabled = False
                 db = None
-        
+
         try:
-            company_name = company_data.get('name', '')
-            location = company_data.get('location', '')
-            focus_area = company_data.get('focus_area', '')  # Changed from 'General Information' to empty string
-            website_url = company_data.get('website_url', '')
+            company_name = company_data.get("name", "")
+            location = company_data.get("location", "")
+            focus_area = company_data.get("focus_area", "")  # Changed from 'General Information' to empty string
+            website_url = company_data.get("website_url", "")
             # Progress: web research started
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'web_progress', 0.10)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "web_progress", 0.10)
             except Exception:
                 pass
-            
+
             print(f"DEBUG: WebResearchAgent using V2 prompts only")
             print(f"DEBUG: Company name: '{company_name}', Location: '{location}', Focus: '{focus_area}'")
-            
-            logger.info("Company data extracted", extra={
-                "run_id": run_id,
-                "company_name": company_name,
-                "location": location,
-                "focus_area": focus_area,
-                "website_url": website_url
-            })
-            
+
+            logger.info(
+                "Company data extracted",
+                extra={"run_id": run_id, "company_name": company_name, "location": location, "focus_area": focus_area, "website_url": website_url},
+            )
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Step 4: Web Research", f"Starting web research for {company_name}")
 
@@ -133,83 +146,79 @@
             # Determine OSINT strategy based on found_by_perplexity flag
             if found_by_perplexity:
                 website_url = found_url
-                company_data['website_url'] = website_url
+                company_data["website_url"] = website_url
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 4: Web Research - Website URL Found", website_url)
-                
+
                 # Execute V2 Perplexity OSINT calls in parallel
                 perp_api_token = get_perplexity_api_key()
                 url = "https://api.perplexity.ai/chat/completions"
-                headers = {
-                    "Authorization": f"Bearer {perp_api_token}",
-                    "Content-Type": "application/json"
-                }
-                
+                headers = {"Authorization": f"Bearer {perp_api_token}", "Content-Type": "application/json"}
+
                 # Set up V2 prompts
                 system_prompt_website = get_osint_system_prompt_website_v2(company_name, website_url, location, focus_area)
                 system_prompt_external = get_osint_system_prompt_external_v2(company_name, website_url, location, focus_area)
                 user_prompt = get_osint_user_prompt_v2(company_name, website_url, location, focus_area)  # Use V2 user prompt
-                
+
                 # Extract domains from all URLs for proper domain filtering
                 domains = []
                 try:
                     from urllib.parse import urlparse
-                    
+
                     # Use found_urls if available (multiple domains), otherwise use single found_url
                     urls_to_process = found_urls if found_urls and len(found_urls) > 1 else [website_url]
-                    
+
                     for url in urls_to_process:
                         if url:
                             parsed_url = urlparse(url)
                             domain = parsed_url.netloc
                             # Remove 'www.' prefix if present
-                            if domain.startswith('www.'):
+                            if domain.startswith("www."):
                                 domain = domain[4:]
                             if domain and domain not in domains:
                                 domains.append(domain)
-                    
+
                     print(f"‚úÖ Found {len(domains)} domains for filtering: {domains}")
                     logger.info("Domains extracted for filtering", extra={"run_id": run_id, "domains_count": len(domains), "domains": domains})
-                    
+
                 except Exception as e:
                     print(f"‚ö†Ô∏è Error parsing domains: {e}")
                     logger.warning("Error parsing domains", extra={"run_id": run_id, "error": str(e)})
                     domains = [website_url]  # Fallback to original URL
-                
+
                 # Use primary domain for compatibility (first domain)
                 primary_domain = domains[0] if domains else website_url
-                
 
-                
                 # Prepare company/ZoomInfo domain early for parallel C-suite search
                 company_domain = None
                 zoominfo_domain = None
                 try:
                     from urllib.parse import urlparse
+
                     if found_urls and len(found_urls) > 0:
                         parsed = urlparse(found_urls[0])
-                        company_domain = parsed.netloc.replace('www.', '')
+                        company_domain = parsed.netloc.replace("www.", "")
                     elif found_url:
                         parsed = urlparse(found_url)
-                        company_domain = parsed.netloc.replace('www.', '')
+                        company_domain = parsed.netloc.replace("www.", "")
                     else:
-                        company_domain = company_data.get('website_url', '').replace('https://', '').replace('http://', '').replace('www.', '')
+                        company_domain = company_data.get("website_url", "").replace("https://", "").replace("http://", "").replace("www.", "")
                     zoominfo_domain = f"{company_name.lower().replace(' ', '')}.zoominfo.com"
                 except Exception:
-                    company_domain = company_data.get('website_url', '').replace('https://', '').replace('http://', '').replace('www.', '')
+                    company_domain = company_data.get("website_url", "").replace("https://", "").replace("http://", "").replace("www.", "")
                     zoominfo_domain = f"{company_name.lower().replace(' ', '')}.zoominfo.com"
 
                 # Run Website, External, and C-Suite searches in parallel
                 website_task = self._perplexity_osint_search(system_prompt_website, user_prompt, "WEBSITE", domains)
                 external_task = self._perplexity_osint_search(system_prompt_external, user_prompt, "EXTERNAL", domains)
                 c_suite_task = self._search_c_suite_members(company_name, company_domain, zoominfo_domain)
-                
+
                 logger.info("Starting parallel OSINT + C-suite searches", extra={"run_id": run_id, "domains_count": len(domains)})
-                
+
                 website_result, external_result, c_suite_result = await asyncio.gather(
                     website_task, external_task, c_suite_task, return_exceptions=True
                 )
-                
+
                 # Handle exceptions from parallel execution
                 if isinstance(website_result, Exception):
                     print(f"‚ö†Ô∏è Website OSINT failed: {website_result}")
@@ -219,8 +228,13 @@
                     # Ensure website_result is always a dict
                     print(f"‚ö†Ô∏è Website OSINT returned unexpected type: {type(website_result)}")
                     logger.error("Website OSINT returned unexpected type", extra={"run_id": run_id, "type": str(type(website_result))})
-                    website_result = {"content": None, "citations": None, "success": False, "error": f"Unexpected result type: {type(website_result)}"}
-                    
+                    website_result = {
+                        "content": None,
+                        "citations": None,
+                        "success": False,
+                        "error": f"Unexpected result type: {type(website_result)}",
+                    }
+
                 if isinstance(external_result, Exception):
                     print(f"‚ö†Ô∏è External OSINT failed: {external_result}")
                     logger.error("External OSINT failed", extra={"run_id": run_id, "error": str(external_result)})
@@ -229,8 +243,13 @@
                     # Ensure external_result is always a dict
                     print(f"‚ö†Ô∏è External OSINT returned unexpected type: {type(external_result)}")
                     logger.error("External OSINT returned unexpected type", extra={"run_id": run_id, "type": str(type(external_result))})
-                    external_result = {"content": None, "citations": None, "success": False, "error": f"Unexpected result type: {type(external_result)}"}
-                
+                    external_result = {
+                        "content": None,
+                        "citations": None,
+                        "success": False,
+                        "error": f"Unexpected result type: {type(external_result)}",
+                    }
+
                 # Normalize c-suite result
                 if isinstance(c_suite_result, Exception):
                     print(f"‚ö†Ô∏è C-suite search failed: {c_suite_result}")
@@ -243,36 +262,49 @@
                 if website_result.get("success") and external_result.get("success"):
                     print(f"‚úÖ Both V2 OSINT calls successful, processing and merging...")
                     logger.info("Both OSINT searches successful", extra={"run_id": run_id})
-                    
+
                     # Parse both contents
-                    website_content = safe_parse_perplexity_content(website_result.get('content'))
-                    external_content = safe_parse_perplexity_content(external_result.get('content'))
-                    
-                    logger.info("OSINT content parsed", extra={"run_id": run_id, "website_content_type": type(website_content).__name__, "external_content_type": type(external_content).__name__})
-                    
+                    website_content = safe_parse_perplexity_content(website_result.get("content"))
+                    external_content = safe_parse_perplexity_content(external_result.get("content"))
+
+                    logger.info(
+                        "OSINT content parsed",
+                        extra={
+                            "run_id": run_id,
+                            "website_content_type": type(website_content).__name__,
+                            "external_content_type": type(external_content).__name__,
+                        },
+                    )
+
                     # Write raw V2 outputs to shared debug file before merge
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
                             "Step 4: Web Research - V2 Website OSINT (Raw)",
-                            f"```json\n{json.dumps(website_content, indent=2, ensure_ascii=False)}\n```"
+                            f"```json\n{json.dumps(website_content, indent=2, ensure_ascii=False)}\n```",
                         )
                         await self.append_markdown(
                             shared_output_file,
                             "Step 4: Web Research - V2 External OSINT (Raw)",
-                            f"```json\n{json.dumps(external_content, indent=2, ensure_ascii=False)}\n```"
+                            f"```json\n{json.dumps(external_content, indent=2, ensure_ascii=False)}\n```",
                         )
 
                     # Merge data
                     merged_data = merge_perplexity_outputs(website_content, external_content)
-                    logger.info("OSINT data merged", extra={"run_id": run_id, "merged_data_keys": list(merged_data.keys()) if isinstance(merged_data, dict) else []})
+                    logger.info(
+                        "OSINT data merged",
+                        extra={"run_id": run_id, "merged_data_keys": list(merged_data.keys()) if isinstance(merged_data, dict) else []},
+                    )
 
                     # Attach C-suite members if available
                     if c_suite_members is not None:
-                        if 'organization_decision_making' not in merged_data or not isinstance(merged_data['organization_decision_making'], dict):
-                            merged_data['organization_decision_making'] = {}
-                        merged_data['organization_decision_making']['c_suite_members'] = c_suite_members
-                        logger.info("C-suite members added to merged data", extra={"run_id": run_id, "c_suite_count": len(c_suite_members) if isinstance(c_suite_members, list) else 0})
+                        if "organization_decision_making" not in merged_data or not isinstance(merged_data["organization_decision_making"], dict):
+                            merged_data["organization_decision_making"] = {}
+                        merged_data["organization_decision_making"]["c_suite_members"] = c_suite_members
+                        logger.info(
+                            "C-suite members added to merged data",
+                            extra={"run_id": run_id, "c_suite_count": len(c_suite_members) if isinstance(c_suite_members, list) else 0},
+                        )
                     else:
                         logger.warning("C-suite search returned no results", extra={"run_id": run_id})
 
@@ -288,150 +320,157 @@
                             "domain_count": len(found_urls) if found_urls else 1,
                             "run_id": run_id,
                             "timestamp": datetime.now().isoformat(),
-                            "found_by_perplexity": found_by_perplexity
+                            "found_by_perplexity": found_by_perplexity,
                         }
-                    
-                    logger.info("Metadata added to merged data", extra={"run_id": run_id, "metadata_keys": list(merged_data.get("metadata", {}).keys())})
-                    
+
+                    logger.info(
+                        "Metadata added to merged data", extra={"run_id": run_id, "metadata_keys": list(merged_data.get("metadata", {}).keys())}
+                    )
+
                     # Append merged results to shared output file
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
                             "Step 4: Web Research - V2 Merged Results",
-                            f"**V2 OSINT Results:**\n\n```json\n{json.dumps(merged_data, indent=2, ensure_ascii=False)}\n```"
+                            f"**V2 OSINT Results:**\n\n```json\n{json.dumps(merged_data, indent=2, ensure_ascii=False)}\n```",
                         )
-                    
+
                     print(f"‚úÖ V2 data merged successfully")
                     logger.info("V2 data merged successfully", extra={"run_id": run_id})
                     # Progress: web research mid (optional)
                     try:
                         if db and run_id:
-                            await ProgressStore.instance().update_subprogress(db, run_id, 'web_progress', 0.50)
+                            await ProgressStore.instance().update_subprogress(db, run_id, "web_progress", 0.50)
                     except Exception:
                         pass
                 else:
                     print(f"‚ùå One or both V2 OSINT calls failed")
-                    logger.error("One or both V2 OSINT calls failed", extra={"run_id": run_id, "website_success": website_result.get("success"), "external_success": external_result.get("success")})
+                    logger.error(
+                        "One or both V2 OSINT calls failed",
+                        extra={
+                            "run_id": run_id,
+                            "website_success": website_result.get("success"),
+                            "external_success": external_result.get("success"),
+                        },
+                    )
                     merged_data = None
-                
+
                 # Prepare results for return (V2 only) - use .get() for safe access
                 results = {
-                    'success': website_result.get("success", False) and external_result.get("success", False),
-                    'website_result': website_result,
-                    'external_result': external_result,
-                    'merged_data': merged_data,
-                    'output_file': shared_output_file,
-                    'agent_name': self.agent_name,
-                    'company_name': company_name,
-                    'c_suite_members': c_suite_members
+                    "success": website_result.get("success", False) and external_result.get("success", False),
+                    "website_result": website_result,
+                    "external_result": external_result,
+                    "merged_data": merged_data,
+                    "output_file": shared_output_file,
+                    "agent_name": self.agent_name,
+                    "company_name": company_name,
+                    "c_suite_members": c_suite_members,
                 }
-                
+
                 # Store in PostgreSQL if enabled and successful
-                if postgres_enabled and db and results.get('success') and merged_data:
+                if postgres_enabled and db and results.get("success") and merged_data:
                     try:
                         # Use provided company_id or create one if not provided
                         storage_company_id = company_id
                         if not storage_company_id:
-                            storage_company_id = await db.store_company(run_id or 'default', company_data, user_id=user_id, session_id=session_id)
+                            storage_company_id = await db.store_company(run_id or "default", company_data, user_id=user_id, session_id=session_id)
                             print(f"‚ö†Ô∏è No company_id provided to Web Research agent, created new one: {storage_company_id}")
                             logger.warning("No company_id provided, created new one", extra={"run_id": run_id, "new_company_id": storage_company_id})
                         else:
                             print(f"‚úÖ Using provided domain-based company_id: {storage_company_id}")
                             logger.info("Using provided company_id", extra={"run_id": run_id, "company_id": storage_company_id})
-                        
+
                         # Store V2 merged data using new agent_data structure
                         await db.store_agent_result(
                             run_id=run_id,
                             user_id=user_id,
                             session_id=session_id,
                             agent_name=self.agent_name,
-                            result_data={
-                                'perplexity_merged_data': merged_data
-                            },
-                            company_id=storage_company_id
+                            result_data={"perplexity_merged_data": merged_data},
+                            company_id=storage_company_id,
                         )
-                        
+
                         print(f"‚úÖ V2 merged data stored in PostgreSQL for company_id: {storage_company_id} (session: {session_id})")
-                        logger.info("V2 merged data stored in PostgreSQL", extra={"run_id": run_id, "company_id": storage_company_id, "session_id": session_id})
-                            
+                        logger.info(
+                            "V2 merged data stored in PostgreSQL",
+                            extra={"run_id": run_id, "company_id": storage_company_id, "session_id": session_id},
+                        )
+
                     except Exception as db_error:
                         print(f"‚ùå Error storing V2 data in PostgreSQL: {db_error}")
                         logger.error("Failed to store V2 data in PostgreSQL", extra={"run_id": run_id, "error": str(db_error)})
-                
+
                 return results
-                
+
             else:
                 # Run external V2 OSINT only when URL was not found by Perplexity
                 print(f"‚ö†Ô∏è URL was not found by Perplexity for {company_name}, running external V2 OSINT only")
                 logger.info("Running external OSINT only (no Perplexity URL)", extra={"run_id": run_id, "company_name": company_name})
-                
+
                 if shared_output_file:
-                    await self.append_markdown(shared_output_file, "Step 4: Web Research - No Perplexity URL", f"URL was not found by Perplexity for {company_name}, proceeding with external sources only")
-                
+                    await self.append_markdown(
+                        shared_output_file,
+                        "Step 4: Web Research - No Perplexity URL",
+                        f"URL was not found by Perplexity for {company_name}, proceeding with external sources only",
+                    )
+
                 # Setup external V2 OSINT call
                 system_prompt_external = get_osint_system_prompt_external_v2(company_name, None, location, focus_area)
                 user_prompt = get_osint_user_prompt_v2(company_name, None, location, focus_area)  # Use V2 user prompt
-                
-                chat = ChatPerplexity(
-                    temperature=0, 
-                    model="sonar-pro",
-                    api_key=get_perplexity_api_key()
-                )
-                
+
+                chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=get_perplexity_api_key())
+
                 try:
                     logger.info("Starting external OSINT search", extra={"run_id": run_id, "company_name": company_name})
-                    with trace_operation("external_osint_search", {
-                        "company_name": company_name,
-                        "model": "sonar-pro",
-                        "search_type": "external_osint",
-                        "source_type": "external",
-                        "provider": "perplexity"
-                    }):
+                    with trace_operation(
+                        "external_osint_search",
+                        {
+                            "company_name": company_name,
+                            "model": "sonar-pro",
+                            "search_type": "external_osint",
+                            "source_type": "external",
+                            "provider": "perplexity",
+                        },
+                    ):
                         response = await chat.ainvoke(
-                            [
-                                ("system", system_prompt_external),
-                                ("user", user_prompt)
-                            ],
-                            extra_body={
-                                "search_domain_filter": [],
-                                "web_search_options": {"search_context_size": "high"}
-                            }
+                            [("system", system_prompt_external), ("user", user_prompt)],
+                            extra_body={"search_domain_filter": [], "web_search_options": {"search_context_size": "high"}},
                         )
-                        
+
                         content = response.content if response else None
-                        citations = getattr(response, 'additional_kwargs', {}).get('citations') if response else None
+                        citations = getattr(response, "additional_kwargs", {}).get("citations") if response else None
                         external_result = {"content": content, "citations": citations, "success": True}
                         logger.info("External OSINT search completed", extra={"run_id": run_id, "content_length": len(content) if content else 0})
                 except Exception as e:
                     external_result = {"content": None, "citations": None, "success": False, "error": str(e)}
                     logger.exception("External OSINT search failed", extra={"run_id": run_id, "error": str(e)})
-                
+
                 if shared_output_file:
                     if external_result.get("success"):
                         await self.append_markdown(
                             shared_output_file,
                             "Step 4: Web Research - External OSINT Only (V2)",
-                            f"**Prompted on:** {company_name} (external sources)\n\n**Result:**\n\n```json\n{external_result.get('content', '')}\n```\n\n**Citations:**\n{external_result.get('citations') if external_result.get('citations') else 'None'}"
+                            f"**Prompted on:** {company_name} (external sources)\n\n**Result:**\n\n```json\n{external_result.get('content', '')}\n```\n\n**Citations:**\n{external_result.get('citations') if external_result.get('citations') else 'None'}",
                         )
                     else:
                         await self.append_markdown(
                             shared_output_file,
                             "Step 4: Web Research - External OSINT (Error)",
-                            f"Error: {external_result.get('error', 'Unknown error')}"
+                            f"Error: {external_result.get('error', 'Unknown error')}",
                         )
-                
+
                 # Parse external content
                 if external_result.get("success"):
-                    external_content = safe_parse_perplexity_content(external_result.get('content'))
-                    
+                    external_content = safe_parse_perplexity_content(external_result.get("content"))
+
                     logger.info("External content parsed", extra={"run_id": run_id, "content_type": type(external_content).__name__})
-                    
+
                     # Ensure it's a dictionary (V2 format)
                     if not isinstance(external_content, dict):
                         print(f"‚ö†Ô∏è WARNING: External content is not a dict: {type(external_content)}")
                         logger.warning("External content is not a dict", extra={"run_id": run_id, "content_type": type(external_content).__name__})
                         external_content = {}
-                    
+
                     # Add metadata to external data only
                     external_content["metadata"] = {
                         "company_name": company_name,
@@ -442,68 +481,84 @@
                         "run_id": run_id,
                         "timestamp": datetime.now().isoformat(),
                         "external_only": True,
-                        "found_by_perplexity": found_by_perplexity
+                        "found_by_perplexity": found_by_perplexity,
                     }
-                    
+
                     print(f"‚úÖ External V2 data processed successfully")
                     logger.info("External V2 data processed successfully", extra={"run_id": run_id})
                 try:
                     if db and run_id:
-                        await ProgressStore.instance().update_subprogress(db, run_id, 'web_progress', 0.50)
+                        await ProgressStore.instance().update_subprogress(db, run_id, "web_progress", 0.50)
                 except Exception:
                     pass
                 else:
                     external_content = None
                     logger.warning("External OSINT search failed, no content to process", extra={"run_id": run_id})
-                
+
                 # Prepare company/ZoomInfo domain for C-suite search
                 company_domain = None
                 zoominfo_domain = None
                 if found_urls and len(found_urls) > 0:
                     from urllib.parse import urlparse
+
                     parsed = urlparse(found_urls[0])
-                    company_domain = parsed.netloc.replace('www.', '')
+                    company_domain = parsed.netloc.replace("www.", "")
                 elif found_url:
                     from urllib.parse import urlparse
+
                     parsed = urlparse(found_url)
-                    company_domain = parsed.netloc.replace('www.', '')
+                    company_domain = parsed.netloc.replace("www.", "")
                 else:
-                    company_domain = company_data.get('website_url', '').replace('https://', '').replace('http://', '').replace('www.', '')
+                    company_domain = company_data.get("website_url", "").replace("https://", "").replace("http://", "").replace("www.", "")
                 zoominfo_domain = f"{company_name.lower().replace(' ', '')}.zoominfo.com"
-                
-                logger.info("Starting C-suite search for external-only mode", extra={"run_id": run_id, "company_domain": company_domain, "zoominfo_domain": zoominfo_domain})
-                
+
+                logger.info(
+                    "Starting C-suite search for external-only mode",
+                    extra={"run_id": run_id, "company_domain": company_domain, "zoominfo_domain": zoominfo_domain},
+                )
+
                 # Run dedicated C-suite search
                 c_suite_members = await self._search_c_suite_members(company_name, company_domain, zoominfo_domain)
-                
+
                 # Add c_suite_members to organization_decision_making in external_content or results dict if available
                 if c_suite_members is not None:
-                    if 'external_content' in locals() and external_content is not None:
-                        if 'organization_decision_making' not in external_content or not isinstance(external_content['organization_decision_making'], dict):
-                            external_content['organization_decision_making'] = {}
-                        external_content['organization_decision_making']['c_suite_members'] = c_suite_members
-                        logger.info("C-suite members added to external content", extra={"run_id": run_id, "c_suite_count": len(c_suite_members) if isinstance(c_suite_members, list) else 0})
-                    elif 'results' in locals() and results is not None:
-                        if 'organization_decision_making' not in results or not isinstance(results['organization_decision_making'], dict):
-                            results['organization_decision_making'] = {}
-                        results['organization_decision_making']['c_suite_members'] = c_suite_members
-                        logger.info("C-suite members added to results", extra={"run_id": run_id, "c_suite_count": len(c_suite_members) if isinstance(c_suite_members, list) else 0})
+                    if "external_content" in locals() and external_content is not None:
+                        if "organization_decision_making" not in external_content or not isinstance(
+                            external_content["organization_decision_making"], dict
+                        ):
+                            external_content["organization_decision_making"] = {}
+                        external_content["organization_decision_making"]["c_suite_members"] = c_suite_members
+                        logger.info(
+                            "C-suite members added to external content",
+                            extra={"run_id": run_id, "c_suite_count": len(c_suite_members) if isinstance(c_suite_members, list) else 0},
+                        )
+                    elif "results" in locals() and results is not None:
+                        if "organization_decision_making" not in results or not isinstance(results["organization_decision_making"], dict):
+                            results["organization_decision_making"] = {}
+                        results["organization_decision_making"]["c_suite_members"] = c_suite_members
+                        logger.info(
+                            "C-suite members added to results",
+                            extra={"run_id": run_id, "c_suite_count": len(c_suite_members) if isinstance(c_suite_members, list) else 0},
+                        )
                 else:
                     logger.warning("C-suite search returned no results for external-only mode", extra={"run_id": run_id})
-                
+
                 # Store in PostgreSQL if enabled and successful
                 if postgres_enabled and db and external_result.get("success") and external_content:
                     try:
                         # Use provided company_id or create one if not provided
                         storage_company_id = company_id
                         if not storage_company_id:
-                            storage_company_id = await db.store_company(run_id or 'default', company_data, user_id=user_id, session_id=session_id)
+                            storage_company_id = await db.store_company(run_id or "default", company_data, user_id=user_id, session_id=session_id)
                             print(f"‚ö†Ô∏è No company_id provided to Web Research agent, created new one: {storage_company_id}")
-                            logger.warning("No company_id provided, created new one for external-only", extra={"run_id": run_id, "new_company_id": storage_company_id})
+                            logger.warning(
+                                "No company_id provided, created new one for external-only",
+                                extra={"run_id": run_id, "new_company_id": storage_company_id},
+                            )
                         else:
                             print(f"‚úÖ Using provided domain-based company_id: {storage_company_id}")
                             logger.info("Using provided company_id for external-only", extra={"run_id": run_id, "company_id": storage_company_id})
-                        
+
                         # Store external V2 data using new agent_data structure
                         execution_time_ms = int((time.time() - start_time) * 1000)
                         await db.store_agent_result(
@@ -512,104 +567,106 @@
                             session_id=session_id,
                             agent_name=self.agent_name,
                             result_data={
-                                'perplexity_merged_data': external_content  # External-only data
+                                "perplexity_merged_data": external_content  # External-only data
                             },
                             company_id=storage_company_id,
-                            execution_time_ms=execution_time_ms
+                            execution_time_ms=execution_time_ms,
                         )
-                        
+
                         print(f"‚úÖ External V2 data stored in PostgreSQL for company_id: {storage_company_id}")
-                        logger.info("External V2 data stored in PostgreSQL", extra={"run_id": run_id, "company_id": storage_company_id, "session_id": session_id})
-                        
+                        logger.info(
+                            "External V2 data stored in PostgreSQL",
+                            extra={"run_id": run_id, "company_id": storage_company_id, "session_id": session_id},
+                        )
+
                         if shared_output_file:
                             await self.append_markdown(
                                 shared_output_file,
                                 "Step 4: Web Research - PostgreSQL Storage",
-                                f"‚úÖ Stored external V2 data in PostgreSQL - Company ID: {storage_company_id}"
+                                f"‚úÖ Stored external V2 data in PostgreSQL - Company ID: {storage_company_id}",
                             )
-                            
+
                     except Exception as db_error:
                         print(f"‚ùå Error storing external V2 data in PostgreSQL: {db_error}")
                         logger.error("Failed to store external V2 data in PostgreSQL", extra={"run_id": run_id, "error": str(db_error)})
                         if shared_output_file:
                             await self.append_markdown(
-                                shared_output_file,
-                                "Step 4: Web Research - PostgreSQL Error",
-                                f"‚ùå PostgreSQL storage failed: {str(db_error)}"
+                                shared_output_file, "Step 4: Web Research - PostgreSQL Error", f"‚ùå PostgreSQL storage failed: {str(db_error)}"
                             )
-                
+
                 return {
-                    'success': external_result.get("success", False),
-                    'external_result': external_result,
-                    'external_data': external_content,
-                    'output_file': shared_output_file,
-                    'agent_name': self.agent_name,
-                    'company_name': company_name,
-                    'external_only': True,
-                    'c_suite_members': c_suite_members
+                    "success": external_result.get("success", False),
+                    "external_result": external_result,
+                    "external_data": external_content,
+                    "output_file": shared_output_file,
+                    "agent_name": self.agent_name,
+                    "company_name": company_name,
+                    "external_only": True,
+                    "c_suite_members": c_suite_members,
                 }
-                
+
         except Exception as e:
             import traceback
+
             print("\n[WebResearchAgent ERROR] Exception in execute():\n" + traceback.format_exc())
             logger.exception("WebResearchAgent execution failed", extra={"run_id": run_id, "user_id": user_id, "session_id": session_id})
-            
+
             # Store error in PostgreSQL if enabled
             if postgres_enabled and db:
                 try:
                     # Use provided company_id or create one if not provided
                     storage_company_id = company_id
                     if not storage_company_id:
-                        storage_company_id = await db.store_company(run_id or 'default', company_data, user_id=user_id, session_id=session_id)
-                    
+                        storage_company_id = await db.store_company(run_id or "default", company_data, user_id=user_id, session_id=session_id)
+
                     execution_time_ms = int((time.time() - start_time) * 1000)
                     await db.store_agent_result(
                         run_id=run_id,
                         user_id=user_id,
                         session_id=session_id,
                         agent_name=self.agent_name,
-                        result_data={'error': str(e)},
+                        result_data={"error": str(e)},
                         company_id=storage_company_id,
-                        execution_time_ms=execution_time_ms
+                        execution_time_ms=execution_time_ms,
                     )
                     logger.info("WebResearchAgent error stored in PostgreSQL", extra={"run_id": run_id, "company_id": storage_company_id})
                 except Exception as db_error:
                     print(f"‚ö†Ô∏è PostgreSQL error storage failed: {db_error}")
                     logger.error("Failed to store WebResearchAgent error in PostgreSQL", extra={"run_id": run_id, "error": str(db_error)})
-            
+
             return {
-                'success': False,
-                'error': str(e),
-                'output_file': company_data.get('output_file'),
-                'agent_name': self.agent_name,
-                'company_name': company_data.get('name', ''),
-                'postgres_stored': False
+                "success": False,
+                "error": str(e),
+                "output_file": company_data.get("output_file"),
+                "agent_name": self.agent_name,
+                "company_name": company_data.get("name", ""),
+                "postgres_stored": False,
             }
         finally:
             # Note: Database connection is managed globally, no need to close here
             # Progress: web research completed
             try:
                 if db and run_id:
-                    await ProgressStore.instance().update_subprogress(db, run_id, 'web_progress', 1.00)
+                    await ProgressStore.instance().update_subprogress(db, run_id, "web_progress", 1.00)
             except Exception:
                 pass
 
     async def _perplexity_osint_search(self, system_prompt: str, user_prompt: str, source_type: str, domains: List[str]) -> Dict[str, Any]:
         """
         Execute a single Perplexity OSINT search with multi-domain filtering.
-        
+
         Args:
             system_prompt: The system prompt for this search
-            user_prompt: The user prompt for this search  
+            user_prompt: The user prompt for this search
             source_type: Either "WEBSITE" or "EXTERNAL"
             domains: List of domains for filtering
-            
+
         Returns:
             Dict with content, citations, success status
         """
         # API monitoring metrics
         api_call_start_time = time.time()
-        
+
         try:
             # Set domain filter based on source type and multiple domains
             if source_type == "WEBSITE":
@@ -618,67 +675,64 @@
             else:  # EXTERNAL
                 domain_filter = [f"-{domain}" for domain in domains]  # Exclude all company domains
                 print(f"üîç External search filtering: Exclude domains {domains}")
-            
+
             logger.info("OSINT search initiated", extra={"source_type": source_type, "domains_count": len(domains), "domain_filter": domain_filter})
-            
-            chat = ChatPerplexity(
-                temperature=0, 
-                model="sonar-pro",
-                api_key=get_perplexity_api_key()
-            )
-            
-            with trace_operation(f"company_{source_type}_osint_search", {
-                "model": "sonar-pro",
-                "search_type": f"company_{source_type}_osint",
-                "source_type": source_type,
-                "domains_count": len(domains)
-            }):
+
+            chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=get_perplexity_api_key())
+
+            with trace_operation(
+                f"company_{source_type}_osint_search",
+                {"model": "sonar-pro", "search_type": f"company_{source_type}_osint", "source_type": source_type, "domains_count": len(domains)},
+            ):
                 try:
                     response = await chat.ainvoke(
-                        [
-                            ("system", system_prompt),
-                            ("user", user_prompt)
-                        ],
-                        extra_body={
-                            "search_domain_filter": domain_filter,
-                            "web_search_options": {"search_context_size": "high"}
-                        }
+                        [("system", system_prompt), ("user", user_prompt)],
+                        extra_body={"search_domain_filter": domain_filter, "web_search_options": {"search_context_size": "high"}},
                     )
-                    
+
                     # API monitoring: Track successful Perplexity API call
                     api_response_time = time.time() - api_call_start_time
-                    logger.info("Perplexity API call successful", extra={
-                        "model": "sonar-pro",
-                        "source_type": source_type,
-                        "response_time": api_response_time,
-                        "api_call_type": "perplexity_osint_search"
-                    })
-                    
+                    logger.info(
+                        "Perplexity API call successful",
+                        extra={
+                            "model": "sonar-pro",
+                            "source_type": source_type,
+                            "response_time": api_response_time,
+                            "api_call_type": "perplexity_osint_search",
+                        },
+                    )
+
                 except Exception as api_error:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track failed Perplexity API call
-                    logger.error("Perplexity API call failed", extra={
-                        "model": "sonar-pro",
-                        "source_type": source_type,
-                        "error": str(api_error),
-                        "response_time": api_response_time,
-                        "api_call_type": "perplexity_osint_search"
-                    })
-                    
+                    logger.error(
+                        "Perplexity API call failed",
+                        extra={
+                            "model": "sonar-pro",
+                            "source_type": source_type,
+                            "error": str(api_error),
+                            "response_time": api_response_time,
+                            "api_call_type": "perplexity_osint_search",
+                        },
+                    )
+
                     raise api_error
-                
+
                 content = response.content if response else None
-                citations = getattr(response, 'additional_kwargs', {}).get('citations') if response else None
-                
-                logger.info("OSINT search completed", extra={"source_type": source_type, "content_length": len(content) if content else 0, "has_citations": bool(citations)})
-                
+                citations = getattr(response, "additional_kwargs", {}).get("citations") if response else None
+
+                logger.info(
+                    "OSINT search completed",
+                    extra={"source_type": source_type, "content_length": len(content) if content else 0, "has_citations": bool(citations)},
+                )
+
                 return {"content": content, "citations": citations, "success": True, "source": source_type}
-                    
+
         except Exception as e:
             logger.exception("OSINT search failed", extra={"source_type": source_type, "error": str(e)})
             return {"content": None, "citations": None, "success": False, "error": str(e), "source": source_type}
-    
+
     async def _search_c_suite_members(self, company_name, company_domain, zoominfo_domain):
         """
         Run a dedicated Perplexity call to extract C-suite members (CEO, CFO, COO, CIO) for the company.
@@ -686,69 +740,62 @@
         """
         import os
         import json
-        
+
         # API monitoring metrics
         api_call_start_time = time.time()
-        
+
         if not get_perplexity_api_key():
             print("‚ùå PERPLEXITY_API_KEY not found in environment variables")
             logger.error("PERPLEXITY_API_KEY not found in environment variables")
             return None
-        
-        chat = ChatPerplexity(
-            temperature=0, 
-            model="sonar-pro",
-            api_key=get_perplexity_api_key()
-        )
-        
+
+        chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=get_perplexity_api_key())
+
         system_prompt, user_prompt = get_c_suite_members_extract_prompt(company_name, company_domain)
-        
+
         logger.info("C-suite search initiated", extra={"company_name": company_name, "company_domain": company_domain})
-        
+
         try:
-            with trace_operation("c_suite_search", {
-                "company_name": company_name,
-                "company_domain": company_domain,
-                "model": "sonar-pro",
-                "search_type": "c_suite_members"
-            }):
+            with trace_operation(
+                "c_suite_search",
+                {"company_name": company_name, "company_domain": company_domain, "model": "sonar-pro", "search_type": "c_suite_members"},
+            ):
                 try:
                     response = await chat.ainvoke(
-                        [
-                            ("system", system_prompt),
-                            ("user", user_prompt)
-                        ],
-                        extra_body={
-                            "web_search_options": {
-                                "search_context_size": "high"
-                            }
-                        }
+                        [("system", system_prompt), ("user", user_prompt)], extra_body={"web_search_options": {"search_context_size": "high"}}
                     )
-                    
+
                     # API monitoring: Track successful Perplexity API call
                     api_response_time = time.time() - api_call_start_time
-                    logger.info("Perplexity API call successful", extra={
-                        "model": "sonar-pro",
-                        "search_type": "c_suite_members",
-                        "response_time": api_response_time,
-                        "api_call_type": "perplexity_c_suite_search"
-                    })
-                    
+                    logger.info(
+                        "Perplexity API call successful",
+                        extra={
+                            "model": "sonar-pro",
+                            "search_type": "c_suite_members",
+                            "response_time": api_response_time,
+                            "api_call_type": "perplexity_c_suite_search",
+                        },
+                    )
+
                 except Exception as api_error:
                     api_response_time = time.time() - api_call_start_time
-                    
+
                     # API monitoring: Track failed Perplexity API call
-                    logger.error("Perplexity API call failed", extra={
-                        "model": "sonar-pro",
-                        "search_type": "c_suite_members",
-                        "error": str(api_error),
-                        "response_time": api_response_time,
-                        "api_call_type": "perplexity_c_suite_search"
-                    })
-                    
+                    logger.error(
+                        "Perplexity API call failed",
+                        extra={
+                            "model": "sonar-pro",
+                            "search_type": "c_suite_members",
+                            "error": str(api_error),
+                            "response_time": api_response_time,
+                            "api_call_type": "perplexity_c_suite_search",
+                        },
+                    )
+
                     raise api_error
-                
+
                 content = response.content or ""
+
                 # Robust parsing: try safe parser first, then strip code fences and extract first JSON object/list
                 def _strip_code_fences(text: str) -> str:
                     text = text.strip()
@@ -758,14 +805,14 @@
                         if first_newline != -1:
                             text = text[first_newline + 1 :]
                     if text.endswith("```"):
-                        text = text[: -3]
+                        text = text[:-3]
                     return text.strip()
 
                 def _extract_first_json_block(text: str) -> Optional[str]:
                     # Try to find first {...} or [...] balanced block
                     # Prefer object
-                    start_obj = text.find('{')
-                    start_arr = text.find('[')
+                    start_obj = text.find("{")
+                    start_arr = text.find("[")
                     start = -1
                     end = -1
                     is_array = False
@@ -773,9 +820,9 @@
                         start = start_obj
                         stack = 0
                         for i, ch in enumerate(text[start:], start=start):
-                            if ch == '{':
+                            if ch == "{":
                                 stack += 1
-                            elif ch == '}':
+                            elif ch == "}":
                                 stack -= 1
                                 if stack == 0:
                                     end = i + 1
@@ -785,9 +832,9 @@
                         is_array = True
                         stack = 0
                         for i, ch in enumerate(text[start:], start=start):
-                            if ch == '[':
+                            if ch == "[":
                                 stack += 1
-                            elif ch == ']':
+                            elif ch == "]":
                                 stack -= 1
                                 if stack == 0:
                                     end = i + 1
@@ -801,10 +848,12 @@
                 try:
                     parsed = safe_parse_perplexity_content(content)
                 except Exception as parse_err:
-                    logger.warning("safe_parse_perplexity_content failed for C-suite content", extra={"company_name": company_name, "error": str(parse_err)})
+                    logger.warning(
+                        "safe_parse_perplexity_content failed for C-suite content", extra={"company_name": company_name, "error": str(parse_err)}
+                    )
 
                 if isinstance(parsed, dict):
-                    c_suite_members = parsed.get('c_suite_members') or parsed.get('value')
+                    c_suite_members = parsed.get("c_suite_members") or parsed.get("value")
                 elif isinstance(parsed, list):
                     c_suite_members = parsed
                 else:
@@ -814,7 +863,7 @@
                         try:
                             data = json.loads(json_str)
                             if isinstance(data, dict):
-                                c_suite_members = data.get('c_suite_members') or data.get('value')
+                                c_suite_members = data.get("c_suite_members") or data.get("value")
                             elif isinstance(data, list):
                                 c_suite_members = data
                         except Exception as json_err:
@@ -824,12 +873,11 @@
                     logger.info("C-suite search completed successfully", extra={"company_name": company_name, "c_suite_count": len(c_suite_members)})
                     return c_suite_members
                 else:
-                    logger.warning("C-suite search: Could not parse valid members list", extra={"company_name": company_name, "content_length": len(content)})
+                    logger.warning(
+                        "C-suite search: Could not parse valid members list", extra={"company_name": company_name, "content_length": len(content)}
+                    )
                     return None
         except Exception as e:
             print(f"‚ùå C-suite Perplexity call failed: {e}")
             logger.exception("C-suite search failed", extra={"company_name": company_name, "error": str(e)})
             return None
-
-    
-   
\ No newline at end of file

--- app/agents/sub_agents/youtube_media_agent.py
+++ app/agents/sub_agents/youtube_media_agent.py
@@ -63,12 +63,7 @@
                 youtube_url = company_data.get("youtube_url")
 
         # Domain hint to tighten Perplexity search results
-        company_domain = (
-            company_data.get("website_url")
-            or company_data.get("domain")
-            or company_data.get("website")
-            or company_data.get("url")
-        )
+        company_domain = company_data.get("website_url") or company_data.get("domain") or company_data.get("website") or company_data.get("url")
         if company_domain:
             from urllib.parse import urlparse
 
@@ -114,12 +109,11 @@
                     agent_name=self.agent_name,
                     result_data=result,
                     company_id=company_id,
-                    execution_time_ms=None
+                    execution_time_ms=None,
                 )
             except Exception as storage_error:
                 self.logger.warning(
-                    "Failed to store YouTube media result",
-                    extra={"run_id": run_id, "company_id": company_id, "error": str(storage_error)}
+                    "Failed to store YouTube media result", extra={"run_id": run_id, "company_id": company_id, "error": str(storage_error)}
                 )
 
         # Progress: finish
@@ -133,10 +127,6 @@
             for summary in result["summaries"]:
                 content_lines.append(f"- Video {summary.get('video_id')}: {summary.get('summary')}")
             content = "\n".join(content_lines)
-            await self.append_markdown(
-                shared_output_file,
-                "Step X: YouTube Media Insights",
-                content or "No insights found."
-            )
+            await self.append_markdown(shared_output_file, "Step X: YouTube Media Insights", content or "No insights found.")
 
         return result

--- app/api/export_routes.py
+++ app/api/export_routes.py
@@ -7,11 +7,13 @@
 from app.utils.auth_dependencies import require_user
 from app.models.current_user import CurrentUser
 import logging
+
 logger = logging.getLogger(__name__)
 
 router = APIRouter()
 security = HTTPBearer()
 
+
 async def get_export_service():
     """Dependency to get ExcelExportService instance."""
     db = ProspectingDB()
@@ -20,107 +22,97 @@
     finally:
         await db.close()
 
-async def validate_session(
-    run_id: str,
-    current_user: CurrentUser = Depends(require_user),
-    db: ProspectingDB = Depends(ProspectingDB)
-) -> bool:
+
+async def validate_session(run_id: str, current_user: CurrentUser = Depends(require_user), db: ProspectingDB = Depends(ProspectingDB)) -> bool:
     """
     Validate that the session exists and belongs to the user.
-    
+
     Args:
         run_id: Run identifier
         user_id: User identifier
         token: JWT token from request
         db: Database connection
-        
+
     Returns:
         True if session is valid
     """
     try:
         # Get session from run_id (format: session_id_run_XXX)
-        session_id = run_id.split('_run_')[0]
-        
+        session_id = run_id.split("_run_")[0]
+
         # Verify session exists and belongs to the authenticated user
         session = await db.get_session(session_id)
-        if not session or session['user_id'] != current_user.user_id:
+        if not session or session["user_id"] != current_user.user_id:
             return False
-            
+
         return True
     except Exception as e:
         print(f"‚ùå Error validating session: {str(e)}")
         return False
 
+
 ## Removed old path with user_id
 
+
 @router.get("/export/company/{run_id}")
 async def export_company_enrichment_jwt(
     run_id: str,
     export_service: ExcelExportService = Depends(get_export_service),
     is_valid: bool = Depends(validate_session),
-    current_user: CurrentUser = Depends(require_user)
+    current_user: CurrentUser = Depends(require_user),
 ):
     """
     Generate and return Excel file for company enrichment data (user derived from JWT).
     """
     try:
         if not is_valid:
-            raise HTTPException(
-                status_code=403,
-                detail="Invalid session or unauthorized access"
-            )
+            raise HTTPException(status_code=403, detail="Invalid session or unauthorized access")
         excel_data = await export_service.generate_company_excel(run_id, current_user.user_id)
         if not excel_data:
-            raise HTTPException(
-                status_code=404,
-                detail="Company enrichment data not found"
-            )
+            raise HTTPException(status_code=404, detail="Company enrichment data not found")
         filename = f"company_enrichment_{run_id}.xlsx"
         return Response(
             content=excel_data,
             media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
             headers={
                 "Content-Disposition": f"attachment; filename={filename}",
-                "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
-            }
+                "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
+            },
         )
     except HTTPException:
         raise
     except Exception:
         logger.exception("Failed to export company enrichment Excel (JWT)")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
+
+
 ## Removed old path with user_id
 
+
 @router.get("/export/person/{run_id}")
 async def export_person_enrichment_jwt(
     run_id: str,
     export_service: ExcelExportService = Depends(get_export_service),
     is_valid: bool = Depends(validate_session),
-    current_user: CurrentUser = Depends(require_user)
+    current_user: CurrentUser = Depends(require_user),
 ):
     """
     Generate and return Excel file for person enrichment data (user derived from JWT).
     """
     try:
         if not is_valid:
-            raise HTTPException(
-                status_code=403,
-                detail="Invalid session or unauthorized access"
-            )
+            raise HTTPException(status_code=403, detail="Invalid session or unauthorized access")
         excel_data = await export_service.generate_person_excel(run_id, current_user.user_id)
         if not excel_data:
-            raise HTTPException(
-                status_code=404,
-                detail="Person enrichment data not found"
-            )
+            raise HTTPException(status_code=404, detail="Person enrichment data not found")
         filename = f"person_enrichment_{run_id}.xlsx"
         return Response(
             content=excel_data,
             media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
             headers={
                 "Content-Disposition": f"attachment; filename={filename}",
-                "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
-            }
+                "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
+            },
         )
     except HTTPException:
         raise

--- app/api/routes/excel_exports.py
+++ app/api/routes/excel_exports.py
@@ -7,32 +7,25 @@
 from app.utils.auth_dependencies import require_user
 from app.models.current_user import CurrentUser
 import logging
+
 logger = logging.getLogger(__name__)
 
 router = APIRouter()
 
 ## Removed old path with user_id
 
+
 @router.get("/api/prospecting/export/company/{run_id}")
-async def export_company_excel_jwt(
-    run_id: str,
-    db = Depends(get_db),
-    current_user: CurrentUser = Depends(require_user)
-):
+async def export_company_excel_jwt(run_id: str, db=Depends(get_db), current_user: CurrentUser = Depends(require_user)):
     try:
         excel_service = ExcelExportService(db)
         excel_data = await excel_service.generate_company_excel(run_id, current_user.user_id)
         if not excel_data:
-            raise HTTPException(
-                status_code=404,
-                detail=f"No company enrichment data found for run: {run_id}"
-            )
+            raise HTTPException(status_code=404, detail=f"No company enrichment data found for run: {run_id}")
         return StreamingResponse(
             io.BytesIO(excel_data),
             media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
-            headers={
-                "Content-Disposition": f"attachment; filename=company_enrichment_{run_id}.xlsx"
-            }
+            headers={"Content-Disposition": f"attachment; filename=company_enrichment_{run_id}.xlsx"},
         )
     except HTTPException:
         raise
@@ -40,28 +33,21 @@
         logger.exception("Failed to export company Excel (JWT)")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 ## Removed old path with user_id
 
+
 @router.get("/api/prospecting/export/person/{run_id}")
-async def export_person_excel_jwt(
-    run_id: str,
-    db = Depends(get_db),
-    current_user: CurrentUser = Depends(require_user)
-):
+async def export_person_excel_jwt(run_id: str, db=Depends(get_db), current_user: CurrentUser = Depends(require_user)):
     try:
         excel_service = ExcelExportService(db)
         excel_data = await excel_service.generate_person_excel(run_id, current_user.user_id)
         if not excel_data:
-            raise HTTPException(
-                status_code=404,
-                detail=f"No person enrichment data found for run: {run_id}"
-            )
+            raise HTTPException(status_code=404, detail=f"No person enrichment data found for run: {run_id}")
         return StreamingResponse(
             io.BytesIO(excel_data),
             media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
-            headers={
-                "Content-Disposition": f"attachment; filename=person_enrichment_{run_id}.xlsx"
-            }
+            headers={"Content-Disposition": f"attachment; filename=person_enrichment_{run_id}.xlsx"},
         )
     except HTTPException:
         raise

--- app/celery_app.py
+++ app/celery_app.py
@@ -10,6 +10,7 @@
 # Load environment variables FIRST, before any imports
 # Celery workers run in separate processes and need .env loaded independently
 from dotenv import load_dotenv
+
 load_dotenv()
 
 import os
@@ -21,6 +22,7 @@
 # Initialize logging FIRST - Celery workers run in separate processes
 # and need logging configured independently from the main FastAPI app
 from app.utils.logging_config import setup_logging
+
 setup_logging()
 
 from app.utils.config import (
@@ -41,6 +43,7 @@
 # Get environment
 environment = get_environment()
 
+
 # Build PostgreSQL connection string for result backend
 def build_db_url() -> str:
     """Build PostgreSQL connection string for Celery result backend."""
@@ -49,12 +52,13 @@
     db = get_postgres_db()
     user = get_postgres_user()
     password = get_postgres_password()
-    
+
     # URL encode password if it contains special characters
     password_encoded = quote_plus(password) if password else ""
-    
+
     return f"db+postgresql://{user}:{password_encoded}@{host}:{port}/{db}"
 
+
 # Environment-based broker selection
 def get_broker_url() -> str:
     """Get broker URL based on environment."""
@@ -71,6 +75,7 @@
         # return os.getenv("REDIS_CLOUD_URL", "redis://redis-cloud-host:port/0")
         return "sqs://"  # Uses IAM role automatically
 
+
 # Get broker configuration based on environment
 def get_broker_config() -> dict:
     """Get broker-specific configuration."""
@@ -81,75 +86,76 @@
         # SQS-specific configuration
         queue_prefix = f"ardessa-{environment}-"
         return {
-            'broker_transport_options': {
-                'region': os.getenv('AWS_REGION', 'eu-west-1'),
-                'queue_name_prefix': queue_prefix,
-                'visibility_timeout': 3600,  # 1 hour
-                'polling_interval': 1,
+            "broker_transport_options": {
+                "region": os.getenv("AWS_REGION", "eu-west-1"),
+                "queue_name_prefix": queue_prefix,
+                "visibility_timeout": 3600,  # 1 hour
+                "polling_interval": 1,
             }
         }
 
+
 # Initialize Celery app
 broker_url = get_broker_url()
 broker_config = get_broker_config()
 
 celery_app = Celery(
-    'ardessa_agent',
+    "ardessa_agent",
     broker=broker_url,
     backend=build_db_url(),  # Use PostgreSQL as result backend
     include=[
-        'app.tasks.tamradar_tasks',
-        'app.tasks.weekly_wrapup_tasks',
-        'app.tasks.meeting_prep_tasks',
-    ]
+        "app.tasks.tamradar_tasks",
+        "app.tasks.weekly_wrapup_tasks",
+        "app.tasks.meeting_prep_tasks",
+    ],
 )
 
 # Celery configuration
 celery_config = {
     # Task serialization
-    'task_serializer': 'json',
-    'accept_content': ['json'],
-    'result_serializer': 'json',
+    "task_serializer": "json",
+    "accept_content": ["json"],
+    "result_serializer": "json",
     # Timezone
-    'timezone': 'UTC',
-    'enable_utc': True,
+    "timezone": "UTC",
+    "enable_utc": True,
     # Task execution settings
-    'task_track_started': True,  # Track when task starts
-    'task_acks_late': True,  # Acknowledge tasks after completion (not before)
-    'worker_prefetch_multiplier': 1,  # Only prefetch one task at a time
+    "task_track_started": True,  # Track when task starts
+    "task_acks_late": True,  # Acknowledge tasks after completion (not before)
+    "worker_prefetch_multiplier": 1,  # Only prefetch one task at a time
     # Retry settings
-    'task_default_retry_delay': 60,  # Default retry delay (seconds)
-    'task_max_retries': 3,  # Maximum retries for tasks
+    "task_default_retry_delay": 60,  # Default retry delay (seconds)
+    "task_max_retries": 3,  # Maximum retries for tasks
     # Result backend settings
-    'result_expires': 3600,  # Results expire after 1 hour
+    "result_expires": 3600,  # Results expire after 1 hour
     # Beat schedule for periodic tasks
-    'beat_schedule': {
-        'send-weekly-wrapup-emails': {
-            'task': 'send_weekly_wrapup_emails',
+    "beat_schedule": {
+        "send-weekly-wrapup-emails": {
+            "task": "send_weekly_wrapup_emails",
             # Run every Friday at 00:00 GMT (midnight Friday morning)
-            'schedule': crontab(hour=0, minute=0, day_of_week=4),
+            "schedule": crontab(hour=0, minute=0, day_of_week=4),
         },
-        'process-daily-meeting-prep': {
-            'task': 'process_daily_meeting_prep_workflow',
+        "process-daily-meeting-prep": {
+            "task": "process_daily_meeting_prep_workflow",
             # Run every day at 04:00 UTC (4:00 AM)
-            'schedule': crontab(hour=4, minute=0),
+            "schedule": crontab(hour=4, minute=0),
         },
     },
     # Task routing to queues
-    'task_routes': {
-        'app.tasks.meeting_prep_tasks.collect_user_integration_data': {'queue': 'data_collection'},
-        'app.tasks.meeting_prep_tasks.identify_important_events_task': {'queue': 'categorization'},
-        'app.tasks.meeting_prep_tasks.categorize_user_meetings': {'queue': 'categorization'},  # Deprecated but kept for compatibility
-        'app.tasks.meeting_prep_tasks.research_meeting_participants': {'queue': 'research'},
-        'app.tasks.meeting_prep_tasks.generate_meeting_report': {'queue': 'report_generation'},
-        'app.tasks.meeting_prep_tasks.save_meeting_report': {'queue': 'storage'},
+    "task_routes": {
+        "app.tasks.meeting_prep_tasks.collect_user_integration_data": {"queue": "data_collection"},
+        "app.tasks.meeting_prep_tasks.identify_important_events_task": {"queue": "categorization"},
+        "app.tasks.meeting_prep_tasks.categorize_user_meetings": {"queue": "categorization"},  # Deprecated but kept for compatibility
+        "app.tasks.meeting_prep_tasks.research_meeting_participants": {"queue": "research"},
+        "app.tasks.meeting_prep_tasks.generate_meeting_report": {"queue": "report_generation"},
+        "app.tasks.meeting_prep_tasks.save_meeting_report": {"queue": "storage"},
     },
     # Rate limits per task
-    'task_rate_limit': {
-        'app.tasks.meeting_prep_tasks.identify_important_events_task': '20/m',
-        'app.tasks.meeting_prep_tasks.categorize_user_meetings': '20/m',  # Deprecated
-        'app.tasks.meeting_prep_tasks.research_meeting_participants': '5/m',
-        'app.tasks.meeting_prep_tasks.generate_meeting_report': '20/m',
+    "task_rate_limit": {
+        "app.tasks.meeting_prep_tasks.identify_important_events_task": "20/m",
+        "app.tasks.meeting_prep_tasks.categorize_user_meetings": "20/m",  # Deprecated
+        "app.tasks.meeting_prep_tasks.research_meeting_participants": "5/m",
+        "app.tasks.meeting_prep_tasks.generate_meeting_report": "20/m",
     },
 }
 
@@ -159,9 +165,9 @@
 # For SQS, explicitly set the broker transport
 # This must be set to ensure kombu uses SQS transport correctly
 if environment != "development":
-    celery_config['broker_transport'] = 'sqs'
+    celery_config["broker_transport"] = "sqs"
     # Force broker_url to be recognized as SQS
-    celery_config['broker_url'] = 'sqs://'
+    celery_config["broker_url"] = "sqs://"
 
 celery_app.conf.update(celery_config)
 
@@ -174,12 +180,12 @@
         "broker": broker_type,
         "broker_url": broker_url if environment == "development" else "sqs://",
         "backend": "postgresql",
-    }
+    },
 )
 
 # For SQS environments, log connection details for debugging
 if environment != "development":
-    aws_region = os.getenv('AWS_REGION', 'eu-west-1')
+    aws_region = os.getenv("AWS_REGION", "eu-west-1")
     queue_prefix = f"ardessa-{environment}-"
     logger.info(
         f"Celery SQS configuration",
@@ -187,12 +193,8 @@
             "aws_region": aws_region,
             "queue_prefix": queue_prefix,
             "environment": environment,
-        }
+        },
     )
     # Log warning if AWS_REGION is not set (might cause connection issues)
-    if not os.getenv('AWS_REGION'):
-        logger.warning(
-            "AWS_REGION environment variable not set - SQS connection may fail",
-            extra={"default_region": "eu-west-1"}
-        )
-
+    if not os.getenv("AWS_REGION"):
+        logger.warning("AWS_REGION environment variable not set - SQS connection may fail", extra={"default_region": "eu-west-1"})

--- app/config/__init__.py
+++ app/config/__init__.py
@@ -1,4 +1,3 @@
 """
 Configuration modules for application settings.
 """
-

--- app/config/relevance_scoring_config.py
+++ app/config/relevance_scoring_config.py
@@ -52,4 +52,3 @@
 
 # Timeout for embedding generation (seconds)
 EMBEDDING_TIMEOUT = int(os.getenv("RELEVANCE_EMBEDDING_TIMEOUT", "30"))
-

--- app/config/tier_queries.py
+++ app/config/tier_queries.py
@@ -9,7 +9,7 @@
 """
 
 TIER_1_QUERIES = {
-    'direct_allocation': """
+    "direct_allocation": """
 Direct allocation announcements to competing funds: Alert immediately when target LPs announce actual commitments to PE/VC funds, including fund close participation with disclosed commitment amounts, new manager selections, and co-investment deals. These announcements prove the LP is actively deploying capital right now and reveal their exact ticket sizes, stage preferences, and competitive positioning.
 
 Direct allocation announcements represent actual capital commitments made by limited partners to private equity or venture capital funds. These include explicit statements about committing capital to specific fund managers, participating in fund closes with disclosed dollar amounts, selecting new general partners or fund managers, and engaging in co-investment opportunities alongside funds.
@@ -20,8 +20,7 @@
 
 Look for language patterns such as "committed to", "allocated capital to", "selected as manager", "participated in fund close", "backed by", "investment in fund", "new commitment", "fund selection", "manager chosen", "ticket size", "commitment amount", and references to specific fund names or general partner names alongside commitment language.
 """,
-    
-    'senior_investment_hires': """
+    "senior_investment_hires": """
 Senior investment staff job changes: Flag all hires, promotions, and departures for CIOs, Heads of Private Equity/Venture Capital, Investment Directors, Portfolio Managers, and Investment Committee members. New decision-makers represent fresh relationship opportunities, while departures may signal strategic shifts or relationship resets that create openings.
 
 Senior investment staff changes encompass all hires, promotions, departures, and role transitions for key decision-makers within limited partner organizations. This includes Chief Investment Officers (CIO), Heads of Private Equity, Heads of Venture Capital, Investment Directors, Portfolio Managers, Managing Directors in investment functions, Partners in investment teams, and members of Investment Committees.
@@ -32,8 +31,7 @@
 
 Look for role titles such as "Chief Investment Officer", "CIO", "Head of Private Equity", "Head of Venture Capital", "Head of PE", "Head of VC", "Investment Director", "Portfolio Manager", "Managing Director", "Investment Partner", "Investment Committee member", combined with action words like "appointed", "joined", "promoted to", "hired as", "departing", "leaving", "transitioning to", or "named as".
 """,
-    
-    'ic_meetings': """
+    "ic_meetings": """
 Investment committee meeting schedules: Surface announcements of upcoming board meetings, investment committee sessions, fiscal year-end decision windows, and annual strategic review timing. These dates define the exact windows when capital allocation decisions are being made and when you need to be positioned in their pipeline.
 
 Investment committee meeting schedules and decision window announcements reveal the exact timing when capital allocation decisions are being made. These include announcements of upcoming board meetings, investment committee session dates, fiscal year-end decision windows, annual strategic review timing, commitment period schedules, and allocation decision timelines.
@@ -44,8 +42,7 @@
 
 Look for language patterns such as "upcoming meeting", "scheduled for", "decision window", "review period", "fiscal year-end", "annual review", "strategic planning session", "allocation decisions", "commitment timeline", and any combination of dates or timeframes with meeting or decision language.
 """,
-    
-    'distribution_reup': """
+    "distribution_reup": """
 Distribution/re-up discussions: Capture mentions of LPs receiving distributions from existing funds, discussions about whether to re-commit to current managers, liquidity events, and cash return expectations. Distributions signal fresh capital capacity available for redeployment, while re-up discussions reveal which existing GP relationships may be opening up.
 
 Distribution and re-up discussions capture critical signals about LP capital capacity and relationship opportunities. These include mentions of LPs receiving distributions from existing funds, discussions about whether to re-commit to current general partners, liquidity events generating cash returns, cash-out expectations, and decisions about not re-upping to existing funds.
@@ -56,8 +53,7 @@
 
 Look for language patterns such as "distribution from", "received proceeds", "cash-out", "liquidity event", "re-up", "recommitment", "not re-upping", "declining to recommit", "redeploying capital", "fresh capital available", "capital capacity", and any discussion combining distribution language with commitment or allocation language.
 """,
-    
-    'seeking_managers': """
+    "seeking_managers": """
 "Seeking managers in [user's category]" statements: Highlight explicit statements where LPs announce they are actively looking for managers in specific categories such as "seeking emerging VC managers," "looking for growth equity funds under $500M," or "interested in climate-focused buyout funds." These are the clearest possible buying signals with stated intent to commit capital.
 
 "Seeking managers" statements represent the clearest possible buying signals with explicit stated intent to commit capital. These include announcements where LPs explicitly state they are actively looking for managers in specific categories, evaluating new fund managers, searching for general partners, or expressing openness to new manager relationships.
@@ -67,11 +63,11 @@
 Key indicators include explicit language such as "seeking managers", "looking for funds", "searching for GPs", "evaluating new managers", "open to new relationships", "manager search", "fund search", "GP search", "actively seeking", "interested in managers", combined with specific category qualifiers such as fund size, stage, sector, geography, or strategy focus.
 
 Look for language patterns such as "seeking", "looking for", "searching for", "evaluating", "open to", "considering", "interested in", combined with "managers", "funds", "GPs", "general partners", "fund managers", and specific qualifiers like "emerging", "growth equity", "venture capital", "private equity", fund size ranges, sector focus, or geographic preferences.
-"""
+""",
 }
 
 TIER_2_QUERIES = {
-    'new_mandate': """
+    "new_mandate": """
 New mandate announcements: Track announcements of new investment mandates including ESG requirements, diversity and inclusion commitments, impact investing goals, climate transition strategies, or other thematic investment focuses. These mandates define what types of managers will receive priority consideration and help you understand if your fund aligns with their strategic direction.
 
 New mandate announcements represent strategic shifts or new focus areas that LPs are prioritizing in their investment approach. These include ESG requirements and commitments, diversity and inclusion mandates, impact investing goals, climate transition strategies, thematic investment focuses, emerging manager programs, budget expansions for new strategies, geographic allocation shifts, sector allocation changes, ticket size preference changes, and fund size preference updates.
@@ -82,8 +78,7 @@
 
 Look for language patterns such as "new mandate", "launching", "expanding into", "strategic shift", "new focus", "program launch", "initiative", "commitment to", "requirement for", combined with thematic terms like "ESG", "diversity", "impact", "climate", "sustainability", sector names, geographic regions, or specific investment themes.
 """,
-    
-    'team_expansion': """
+    "team_expansion": """
 Team expansion in relevant asset class: Monitor when LPs hire additional investment professionals specifically within private equity, venture capital, or other alternative asset teams. Team expansion signals increased deployment capacity, growing allocations to the asset class, and potentially new relationship bandwidth for evaluating managers.
 
 Team expansion in relevant asset classes indicates organizational growth and increased capacity for investment activities. This includes hiring additional investment professionals specifically within private equity teams, venture capital teams, or other alternative asset class teams. Multiple hires in a short timeframe, explicit announcements about growing investment teams, and expansion of specific asset class coverage all signal increased deployment capacity.
@@ -94,8 +89,7 @@
 
 Look for language patterns such as "expanding", "growing team", "adding professionals", "building out", "hiring for", "team growth", "increasing headcount", combined with asset class terms like "private equity", "venture capital", "PE", "VC", "alternative investments", or specific investment function names.
 """,
-    
-    'conference_speaking': """
+    "conference_speaking": """
 Conference speaking on relevant topics: Identify when LP investment staff are speaking at industry conferences, participating in panel discussions, or presenting on specific topics like emerging managers, sector strategies, or market outlooks. Speaking topics reveal current strategic priorities and provide natural outreach opportunities around their areas of focus.
 
 Conference speaking engagements by LP investment staff provide visibility into their current strategic priorities and areas of focus. These include speaking at industry conferences, participating in panel discussions, presenting on specific investment topics, keynote addresses at investment forums, and public appearances discussing investment strategies or market outlooks.
@@ -106,8 +100,7 @@
 
 Look for language patterns such as "speaking at", "presenting", "panel discussion", "keynote", "conference", "summit", "forum", "event", combined with topics like "emerging managers", "sector focus", "market outlook", "investment strategy", "portfolio allocation", or specific investment themes.
 """,
-    
-    'allocation_announcements': """
+    "allocation_announcements": """
 New allocation announcements: Surface updates about broad portfolio allocation changes such as increasing private equity target from 10% to 15%, shifting from buyout to growth equity focus, or expanding into new geographies. These strategic shifts indicate where fresh capital will flow and whether your fund fits their evolving portfolio construction.
 
 New allocation announcements represent strategic portfolio construction changes that indicate where fresh capital will flow. These include increasing target allocations to specific asset classes (e.g., "increasing PE target from 10% to 15%"), shifting focus between sub-asset classes (e.g., "shifting from buyout to growth equity"), expanding into new geographies, changing sector focus, adjusting ticket size preferences, or modifying fund size requirements.
@@ -118,8 +111,7 @@
 
 Look for language patterns such as "increasing allocation", "shifting from", "expanding into", "target allocation", "portfolio change", "strategic shift", "allocation strategy", combined with asset class names (private equity, venture capital), sub-asset classes (buyout, growth equity, early stage), geographic regions, or percentage changes.
 """,
-    
-    'gp_relationships': """
+    "gp_relationships": """
 Partners that they work with: Track mentions of existing GP relationships, fund managers they currently work with, satisfaction or dissatisfaction with current partnerships, and discussions about their existing manager roster. Understanding their current relationships helps you position competitively and identify potential openings from underperformance or strategy changes.
 
 GP relationship mentions provide insights into the competitive landscape and potential relationship opportunities. These include mentions of existing GP relationships, fund managers they currently work with, satisfaction or dissatisfaction with current partnerships, discussions about their existing manager roster, performance feedback on current managers, re-up decisions, and manager consolidation efforts.
@@ -129,11 +121,11 @@
 Key indicators include explicit mentions of "working with", "partnered with", "invested with", "current manager", "existing GP", "fund manager", "general partner", "relationship with", "satisfaction with", "dissatisfaction", "re-up", "not re-upping", "consolidating managers", and discussions about existing fund manager relationships.
 
 Look for language patterns such as "working with", "partnered with", "invested with", "current manager", "existing GP", "fund manager", "general partner", "relationship", "satisfaction", "dissatisfaction", "re-up", "consolidating", combined with fund names, manager names, or discussions about manager performance or relationships.
-"""
+""",
 }
 
 TIER_3_QUERIES = {
-    'thought_leadership': """
+    "thought_leadership": """
 General thought leadership: Monitor white papers, research publications, blog posts, and general market commentary from LP investment teams about industry trends, market perspectives, and investment philosophies. While less actionable than direct buying signals, this content reveals their intellectual framework and can inform relationship-building conversations.
 
 General thought leadership content from LP investment teams provides insights into their intellectual framework and investment philosophy. These include white papers, research publications, blog posts, market commentary, industry trend analysis, market perspectives, investment philosophy discussions, outlook forecasts, opinion pieces on investment topics, and general commentary on markets and investment strategies.
@@ -144,8 +136,7 @@
 
 Look for language patterns such as "insights", "perspective", "viewpoint", "analysis", "trends", "outlook", "forecast", "opinion", "philosophy", "framework", "commentary", combined with investment topics, market discussions, or strategic thinking. Content is typically longer-form and analytical rather than announcement-focused.
 """,
-    
-    'lower_seniority_hires': """
+    "lower_seniority_hires": """
 Lower-seniority hires in investment teams: Track analyst, associate, and junior investment professional hires within private markets teams. While these individuals aren't decision-makers, they often conduct initial due diligence and manager research, making them valuable future relationships as they advance in their careers.
 
 Lower-seniority hires in investment teams represent early-career investment professionals who play important roles in the manager evaluation process. These include analyst positions, associate roles, junior investment professionals, research analysts, investment analysts, and entry-level to mid-level positions within private equity, venture capital, or alternative investment teams.
@@ -156,8 +147,7 @@
 
 Look for language patterns such as "analyst", "associate", "junior", "assistant", "research analyst", "investment analyst", combined with action words like "joined", "hired", "appointed", and department names like "private equity", "venture capital", "investment team", or "alternative investments". These roles are typically below Director level.
 """,
-    
-    'committee_participation': """
+    "committee_participation": """
 Industry committee participation: Note participation in industry organizations like Institutional Limited Partners Association, institutional investor working groups, pension fund coalitions, and standards-setting bodies. Committee involvement shows areas of professional focus and provides context for their approach to manager evaluation and portfolio construction.
 
 Industry committee participation reveals areas of professional focus and provides context for an LP's approach to manager evaluation and portfolio construction. These include participation in industry organizations like Institutional Limited Partners Association (ILPA), institutional investor working groups, pension fund coalitions, standards-setting bodies, advisory boards, industry committees, professional organizations, and collaborative initiatives focused on best practices or industry standards.
@@ -167,7 +157,7 @@
 Key indicators include mentions of industry organizations like "Institutional Limited Partners Association", "ILPA", "institutional investor", "working group", "coalition", "standards-setting", "advisory board", "committee member", "serving on", "board member", "industry organization", combined with professional or industry-focused contexts.
 
 Look for language patterns such as "committee member", "serving on", "board member", "advisory board", "working group", "coalition", "industry organization", "Institutional Limited Partners Association", "ILPA", "participating in", "member of", combined with professional or industry-focused contexts. These are typically longer-term commitments rather than one-time events.
-"""
+""",
 }
 
 DEPRIORITIZE_QUERIES = {}
@@ -176,4 +166,3 @@
 EMBEDDING_MODEL = "text-embedding-3-large"
 EMBEDDING_DIMENSION = 3072
 QUERY_VERSION = "v1"
-

--- app/google/__init__.py
+++ app/google/__init__.py
@@ -1,6 +1 @@
 
-
-
-
-
-

--- app/google/calendar.py
+++ app/google/calendar.py
@@ -54,7 +54,3 @@
     if value.tzinfo is None:
         value = value.replace(tzinfo=timezone.utc)
     return value.isoformat()
-
-
-
-

--- app/google/email.py
+++ app/google/email.py
@@ -64,7 +64,3 @@
         )
         raise HTTPException(status_code=exc.response.status_code, detail="gmail_thread_error") from exc
     return response.json()
-
-
-
-

--- app/google/oauth.py
+++ app/google/oauth.py
@@ -171,4 +171,4 @@
     value = expires_at
     if value.tzinfo is None:
         value = value.replace(tzinfo=timezone.utc)
-    return value <= datetime.now(timezone.utc)
\ No newline at end of file
+    return value <= datetime.now(timezone.utc)

--- app/google/repositories/__init__.py
+++ app/google/repositories/__init__.py
@@ -1,6 +1 @@
 
-
-
-
-
-

--- app/google/repositories/google_tokens_repository.py
+++ app/google/repositories/google_tokens_repository.py
@@ -94,7 +94,3 @@
             expires_at,
             token_id,
         )
-
-
-
-

--- app/google/router.py
+++ app/google/router.py
@@ -95,4 +95,3 @@
     if not current_user.user_id:
         raise HTTPException(status_code=400, detail="user_id_missing")
     return current_user.user_id
-

--- app/hubspot/__init__.py
+++ app/hubspot/__init__.py
@@ -1,2 +1 @@
 
-

--- app/hubspot/crm/__init__.py
+++ app/hubspot/crm/__init__.py
@@ -21,4 +21,3 @@
     "get_account_summary",
     "get_context_by_email",
 ]
-

--- app/hubspot/crm/associations.py
+++ app/hubspot/crm/associations.py
@@ -10,11 +10,11 @@
 async def get_contact_company_associations(access_token: str, contact_id: str) -> List[str]:
     """
     Get company IDs associated with a contact.
-    
+
     Args:
         access_token: HubSpot access token
         contact_id: HubSpot contact ID
-        
+
     Returns:
         List of company IDs
     """
@@ -24,14 +24,14 @@
             method="GET",
             path=f"/crm/v3/objects/contacts/{contact_id}/associations/companies",
         )
-        
+
         company_ids = []
         results = response.get("results", [])
         for result in results:
             company_id = result.get("id")
             if company_id:
                 company_ids.append(company_id)
-        
+
         return company_ids
     except Exception as e:
         logger.warning(
@@ -44,11 +44,11 @@
 async def get_deals_by_company_id(access_token: str, company_id: str) -> List[str]:
     """
     Get deal IDs associated with a company.
-    
+
     Args:
         access_token: HubSpot access token
         company_id: HubSpot company ID
-        
+
     Returns:
         List of deal IDs
     """
@@ -58,14 +58,14 @@
             method="GET",
             path=f"/crm/v3/objects/companies/{company_id}/associations/deals",
         )
-        
+
         deal_ids = []
         results = response.get("results", [])
         for result in results:
             deal_id = result.get("id")
             if deal_id:
                 deal_ids.append(deal_id)
-        
+
         return deal_ids
     except Exception as e:
         logger.warning(
@@ -78,11 +78,11 @@
 async def get_deals_by_contact_id(access_token: str, contact_id: str) -> List[str]:
     """
     Get deal IDs associated with a contact.
-    
+
     Args:
         access_token: HubSpot access token
         contact_id: HubSpot contact ID
-        
+
     Returns:
         List of deal IDs
     """
@@ -92,14 +92,14 @@
             method="GET",
             path=f"/crm/v3/objects/contacts/{contact_id}/associations/deals",
         )
-        
+
         deal_ids = []
         results = response.get("results", [])
         for result in results:
             deal_id = result.get("id")
             if deal_id:
                 deal_ids.append(deal_id)
-        
+
         return deal_ids
     except Exception as e:
         logger.warning(
@@ -107,4 +107,3 @@
             extra={"contact_id": contact_id, "error": str(e)},
         )
         return []
-

--- app/hubspot/crm/client.py
+++ app/hubspot/crm/client.py
@@ -15,17 +15,20 @@
 
 class HubSpotAPIError(HTTPException):
     """Base exception for HubSpot API errors."""
+
     pass
 
 
 class HubSpotAuthenticationError(HubSpotAPIError):
     """Raised when authentication fails (401/403)."""
+
     def __init__(self, detail: str = "HubSpot authentication failed. Please reconnect your account."):
         super().__init__(status_code=401, detail=detail)
 
 
 class HubSpotRateLimitError(HubSpotAPIError):
     """Raised when rate limit is exceeded (429)."""
+
     def __init__(self, detail: str = "HubSpot rate limit exceeded. Please try again later."):
         super().__init__(status_code=429, detail=detail)
 
@@ -39,17 +42,17 @@
 ) -> Dict:
     """
     Generic helper for making HTTP requests to HubSpot API.
-    
+
     Args:
         access_token: HubSpot access token
         method: HTTP method (GET, POST, etc.)
         path: Relative API path (e.g., /crm/v3/objects/contacts/search)
         query_params: Optional query parameters
         body: Optional request body for POST/PUT requests
-        
+
     Returns:
         JSON response as dict
-        
+
     Raises:
         HubSpotAuthenticationError: For 401/403 errors
         HubSpotRateLimitError: For 429 errors
@@ -62,7 +65,7 @@
         "Authorization": f"Bearer {access_token}",
         "Content-Type": "application/json",
     }
-    
+
     async with httpx.AsyncClient(timeout=30.0) as client:
         try:
             if method.upper() == "GET":
@@ -77,7 +80,7 @@
                 response = await client.delete(url, headers=headers, params=query_params)
             else:
                 raise ValueError(f"Unsupported HTTP method: {method}")
-            
+
             if 200 <= response.status_code < 300:
                 return response.json()
             elif response.status_code == 401 or response.status_code == 403:
@@ -139,25 +142,25 @@
 ) -> List[Dict]:
     """
     Execute a HubSpot search request with pagination support.
-    
+
     Args:
         access_token: HubSpot access token
         method: HTTP method (usually POST for search)
         path: API path
         body: Request body
         query_params: Optional query parameters
-        
+
     Returns:
         List of all results from all pages
     """
     all_results = []
     after = None
-    
+
     while True:
         current_query_params = query_params.copy() if query_params else {}
         if after:
             current_query_params["after"] = after
-        
+
         response = await _make_hubspot_request(
             access_token=access_token,
             method=method,
@@ -165,10 +168,10 @@
             query_params=current_query_params if current_query_params else None,
             body=body,
         )
-        
+
         results = response.get("results", [])
         all_results.extend(results)
-        
+
         paging = response.get("paging", {})
         next_page = paging.get("next")
         if next_page:
@@ -177,6 +180,5 @@
                 break
         else:
             break
-    
-    return all_results
 
+    return all_results

--- app/hubspot/crm/companies.py
+++ app/hubspot/crm/companies.py
@@ -5,38 +5,34 @@
 from .client import _make_hubspot_request, _paginate_hubspot_search
 
 
-async def search_companies_by_domain(
-    access_token: str, domain: str
-) -> List[HubSpotCompanySummary]:
+async def search_companies_by_domain(access_token: str, domain: str) -> List[HubSpotCompanySummary]:
     """
     Search for companies by domain (single domain for backward compatibility).
-    
+
     Args:
         access_token: HubSpot access token
         domain: Domain to search for
-        
+
     Returns:
         List of HubSpotCompanySummary objects
     """
     return await search_companies_by_domains(access_token, [domain])
 
 
-async def search_companies_by_domains(
-    access_token: str, domains: List[str]
-) -> List[HubSpotCompanySummary]:
+async def search_companies_by_domains(access_token: str, domains: List[str]) -> List[HubSpotCompanySummary]:
     """
     Search for companies by multiple domains.
-    
+
     Args:
         access_token: HubSpot access token
         domains: List of domains to search for
-        
+
     Returns:
         List of HubSpotCompanySummary objects
     """
     if not domains:
         return []
-    
+
     properties = [
         "name",
         "domain",
@@ -46,7 +42,7 @@
         "lifecyclestage",
         "hubspot_owner_id",
     ]
-    
+
     filter_group = {
         "filters": [
             {
@@ -56,20 +52,20 @@
             }
         ]
     }
-    
+
     body = {
         "filterGroups": [filter_group],
         "properties": properties,
         "limit": 100,
     }
-    
+
     results_data = await _paginate_hubspot_search(
         access_token=access_token,
         method="POST",
         path="/crm/v3/objects/companies/search",
         body=body,
     )
-    
+
     results = []
     for result in results_data:
         props = result.get("properties", {})
@@ -85,26 +81,24 @@
             raw_properties=props,
         )
         results.append(company)
-    
+
     return results
 
 
-async def search_companies_by_ids(
-    access_token: str, company_ids: List[str]
-) -> List[HubSpotCompanySummary]:
+async def search_companies_by_ids(access_token: str, company_ids: List[str]) -> List[HubSpotCompanySummary]:
     """
     Search for companies by their IDs.
-    
+
     Args:
         access_token: HubSpot access token
         company_ids: List of company IDs to search for
-        
+
     Returns:
         List of HubSpotCompanySummary objects
     """
     if not company_ids:
         return []
-    
+
     properties = [
         "name",
         "domain",
@@ -114,7 +108,7 @@
         "lifecyclestage",
         "hubspot_owner_id",
     ]
-    
+
     filter_group = {
         "filters": [
             {
@@ -124,20 +118,20 @@
             }
         ]
     }
-    
+
     body = {
         "filterGroups": [filter_group],
         "properties": properties,
         "limit": len(company_ids),
     }
-    
+
     results_data = await _paginate_hubspot_search(
         access_token=access_token,
         method="POST",
         path="/crm/v3/objects/companies/search",
         body=body,
     )
-    
+
     results = []
     for result in results_data:
         props = result.get("properties", {})
@@ -153,6 +147,5 @@
             raw_properties=props,
         )
         results.append(company)
-    
-    return results
 
+    return results

--- app/hubspot/crm/contacts.py
+++ app/hubspot/crm/contacts.py
@@ -8,16 +8,14 @@
 logger = get_logger(__name__)
 
 
-async def search_contacts_by_email(
-    access_token: str, emails: List[str]
-) -> List[HubSpotContactSummary]:
+async def search_contacts_by_email(access_token: str, emails: List[str]) -> List[HubSpotContactSummary]:
     """
     Search for contacts by email address(es).
-    
+
     Args:
         access_token: HubSpot access token
         emails: List of email addresses to search for
-        
+
     Returns:
         List of HubSpotContactSummary objects
     """
@@ -31,11 +29,11 @@
         "hs_lead_status",
         "hubspot_owner_id",
     ]
-    
+
     batch_size = 100
     for i in range(0, len(emails), batch_size):
-        email_batch = emails[i:i + batch_size]
-        
+        email_batch = emails[i : i + batch_size]
+
         filter_group = {
             "filters": [
                 {
@@ -45,13 +43,13 @@
                 }
             ]
         }
-        
+
         body = {
             "filterGroups": [filter_group],
             "properties": properties,
             "limit": 100,
         }
-        
+
         try:
             results_data = await _paginate_hubspot_search(
                 access_token=access_token,
@@ -59,7 +57,7 @@
                 path="/crm/v3/objects/contacts/search",
                 body=body,
             )
-            
+
             for result in results_data:
                 props = result.get("properties", {})
                 contact = HubSpotContactSummary(
@@ -79,6 +77,5 @@
                 f"Failed to search contacts for email batch",
                 extra={"email_count": len(email_batch), "error": str(e)},
             )
-    
+
     return results
-

--- app/hubspot/crm/context.py
+++ app/hubspot/crm/context.py
@@ -7,32 +7,30 @@
 from .owners import list_owners
 
 
-async def get_context_by_email(
-    access_token: str, email: str
-) -> HubSpotContextByEmailResponse:
+async def get_context_by_email(access_token: str, email: str) -> HubSpotContextByEmailResponse:
     """
     Get complete context for an email address, including contacts, companies, deals, and owners.
-    
+
     Args:
         access_token: HubSpot access token
         email: Email address to search for
-        
+
     Returns:
         HubSpotContextByEmailResponse with all related data
     """
     contacts = await search_contacts_by_email(access_token, [email])
-    
+
     company_ids = set()
     owner_ids = set()
-    
+
     for contact in contacts:
         if contact.owner_id:
             owner_ids.add(contact.owner_id)
-        
+
         contact_company_ids = await get_contact_company_associations(access_token, contact.id)
         for company_id in contact_company_ids:
             company_ids.add(company_id)
-    
+
     companies = []
     if company_ids:
         companies_by_ids = await search_companies_by_ids(access_token, list(company_ids))
@@ -40,7 +38,7 @@
         for company in companies_by_ids:
             if company.owner_id:
                 owner_ids.add(company.owner_id)
-    
+
     domain = email.split("@")[1] if "@" in email else None
     if domain:
         companies_by_domain = await search_companies_by_domain(access_token, domain)
@@ -51,31 +49,30 @@
                 company_ids.add(company.id)
             if company.owner_id:
                 owner_ids.add(company.owner_id)
-    
+
     deals = []
     if company_ids:
         for company_id in company_ids:
             deal_request = HubSpotDealSearchRequest(company_id=company_id, limit=50)
             company_deals = await search_deals(access_token, deal_request)
             deals.extend(company_deals)
-        
+
         for deal in deals:
             if deal.raw_properties.get("hubspot_owner_id"):
                 owner_ids.add(deal.raw_properties.get("hubspot_owner_id"))
-    
+
     all_owners = await list_owners(access_token)
     owner_map = {owner.id: owner for owner in all_owners}
-    
+
     for contact in contacts:
         if contact.owner_id and contact.owner_id in owner_map:
             contact.owner_email = owner_map[contact.owner_id].email
-    
+
     used_owners = [owner_map[oid] for oid in owner_ids if oid in owner_map]
-    
+
     return HubSpotContextByEmailResponse(
         contacts=contacts,
         companies=companies,
         deals=deals,
         owners=used_owners,
     )
-

--- app/hubspot/crm/deals.py
+++ app/hubspot/crm/deals.py
@@ -7,16 +7,14 @@
 from .client import _paginate_hubspot_search
 
 
-async def search_deals(
-    access_token: str, request: HubSpotDealSearchRequest
-) -> List[HubSpotDealSummary]:
+async def search_deals(access_token: str, request: HubSpotDealSearchRequest) -> List[HubSpotDealSummary]:
     """
     Search for deals with optional filters.
-    
+
     Args:
         access_token: HubSpot access token
         request: HubSpotDealSearchRequest with filters
-        
+
     Returns:
         List of HubSpotDealSummary objects
     """
@@ -29,78 +27,80 @@
         "hs_lastmodifieddate",
         "dealtype",
     ]
-    
+
     deal_ids: Set[str] = set()
     if request.company_id:
         company_deal_ids = await get_deals_by_company_id(access_token, request.company_id)
         deal_ids.update(company_deal_ids)
-    
+
     if request.contact_id:
         contact_deal_ids = await get_deals_by_contact_id(access_token, request.contact_id)
         if deal_ids:
             deal_ids = deal_ids.intersection(set(contact_deal_ids))
         else:
             deal_ids.update(contact_deal_ids)
-    
+
     if (request.company_id or request.contact_id) and not deal_ids:
         return []
-    
+
     filters = []
-    
+
     if deal_ids:
-        filters.append({
-            "propertyName": "hs_object_id",
-            "operator": "IN",
-            "values": list(deal_ids),
-        })
-    
+        filters.append(
+            {
+                "propertyName": "hs_object_id",
+                "operator": "IN",
+                "values": list(deal_ids),
+            }
+        )
+
     if request.stages:
-        filters.append({
-            "propertyName": "dealstage",
-            "operator": "IN",
-            "values": request.stages,
-        })
-    
+        filters.append(
+            {
+                "propertyName": "dealstage",
+                "operator": "IN",
+                "values": request.stages,
+            }
+        )
+
     if request.pipelines:
-        filters.append({
-            "propertyName": "pipeline",
-            "operator": "IN",
-            "values": request.pipelines,
-        })
-    
+        filters.append(
+            {
+                "propertyName": "pipeline",
+                "operator": "IN",
+                "values": request.pipelines,
+            }
+        )
+
     body = {
         "properties": properties,
         "limit": request.limit,
     }
-    
+
     if filters:
         body["filterGroups"] = [{"filters": filters}]
-    
+
     results_data = await _paginate_hubspot_search(
         access_token=access_token,
         method="POST",
         path="/crm/v3/objects/deals/search",
         body=body,
     )
-    
+
     results = []
     for result in results_data:
         props = result.get("properties", {})
         associations = result.get("associations", {})
-        
+
         company_ids = []
         contact_ids = []
-        
+
         if "companies" in associations:
-            company_ids = [
-                assoc.get("id") for assoc in associations["companies"].get("results", [])
-            ]
-        
+            company_ids = [assoc.get("id") for assoc in associations["companies"].get("results", [])]
+
         if "contacts" in associations:
-            contact_ids = [
-                assoc.get("id") for assoc in associations["contacts"].get("results", [])
-            ]
-        
+            contact_ids = [assoc.get("id") for assoc in associations["contacts"].get("results", [])]
+
         close_date = None
         if props.get("closedate"):
             try:
@@ -108,7 +108,7 @@
                 close_date = datetime.fromtimestamp(timestamp_ms / 1000)
             except (ValueError, TypeError):
                 pass
-        
+
         last_modified = None
         if props.get("hs_lastmodifieddate"):
             try:
@@ -116,7 +116,7 @@
                 last_modified = datetime.fromtimestamp(timestamp_ms / 1000)
             except (ValueError, TypeError):
                 pass
-        
+
         deal = HubSpotDealSummary(
             id=result.get("id", ""),
             name=props.get("dealname"),
@@ -131,6 +131,5 @@
             raw_properties=props,
         )
         results.append(deal)
-    
+
     return results
-

--- app/hubspot/crm/owners.py
+++ app/hubspot/crm/owners.py
@@ -14,7 +14,7 @@
 async def get_account_summary(access_token: str) -> Dict:
     """
     Get HubSpot account summary from token info endpoint.
-    
+
     Returns:
         Dict with hub_id, email, and scopes
     """
@@ -44,10 +44,10 @@
 async def list_owners(access_token: str) -> List[HubSpotOwnerSummary]:
     """
     List all HubSpot owners.
-    
+
     Args:
         access_token: HubSpot access token
-        
+
     Returns:
         List of HubSpotOwnerSummary objects
     """
@@ -56,7 +56,7 @@
         method="GET",
         path="/crm/v3/owners",
     )
-    
+
     results = []
     for owner_data in response.get("results", []):
         owner = HubSpotOwnerSummary(
@@ -69,6 +69,5 @@
             teams=owner_data.get("teams", []),
         )
         results.append(owner)
-    
+
     return results
-

--- app/hubspot/models.py
+++ app/hubspot/models.py
@@ -6,7 +6,7 @@
 
 class HubSpotContactSummary(BaseModel):
     """Summary of a HubSpot contact."""
-    
+
     id: str = Field(..., description="HubSpot internal ID of the contact")
     email: Optional[str] = Field(None, description="Contact email address")
     first_name: Optional[str] = Field(None, description="Contact first name")
@@ -21,7 +21,7 @@
 
 class HubSpotCompanySummary(BaseModel):
     """Summary of a HubSpot company."""
-    
+
     id: str = Field(..., description="HubSpot internal ID of the company")
     name: Optional[str] = Field(None, description="Company name")
     domain: Optional[str] = Field(None, description="Company domain")
@@ -35,7 +35,7 @@
 
 class HubSpotDealSummary(BaseModel):
     """Summary of a HubSpot deal."""
-    
+
     id: str = Field(..., description="HubSpot internal ID of the deal")
     name: Optional[str] = Field(None, description="Deal name")
     amount: Optional[str] = Field(None, description="Deal amount")
@@ -51,7 +51,7 @@
 
 class HubSpotOwnerSummary(BaseModel):
     """Summary of a HubSpot owner."""
-    
+
     id: str = Field(..., description="HubSpot owner ID")
     email: Optional[str] = Field(None, description="Owner email")
     first_name: Optional[str] = Field(None, description="Owner first name")
@@ -63,7 +63,7 @@
 
 class HubSpotContextByEmailResponse(BaseModel):
     """Complete context for an email address, including contacts, companies, deals, and owners."""
-    
+
     contacts: List[HubSpotContactSummary] = Field(default_factory=list, description="Contacts found for this email")
     companies: List[HubSpotCompanySummary] = Field(default_factory=list, description="Companies associated with contacts")
     deals: List[HubSpotDealSummary] = Field(default_factory=list, description="Deals associated with companies")
@@ -72,24 +72,24 @@
 
 # Request Models
 
+
 class HubSpotContactSearchRequest(BaseModel):
     """Request model for searching contacts by email."""
-    
+
     emails: List[str] = Field(..., description="List of email addresses to search for", min_items=1)
 
 
 class HubSpotCompanySearchRequest(BaseModel):
     """Request model for searching companies by domain."""
-    
+
     domains: List[str] = Field(..., description="List of domains to search for", min_items=1)
 
 
 class HubSpotDealSearchRequest(BaseModel):
     """Request model for searching deals with filters."""
-    
+
     company_id: Optional[str] = Field(None, description="Filter by company ID")
     contact_id: Optional[str] = Field(None, description="Filter by contact ID")
     stages: Optional[List[str]] = Field(None, description="Filter by deal stages (IN operator)")
     pipelines: Optional[List[str]] = Field(None, description="Filter by pipelines (IN operator)")
     limit: int = Field(20, description="Maximum number of results to return", ge=1, le=100)
-

--- app/hubspot/oauth.py
+++ app/hubspot/oauth.py
@@ -174,4 +174,3 @@
     if value.tzinfo is None:
         value = value.replace(tzinfo=timezone.utc)
     return value <= datetime.now(timezone.utc)
-

--- app/hubspot/repositories/__init__.py
+++ app/hubspot/repositories/__init__.py
@@ -1,2 +1 @@
 
-

--- app/hubspot/repositories/hubspot_tokens_repository.py
+++ app/hubspot/repositories/hubspot_tokens_repository.py
@@ -99,4 +99,3 @@
             expires_at,
             token_id,
         )
-

--- app/hubspot/router.py
+++ app/hubspot/router.py
@@ -110,7 +110,7 @@
     access_token = await get_valid_hubspot_access_token(user_id)
     if not access_token:
         raise HTTPException(status_code=401, detail="hubspot_not_connected")
-    
+
     contacts = await search_contacts_by_email(access_token, [email])
     return contacts
 
@@ -127,7 +127,7 @@
     access_token = await get_valid_hubspot_access_token(user_id)
     if not access_token:
         raise HTTPException(status_code=401, detail="hubspot_not_connected")
-    
+
     contacts = await search_contacts_by_email(access_token, request.emails)
     return contacts
 
@@ -144,7 +144,7 @@
     access_token = await get_valid_hubspot_access_token(user_id)
     if not access_token:
         raise HTTPException(status_code=401, detail="hubspot_not_connected")
-    
+
     companies = await search_companies_by_domain(access_token, domain)
     return companies
 
@@ -161,7 +161,7 @@
     access_token = await get_valid_hubspot_access_token(user_id)
     if not access_token:
         raise HTTPException(status_code=401, detail="hubspot_not_connected")
-    
+
     companies = await search_companies_by_domains(access_token, request.domains)
     return companies
 
@@ -178,7 +178,7 @@
     access_token = await get_valid_hubspot_access_token(user_id)
     if not access_token:
         raise HTTPException(status_code=401, detail="hubspot_not_connected")
-    
+
     deals = await search_deals(access_token, request)
     return deals
 
@@ -195,12 +195,12 @@
     access_token = await get_valid_hubspot_access_token(user_id)
     if not access_token:
         raise HTTPException(status_code=401, detail="hubspot_not_connected")
-    
+
     owners = await list_owners(access_token)
-    
+
     if active_only:
         owners = [owner for owner in owners if owner.active]
-    
+
     return owners
 
 
@@ -217,7 +217,6 @@
     access_token = await get_valid_hubspot_access_token(user_id)
     if not access_token:
         raise HTTPException(status_code=401, detail="hubspot_not_connected")
-    
+
     context = await get_context_by_email(access_token, email)
     return context
-

--- app/main.py
+++ app/main.py
@@ -24,11 +24,13 @@
 
 # Load environment variables FIRST, before any imports
 from dotenv import load_dotenv
+
 load_dotenv()
 
 # Initialize logging BEFORE any other imports
 from app.utils.logging_config import setup_logging, get_logger
 from app.utils.config import get_backend_url
+
 setup_logging()
 
 import os
@@ -65,9 +67,9 @@
     """Application lifespan management with proper logging."""
     # Check if we're in testing mode - skip expensive initializations
     is_testing = os.getenv("TESTING") == "true" or os.getenv("TESTING") == "1"
-    
+
     logger.info("Starting application initialization")
-    
+
     try:
         # Initialize LangSmith (skip in testing for speed)
         if not is_testing:
@@ -75,9 +77,10 @@
             initialize_langsmith()
         else:
             logger.info("Skipping LangSmith initialization (TESTING mode)")
-        
+
         # Initialize global database instance (conditional)
         from app.utils.config import get_enable_postgres_storage, get_admin_emails
+
         enable_postgres_storage = get_enable_postgres_storage()
         if enable_postgres_storage:
             # Check if GlobalDB is already initialized (e.g., by tests)
@@ -103,6 +106,7 @@
                 logger.info("Starting admin bootstrapping", extra={"admin_emails_count": len(admin_emails)})
                 # Use global DB instance instead of creating a new pool
                 from app.utils.global_db import get_global_db
+
                 db = await get_global_db()
                 async with db.pool.acquire() as conn:
                     for email in admin_emails:
@@ -131,7 +135,8 @@
                             # Pre-approve the email for admin registration
                             await conn.execute(
                                 "INSERT INTO approved_users (email, approved_at, approved_by) VALUES ($1, NOW(), $2) ON CONFLICT DO NOTHING",
-                                email, "bootstrap"
+                                email,
+                                "bootstrap",
                             )
                             logger.info("Pre-approved admin email", extra={"email": email, "action": "pre_approve_admin"})
                 # Don't close the global DB instance - it's shared across the application
@@ -139,29 +144,29 @@
                 logger.info("Skipping admin bootstrapping (ENABLE_POSTGRES_STORAGE=false)")
         else:
             logger.info("Skipping admin bootstrapping (TESTING mode)")
-        
+
         # Start TAMradar balance monitoring background task (skip in testing)
         if enable_postgres_storage and not is_testing:
             try:
                 from app.services.tamradar_balance_monitor import tamradar_balance_monitor
                 from app.utils.config import get_tamradar_balance_check_interval
-                
+
                 async def check_balance_periodically():
                     """Background task to check TAMradar balance every 4 hours."""
                     interval_minutes = get_tamradar_balance_check_interval()
                     # Override to 4 hours (240 minutes) as requested
                     interval_seconds = 240 * 60  # 4 hours
-                    
+
                     # Wait a bit before first check to let app fully start
                     await asyncio.sleep(60)
-                    
+
                     while True:
                         try:
                             await tamradar_balance_monitor.check_balance_and_alert()
                         except Exception as e:
                             logger.error(f"TAMradar balance check failed: {e}", exc_info=True)
                         await asyncio.sleep(interval_seconds)
-                
+
                 # Start background task
                 asyncio.create_task(check_balance_periodically())
                 logger.info("Started TAMradar balance monitoring background task (checks every 4 hours)")
@@ -169,10 +174,10 @@
                 logger.warning(f"Failed to start TAMradar balance monitoring: {e}")
         elif is_testing:
             logger.info("Skipping TAMradar balance monitoring (TESTING mode)")
-        
+
         logger.info("Application startup completed successfully")
         yield
-        
+
     except Exception as e:
         logger.error("Application startup failed", extra={"error": str(e), "error_type": type(e).__name__})
         raise
@@ -188,15 +193,17 @@
         except Exception as e:
             logger.error("Error during application shutdown", extra={"error": str(e), "error_type": type(e).__name__})
 
+
 app = FastAPI(title="Prospecting API", lifespan=lifespan)
 
+
 # Configure CORS - MUST BE FIRST to handle preflight requests
 def get_cors_config():
     """Return environment-specific CORS settings."""
     from app.utils.config import get_environment, get_staging_additional_origins
-    
+
     environment = get_environment()
-    
+
     if environment == "staging":
         base_origins = [
             "https://stage-dashboard.ardessa.com",  # Staging frontend
@@ -290,21 +297,26 @@
 logger.info("Adding request logging middleware", extra={"log_request_body": False, "log_response_body": False})
 app.add_middleware(RequestLoggingMiddleware, log_request_body=False, log_response_body=False)
 
+
 # Add CORS debug middleware
 class CORSDebugMiddleware(BaseHTTPMiddleware):
     async def dispatch(self, request, call_next):
         response = await call_next(request)
-        logger.debug("CORS Debug", extra={
-            "origin": request.headers.get("origin"),
-            "method": request.method,
-            "cors_headers": {
-                "allow_origin": response.headers.get("access-control-allow-origin"),
-                "allow_methods": response.headers.get("access-control-allow-methods"),
-                "allow_headers": response.headers.get("access-control-allow-headers")
-            }
-        })
+        logger.debug(
+            "CORS Debug",
+            extra={
+                "origin": request.headers.get("origin"),
+                "method": request.method,
+                "cors_headers": {
+                    "allow_origin": response.headers.get("access-control-allow-origin"),
+                    "allow_methods": response.headers.get("access-control-allow-methods"),
+                    "allow_headers": response.headers.get("access-control-allow-headers"),
+                },
+            },
+        )
         return response
 
+
 logger.debug("Adding CORS debug middleware")
 app.add_middleware(CORSDebugMiddleware)
 
@@ -326,6 +338,7 @@
     ],
 )
 
+
 # Security Headers Middleware
 class SecurityHeadersMiddleware(BaseHTTPMiddleware):
     async def dispatch(self, request, call_next):
@@ -342,22 +355,25 @@
         # Note: FE should progressively tighten and eventually enforce via headers in prod.
         # Get environment-specific backend URL for CSP
         backend_url = get_backend_url()
-        csp_policy = "; ".join([
-            "default-src 'self'",
-            "script-src 'self' https://browser.sentry-cdn.com 'unsafe-inline' 'unsafe-eval'",
-            "style-src 'self' 'unsafe-inline'",
-            "img-src 'self' data: https:",
-            f"connect-src 'self' {backend_url} https://*.sentry.io",
-            "font-src 'self' data:",
-            "object-src 'none'",
-            "base-uri 'self'",
-            "frame-ancestors 'none'",
-        ])
+        csp_policy = "; ".join(
+            [
+                "default-src 'self'",
+                "script-src 'self' https://browser.sentry-cdn.com 'unsafe-inline' 'unsafe-eval'",
+                "style-src 'self' 'unsafe-inline'",
+                "img-src 'self' data: https:",
+                f"connect-src 'self' {backend_url} https://*.sentry.io",
+                "font-src 'self' data:",
+                "object-src 'none'",
+                "base-uri 'self'",
+                "frame-ancestors 'none'",
+            ]
+        )
         # Use Report-Only header to monitor; switch to Content-Security-Policy to enforce later
         response.headers.setdefault("Content-Security-Policy-Report-Only", csp_policy)
         logger.debug("Applied security headers", extra={"path": str(request.url.path), "method": request.method})
         return response
 
+
 logger.info("Adding security headers middleware")
 app.add_middleware(SecurityHeadersMiddleware)
 
@@ -409,13 +425,14 @@
 from fastapi.responses import JSONResponse
 from fastapi.encoders import jsonable_encoder
 
+
 # Add exception handler for validation errors to log webhook failures
 @app.exception_handler(RequestValidationError)
 async def validation_exception_handler(request: Request, exc: RequestValidationError):
     """Log validation errors with full details, especially for webhook endpoints."""
     errors = exc.errors()
     error_details = jsonable_encoder(errors)
-    
+
     # Try to read request body for webhook debugging
     request_body = None
     if "/api/webhooks/tamradar/internal" in str(request.url.path):
@@ -424,12 +441,13 @@
             if body_bytes:
                 try:
                     import json
-                    request_body = json.loads(body_bytes.decode('utf-8'))
+
+                    request_body = json.loads(body_bytes.decode("utf-8"))
                 except (json.JSONDecodeError, UnicodeDecodeError):
-                    request_body = body_bytes.decode('utf-8', errors='replace')[:1000]
+                    request_body = body_bytes.decode("utf-8", errors="replace")[:1000]
         except Exception:
             pass  # Don't fail if we can't read body
-    
+
     # Log validation errors with full context
     if "/api/webhooks/tamradar/internal" in str(request.url.path):
         logger.error(
@@ -441,9 +459,9 @@
                 "validation_errors": error_details,
                 "request_body": request_body,
                 "query_params": dict(request.query_params),
-                "headers": {k: v for k, v in request.headers.items() if k.lower() not in ['authorization', 'cookie']},
+                "headers": {k: v for k, v in request.headers.items() if k.lower() not in ["authorization", "cookie"]},
             },
-            exc_info=True
+            exc_info=True,
         )
     else:
         logger.warning(
@@ -452,14 +470,15 @@
                 "path": request.url.path,
                 "method": request.method,
                 "validation_errors": error_details,
-            }
+            },
         )
-    
+
     return JSONResponse(
         status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
         content={"detail": error_details},
     )
 
+
 # Register internal webhook endpoint (for Lambda fan-out)
 # This is the only webhook endpoint needed - Lambda routes all TAMradar webhooks here
 app.post(
@@ -476,6 +495,7 @@
     logger.info("Health check request received", extra={"endpoint": "/", "method": "GET"})
     return {"status": "ok", "message": "Prospecting API is running"}
 
+
 # Cache for Celery status check to avoid SQS rate limits
 # SQS only allows 1 PurgeQueue operation per queue every 60 seconds
 _celery_status_cache = {
@@ -486,26 +506,28 @@
 _celery_status_cache_lock = asyncio.Lock()
 CELERY_STATUS_CACHE_TTL = 90  # Cache for 90 seconds (longer than SQS 60s rate limit)
 
+
 async def _get_celery_status_cached():
     """Get Celery status with caching to avoid SQS PurgeQueue rate limits."""
     import time
     from botocore.exceptions import ClientError
-    
+
     current_time = time.time()
-    
+
     # Check if cache is still valid (with lock to prevent race conditions)
     async with _celery_status_cache_lock:
         cache_age = current_time - _celery_status_cache["timestamp"]
         if cache_age < CELERY_STATUS_CACHE_TTL and _celery_status_cache["timestamp"] > 0:
             # Return cached status
             return _celery_status_cache["status"], _celery_status_cache["error"]
-    
+
     # Cache expired or invalid, check Celery status
     celery_status = "unknown"
     celery_error = None
-    
+
     try:
         from app.celery_app import celery_app
+
         # Try to inspect active workers
         inspect = celery_app.control.inspect()
         active = inspect.active()
@@ -536,30 +558,28 @@
         celery_status = "error"
         celery_error = str(e)
         logger.warning(f"Celery status check failed: {e}", exc_info=True)
-    
+
     # Update cache (with lock to prevent race conditions)
     async with _celery_status_cache_lock:
         _celery_status_cache["status"] = celery_status
         _celery_status_cache["error"] = celery_error
         _celery_status_cache["timestamp"] = current_time
-    
+
     return celery_status, celery_error
 
+
 @app.get("/health")
 async def health_check():
     """Health check endpoint for AWS App Runner."""
     logger.info("Health check request received", extra={"endpoint": "/health", "method": "GET"})
-    
+
     # Check Celery status (with caching to avoid SQS rate limits)
     celery_status, celery_error = await _get_celery_status_cached()
-    
+
     return {
         "status": "healthy",
         "timestamp": "2024-01-01T00:00:00Z",
         "service": "Prospecting API",
         "version": "1.0.0",
-        "celery": {
-            "status": celery_status,
-            "error": celery_error
-        }
-    } 
+        "celery": {"status": celery_status, "error": celery_error},
+    }

--- app/meeting_prep/__init__.py
+++ app/meeting_prep/__init__.py
@@ -1,3 +1,3 @@
 from app.meeting_prep.router import router
 
-__all__ = ['router']
+__all__ = ["router"]

--- app/meeting_prep/analysis/communication.py
+++ app/meeting_prep/analysis/communication.py
@@ -10,20 +10,18 @@
 from app.meeting_prep.prompts.prompts import get_communication_style_prompt
 
 
-async def analyze_communication_style(
-    related_emails: List[Dict[str, Any]]
-) -> CommunicationStyleModel:
+async def analyze_communication_style(related_emails: List[Dict[str, Any]]) -> CommunicationStyleModel:
     if not related_emails:
         return CommunicationStyleModel()
-    
+
     api_key = get_openai_api_key()
     if not api_key:
         raise LLMAPIError("OpenAI API key not configured")
-    
+
     try:
         config = get_llm_config()
         prompt = get_communication_style_prompt(related_emails)
-        
+
         chat = ChatOpenAI(
             model=config.openai_model,
             temperature=config.openai_temperature,
@@ -31,15 +29,14 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", prompt)])
         content = response.content.strip()
-        
+
         result = parse_llm_json_response(content)
         return CommunicationStyleModel(**result)
-        
+
     except Exception as e:
         if isinstance(e, LLMAPIError):
             raise
         return CommunicationStyleModel()
-

--- app/meeting_prep/analysis/metadata.py
+++ app/meeting_prep/analysis/metadata.py
@@ -4,52 +4,45 @@
 from app.meeting_prep.models import MeetingMetadataModel, AttendeeModel
 
 
-def extract_meeting_metadata(
-    event: Dict[str, Any],
-    related_emails: List[Dict[str, Any]]
-) -> MeetingMetadataModel:
+def extract_meeting_metadata(event: Dict[str, Any], related_emails: List[Dict[str, Any]]) -> MeetingMetadataModel:
     try:
-        meeting_title = event.get('title') or event.get('summary') or event.get('subject')
-        meeting_time_str = event.get('time') or event.get('start', {}).get('dateTime') or event.get('start', {}).get('date')
-        
-        duration_minutes = event.get('duration_minutes')
+        meeting_title = event.get("title") or event.get("summary") or event.get("subject")
+        meeting_time_str = event.get("time") or event.get("start", {}).get("dateTime") or event.get("start", {}).get("date")
+
+        duration_minutes = event.get("duration_minutes")
         if not duration_minutes:
-            start = event.get('start', {})
-            end = event.get('end', {})
+            start = event.get("start", {})
+            end = event.get("end", {})
             if isinstance(start, dict) and isinstance(end, dict):
                 try:
-                    start_dt = datetime.fromisoformat(start.get('dateTime', '').replace('Z', '+00:00'))
-                    end_dt = datetime.fromisoformat(end.get('dateTime', '').replace('Z', '+00:00'))
+                    start_dt = datetime.fromisoformat(start.get("dateTime", "").replace("Z", "+00:00"))
+                    end_dt = datetime.fromisoformat(end.get("dateTime", "").replace("Z", "+00:00"))
                     duration_minutes = int((end_dt - start_dt).total_seconds() / 60)
                 except Exception:
                     duration_minutes = None
-        
-        location = event.get('location') or event.get('location', {}).get('displayName')
-        
+
+        location = event.get("location") or event.get("location", {}).get("displayName")
+
         attendees = []
-        event_attendees = event.get('attendees', [])
+        event_attendees = event.get("attendees", [])
         for att in event_attendees:
             if isinstance(att, dict):
-                name = att.get('displayName') or att.get('name') or att.get('emailAddress', {}).get('name')
-                email = att.get('email') or att.get('emailAddress', {}).get('address')
+                name = att.get("displayName") or att.get("name") or att.get("emailAddress", {}).get("name")
+                email = att.get("email") or att.get("emailAddress", {}).get("address")
                 if name and email:
-                    attendees.append(AttendeeModel(
-                        name=name,
-                        email=email,
-                        role=None
-                    ))
-        
+                    attendees.append(AttendeeModel(name=name, email=email, role=None))
+
         meeting_date = None
         meeting_time = None
         if meeting_time_str:
             try:
                 if isinstance(meeting_time_str, str):
-                    dt = datetime.fromisoformat(meeting_time_str.replace('Z', '+00:00'))
+                    dt = datetime.fromisoformat(meeting_time_str.replace("Z", "+00:00"))
                     meeting_date = dt.date().isoformat()
                     meeting_time = dt.isoformat()
             except Exception:
                 pass
-        
+
         return MeetingMetadataModel(
             meeting_title=meeting_title,
             meeting_date=meeting_date,
@@ -60,4 +53,3 @@
         )
     except Exception:
         return MeetingMetadataModel()
-

--- app/meeting_prep/analysis/relationship.py
+++ app/meeting_prep/analysis/relationship.py
@@ -9,98 +9,86 @@
     emails = set()
     for participant in participants:
         if isinstance(participant, dict):
-            email = participant.get('email') or participant.get('emailAddress', {}).get('address')
+            email = participant.get("email") or participant.get("emailAddress", {}).get("address")
             if email:
                 emails.add(email.lower())
     return emails
 
 
-def _find_previous_meetings(
-    target_event: Dict[str, Any],
-    calendar_events: List[Dict[str, Any]]
-) -> List[Dict[str, Any]]:
+def _find_previous_meetings(target_event: Dict[str, Any], calendar_events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
     """Find previous meetings with overlapping participants."""
     if not target_event or not calendar_events:
         return []
-    
+
     # Get target event participants
-    target_participants = target_event.get('participants', []) or target_event.get('attendees', [])
+    target_participants = target_event.get("participants", []) or target_event.get("attendees", [])
     target_emails = _extract_emails_from_participants(target_participants)
-    
+
     if not target_emails:
         return []
-    
+
     # Get target event time
-    target_time_str = (
-        target_event.get('time') or 
-        target_event.get('start', {}).get('dateTime') or 
-        target_event.get('start', {}).get('date')
-    )
+    target_time_str = target_event.get("time") or target_event.get("start", {}).get("dateTime") or target_event.get("start", {}).get("date")
     if not target_time_str:
         return []
-    
+
     try:
         if isinstance(target_time_str, str):
-            target_time = datetime.fromisoformat(target_time_str.replace('Z', '+00:00'))
+            target_time = datetime.fromisoformat(target_time_str.replace("Z", "+00:00"))
         else:
             return []
     except Exception:
         return []
-    
+
     previous_meetings = []
     now = datetime.now(timezone.utc)
-    
+
     for event in calendar_events:
         # Skip the target event itself
-        event_id = event.get('event_id') or event.get('id', '')
-        target_id = target_event.get('event_id') or target_event.get('id', '')
+        event_id = event.get("event_id") or event.get("id", "")
+        target_id = target_event.get("event_id") or target_event.get("id", "")
         if event_id == target_id:
             continue
-        
+
         # Get event time
-        event_time_str = (
-            event.get('time') or 
-            event.get('start', {}).get('dateTime') or 
-            event.get('start', {}).get('date')
-        )
+        event_time_str = event.get("time") or event.get("start", {}).get("dateTime") or event.get("start", {}).get("date")
         if not event_time_str:
             continue
-        
+
         try:
             if isinstance(event_time_str, str):
-                event_time = datetime.fromisoformat(event_time_str.replace('Z', '+00:00'))
+                event_time = datetime.fromisoformat(event_time_str.replace("Z", "+00:00"))
             else:
                 continue
         except Exception:
             continue
-        
+
         # Only consider past meetings
         if event_time >= target_time or event_time > now:
             continue
-        
+
         # Check for overlapping participants
-        event_participants = event.get('participants', []) or event.get('attendees', [])
+        event_participants = event.get("participants", []) or event.get("attendees", [])
         event_emails = _extract_emails_from_participants(event_participants)
-        
+
         # Check if there's overlap (at least one common external participant)
         common_emails = target_emails.intersection(event_emails)
         if common_emails:
-            previous_meetings.append({
-                'title': event.get('summary') or event.get('title', 'Untitled Meeting'),
-                'time': event_time,
-                'participants_count': len(event_emails),
-            })
-    
+            previous_meetings.append(
+                {
+                    "title": event.get("summary") or event.get("title", "Untitled Meeting"),
+                    "time": event_time,
+                    "participants_count": len(event_emails),
+                }
+            )
+
     # Sort by time, most recent first
-    previous_meetings.sort(key=lambda x: x['time'], reverse=True)
+    previous_meetings.sort(key=lambda x: x["time"], reverse=True)
     return previous_meetings[:5]  # Return up to 5 most recent
 
 
 def build_relationship_history(
-    related_emails: List[Dict[str, Any]],
-    calendar_events: List[Dict[str, Any]],
-    user_id: str,
-    target_event: Optional[Dict[str, Any]] = None
+    related_emails: List[Dict[str, Any]], calendar_events: List[Dict[str, Any]], user_id: str, target_event: Optional[Dict[str, Any]] = None
 ) -> RelationshipHistoryModel:
     try:
         last_contact_date = None
@@ -108,27 +96,27 @@
         relationship_temperature = None
         email_thread_summary = None
         previous_meetings_summary = None
-        
+
         # Process email data
         if related_emails:
             email_dates = []
             for email in related_emails:
-                date_str = email.get('date') or email.get('receivedDateTime') or email.get('received_at')
+                date_str = email.get("date") or email.get("receivedDateTime") or email.get("received_at")
                 if date_str:
                     try:
                         if isinstance(date_str, str):
-                            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
+                            dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                             email_dates.append(dt)
                     except Exception:
                         pass
-            
+
             if email_dates:
                 last_contact = max(email_dates)
                 # Store ISO format for API, but also format for display
                 last_contact_date = last_contact.isoformat()
                 days_since = (datetime.now(timezone.utc) - last_contact).days
                 days_since_last_contact = days_since
-                
+
                 # Calculate relationship temperature
                 if days_since_last_contact <= 7:
                     relationship_temperature = "hot"
@@ -138,7 +126,7 @@
                     relationship_temperature = "cool"
                 else:
                     relationship_temperature = "cold"
-            
+
             # Enhanced email thread summary
             if len(related_emails) > 0:
                 email_thread_summary = f"{len(related_emails)} email(s) exchanged"
@@ -151,21 +139,21 @@
                         email_thread_summary += f" over {days_span} day(s)"
             else:
                 email_thread_summary = "No related emails found"
-        
+
         # Find previous meetings if target event is provided
         if target_event and calendar_events:
             previous_meetings = _find_previous_meetings(target_event, calendar_events)
             if previous_meetings:
                 meeting_descriptions = []
                 for meeting in previous_meetings:
-                    time_str = meeting['time'].strftime("%B %d, %Y")
+                    time_str = meeting["time"].strftime("%B %d, %Y")
                     meeting_descriptions.append(f"{meeting['title']} on {time_str}")
-                
+
                 if len(previous_meetings) == 1:
                     previous_meetings_summary = f"1 previous meeting: {meeting_descriptions[0]}"
                 else:
                     previous_meetings_summary = f"{len(previous_meetings)} previous meetings: {', '.join(meeting_descriptions)}"
-        
+
         return RelationshipHistoryModel(
             last_contact_date=last_contact_date,
             days_since_last_contact=days_since_last_contact,
@@ -176,6 +164,6 @@
     except Exception as e:
         print(f"[RELATIONSHIP] Error building relationship history: {e}")
         import traceback
+
         traceback.print_exc()
         return RelationshipHistoryModel()
-

--- app/meeting_prep/cache/cache_manager.py
+++ app/meeting_prep/cache/cache_manager.py
@@ -21,13 +21,13 @@
     Distributed cache manager with Redis backend and in-memory fallback.
     Supports LRU eviction for memory cache.
     """
-    
+
     def __init__(self):
         self.use_redis = get_use_redis_rate_limiting() and redis is not None
         self._redis_client = None
         self._memory_cache: Dict[str, Tuple[Any, datetime, int]] = {}  # key -> (value, timestamp, ttl_hours)
         self._max_memory_entries = 1000  # LRU limit
-        
+
         if self.use_redis:
             try:
                 self._redis_client = redis.Redis(
@@ -42,30 +42,18 @@
             except Exception:
                 # Fallback to memory if Redis connection fails
                 self.use_redis = False
-    
-    def _make_key(
-        self,
-        firm_name: Optional[str],
-        person_name: Optional[str],
-        person_email: Optional[str],
-        section: str
-    ) -> str:
+
+    def _make_key(self, firm_name: Optional[str], person_name: Optional[str], person_email: Optional[str], section: str) -> str:
         """Generate cache key from parameters."""
         person_key = f"{person_name}+{person_email}" if person_name and person_email else ""
         return f"meeting_prep:{firm_name or ''}:{person_key}:{section}"
-    
-    async def get(
-        self,
-        firm_name: Optional[str],
-        person_name: Optional[str],
-        person_email: Optional[str],
-        section: str
-    ) -> Optional[Any]:
+
+    async def get(self, firm_name: Optional[str], person_name: Optional[str], person_email: Optional[str], section: str) -> Optional[Any]:
         """
         Get cached value. Returns None if not found or expired.
         """
         key = self._make_key(firm_name, person_name, person_email, section)
-        
+
         # Try Redis first
         if self.use_redis and self._redis_client:
             try:
@@ -75,7 +63,7 @@
             except Exception:
                 # Fallback to memory cache on Redis error
                 pass
-        
+
         # Memory cache fallback
         if key in self._memory_cache:
             value, timestamp, ttl_hours = self._memory_cache[key]
@@ -83,9 +71,9 @@
                 return value
             # Expired - remove it
             del self._memory_cache[key]
-        
+
         return None
-    
+
     async def set(
         self,
         firm_name: Optional[str],
@@ -98,33 +86,26 @@
         Set cached value with TTL based on section type.
         """
         key = self._make_key(firm_name, person_name, person_email, section)
-        ttl_hours = 24 if section == 'firm_intel' else 12
-        
+        ttl_hours = 24 if section == "firm_intel" else 12
+
         # Try Redis first
         if self.use_redis and self._redis_client:
             try:
-                await self._redis_client.setex(
-                    key,
-                    ttl_hours * 3600,
-                    json.dumps(value)
-                )
+                await self._redis_client.setex(key, ttl_hours * 3600, json.dumps(value))
                 return
             except Exception:
                 # Fallback to memory cache on Redis error
                 pass
-        
+
         # Memory cache fallback with LRU
         now = datetime.now()
         self._memory_cache[key] = (value, now, ttl_hours)
-        
+
         # LRU eviction: Remove oldest if cache too large
         if len(self._memory_cache) > self._max_memory_entries:
-            oldest_key = min(
-                self._memory_cache.keys(),
-                key=lambda k: self._memory_cache[k][1]
-            )
+            oldest_key = min(self._memory_cache.keys(), key=lambda k: self._memory_cache[k][1])
             del self._memory_cache[oldest_key]
-    
+
     async def clear(self) -> None:
         """Clear all cache entries."""
         if self.use_redis and self._redis_client:
@@ -135,7 +116,7 @@
                     await self._redis_client.delete(*keys)
             except Exception:
                 pass
-        
+
         self._memory_cache.clear()
 
 

--- app/meeting_prep/config/settings.py
+++ app/meeting_prep/config/settings.py
@@ -11,7 +11,7 @@
     perplexity_model: str = "sonar-pro"
     perplexity_temperature: float = 0
     perplexity_timeout: int = 120
-    
+
     # Section-specific configurations
     executive_summary_temperature: float = 0.7
     executive_summary_max_tokens: int = 1000
@@ -33,4 +33,3 @@
     if _config_instance is None:
         _config_instance = LLMConfig()
     return _config_instance
-

--- app/meeting_prep/core/data_collector.py
+++ app/meeting_prep/core/data_collector.py
@@ -6,28 +6,22 @@
 from app.meeting_prep.core.exceptions import DataCollectionError
 
 
-async def collect_calendar_events(
-    user_id: str, 
-    integration_type: str,
-    days_ahead: int = 7,
-    days_back: int = 0
-) -> List[Dict[str, Any]]:
+async def collect_calendar_events(user_id: str, integration_type: str, days_ahead: int = 7, days_back: int = 0) -> List[Dict[str, Any]]:
     now = datetime.now(timezone.utc)
     event_start = now - timedelta(days=days_back)
     event_end = now + timedelta(days=days_ahead)
-    
+
     try:
-        if integration_type == 'google':
+        if integration_type == "google":
             events = await list_google_calendar_events(user_id, event_start, event_end)
-        elif integration_type == 'outlook':
+        elif integration_type == "outlook":
             events = await list_outlook_calendar_events(user_id, event_start, event_end)
         else:
             raise DataCollectionError(f"Unknown integration type: {integration_type}")
-        
+
         return events
-        
+
     except DataCollectionError:
         raise
     except Exception as e:
         raise DataCollectionError(f"Failed to collect calendar events for user {user_id}: {str(e)}") from e
-

--- app/meeting_prep/core/event_processor.py
+++ app/meeting_prep/core/event_processor.py
@@ -12,15 +12,15 @@
 async def identify_important_events(events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
     if not events:
         return []
-    
+
     api_key = get_openai_api_key()
     if not api_key:
         raise LLMAPIError("OpenAI API key not configured")
-    
+
     try:
         config = get_llm_config()
         prompt = get_important_events_prompt(events)
-        
+
         chat = ChatOpenAI(
             model=config.openai_model,
             temperature=config.openai_temperature,
@@ -28,17 +28,16 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", prompt)])
         content = response.content.strip()
-        
+
         result = parse_llm_json_response(content)
-        important_events = result.get('events', [])
-        
+        important_events = result.get("events", [])
+
         return important_events
-        
+
     except Exception as e:
         if isinstance(e, LLMAPIError):
             raise
         raise LLMAPIError(f"Failed to identify important events: {str(e)}") from e
-

--- app/meeting_prep/core/exceptions.py
+++ app/meeting_prep/core/exceptions.py
@@ -16,4 +16,3 @@
 
 class ReportGenerationError(MeetingPrepError):
     pass
-

--- app/meeting_prep/core/orchestrator.py
+++ app/meeting_prep/core/orchestrator.py
@@ -28,147 +28,131 @@
 from app.meeting_prep.core.exceptions import ReportGenerationError
 
 
-def extract_organizer_email(
-    target_event: Dict[str, Any],
-    events: List[Dict[str, Any]],
-    meeting_id: str
-) -> Optional[str]:
+def extract_organizer_email(target_event: Dict[str, Any], events: List[Dict[str, Any]], meeting_id: str) -> Optional[str]:
     """
     Extract organizer email from event or fallback to events list.
-    
+
     Args:
         target_event: The target meeting event
         events: List of all events (for fallback lookup)
         meeting_id: Meeting ID for matching events
-        
+
     Returns:
         Organizer email address or None if not found
     """
     # Try from target_event first
-    if target_event.get('organizer'):
-        org = target_event.get('organizer', {})
-        email = org.get('email') or org.get('emailAddress', {}).get('address', '')
+    if target_event.get("organizer"):
+        org = target_event.get("organizer", {})
+        email = org.get("email") or org.get("emailAddress", {}).get("address", "")
         if email:
             return email
-    
+
     # Fallback to events list
     for orig_event in events:
-        event_id = orig_event.get('id', '')
-        if event_id == meeting_id or event_id == target_event.get('event_id'):
-            org = orig_event.get('organizer', {})
-            email = org.get('email') or org.get('emailAddress', {}).get('address', '')
+        event_id = orig_event.get("id", "")
+        if event_id == meeting_id or event_id == target_event.get("event_id"):
+            org = orig_event.get("organizer", {})
+            email = org.get("email") or org.get("emailAddress", {}).get("address", "")
             if email:
                 return email
-    
+
     return None
 
 
-async def prepare_meeting_report(
-    user_id: str,
-    meeting_id: str,
-    integration_type: str
-) -> str:
+async def prepare_meeting_report(user_id: str, meeting_id: str, integration_type: str) -> str:
     try:
         existing_report = await get_report_by_meeting_id(user_id, meeting_id, integration_type)
         if existing_report:
-            created_at = existing_report.get('created_at')
+            created_at = existing_report.get("created_at")
             if created_at:
                 if isinstance(created_at, datetime):
                     created_at_utc = created_at.replace(tzinfo=timezone.utc) if created_at.tzinfo is None else created_at
                     if (datetime.now(timezone.utc) - created_at_utc) < timedelta(hours=1):
-                        return existing_report.get('report_content', '')
+                        return existing_report.get("report_content", "")
                 elif isinstance(created_at, str):
                     try:
-                        created_at_dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
+                        created_at_dt = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                         if (datetime.now(timezone.utc) - created_at_dt) < timedelta(hours=1):
-                            return existing_report.get('report_content', '')
+                            return existing_report.get("report_content", "")
                     except Exception:
                         pass
-        
+
         # Use wider date range when generating report for specific meeting
         events_task = collect_calendar_events(user_id, integration_type, days_ahead=30, days_back=7)
         emails_task = collect_emails(user_id, integration_type)
-        
-        events, emails = await asyncio.gather(
-            events_task,
-            emails_task,
-            return_exceptions=True
-        )
-        
+
+        events, emails = await asyncio.gather(events_task, emails_task, return_exceptions=True)
+
         # Handle exceptions gracefully
         if isinstance(events, Exception):
             events = []
         if isinstance(emails, Exception):
             emails = []
-        
+
         # First, try to find the event directly in all events (for manual generation)
         target_event = None
         print(f"[ORCHESTRATOR] Looking for meeting_id: {meeting_id} in {len(events)} events")
         for event in events:
-            event_id = event.get('event_id') or event.get('id', '')
+            event_id = event.get("event_id") or event.get("id", "")
             if event_id == meeting_id:
                 target_event = event
                 print(f"[ORCHESTRATOR] ‚úÖ Found event directly: {event.get('summary') or event.get('title', 'Untitled')}")
                 break
-        
+
         # If not found, check in important events (for cronjob/week organization)
         if not target_event:
             print(f"[ORCHESTRATOR] Event not found in all events, checking important events")
             important_events = await identify_important_events(events)
             print(f"[ORCHESTRATOR] Found {len(important_events)} important events")
             for event in important_events:
-                event_id = event.get('event_id') or event.get('id', '')
+                event_id = event.get("event_id") or event.get("id", "")
                 if event_id == meeting_id:
                     target_event = event
                     print(f"[ORCHESTRATOR] ‚úÖ Found event in important events: {event.get('summary') or event.get('title', 'Untitled')}")
                     break
-        
+
         if not target_event:
             print(f"[ORCHESTRATOR] ‚ùå Event with meeting_id {meeting_id} not found in {len(events)} events")
             # Print first few event IDs for debugging
-            event_ids = [e.get('event_id') or e.get('id', 'N/A') for e in events[:5]]
+            event_ids = [e.get("event_id") or e.get("id", "N/A") for e in events[:5]]
             print(f"[ORCHESTRATOR] Sample event IDs: {event_ids}")
             return ""
-        
+
         related_emails = await find_related_emails_for_event(target_event, emails)
-        
+
         meeting_metadata = extract_meeting_metadata(target_event, related_emails)
-        
+
         relationship_history = build_relationship_history(related_emails, events, user_id, target_event=target_event)
-        
+
         organizer_email = extract_organizer_email(target_event, events, meeting_id)
-        
+
         meeting_data = {
-            'meeting_id': meeting_id,
-            'title': target_event.get('title', ''),
-            'participants': target_event.get('participants', []),
-            'related_emails': related_emails,
-            'organizer_email': organizer_email,
+            "meeting_id": meeting_id,
+            "title": target_event.get("title", ""),
+            "participants": target_event.get("participants", []),
+            "related_emails": related_emails,
+            "organizer_email": organizer_email,
         }
-        
+
         if not validate_meeting_data(meeting_data):
             print(f"[ORCHESTRATOR] ‚ùå Meeting data validation failed for meeting_id: {meeting_id}")
             print(f"[ORCHESTRATOR] Meeting data: {meeting_data}")
             return ""
-        
+
         intelligence_task = collect_intelligence_data(meeting_data, user_id)
         communication_task = analyze_communication_style(related_emails)
-        
-        intelligence_result, communication_style = await asyncio.gather(
-            intelligence_task,
-            communication_task,
-            return_exceptions=True
-        )
-        
+
+        intelligence_result, communication_style = await asyncio.gather(intelligence_task, communication_task, return_exceptions=True)
+
         if isinstance(intelligence_result, Exception):
             intelligence_result = {}
         if isinstance(communication_style, Exception):
             communication_style = CommunicationStyleModel()
-        
+
         firm_intelligence = None
         person_intelligence = None
-        
-        firm_intel_data = intelligence_result.get('firm_intel', {})
+
+        firm_intel_data = intelligence_result.get("firm_intel", {})
         if firm_intel_data:
             try:
                 firm_intelligence = FirmIntelligenceModel(**firm_intel_data)
@@ -178,40 +162,40 @@
             except Exception:
                 # Unexpected error - still use None
                 firm_intelligence = None
-        
-        person_intel_data = intelligence_result.get('person_intel', {})
+
+        person_intel_data = intelligence_result.get("person_intel", {})
         person_intelligence = None
         if person_intel_data:
             try:
                 # Create model with all data (including extra CoreSignal fields)
                 # extra='allow' in model_config allows extra fields to be preserved
                 person_intelligence = PersonIntelligenceModel(**person_intel_data)
-                
+
                 # Log what fields were included
                 logger.info(
                     "PersonIntelligenceModel created",
                     extra={
-                        "has_employment_history": bool(person_intel_data.get('employment_history')),
-                        "has_education": bool(person_intel_data.get('education')),
-                        "has_skills": bool(person_intel_data.get('skills')),
-                        "all_fields": list(person_intel_data.keys())
-                    }
+                        "has_employment_history": bool(person_intel_data.get("employment_history")),
+                        "has_education": bool(person_intel_data.get("education")),
+                        "has_skills": bool(person_intel_data.get("skills")),
+                        "all_fields": list(person_intel_data.keys()),
+                    },
                 )
             except (ValueError, TypeError, KeyError) as e:
                 logger.warning(f"PersonIntelligenceModel validation failed: {e}, trying explicit fields")
                 try:
                     model_data = {
-                        'basic_info': person_intel_data.get('basic_info', {}),
-                        'conferences_public_speaking': person_intel_data.get('conferences_public_speaking', {'speaking_engagements': []}),
-                        'recent_news': person_intel_data.get('recent_news', {'recent_news_mentions': []}),
-                        'recent_social_posts': person_intel_data.get('recent_social_posts'),
-                        'media': person_intel_data.get('media'),
-                        'employment_history': person_intel_data.get('employment_history'),
-                        'education': person_intel_data.get('education'),
-                        'skills': person_intel_data.get('skills'),
-                        'location': person_intel_data.get('location'),
-                        'key_strengths': person_intel_data.get('key_strengths'),
-                        'meeting_relevance': person_intel_data.get('meeting_relevance'),
+                        "basic_info": person_intel_data.get("basic_info", {}),
+                        "conferences_public_speaking": person_intel_data.get("conferences_public_speaking", {"speaking_engagements": []}),
+                        "recent_news": person_intel_data.get("recent_news", {"recent_news_mentions": []}),
+                        "recent_social_posts": person_intel_data.get("recent_social_posts"),
+                        "media": person_intel_data.get("media"),
+                        "employment_history": person_intel_data.get("employment_history"),
+                        "education": person_intel_data.get("education"),
+                        "skills": person_intel_data.get("skills"),
+                        "location": person_intel_data.get("location"),
+                        "key_strengths": person_intel_data.get("key_strengths"),
+                        "meeting_relevance": person_intel_data.get("meeting_relevance"),
                     }
                     person_intelligence = PersonIntelligenceModel(**model_data)
                 except Exception as e2:
@@ -221,16 +205,13 @@
                 # Unexpected error - log and continue
                 logger.error(f"Unexpected error creating PersonIntelligenceModel: {e}", exc_info=True)
                 person_intelligence = None
-        
+
         user_profile = None
         try:
             db = await get_global_db()
             if db:
                 async with db.pool.acquire() as conn:
-                    row = await conn.fetchrow(
-                        "SELECT * FROM user_profiles WHERE user_id = $1",
-                        user_id
-                    )
+                    row = await conn.fetchrow("SELECT * FROM user_profiles WHERE user_id = $1", user_id)
                     if row:
                         user_profile = dict(row)
         except (AttributeError, KeyError, TypeError):
@@ -239,21 +220,21 @@
         except Exception:
             # Unexpected error - still use None
             user_profile = None
-        
+
         # Store raw person_intel_data separately to ensure CoreSignal fields are preserved
         # even if PersonIntelligenceModel doesn't include them
         all_data = {
-            'meeting_metadata': meeting_metadata.model_dump() if meeting_metadata else {},
-            'relationship_history': relationship_history.model_dump() if relationship_history else {},
-            'firm_intel': firm_intel_data,
-            'person_intel': person_intel_data,  # This includes CoreSignal fields like employment_history, education, etc.
-            'communication_style': communication_style.model_dump() if communication_style else {},
-            'related_emails': related_emails,
+            "meeting_metadata": meeting_metadata.model_dump() if meeting_metadata else {},
+            "relationship_history": relationship_history.model_dump() if relationship_history else {},
+            "firm_intel": firm_intel_data,
+            "person_intel": person_intel_data,  # This includes CoreSignal fields like employment_history, education, etc.
+            "communication_style": communication_style.model_dump() if communication_style else {},
+            "related_emails": related_emails,
         }
-        
+
         # Store raw person_intel_data for use in report generation if needed
         raw_person_intel_data = person_intel_data.copy() if person_intel_data else {}
-        
+
         # Prepare all generation tasks
         exec_summary_task = generate_executive_summary(
             email_thread=related_emails,
@@ -261,37 +242,32 @@
             firm_intel=firm_intel_data,
             person_intel=person_intel_data,
         )
-        
+
         starters_task = generate_conversation_starters(
             all_data=all_data,
             user_profile=user_profile,
         )
-        
+
         objectives_task = generate_meeting_objectives(all_data=all_data)
-        
+
         # Execute all generation tasks in parallel
-        generation_results = await asyncio.gather(
-            exec_summary_task,
-            starters_task,
-            objectives_task,
-            return_exceptions=True
-        )
-        
+        generation_results = await asyncio.gather(exec_summary_task, starters_task, objectives_task, return_exceptions=True)
+
         # Process results with exception handling
         executive_summary = generation_results[0] if not isinstance(generation_results[0], Exception) else None
         conversation_starters = generation_results[1] if not isinstance(generation_results[1], Exception) else []
         meeting_objectives = generation_results[2] if not isinstance(generation_results[2], Exception) else None
-        
+
         if person_intelligence and raw_person_intel_data:
-            person_intel_from_model = person_intelligence.model_dump(exclude_none=False, mode='python')
-            for key in ['employment_history', 'education', 'skills', 'location', 'key_strengths', 'meeting_relevance']:
+            person_intel_from_model = person_intelligence.model_dump(exclude_none=False, mode="python")
+            for key in ["employment_history", "education", "skills", "location", "key_strengths", "meeting_relevance"]:
                 if key in raw_person_intel_data:
                     person_intel_from_model[key] = raw_person_intel_data[key]
-            if hasattr(person_intelligence, '__dict__'):
+            if hasattr(person_intelligence, "__dict__"):
                 for key, value in person_intel_from_model.items():
                     if key not in person_intelligence.model_fields:
                         setattr(person_intelligence, key, value)
-        
+
         try:
             report_content = await generate_report(
                 meeting_metadata=meeting_metadata,
@@ -305,20 +281,21 @@
                 related_emails=related_emails,
                 raw_person_intel_data=raw_person_intel_data,
             )
-            
+
             if not report_content or len(report_content) == 0:
                 print("[ORCHESTRATOR] ‚ùå Report generation returned empty content!")
         except Exception as e:
             print(f"[ORCHESTRATOR] ‚ùå Error generating report: {e}")
             import traceback
+
             traceback.print_exc()
             raise
-        
+
         return report_content
-        
+
     except Exception as e:
         print(f"[ORCHESTRATOR] ‚ùå Error in prepare_meeting_report: {e}")
         import traceback
+
         traceback.print_exc()
         return ""
-

--- app/meeting_prep/core/utils.py
+++ app/meeting_prep/core/utils.py
@@ -4,19 +4,18 @@
 
 def extract_text_from_markdown_code_blocks(text: str) -> str:
     content = text.strip()
-    
+
     if content.startswith("```json"):
         content = content[7:]
     elif content.startswith("```"):
         content = content[3:]
-    
+
     if content.endswith("```"):
         content = content[:-3]
-    
+
     return content.strip()
 
 
 def parse_llm_json_response(content: str) -> Dict[str, Any]:
     cleaned_content = extract_text_from_markdown_code_blocks(content)
     return json.loads(cleaned_content)
-

--- app/meeting_prep/email/collector.py
+++ app/meeting_prep/email/collector.py
@@ -11,76 +11,71 @@
 async def collect_emails(user_id: str, integration_type: str) -> List[Dict[str, Any]]:
     now = datetime.now(timezone.utc)
     email_start = now - timedelta(hours=24)
-    
+
     emails = []
-    
+
     try:
-        if integration_type == 'google':
+        if integration_type == "google":
             threads = await list_gmail_threads(user_id, max_results=50)
-            
+
             # Helper function to fetch emails from a single thread
             async def fetch_thread_emails(thread: Dict[str, Any]) -> List[Dict[str, Any]]:
                 """Fetch and normalize emails from a single Gmail thread."""
                 thread_emails = []
                 try:
-                    thread_id = thread.get('id')
+                    thread_id = thread.get("id")
                     if not thread_id:
                         return thread_emails
-                    
+
                     thread_data = await get_gmail_thread(user_id, thread_id)
-                    messages = thread_data.get('messages', [])
+                    messages = thread_data.get("messages", [])
                     for msg in messages:
-                        email_data = normalize_email_message(msg, 'google', thread_id)
+                        email_data = normalize_email_message(msg, "google", thread_id)
                         thread_emails.append(email_data)
                 except Exception:
                     # Silently fail individual threads to not break entire collection
                     pass
                 return thread_emails
-            
+
             # Parallelize fetching up to 20 threads
-            thread_tasks = [
-                fetch_thread_emails(thread)
-                for thread in threads[:20]
-                if thread.get('id')
-            ]
-            
+            thread_tasks = [fetch_thread_emails(thread) for thread in threads[:20] if thread.get("id")]
+
             # Execute all thread fetches in parallel
             thread_results = await asyncio.gather(*thread_tasks, return_exceptions=True)
-            
+
             # Flatten results
             for result in thread_results:
                 if isinstance(result, list):
                     emails.extend(result)
                 # If result is Exception, silently skip (already handled in fetch_thread_emails)
-            
-        elif integration_type == 'outlook':
+
+        elif integration_type == "outlook":
             messages = await list_outlook_messages(user_id, top=50)
-            
+
             for msg in messages:
-                received = msg.get('receivedDateTime')
+                received = msg.get("receivedDateTime")
                 if received:
                     try:
                         date_str = received
-                        if date_str.endswith('Z'):
-                            date_str = date_str[:-1] + '+00:00'
-                        if date_str.endswith('+00:00+00:00'):
-                            date_str = date_str.replace('+00:00+00:00', '+00:00')
-                        if '+' not in date_str and '-' not in date_str[-6:]:
-                            date_str = date_str + '+00:00'
-                        
+                        if date_str.endswith("Z"):
+                            date_str = date_str[:-1] + "+00:00"
+                        if date_str.endswith("+00:00+00:00"):
+                            date_str = date_str.replace("+00:00+00:00", "+00:00")
+                        if "+" not in date_str and "-" not in date_str[-6:]:
+                            date_str = date_str + "+00:00"
+
                         msg_time = datetime.fromisoformat(date_str)
                         if msg_time >= email_start:
-                            email_data = normalize_email_message(msg, 'outlook')
+                            email_data = normalize_email_message(msg, "outlook")
                             emails.append(email_data)
                     except Exception:
                         continue
         else:
             raise DataCollectionError(f"Unknown integration type: {integration_type}")
-        
+
         return emails
-        
+
     except DataCollectionError:
         raise
     except Exception as e:
         raise DataCollectionError(f"Failed to collect emails for user {user_id}: {str(e)}") from e
-

--- app/meeting_prep/email/matcher.py
+++ app/meeting_prep/email/matcher.py
@@ -12,78 +12,72 @@
 
 def _simple_email_matching(event: Dict[str, Any], emails: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
     related = []
-    event_title = (event.get('title') or event.get('summary') or event.get('subject') or '').lower()
+    event_title = (event.get("title") or event.get("summary") or event.get("subject") or "").lower()
     event_participants = []
-    
-    attendees = event.get('attendees', []) or event.get('participants', [])
+
+    attendees = event.get("attendees", []) or event.get("participants", [])
     for attendee in attendees:
         email = None
         if isinstance(attendee, dict):
-            email = attendee.get('email') or attendee.get('emailAddress', {}).get('address', '')
+            email = attendee.get("email") or attendee.get("emailAddress", {}).get("address", "")
         if email:
             event_participants.append(email.lower())
-    
-    if event.get('participants'):
-        for participant in event.get('participants', []):
+
+    if event.get("participants"):
+        for participant in event.get("participants", []):
             if isinstance(participant, dict):
-                email = participant.get('email', '')
+                email = participant.get("email", "")
                 if email:
                     event_participants.append(email.lower())
-    
+
     event_participants = list(set(event_participants))
-    
+
     def extract_email(text):
         if not text:
-            return ''
+            return ""
         if isinstance(text, list):
-            text = ' '.join(str(t) for t in text)
+            text = " ".join(str(t) for t in text)
         text = str(text).lower()
-        match = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', text)
+        match = re.search(r"[\w\.-]+@[\w\.-]+\.\w+", text)
         if match:
             return match.group(0)
         return text
-    
+
     for email in emails:
-        email_from = email.get('from', '')
-        email_to = email.get('to', '')
-        
+        email_from = email.get("from", "")
+        email_to = email.get("to", "")
+
         email_from_addr = extract_email(email_from)
         email_to_addr = extract_email(email_to)
-        email_subject = (email.get('subject') or '').lower()
-        
-        participant_match = any(
-            participant in email_from_addr or participant in email_to_addr
-            for participant in event_participants
-        )
+        email_subject = (email.get("subject") or "").lower()
+
+        participant_match = any(participant in email_from_addr or participant in email_to_addr for participant in event_participants)
         if participant_match:
             related.append(email)
             continue
-        
+
         if event_title:
             title_words = [w for w in event_title.split() if len(w) > 3]
             subject_match = title_words and any(word in email_subject for word in title_words[:5])
             if subject_match:
                 related.append(email)
                 continue
-    
+
     return related
 
 
-async def find_related_emails_for_event(
-    event: Dict[str, Any],
-    emails: List[Dict[str, Any]]
-) -> List[Dict[str, Any]]:
+async def find_related_emails_for_event(event: Dict[str, Any], emails: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
     if not emails:
         return []
-    
+
     api_key = get_openai_api_key()
     if not api_key:
         return _simple_email_matching(event, emails)
-    
+
     try:
         config = get_llm_config()
         prompt = get_email_matching_prompt(event, emails)
-        
+
         chat = ChatOpenAI(
             model=config.openai_model,
             temperature=config.openai_temperature,
@@ -91,28 +85,27 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", prompt)])
         content = response.content.strip()
-        
+
         result = parse_llm_json_response(content)
-        related_email_ids = result.get('related_email_ids', [])
-        
+        related_email_ids = result.get("related_email_ids", [])
+
         related_emails = []
         for email in emails:
-            email_id = email.get('id') or email.get('msg_id')
-            thread_id = email.get('thread_id') or email.get('threadId')
-            
+            email_id = email.get("id") or email.get("msg_id")
+            thread_id = email.get("thread_id") or email.get("threadId")
+
             if email_id in related_email_ids or thread_id in related_email_ids:
                 related_emails.append(email)
-        
+
         if not related_emails:
             related_emails = _simple_email_matching(event, emails)
-        
+
         return related_emails
-        
+
     except Exception as e:
         if isinstance(e, (LLMAPIError, DataCollectionError)):
             raise
         return _simple_email_matching(event, emails)
-

--- app/meeting_prep/email/normalizer.py
+++ app/meeting_prep/email/normalizer.py
@@ -3,97 +3,92 @@
 
 
 def normalize_gmail_message(msg: Dict[str, Any], thread_id: str) -> Dict[str, Any]:
-    payload = msg.get('payload', {})
-    headers = payload.get('headers', [])
-    
+    payload = msg.get("payload", {})
+    headers = payload.get("headers", [])
+
     email_data = {
-        'id': msg.get('id'),
-        'thread_id': thread_id,
-        'snippet': msg.get('snippet', ''),
-        'subject': '',
-        'from': '',
-        'to': '',
-        'date': '',
+        "id": msg.get("id"),
+        "thread_id": thread_id,
+        "snippet": msg.get("snippet", ""),
+        "subject": "",
+        "from": "",
+        "to": "",
+        "date": "",
     }
-    
+
     for header in headers:
-        name = header.get('name', '').lower()
-        value = header.get('value', '')
-        if name == 'subject':
-            email_data['subject'] = value
-        elif name == 'from':
-            email_data['from'] = value
-        elif name == 'to':
-            email_data['to'] = value
-        elif name == 'date':
-            email_data['date'] = value
-    
+        name = header.get("name", "").lower()
+        value = header.get("value", "")
+        if name == "subject":
+            email_data["subject"] = value
+        elif name == "from":
+            email_data["from"] = value
+        elif name == "to":
+            email_data["to"] = value
+        elif name == "date":
+            email_data["date"] = value
+
     return email_data
 
 
 def normalize_outlook_message(msg: Dict[str, Any]) -> Dict[str, Any]:
-    received = msg.get('receivedDateTime')
+    received = msg.get("receivedDateTime")
     if not received:
         return {
-            'id': msg.get('id'),
-            'subject': msg.get('subject', ''),
-            'from': '',
-            'to': '',
-            'body_preview': msg.get('bodyPreview', ''),
-            'date': '',
+            "id": msg.get("id"),
+            "subject": msg.get("subject", ""),
+            "from": "",
+            "to": "",
+            "body_preview": msg.get("bodyPreview", ""),
+            "date": "",
         }
-    
+
     try:
         date_str = received
-        if date_str.endswith('Z'):
-            date_str = date_str[:-1] + '+00:00'
-        if date_str.endswith('+00:00+00:00'):
-            date_str = date_str.replace('+00:00+00:00', '+00:00')
-        if '+' not in date_str and '-' not in date_str[-6:]:
-            date_str = date_str + '+00:00'
-        
+        if date_str.endswith("Z"):
+            date_str = date_str[:-1] + "+00:00"
+        if date_str.endswith("+00:00+00:00"):
+            date_str = date_str.replace("+00:00+00:00", "+00:00")
+        if "+" not in date_str and "-" not in date_str[-6:]:
+            date_str = date_str + "+00:00"
+
         datetime.fromisoformat(date_str)
     except Exception:
         date_str = received
-    
-    from_addr = msg.get('from', {})
-    from_email = from_addr.get('emailAddress', {}).get('address', '') if isinstance(from_addr, dict) else ''
-    from_name = from_addr.get('emailAddress', {}).get('name', '') if isinstance(from_addr, dict) else ''
-    
+
+    from_addr = msg.get("from", {})
+    from_email = from_addr.get("emailAddress", {}).get("address", "") if isinstance(from_addr, dict) else ""
+    from_name = from_addr.get("emailAddress", {}).get("name", "") if isinstance(from_addr, dict) else ""
+
     from_str = f"{from_name} <{from_email}>" if from_name and from_email else from_email
-    
-    to_recipients = msg.get('toRecipients', [])
+
+    to_recipients = msg.get("toRecipients", [])
     to_list = []
     for addr in to_recipients:
         if isinstance(addr, dict):
-            email_addr = addr.get('emailAddress', {})
+            email_addr = addr.get("emailAddress", {})
             if isinstance(email_addr, dict):
-                email = email_addr.get('address', '')
-                name = email_addr.get('name', '')
+                email = email_addr.get("address", "")
+                name = email_addr.get("name", "")
                 if email:
                     to_list.append(f"{name} <{email}>" if name else email)
-    
+
     return {
-        'id': msg.get('id'),
-        'subject': msg.get('subject', ''),
-        'from': from_str,
-        'to': to_list if to_list else from_email,
-        'body_preview': msg.get('bodyPreview', ''),
-        'date': received,
+        "id": msg.get("id"),
+        "subject": msg.get("subject", ""),
+        "from": from_str,
+        "to": to_list if to_list else from_email,
+        "body_preview": msg.get("bodyPreview", ""),
+        "date": received,
     }
 
 
-def normalize_email_message(
-    msg: Dict[str, Any],
-    integration_type: str,
-    thread_id: Optional[str] = None
-) -> Dict[str, Any]:
-    if integration_type == 'google':
+def normalize_email_message(msg: Dict[str, Any], integration_type: str, thread_id: Optional[str] = None) -> Dict[str, Any]:
+    if integration_type == "google":
         if not thread_id:
-            thread_id = msg.get('threadId', '')
+            thread_id = msg.get("threadId", "")
         return normalize_gmail_message(msg, thread_id)
-    elif integration_type == 'outlook':
+    elif integration_type == "outlook":
         return normalize_outlook_message(msg)
     else:
         return msg
-

--- app/meeting_prep/generation/conversation_starters.py
+++ app/meeting_prep/generation/conversation_starters.py
@@ -10,21 +10,18 @@
 from app.meeting_prep.prompts.prompts import get_conversation_starters_prompt
 
 
-async def generate_conversation_starters(
-    all_data: Dict[str, Any],
-    user_profile: Optional[Dict[str, Any]] = None
-) -> List[ConversationStarterModel]:
+async def generate_conversation_starters(all_data: Dict[str, Any], user_profile: Optional[Dict[str, Any]] = None) -> List[ConversationStarterModel]:
     api_key = get_openai_api_key()
     if not api_key:
         raise LLMAPIError("OpenAI API key not configured")
-    
+
     try:
         config = get_llm_config()
         prompt = get_conversation_starters_prompt(
             all_data=all_data,
             user_profile=user_profile,
         )
-        
+
         chat = ChatOpenAI(
             model=config.openai_model,
             temperature=config.conversation_starters_temperature,
@@ -32,19 +29,16 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", prompt)])
         content = response.content.strip()
-        
+
         starters_data = parse_llm_json_response(content)
-        if 'timely_topics' in starters_data:
-            return [
-                ConversationStarterModel(**topic) for topic in starters_data['timely_topics']
-            ]
+        if "timely_topics" in starters_data:
+            return [ConversationStarterModel(**topic) for topic in starters_data["timely_topics"]]
         return []
-        
+
     except Exception as e:
         if isinstance(e, (LLMAPIError, ReportGenerationError)):
             raise
         raise ReportGenerationError(f"Failed to generate conversation starters: {str(e)}") from e
-

--- app/meeting_prep/generation/executive_summary.py
+++ app/meeting_prep/generation/executive_summary.py
@@ -11,15 +11,12 @@
 
 
 async def generate_executive_summary(
-    email_thread: List[Dict[str, Any]],
-    meeting_context: Dict[str, Any],
-    firm_intel: Optional[Dict[str, Any]],
-    person_intel: Optional[Dict[str, Any]]
+    email_thread: List[Dict[str, Any]], meeting_context: Dict[str, Any], firm_intel: Optional[Dict[str, Any]], person_intel: Optional[Dict[str, Any]]
 ) -> Optional[ExecutiveSummaryModel]:
     api_key = get_openai_api_key()
     if not api_key:
         raise LLMAPIError("OpenAI API key not configured")
-    
+
     try:
         config = get_llm_config()
         prompt = get_executive_summary_prompt(
@@ -28,7 +25,7 @@
             firm_intel=firm_intel,
             person_intel=person_intel,
         )
-        
+
         chat = ChatOpenAI(
             model=config.openai_model,
             temperature=config.executive_summary_temperature,
@@ -36,15 +33,14 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", prompt)])
         content = response.content.strip()
-        
+
         summary_data = parse_llm_json_response(content)
         return ExecutiveSummaryModel(**summary_data)
-        
+
     except Exception as e:
         if isinstance(e, (LLMAPIError, ReportGenerationError)):
             raise
         raise ReportGenerationError(f"Failed to generate executive summary: {str(e)}") from e
-

--- app/meeting_prep/generation/objectives.py
+++ app/meeting_prep/generation/objectives.py
@@ -10,17 +10,15 @@
 from app.meeting_prep.prompts.prompts import get_meeting_objectives_prompt
 
 
-async def generate_meeting_objectives(
-    all_data: Dict[str, Any]
-) -> Optional[MeetingObjectivesModel]:
+async def generate_meeting_objectives(all_data: Dict[str, Any]) -> Optional[MeetingObjectivesModel]:
     api_key = get_openai_api_key()
     if not api_key:
         raise LLMAPIError("OpenAI API key not configured")
-    
+
     try:
         config = get_llm_config()
         prompt = get_meeting_objectives_prompt(all_data=all_data)
-        
+
         chat = ChatOpenAI(
             model=config.openai_model,
             temperature=config.meeting_objectives_temperature,
@@ -28,15 +26,14 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", prompt)])
         content = response.content.strip()
-        
+
         objectives_data = parse_llm_json_response(content)
         return MeetingObjectivesModel(**objectives_data)
-        
+
     except Exception as e:
         if isinstance(e, (LLMAPIError, ReportGenerationError)):
             raise
         raise ReportGenerationError(f"Failed to generate meeting objectives: {str(e)}") from e
-

--- app/meeting_prep/generation/report_generator.py
+++ app/meeting_prep/generation/report_generator.py
@@ -33,37 +33,38 @@
     api_key = get_openai_api_key()
     if not api_key:
         raise LLMAPIError("OpenAI API key not configured")
-    
+
     try:
         config = get_llm_config()
-        
+
         metadata_dict = meeting_metadata.model_dump() if meeting_metadata else None
         exec_summary_dict = executive_summary.model_dump() if executive_summary else None
         relationship_dict = relationship_history.model_dump() if relationship_history else None
         firm_intel_dict = firm_intelligence.model_dump() if firm_intelligence else None
 
         if person_intelligence:
-            person_intel_dict = person_intelligence.model_dump(exclude_none=False, mode='python')
-            
-            if hasattr(person_intelligence, '__pydantic_extra__') and person_intelligence.__pydantic_extra__:
+            person_intel_dict = person_intelligence.model_dump(exclude_none=False, mode="python")
+
+            if hasattr(person_intelligence, "__pydantic_extra__") and person_intelligence.__pydantic_extra__:
                 person_intel_dict.update(person_intelligence.__pydantic_extra__)
-            
+
             if raw_person_intel_data:
-                for key in ['employment_history', 'education', 'skills', 'location', 'key_strengths', 'meeting_relevance']:
+                for key in ["employment_history", "education", "skills", "location", "key_strengths", "meeting_relevance"]:
                     if key in raw_person_intel_data:
                         person_intel_dict[key] = raw_person_intel_data[key]
-            
+
             import logging
+
             logger = logging.getLogger(__name__)
             logger.info(
                 "Person intelligence data for report generation",
                 extra={
-                    "has_employment_history": bool(person_intel_dict.get('employment_history')),
-                    "has_education": bool(person_intel_dict.get('education')),
-                    "has_skills": bool(person_intel_dict.get('skills')),
-                    "has_key_strengths": bool(person_intel_dict.get('key_strengths')),
-                    "all_keys": list(person_intel_dict.keys())
-                }
+                    "has_employment_history": bool(person_intel_dict.get("employment_history")),
+                    "has_education": bool(person_intel_dict.get("education")),
+                    "has_skills": bool(person_intel_dict.get("skills")),
+                    "has_key_strengths": bool(person_intel_dict.get("key_strengths")),
+                    "all_keys": list(person_intel_dict.keys()),
+                },
             )
         elif raw_person_intel_data:
             person_intel_dict = raw_person_intel_data
@@ -72,7 +73,7 @@
         starters_dict = [cs.model_dump() for cs in conversation_starters] if conversation_starters else None
         objectives_dict = meeting_objectives.model_dump() if meeting_objectives else None
         communication_dict = communication_style.model_dump() if communication_style else None
-        
+
         prompt = get_report_generation_prompt(
             meeting_metadata=metadata_dict,
             executive_summary=exec_summary_dict,
@@ -84,7 +85,7 @@
             communication_style=communication_dict,
             related_emails=related_emails,
         )
-        
+
         chat = ChatOpenAI(
             model=config.openai_model,
             temperature=config.report_generation_temperature,
@@ -92,17 +93,16 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", prompt)])
         report = response.content.strip()
-        
+
         if len(report) == 0:
             logger.warning("Report generation returned empty content")
-        
+
         return report
-        
+
     except Exception as e:
         if isinstance(e, (LLMAPIError, ReportGenerationError)):
             raise
         raise ReportGenerationError(f"Failed to generate report: {str(e)}") from e
-

--- app/meeting_prep/intelligence/collector.py
+++ app/meeting_prep/intelligence/collector.py
@@ -33,86 +33,76 @@
 ) -> Dict[str, Any]:
     """
     Query CoreSignal for person intelligence and extract relevant information.
-    
+
     Args:
         person_name: Person's full name
         email: Person's email address
         company_name: Optional company name for filtering
         meeting_context: Context about the meeting
-        
+
     Returns:
         Dictionary with extracted person intelligence data, or empty dict on error
     """
     try:
-        print(f"\n{'='*60}")
+        print(f"\n{'=' * 60}")
         print(f"üîç CORESIGNAL: Starting person intelligence query")
         print(f"   Person: {person_name}")
         print(f"   Email: {email}")
         print(f"   Company: {company_name}")
-        print(f"{'='*60}\n")
-        
+        print(f"{'=' * 60}\n")
+
         logger.info(
             "CoreSignal: Starting person intelligence query",
-            extra={
-                "person_name": person_name,
-                "email": email,
-                "company_name": company_name,
-                "meeting_context": meeting_context
-            }
+            extra={"person_name": person_name, "email": email, "company_name": company_name, "meeting_context": meeting_context},
         )
-        
+
         company_abbreviation = None
         if company_name:
             from app.meeting_prep.intelligence.firm_extractor import extract_company_abbreviation
+
             company_abbreviation = extract_company_abbreviation(company_name)
-        
+
         candidates = await search_person_by_name_and_email(
-            name=person_name,
-            email=email,
-            company_name=company_name,
-            company_abbreviation=company_abbreviation
+            name=person_name, email=email, company_name=company_name, company_abbreviation=company_abbreviation
         )
-        
+
         if not candidates:
             print(f"   ‚ö†Ô∏è No CoreSignal candidates found")
-            logger.info(
-                f"No CoreSignal candidates found for {person_name} ({email})",
-                extra={"person_name": person_name, "email": email}
-            )
+            logger.info(f"No CoreSignal candidates found for {person_name} ({email})", extra={"person_name": person_name, "email": email})
             return {}
-        
+
         selected_parent_id = None
         selected_candidate = None
         confidence = 0
-        reasoning = ''
-        
+        reasoning = ""
+
         if len(candidates) == 1:
             candidate = candidates[0]
-            candidate_name = (candidate.get('full_name') or '').lower()
-            candidate_email_raw = candidate.get('email')
-            candidate_email = (candidate_email_raw or '').lower() if candidate_email_raw else ''
-            candidate_company = (candidate.get('company_name') or '').lower()
+            candidate_name = (candidate.get("full_name") or "").lower()
+            candidate_email_raw = candidate.get("email")
+            candidate_email = (candidate_email_raw or "").lower() if candidate_email_raw else ""
+            candidate_company = (candidate.get("company_name") or "").lower()
             target_name = person_name.lower()
             target_email = email.lower()
-            target_company = (company_name or '').lower()
-            target_company_abbr = (company_abbreviation or '').lower()
-            
+            target_company = (company_name or "").lower()
+            target_company_abbr = (company_abbreviation or "").lower()
+
             name_exact_match = candidate_name == target_name
             email_exact_match = candidate_email == target_email if candidate_email else False
-            
+
             company_match = False
             if target_company and candidate_company:
                 company_match = (
-                    candidate_company == target_company or
-                    candidate_company == target_company_abbr or
-                    target_company in candidate_company or
-                    candidate_company in target_company
+                    candidate_company == target_company
+                    or candidate_company == target_company_abbr
+                    or target_company in candidate_company
+                    or candidate_company in target_company
                 )
-            
+
             should_auto_select = False
             auto_confidence = 0
             auto_reasoning = ""
-            
+
             if name_exact_match:
                 if email_exact_match:
                     should_auto_select = True
@@ -126,16 +116,16 @@
                     should_auto_select = True
                     auto_confidence = 8
                     auto_reasoning = "Auto-selected single candidate: name exact match (no company to verify)"
-            
+
             if should_auto_select:
-                selected_parent_id = candidate.get('parent_id')
+                selected_parent_id = candidate.get("parent_id")
                 selected_candidate = candidate
                 confidence = auto_confidence
                 reasoning = auto_reasoning
-                
+
                 print(f"   ‚úÖ AUTO-SELECTED candidate: {person_name} (confidence: {confidence})")
                 print(f"      Reasoning: {reasoning}")
-                
+
                 logger.info(
                     f"Auto-selected CoreSignal candidate for {person_name} (confidence: {confidence})",
                     extra={
@@ -143,15 +133,15 @@
                         "selected_parent_id": selected_parent_id,
                         "confidence": confidence,
                         "reasoning": reasoning,
-                        "has_full_source": '_full_source' in candidate
-                    }
+                        "has_full_source": "_full_source" in candidate,
+                    },
                 )
-        
+
         api_key = get_openai_api_key()
         if not api_key:
             logger.warning("OpenAI API key not configured, cannot process CoreSignal profiles")
             return {}
-        
+
         config = get_llm_config()
         chat = ChatOpenAI(
             model=config.openai_model,
@@ -160,79 +150,53 @@
             api_key=api_key,
             timeout=config.openai_timeout,
         )
-        
+
         if not selected_parent_id:
             print(f"   üîÑ Found {len(candidates)} candidates, using GPT matching...")
             logger.info(
                 f"Found {len(candidates)} CoreSignal candidates, proceeding to GPT matching",
-                extra={
-                    "person_name": person_name,
-                    "candidates_count": len(candidates)
-                }
+                extra={"person_name": person_name, "candidates_count": len(candidates)},
             )
-            
+
             matching_prompt = get_coresignal_profile_matching_prompt(
                 person_name=person_name,
                 person_email=email,
                 candidates=candidates,
                 meeting_context=meeting_context,
-                company_abbreviation=company_abbreviation
+                company_abbreviation=company_abbreviation,
             )
-            
+
             logger.info(
                 "CoreSignal: Sending matching prompt to GPT",
-                extra={
-                    "person_name": person_name,
-                    "candidates_count": len(candidates),
-                    "prompt_length": len(matching_prompt)
-                }
+                extra={"person_name": person_name, "candidates_count": len(candidates), "prompt_length": len(matching_prompt)},
             )
-            
+
             response = await chat.ainvoke([("user", matching_prompt)])
             content = response.content.strip()
-            
-            logger.info(
-                "CoreSignal: Received GPT matching response",
-                extra={
-                    "person_name": person_name,
-                    "response_length": len(content)
-                }
-            )
-            
+
+            logger.info("CoreSignal: Received GPT matching response", extra={"person_name": person_name, "response_length": len(content)})
+
             matching_result = parse_llm_json_response(content)
-            selected_parent_id = matching_result.get('parent_id')
-            confidence = matching_result.get('confidence_score', 0)
-            reasoning = matching_result.get('reasoning', '')
-            
+            selected_parent_id = matching_result.get("parent_id")
+            confidence = matching_result.get("confidence_score", 0)
+            reasoning = matching_result.get("reasoning", "")
+
             logger.info(
                 "CoreSignal: GPT matching result",
-                extra={
-                    "person_name": person_name,
-                    "selected_parent_id": selected_parent_id,
-                    "confidence": confidence,
-                    "reasoning": reasoning
-                }
+                extra={"person_name": person_name, "selected_parent_id": selected_parent_id, "confidence": confidence, "reasoning": reasoning},
             )
-            
-            has_name_match = any(
-                cand.get('full_name', '').lower() == person_name.lower() 
-                for cand in candidates
-            )
-            
+
+            has_name_match = any(cand.get("full_name", "").lower() == person_name.lower() for cand in candidates)
+
             min_confidence = 5 if has_name_match else 6
-            
+
             if not selected_parent_id:
                 logger.info(
                     f"No CoreSignal candidate selected for {person_name}",
-                    extra={
-                        "person_name": person_name,
-                        "confidence": confidence,
-                        "reasoning": reasoning,
-                        "has_name_match": has_name_match
-                    }
+                    extra={"person_name": person_name, "confidence": confidence, "reasoning": reasoning, "has_name_match": has_name_match},
                 )
                 return {}
-            
+
             if confidence < min_confidence:
                 if has_name_match and confidence >= 5:
                     logger.info(
@@ -241,38 +205,31 @@
                             "person_name": person_name,
                             "confidence": confidence,
                             "reasoning": reasoning,
-                            "selected_parent_id": selected_parent_id
-                        }
+                            "selected_parent_id": selected_parent_id,
+                        },
                     )
                 else:
                     logger.info(
                         f"No high-confidence CoreSignal match for {person_name} (confidence: {confidence})",
-                        extra={
-                            "person_name": person_name,
-                            "confidence": confidence,
-                            "reasoning": reasoning
-                        }
+                        extra={"person_name": person_name, "confidence": confidence, "reasoning": reasoning},
                     )
                     return {}
-            
-            selected_candidate = next(
-                (cand for cand in candidates if cand.get('parent_id') == selected_parent_id),
-                None
-            )
-        
+
+            selected_candidate = next((cand for cand in candidates if cand.get("parent_id") == selected_parent_id), None)
+
         if not selected_parent_id:
             logger.warning("No candidate selected, cannot proceed with extraction")
             return {}
-        
+
         profile_data = None
         use_full_source = False
-        
-        if selected_candidate and '_full_source' in selected_candidate:
-            full_source = selected_candidate['_full_source']
-            has_experience = bool(full_source.get('experience')) if isinstance(full_source, dict) else False
-            has_education = bool(full_source.get('education')) if isinstance(full_source, dict) else False
-            has_skills = bool(full_source.get('skills')) if isinstance(full_source, dict) else False
-            
+
+        if selected_candidate and "_full_source" in selected_candidate:
+            full_source = selected_candidate["_full_source"]
+            has_experience = bool(full_source.get("experience")) if isinstance(full_source, dict) else False
+            has_education = bool(full_source.get("education")) if isinstance(full_source, dict) else False
+            has_skills = bool(full_source.get("skills")) if isinstance(full_source, dict) else False
+
             logger.info(
                 f"Checking full source data from search result for parent_id: {selected_parent_id}",
                 extra={
@@ -280,76 +237,59 @@
                     "has_experience": has_experience,
                     "has_education": has_education,
                     "has_skills": has_skills,
-                    "source_keys": list(full_source.keys())[:15] if isinstance(full_source, dict) else []
-                }
+                    "source_keys": list(full_source.keys())[:15] if isinstance(full_source, dict) else [],
+                },
             )
-            
+
             if has_experience or has_education or has_skills:
                 profile_data = full_source
                 use_full_source = True
                 logger.info(f"Using full source data (has structured data)")
             else:
                 logger.info(f"Full source lacks structured data, will fetch from /collect endpoint")
-        
+
         if not profile_data:
             logger.info(
                 f"Fetching detailed profile for parent_id: {selected_parent_id}",
-                extra={"parent_id": selected_parent_id, "use_full_source_attempted": use_full_source}
+                extra={"parent_id": selected_parent_id, "use_full_source_attempted": use_full_source},
             )
             profile_data = await get_person_details(selected_parent_id)
             if not profile_data:
                 logger.warning(
-                    f"Could not fetch CoreSignal profile details for parent_id: {selected_parent_id}",
-                    extra={"parent_id": selected_parent_id}
+                    f"Could not fetch CoreSignal profile details for parent_id: {selected_parent_id}", extra={"parent_id": selected_parent_id}
                 )
                 return {}
-        
+
         if not profile_data or not isinstance(profile_data, dict):
-            logger.error(
-                f"Invalid profile_data after retrieval: {type(profile_data)}",
-                extra={"parent_id": selected_parent_id}
-            )
+            logger.error(f"Invalid profile_data after retrieval: {type(profile_data)}", extra={"parent_id": selected_parent_id})
             return {}
-        
+
         logger.info(
             f"Profile data prepared for extraction",
             extra={
                 "parent_id": selected_parent_id,
                 "source": "full_source" if use_full_source else "collect_endpoint",
-                "has_experience": bool(profile_data.get('experience')),
-                "has_education": bool(profile_data.get('education')),
-                "has_skills": bool(profile_data.get('skills')),
-                "profile_keys": list(profile_data.keys())[:20]
-            }
+                "has_experience": bool(profile_data.get("experience")),
+                "has_education": bool(profile_data.get("education")),
+                "has_skills": bool(profile_data.get("skills")),
+                "profile_keys": list(profile_data.keys())[:20],
+            },
         )
-        
+
         extraction_prompt = get_coresignal_data_extraction_prompt(
-            person_name=person_name,
-            person_email=email,
-            coresignal_profile=profile_data,
-            meeting_context=meeting_context
+            person_name=person_name, person_email=email, coresignal_profile=profile_data, meeting_context=meeting_context
         )
-        
+
         logger.info(
             "CoreSignal: Sending extraction prompt to GPT",
-            extra={
-                "person_name": person_name,
-                "parent_id": selected_parent_id,
-                "prompt_length": len(extraction_prompt)
-            }
+            extra={"person_name": person_name, "parent_id": selected_parent_id, "prompt_length": len(extraction_prompt)},
         )
-        
+
         extraction_response = await chat.ainvoke([("user", extraction_prompt)])
         extraction_content = extraction_response.content.strip()
-        
-        logger.info(
-            "CoreSignal: Received GPT extraction response",
-            extra={
-                "person_name": person_name,
-                "response_length": len(extraction_content)
-            }
-        )
-        
+
+        logger.info("CoreSignal: Received GPT extraction response", extra={"person_name": person_name, "response_length": len(extraction_content)})
+
         try:
             extracted_data = parse_llm_json_response(extraction_content)
         except Exception as parse_error:
@@ -358,12 +298,12 @@
                 extra={
                     "person_name": person_name,
                     "parent_id": selected_parent_id,
-                    "response_preview": extraction_content[:500] if extraction_content else "empty"
+                    "response_preview": extraction_content[:500] if extraction_content else "empty",
                 },
-                exc_info=True
+                exc_info=True,
             )
             return {}
-        
+
         if not extracted_data or not isinstance(extracted_data, dict):
             logger.error(
                 f"Extraction returned invalid data: {type(extracted_data)}",
@@ -371,22 +311,22 @@
                     "person_name": person_name,
                     "parent_id": selected_parent_id,
                     "extracted_data_type": type(extracted_data).__name__,
-                    "extracted_data_preview": str(extracted_data)[:200] if extracted_data else "None"
-                }
+                    "extracted_data_preview": str(extracted_data)[:200] if extracted_data else "None",
+                },
             )
             return {}
-        
-        has_emp = bool(extracted_data.get('employment_history'))
-        has_edu = bool(extracted_data.get('education'))
-        has_skills = bool(extracted_data.get('skills'))
-        
+
+        has_emp = bool(extracted_data.get("employment_history"))
+        has_edu = bool(extracted_data.get("education"))
+        has_skills = bool(extracted_data.get("skills"))
+
         print(f"\n   ‚úÖ Successfully extracted CoreSignal data for {person_name}")
         print(f"      Has employment_history: {has_emp}")
         print(f"      Has education: {has_edu}")
         print(f"      Has skills: {has_skills}")
         print(f"      Extracted fields: {list(extracted_data.keys())}")
-        print(f"{'='*60}\n")
-        
+        print(f"{'=' * 60}\n")
+
         logger.info(
             f"Successfully extracted CoreSignal data for {person_name}",
             extra={
@@ -395,165 +335,152 @@
                 "has_employment_history": has_emp,
                 "has_education": has_edu,
                 "has_skills": has_skills,
-                "has_key_strengths": bool(extracted_data.get('key_strengths')),
-                "has_meeting_relevance": bool(extracted_data.get('meeting_relevance')),
-                "has_location": bool(extracted_data.get('location')),
+                "has_key_strengths": bool(extracted_data.get("key_strengths")),
+                "has_meeting_relevance": bool(extracted_data.get("meeting_relevance")),
+                "has_location": bool(extracted_data.get("location")),
                 "extracted_fields": list(extracted_data.keys()),
-                "employment_history_type": type(extracted_data.get('employment_history')).__name__,
-                "education_type": type(extracted_data.get('education')).__name__
-            }
+                "employment_history_type": type(extracted_data.get("employment_history")).__name__,
+                "education_type": type(extracted_data.get("education")).__name__,
+            },
         )
-        
+
         return extracted_data
-        
+
     except Exception as e:
         import traceback
+
         error_traceback = traceback.format_exc()
         print(f"\n   ‚ùå ERROR in CoreSignal query: {str(e)}")
         print(f"   Traceback: {error_traceback[:500]}\n")
         logger.error(
             f"CoreSignal person intelligence query failed: {str(e)}",
             exc_info=True,
-            extra={
-                "person_name": person_name,
-                "email": email,
-                "error_type": type(e).__name__,
-                "traceback": error_traceback
-            }
+            extra={"person_name": person_name, "email": email, "error_type": type(e).__name__, "traceback": error_traceback},
         )
         return {}
 
 
-def merge_person_intelligence(
-    perplexity_data: Dict[str, Any],
-    coresignal_data: Dict[str, Any]
-) -> Dict[str, Any]:
+def merge_person_intelligence(perplexity_data: Dict[str, Any], coresignal_data: Dict[str, Any]) -> Dict[str, Any]:
     """
     Merge person intelligence data from Perplexity and CoreSignal.
-    
+
     Prioritizes CoreSignal for structured data (basic_info, employment_history, education, skills)
     and keeps Perplexity data for dynamic content (conferences, recent_news).
-    
+
     Args:
         perplexity_data: Person intelligence from Perplexity
         coresignal_data: Person intelligence from CoreSignal
-        
+
     Returns:
         Merged person intelligence dictionary
     """
     # Start with Perplexity data as base
     merged = perplexity_data.copy() if perplexity_data else {}
-    
+
     # Ensure basic structure
-    if 'basic_info' not in merged:
-        merged['basic_info'] = {}
-    
-    if 'conferences_public_speaking' not in merged:
-        merged['conferences_public_speaking'] = {'speaking_engagements': []}
-    
-    if 'recent_news' not in merged:
-        merged['recent_news'] = {'recent_news_mentions': []}
-    
+    if "basic_info" not in merged:
+        merged["basic_info"] = {}
+
+    if "conferences_public_speaking" not in merged:
+        merged["conferences_public_speaking"] = {"speaking_engagements": []}
+
+    if "recent_news" not in merged:
+        merged["recent_news"] = {"recent_news_mentions": []}
+
     if not coresignal_data:
         return merged
-    
+
     # Merge basic_info - prioritize CoreSignal if it has more complete data
-    if coresignal_data.get('basic_info'):
-        cs_basic = coresignal_data['basic_info']
+    if coresignal_data.get("basic_info"):
+        cs_basic = coresignal_data["basic_info"]
         # Prefer CoreSignal LinkedIn URL if available (usually more accurate)
-        if cs_basic.get('linkedin_url') and not merged['basic_info'].get('linkedin_url'):
-            merged['basic_info']['linkedin_url'] = cs_basic['linkedin_url']
+        if cs_basic.get("linkedin_url") and not merged["basic_info"].get("linkedin_url"):
+            merged["basic_info"]["linkedin_url"] = cs_basic["linkedin_url"]
         # Prefer CoreSignal email if it matches
-        if cs_basic.get('email'):
-            merged['basic_info']['email'] = cs_basic['email']
+        if cs_basic.get("email"):
+            merged["basic_info"]["email"] = cs_basic["email"]
         # Prefer CoreSignal name if available
-        if cs_basic.get('full_name'):
-            merged['basic_info']['full_name'] = cs_basic['full_name']
-    
+        if cs_basic.get("full_name"):
+            merged["basic_info"]["full_name"] = cs_basic["full_name"]
+
     # Add CoreSignal-specific fields (employment_history, education, skills, location)
     # Always set these fields, even if empty, to ensure they're available for the report
-    if 'employment_history' in coresignal_data:
-        merged['employment_history'] = coresignal_data['employment_history']
-    
-    if 'education' in coresignal_data:
-        merged['education'] = coresignal_data['education']
-    
-    if 'skills' in coresignal_data:
-        merged['skills'] = coresignal_data['skills']
-    
-    if 'location' in coresignal_data:
-        merged['location'] = coresignal_data['location']
-    
+    if "employment_history" in coresignal_data:
+        merged["employment_history"] = coresignal_data["employment_history"]
+
+    if "education" in coresignal_data:
+        merged["education"] = coresignal_data["education"]
+
+    if "skills" in coresignal_data:
+        merged["skills"] = coresignal_data["skills"]
+
+    if "location" in coresignal_data:
+        merged["location"] = coresignal_data["location"]
+
     # Add analysis fields
-    if 'key_strengths' in coresignal_data:
-        merged['key_strengths'] = coresignal_data['key_strengths']
-    
-    if 'meeting_relevance' in coresignal_data:
-        merged['meeting_relevance'] = coresignal_data['meeting_relevance']
-    
+    if "key_strengths" in coresignal_data:
+        merged["key_strengths"] = coresignal_data["key_strengths"]
+
+    if "meeting_relevance" in coresignal_data:
+        merged["meeting_relevance"] = coresignal_data["meeting_relevance"]
+
     import logging
+
     merge_logger = logging.getLogger(__name__)
     merge_logger.info(
         f"Merged CoreSignal data into person intelligence",
         extra={
-            "coresignal_employment_history": bool(coresignal_data.get('employment_history')),
-            "coresignal_education": bool(coresignal_data.get('education')),
-            "coresignal_skills": bool(coresignal_data.get('skills')),
-            "merged_employment_history": bool(merged.get('employment_history')),
-            "merged_education": bool(merged.get('education')),
-            "merged_skills": bool(merged.get('skills')),
+            "coresignal_employment_history": bool(coresignal_data.get("employment_history")),
+            "coresignal_education": bool(coresignal_data.get("education")),
+            "coresignal_skills": bool(coresignal_data.get("skills")),
+            "merged_employment_history": bool(merged.get("employment_history")),
+            "merged_education": bool(merged.get("education")),
+            "merged_skills": bool(merged.get("skills")),
             "coresignal_keys": list(coresignal_data.keys()),
-            "merged_keys": list(merged.keys())
-        }
+            "merged_keys": list(merged.keys()),
+        },
     )
-    
+
     # Keep Perplexity data for conferences and news (CoreSignal doesn't have this)
     # These are already in merged from perplexity_data
-    
+
     return merged
 
 
-async def collect_intelligence_data(
-    meeting_data: Dict[str, Any],
-    user_id: str
-) -> Dict[str, Any]:
-    meeting_id = meeting_data.get('meeting_id', '')
-    participants = meeting_data.get('participants', [])
+async def collect_intelligence_data(meeting_data: Dict[str, Any], user_id: str) -> Dict[str, Any]:
+    meeting_id = meeting_data.get("meeting_id", "")
+    participants = meeting_data.get("participants", [])
     meeting_context = {
-        'title': meeting_data.get('title', ''),
-        'other_participants': [p.get('name', '') for p in participants if p.get('name')],
+        "title": meeting_data.get("title", ""),
+        "other_participants": [p.get("name", "") for p in participants if p.get("name")],
     }
-    
+
     primary_firm = None
     primary_person_name = None
     primary_person_email = None
-    
-    external_participants = [p for p in participants if p.get('is_external', False)]
-    
+
+    external_participants = [p for p in participants if p.get("is_external", False)]
+
     if not external_participants and len(participants) > 1:
-        organizer_email = meeting_data.get('organizer_email') or (participants[0].get('email', '') if participants else '')
-        organizer_domain = organizer_email.split('@')[1] if '@' in organizer_email else None
-        
+        organizer_email = meeting_data.get("organizer_email") or (participants[0].get("email", "") if participants else "")
+        organizer_domain = organizer_email.split("@")[1] if "@" in organizer_email else None
+
         for p in participants:
-            email = p.get('email', '')
-            p_domain = email.split('@')[1] if '@' in email else None
+            email = p.get("email", "")
+            p_domain = email.split("@")[1] if "@" in email else None
             if organizer_domain and p_domain and p_domain != organizer_domain:
-                p['is_external'] = True
-        
-        external_participants = [p for p in participants if p.get('is_external', False)]
-    
+                p["is_external"] = True
+
+        external_participants = [p for p in participants if p.get("is_external", False)]
+
     if external_participants:
         primary_person = external_participants[0]
-        primary_person_name = primary_person.get('name', '')
-        primary_person_email = primary_person.get('email', '')
-        
-        meeting_title = meeting_data.get('title', '')
-        primary_firm, primary_firm_abbreviation = extract_firm_name_and_abbreviation(
-            meeting_title,
-            participants,
-            meeting_data.get('organizer_email')
-        )
-    
+        primary_person_name = primary_person.get("name", "")
+        primary_person_email = primary_person.get("email", "")
+
+        meeting_title = meeting_data.get("title", "")
+        primary_firm, primary_firm_abbreviation = extract_firm_name_and_abbreviation(meeting_title, participants, meeting_data.get("organizer_email"))
+
     # Firm intelligence task (Perplexity only)
     firm_task = None
     if primary_firm:
@@ -563,16 +490,16 @@
             meeting_id=meeting_id,
             additional_context=meeting_context,
         )
-    
+
     # Person intelligence tasks (Perplexity and CoreSignal in parallel)
     person_perplexity_task = None
     person_coresignal_task = None
-    
+
     print(f"\nüìã Preparing intelligence tasks:")
     print(f"   Primary person: {primary_person_name}")
     print(f"   Primary email: {primary_person_email}")
     print(f"   Primary firm: {primary_firm}")
-    
+
     if primary_person_name and primary_person_email:
         person_perplexity_task = query_person_intelligence(
             person_name=primary_person_name,
@@ -584,46 +511,40 @@
         # Enhance meeting context with company name for better CoreSignal matching
         enhanced_meeting_context = meeting_context.copy()
         if primary_firm:
-            enhanced_meeting_context['company'] = primary_firm
-        
+            enhanced_meeting_context["company"] = primary_firm
+
         person_coresignal_task = query_coresignal_person_intelligence(
             person_name=primary_person_name,
             email=primary_person_email,
             company_name=primary_firm,
             meeting_context=enhanced_meeting_context,
         )
-    
+
     try:
         # Execute all tasks in parallel
         async def empty_dict():
             return {}
-        
+
         tasks_to_gather = []
         tasks_to_gather.append(firm_task if firm_task else empty_dict())
         tasks_to_gather.append(person_perplexity_task if person_perplexity_task else empty_dict())
         tasks_to_gather.append(person_coresignal_task if person_coresignal_task else empty_dict())
-        
-        results = await asyncio.gather(
-            *tasks_to_gather,
-            return_exceptions=True
-        )
-        
+
+        results = await asyncio.gather(*tasks_to_gather, return_exceptions=True)
+
         firm_intel = results[0] if not isinstance(results[0], Exception) else {}
         person_perplexity_intel = results[1] if not isinstance(results[1], Exception) else {}
-        
+
         if isinstance(results[2], Exception):
             logger.error(
                 f"CoreSignal task raised exception: {results[2]}",
                 exc_info=results[2],
-                extra={
-                    "exception_type": type(results[2]).__name__,
-                    "exception_message": str(results[2])
-                }
+                extra={"exception_type": type(results[2]).__name__, "exception_message": str(results[2])},
             )
             person_coresignal_intel = {}
         else:
             person_coresignal_intel = results[2]
-        
+
         logger.info(
             "Merging person intelligence from Perplexity and CoreSignal",
             extra={
@@ -631,29 +552,25 @@
                 "has_coresignal": bool(person_coresignal_intel),
                 "perplexity_keys": list(person_perplexity_intel.keys()) if person_perplexity_intel else [],
                 "coresignal_keys": list(person_coresignal_intel.keys()) if person_coresignal_intel else [],
-                "coresignal_is_exception": isinstance(results[2], Exception)
-            }
+                "coresignal_is_exception": isinstance(results[2], Exception),
+            },
         )
-        
-        person_intel = merge_person_intelligence(
-            perplexity_data=person_perplexity_intel,
-            coresignal_data=person_coresignal_intel
-        )
-        
+
+        person_intel = merge_person_intelligence(perplexity_data=person_perplexity_intel, coresignal_data=person_coresignal_intel)
+
         logger.info(
             "Merged person intelligence complete",
             extra={
                 "merged_keys": list(person_intel.keys()),
-                "has_employment_history": bool(person_intel.get('employment_history')),
-                "has_education": bool(person_intel.get('education')),
-                "has_skills": bool(person_intel.get('skills'))
-            }
+                "has_employment_history": bool(person_intel.get("employment_history")),
+                "has_education": bool(person_intel.get("education")),
+                "has_skills": bool(person_intel.get("skills")),
+            },
         )
-        
+
         return {
-            'firm_intel': firm_intel,
-            'person_intel': person_intel,
+            "firm_intel": firm_intel,
+            "person_intel": person_intel,
         }
     except Exception as e:
         raise IntelligenceCollectionError(f"Failed to collect intelligence data: {str(e)}") from e
-

--- app/meeting_prep/intelligence/coresignal_client.py
+++ app/meeting_prep/intelligence/coresignal_client.py
@@ -17,85 +17,84 @@
 logger = logging.getLogger(__name__)
 
 
-def _extract_candidate_from_source(parent_id: str, source: Dict[str, Any], score: float = 0, index: int = 0, store_full_source: bool = False) -> Dict[str, Any]:
+def _extract_candidate_from_source(
+    parent_id: str, source: Dict[str, Any], score: float = 0, index: int = 0, store_full_source: bool = False
+) -> Dict[str, Any]:
     """Extract candidate information from CoreSignal source data."""
-    candidate_name = source.get('full_name', '')
-    job_title = source.get('job_title', '')
-    
-    company_name = ''
-    if source.get('experience') and isinstance(source['experience'], list):
-        first_exp = source['experience'][0] if source['experience'] else {}
-        company_name = first_exp.get('company_name', '') if isinstance(first_exp, dict) else ''
-    
+    candidate_name = source.get("full_name", "")
+    job_title = source.get("job_title", "")
+
+    company_name = ""
+    if source.get("experience") and isinstance(source["experience"], list):
+        first_exp = source["experience"][0] if source["experience"] else {}
+        company_name = first_exp.get("company_name", "") if isinstance(first_exp, dict) else ""
+
     email = None
     all_emails = []
-    
-    emails_field = source.get('emails') or source.get('email')
+
+    emails_field = source.get("emails") or source.get("email")
     if emails_field:
         if isinstance(emails_field, list):
             for e in emails_field:
-                if isinstance(e, str) and '@' in e:
+                if isinstance(e, str) and "@" in e:
                     all_emails.append(e)
-        elif isinstance(emails_field, str) and '@' in emails_field:
+        elif isinstance(emails_field, str) and "@" in emails_field:
             all_emails.append(emails_field)
-    
+
     if not all_emails:
-        contact_info = source.get('contact_info') or source.get('contact')
+        contact_info = source.get("contact_info") or source.get("contact")
         if contact_info and isinstance(contact_info, dict):
-            contact_emails = contact_info.get('emails') or contact_info.get('email')
+            contact_emails = contact_info.get("emails") or contact_info.get("email")
             if contact_emails:
                 if isinstance(contact_emails, list):
                     for e in contact_emails:
-                        if isinstance(e, str) and '@' in e:
+                        if isinstance(e, str) and "@" in e:
                             all_emails.append(e)
-                elif isinstance(contact_emails, str) and '@' in contact_emails:
+                elif isinstance(contact_emails, str) and "@" in contact_emails:
                     all_emails.append(contact_emails)
-    
+
     email = all_emails[0] if all_emails else None
-    
+
     linkedin_url = None
-    linkedin_field = source.get('websites_linkedin') or source.get('websites_professional_network')
+    linkedin_field = source.get("websites_linkedin") or source.get("websites_professional_network")
     if linkedin_field:
         if isinstance(linkedin_field, list) and linkedin_field:
             linkedin_url = linkedin_field[0]
         elif isinstance(linkedin_field, str):
             linkedin_url = linkedin_field
-    
+
     candidate = {
-        'parent_id': parent_id,
-        'full_name': candidate_name,
-        'job_title': job_title,
-        'company_name': company_name,
-        'email': email,
-        'linkedin_url': linkedin_url,
-        'score': score,
-        '_source': source
+        "parent_id": parent_id,
+        "full_name": candidate_name,
+        "job_title": job_title,
+        "company_name": company_name,
+        "email": email,
+        "linkedin_url": linkedin_url,
+        "score": score,
+        "_source": source,
     }
-    
+
     if store_full_source:
-        candidate['_full_source'] = source
-    
+        candidate["_full_source"] = source
+
     return candidate
 
 
 async def search_person_by_name_and_email(
-    name: str,
-    email: str,
-    company_name: Optional[str] = None,
-    company_abbreviation: Optional[str] = None
+    name: str, email: str, company_name: Optional[str] = None, company_abbreviation: Optional[str] = None
 ) -> List[Dict[str, Any]]:
     """
     Search for a person in CoreSignal by name and email.
-    
+
     Args:
         name: Person's full name
         email: Person's email address
         company_name: Optional company name to filter results (REQUIRED if provided)
         company_abbreviation: Optional company abbreviation (e.g., "EGPS" for "Economic Group Pension Services")
-        
+
     Returns:
         List of candidate profiles found (each with parent_id, full_name, etc.)
-        
+
     Raises:
         IntelligenceCollectionError: If API call fails
     """
@@ -103,91 +102,43 @@
     if not api_key:
         logger.warning("CORESIG_API_KEY not configured, skipping CoreSignal search")
         return []
-    
+
     search_url = "https://api.coresignal.com/cdapi/v2/employee_clean/search/es_dsl"
-    headers = {
-        "apikey": api_key,
-        "Content-Type": "application/json",
-        "accept": "application/json"
-    }
-    
-    email_domain = email.split('@')[1] if '@' in email else None
-    
+    headers = {"apikey": api_key, "Content-Type": "application/json", "accept": "application/json"}
+
+    email_domain = email.split("@")[1] if "@" in email else None
+
     must_clauses = []
     should_clauses = []
-    
-    must_clauses.append({
-        "query_string": {
-            "query": name,
-            "default_field": "full_name",
-            "default_operator": "and"
-        }
-    })
-    
+
+    must_clauses.append({"query_string": {"query": name, "default_field": "full_name", "default_operator": "and"}})
+
     if company_name:
         company_queries = []
-        company_queries.append({
-            "query_string": {
-                "query": company_name,
-                "default_field": "experience.company_name",
-                "default_operator": "and"
-            }
-        })
+        company_queries.append({"query_string": {"query": company_name, "default_field": "experience.company_name", "default_operator": "and"}})
         if company_abbreviation:
-            company_queries.append({
-                "query_string": {
-                    "query": company_abbreviation,
-                    "default_field": "experience.company_name",
-                    "default_operator": "and"
-                }
-            })
-        
+            company_queries.append(
+                {"query_string": {"query": company_abbreviation, "default_field": "experience.company_name", "default_operator": "and"}}
+            )
+
         if len(company_queries) > 1:
-            must_clauses.append({
-                "nested": {
-                    "path": "experience",
-                    "query": {
-                        "bool": {
-                            "should": company_queries,
-                            "minimum_should_match": 1
-                        }
-                    }
-                }
-            })
+            must_clauses.append({"nested": {"path": "experience", "query": {"bool": {"should": company_queries, "minimum_should_match": 1}}}})
         else:
-            must_clauses.append({
-                "nested": {
-                    "path": "experience",
-                    "query": company_queries[0]
-                }
-            })
-    
+            must_clauses.append({"nested": {"path": "experience", "query": company_queries[0]}})
+
     if email_domain:
-        should_clauses.append({
-            "query_string": {
-                "query": email_domain,
-                "default_field": "emails",
-                "default_operator": "and"
-            }
-        })
-    
+        should_clauses.append({"query_string": {"query": email_domain, "default_field": "emails", "default_operator": "and"}})
+
     if not must_clauses:
-        bool_query = {
-            "should": should_clauses if should_clauses else [],
-            "minimum_should_match": 1 if should_clauses else 0
-        }
+        bool_query = {"should": should_clauses if should_clauses else [], "minimum_should_match": 1 if should_clauses else 0}
     else:
         bool_query = {"must": must_clauses}
         if should_clauses:
             bool_query["should"] = should_clauses
             bool_query["minimum_should_match"] = 0
-    
-    search_query = {
-        "query": {
-            "bool": bool_query
-        }
-    }
-    
+
+    search_query = {"query": {"bool": bool_query}}
+
     logger.info(
         "CoreSignal: Starting person search",
         extra={
@@ -195,122 +146,100 @@
             "email": email,
             "company_name": company_name,
             "company_abbreviation": company_abbreviation,
-            "email_domain": email_domain
-        }
+            "email_domain": email_domain,
+        },
     )
-    
+
     try:
         async with aiohttp.ClientSession() as session:
-            async with session.post(
-                search_url,
-                headers=headers,
-                json=search_query,
-                timeout=aiohttp.ClientTimeout(total=30)
-            ) as response:
+            async with session.post(search_url, headers=headers, json=search_query, timeout=aiohttp.ClientTimeout(total=30)) as response:
                 if response.status != 200:
                     error_text = await response.text()
                     logger.error(
                         f"CoreSignal search API error: {response.status} - {error_text}",
-                        extra={
-                            "name": name,
-                            "email": email,
-                            "status": response.status,
-                            "error_text": error_text
-                        }
+                        extra={"name": name, "email": email, "status": response.status, "error_text": error_text},
                     )
                     return []
-                
+
                 try:
                     search_data = await response.json()
                 except Exception as json_error:
                     logger.error(f"Failed to parse JSON response: {json_error}")
                     return []
-                
+
                 parent_ids = []
                 hits = []
-                
+
                 if isinstance(search_data, list):
                     parent_ids = search_data
                 elif isinstance(search_data, dict):
-                    if 'hits' in search_data:
-                        hits_obj = search_data['hits']
-                        if isinstance(hits_obj, dict) and 'hits' in hits_obj:
-                            hits = hits_obj['hits']
-                            parent_ids = [hit.get('_id') for hit in hits if hit.get('_id')]
+                    if "hits" in search_data:
+                        hits_obj = search_data["hits"]
+                        if isinstance(hits_obj, dict) and "hits" in hits_obj:
+                            hits = hits_obj["hits"]
+                            parent_ids = [hit.get("_id") for hit in hits if hit.get("_id")]
                         elif isinstance(hits_obj, list):
                             hits = hits_obj
-                            parent_ids = [hit.get('_id') for hit in hits if hit.get('_id')]
-                
+                            parent_ids = [hit.get("_id") for hit in hits if hit.get("_id")]
+
                 logger.info(
-                    f"CoreSignal search found {len(parent_ids)} parent_ids",
-                    extra={
-                        "name": name,
-                        "email": email,
-                        "parent_ids_count": len(parent_ids)
-                    }
+                    f"CoreSignal search found {len(parent_ids)} parent_ids", extra={"name": name, "email": email, "parent_ids_count": len(parent_ids)}
                 )
-                
+
                 if not parent_ids:
                     if company_name and len(must_clauses) > 1:
                         relaxed_must = [must_clauses[0]]
                         relaxed_should = [must_clauses[1]]
                         if email_domain and should_clauses:
                             relaxed_should.append(should_clauses[0])
-                        
+
                         relaxed_bool_query = {"must": relaxed_must}
                         if relaxed_should:
                             relaxed_bool_query["should"] = relaxed_should
                             relaxed_bool_query["minimum_should_match"] = 0
-                        
-                        relaxed_search_query = {
-                            "query": {
-                                "bool": relaxed_bool_query
-                            }
-                        }
-                        
+
+                        relaxed_search_query = {"query": {"bool": relaxed_bool_query}}
+
                         async with session.post(
-                            search_url,
-                            headers=headers,
-                            json=relaxed_search_query,
-                            timeout=aiohttp.ClientTimeout(total=30)
+                            search_url, headers=headers, json=relaxed_search_query, timeout=aiohttp.ClientTimeout(total=30)
                         ) as relaxed_response:
                             if relaxed_response.status == 200:
                                 relaxed_data = await relaxed_response.json()
                                 if isinstance(relaxed_data, list):
                                     parent_ids = relaxed_data
-                                elif isinstance(relaxed_data, dict) and 'hits' in relaxed_data:
-                                    hits_obj = relaxed_data['hits']
-                                    if isinstance(hits_obj, dict) and 'hits' in hits_obj:
-                                        parent_ids = [hit.get('_id') for hit in hits_obj['hits'] if hit.get('_id')]
-                    
+                                elif isinstance(relaxed_data, dict) and "hits" in relaxed_data:
+                                    hits_obj = relaxed_data["hits"]
+                                    if isinstance(hits_obj, dict) and "hits" in hits_obj:
+                                        parent_ids = [hit.get("_id") for hit in hits_obj["hits"] if hit.get("_id")]
+
                     if not parent_ids:
                         return []
-                
+
                 candidates = []
-                
+
                 if hits:
                     for idx, hit in enumerate(hits):
-                        hit_id = hit.get('_id')
-                        source = hit.get('_source', {})
-                        candidates.append(_extract_candidate_from_source(str(hit_id), source, hit.get('_score', 0), idx + 1, store_full_source=True))
+                        hit_id = hit.get("_id")
+                        source = hit.get("_source", {})
+                        candidates.append(_extract_candidate_from_source(str(hit_id), source, hit.get("_score", 0), idx + 1, store_full_source=True))
                 else:
                     ids_to_fetch = parent_ids[:10]
-                    
+
                     async def fetch_candidate(pid, idx):
                         profile_data = await get_person_details(str(pid))
                         if profile_data:
                             return _extract_candidate_from_source(str(pid), profile_data, 0, idx, store_full_source=True)
                         return None
-                    
+
                     fetch_tasks = [fetch_candidate(pid, idx + 1) for idx, pid in enumerate(ids_to_fetch)]
                     fetched_candidates = await asyncio.gather(*fetch_tasks, return_exceptions=True)
-                    
+
                     for candidate in fetched_candidates:
                         if candidate and not isinstance(candidate, Exception):
                             candidates.append(candidate)
-                
+
                 return candidates
-                
+
     except aiohttp.ClientError as e:
         logger.error(f"CoreSignal search API client error: {str(e)}", exc_info=True)
         return []
@@ -322,13 +251,13 @@
 async def get_person_details(parent_id: str) -> Optional[Dict[str, Any]]:
     """
     Get detailed profile information for a person by parent_id.
-    
+
     Args:
         parent_id: CoreSignal employee parent_id
-        
+
     Returns:
         Full employee profile data or None if not found
-        
+
     Raises:
         IntelligenceCollectionError: If API call fails
     """
@@ -336,39 +265,26 @@
     if not api_key:
         logger.warning("CORESIG_API_KEY not configured, cannot fetch person details")
         return None
-    
+
     collect_url = f"https://api.coresignal.com/cdapi/v2/employee_clean/collect/{parent_id}"
-    headers = {
-        "apikey": api_key,
-        "Content-Type": "application/json"
-    }
-    
+    headers = {"apikey": api_key, "Content-Type": "application/json"}
+
     try:
         async with aiohttp.ClientSession() as session:
-            async with session.get(
-                collect_url,
-                headers=headers,
-                timeout=aiohttp.ClientTimeout(total=30)
-            ) as response:
+            async with session.get(collect_url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as response:
                 if response.status != 200:
                     error_text = await response.text()
-                    logger.error(
-                        f"CoreSignal collect API error: {response.status} - {error_text}",
-                        extra={"parent_id": parent_id}
-                    )
+                    logger.error(f"CoreSignal collect API error: {response.status} - {error_text}", extra={"parent_id": parent_id})
                     return None
-                
+
                 employee_data = await response.json()
-                
-                logger.info(
-                    f"CoreSignal collect successful for parent_id: {parent_id}",
-                    extra={"parent_id": parent_id}
-                )
-                
-                if isinstance(employee_data, dict) and 'data' in employee_data:
-                    return employee_data['data']
+
+                logger.info(f"CoreSignal collect successful for parent_id: {parent_id}", extra={"parent_id": parent_id})
+
+                if isinstance(employee_data, dict) and "data" in employee_data:
+                    return employee_data["data"]
                 return employee_data
-                
+
     except aiohttp.ClientError as e:
         logger.error(f"CoreSignal collect API client error: {str(e)}", exc_info=True)
         return None

--- app/meeting_prep/intelligence/firm_extractor.py
+++ app/meeting_prep/intelligence/firm_extractor.py
@@ -2,122 +2,116 @@
 from typing import Optional, List, Dict, Any, Tuple
 
 
-def extract_firm_name(
-    meeting_title: str,
-    participants: List[Dict[str, Any]],
-    organizer_email: Optional[str] = None
-) -> Optional[str]:
+def extract_firm_name(meeting_title: str, participants: List[Dict[str, Any]], organizer_email: Optional[str] = None) -> Optional[str]:
     if not meeting_title:
         return None
-    
+
     words = meeting_title.split()
-    company_indicators = ['group', 'services', 'capital', 'partners', 'fund', 'management', 'holdings', 'inc', 'llc', 'corp', 'pension']
-    
+    company_indicators = ["group", "services", "capital", "partners", "fund", "management", "holdings", "inc", "llc", "corp", "pension"]
+
     for i, word in enumerate(words):
-        if word.lower() in ['x', 'with', 'and'] and i < len(words) - 1:
-            potential_firm = ' '.join(words[i+1:])
+        if word.lower() in ["x", "with", "and"] and i < len(words) - 1:
+            potential_firm = " ".join(words[i + 1 :])
             if len(potential_firm) > 5:
                 return potential_firm
-    
+
     for i, word in enumerate(words):
         if word.lower() in company_indicators and i > 0:
             start_idx = max(0, i - 3)
-            potential_firm = ' '.join(words[start_idx:i+1])
+            potential_firm = " ".join(words[start_idx : i + 1])
             if len(potential_firm) > 5:
                 return potential_firm
-    
+
     if not participants:
         return None
-    
-    external_participants = [p for p in participants if p.get('is_external', False)]
-    
+
+    external_participants = [p for p in participants if p.get("is_external", False)]
+
     if not external_participants and len(participants) > 1 and organizer_email:
-        organizer_domain = organizer_email.split('@')[1] if '@' in organizer_email else None
+        organizer_domain = organizer_email.split("@")[1] if "@" in organizer_email else None
         for p in participants:
-            email = p.get('email', '')
-            p_domain = email.split('@')[1] if '@' in email else None
+            email = p.get("email", "")
+            p_domain = email.split("@")[1] if "@" in email else None
             if organizer_domain and p_domain and p_domain != organizer_domain:
-                p['is_external'] = True
+                p["is_external"] = True
                 external_participants.append(p)
-    
+
     if external_participants:
         primary_person = external_participants[0]
-        primary_person_email = primary_person.get('email', '')
-        
+        primary_person_email = primary_person.get("email", "")
+
         if primary_person_email:
-            domain = primary_person_email.split('@')[1] if '@' in primary_person_email else None
+            domain = primary_person_email.split("@")[1] if "@" in primary_person_email else None
             if domain:
-                domain_parts = domain.split('.')
+                domain_parts = domain.split(".")
                 if domain_parts:
                     domain_name = domain_parts[0]
-                    domain_name = re.sub(r'(?<!^)(?=[A-Z])', ' ', domain_name)
-                    domain_name = re.sub(r'([a-z])([A-Z])', r'\1 \2', domain_name)
+                    domain_name = re.sub(r"(?<!^)(?=[A-Z])", " ", domain_name)
+                    domain_name = re.sub(r"([a-z])([A-Z])", r"\1 \2", domain_name)
                     return domain_name.title()
-    
+
     return None
 
 
 def extract_company_abbreviation(company_name: Optional[str]) -> Optional[str]:
     """
     Extract abbreviation from company name.
-    
+
     Examples:
     - "Economic Group Pension Services" -> "EGPS"
     - "Apple Inc" -> "AI" (or None if too short)
     - "International Business Machines" -> "IBM"
-    
+
     Args:
         company_name: Full company name
-        
+
     Returns:
         Abbreviation (usually 3-6 letters) or None if not extractable
     """
     if not company_name:
         return None
-    
+
     # Remove ONLY legal entity suffixes (not descriptive words like "Services", "Group")
     # Legal suffixes that should be removed: Inc, LLC, Ltd, Corp, Corporation, Limited, Company, Co
     # NOTE: We DON'T remove "Services", "Group", etc. as they are part of the company name
-    legal_suffixes = [' Inc', ' LLC', ' Ltd', ' Corp', ' Corporation', ' Limited', ' Company', ' Co']
+    legal_suffixes = [" Inc", " LLC", " Ltd", " Corp", " Corporation", " Limited", " Company", " Co"]
     clean_name = company_name
     for suffix in legal_suffixes:
         if clean_name.endswith(suffix):
-            clean_name = clean_name[:-len(suffix)].strip()
+            clean_name = clean_name[: -len(suffix)].strip()
             break
-    
+
     words = clean_name.split()
-    
-    stop_words = {'the', 'of', 'and', 'for', 'in', 'on', 'at', 'to', 'a', 'an'}
+
+    stop_words = {"the", "of", "and", "for", "in", "on", "at", "to", "a", "an"}
     meaningful_words = [w for w in words if w.lower() not in stop_words and len(w) > 0]
-    
+
     if len(meaningful_words) >= 2:
-        abbreviation = ''.join(word[0].upper() for word in meaningful_words if word and word[0].isalnum())
+        abbreviation = "".join(word[0].upper() for word in meaningful_words if word and word[0].isalnum())
         # Only return if it's 2-8 characters (reasonable abbreviation length)
         # Examples: "Economic Group Pension Services" -> "EGPS" (4 chars)
         if 2 <= len(abbreviation) <= 8:
             return abbreviation
-    
+
     if len(words) == 1:
         word = words[0]
         # If it's all caps and 3-6 letters, might already be an abbreviation
         if word.isupper() and 3 <= len(word) <= 6:
             return word
-    
+
     return None
 
 
 def extract_firm_name_and_abbreviation(
-    meeting_title: str,
-    participants: List[Dict[str, Any]],
-    organizer_email: Optional[str] = None
+    meeting_title: str, participants: List[Dict[str, Any]], organizer_email: Optional[str] = None
 ) -> Tuple[Optional[str], Optional[str]]:
     """
     Extract firm name and its abbreviation from meeting context.
-    
+
     Returns:
         Tuple of (company_name, company_abbreviation)
     """
     company_name = extract_firm_name(meeting_title, participants, organizer_email)
     company_abbreviation = extract_company_abbreviation(company_name) if company_name else None
-    
+
     return company_name, company_abbreviation

--- app/meeting_prep/intelligence/perplexity_client.py
+++ app/meeting_prep/intelligence/perplexity_client.py
@@ -25,7 +25,7 @@
     cached_result = await cache.get(firm_name, person_name, person_email, section)
     if cached_result:
         return cached_result
-    
+
     api_key = get_perplexity_api_key()
     if not api_key:
         error_msg = "Perplexity API key not configured"
@@ -34,23 +34,23 @@
             meeting_id=meeting_id,
             section=section,
             query_text=query_text,
-            model=model or 'sonar-pro',
+            model=model or "sonar-pro",
             error_message=error_msg,
         )
         result = {
-            'raw_response': None,
-            'parsed_response': None,
-            'query_id': query_id,
-            'error': error_msg,
+            "raw_response": None,
+            "parsed_response": None,
+            "query_id": query_id,
+            "error": error_msg,
         }
         await cache.set(firm_name, person_name, person_email, section, result)
         return result
-    
+
     config = get_llm_config()
     parsed_response = None
     raw_response = None
     error_message = None
-    
+
     try:
         chat = ChatPerplexity(
             temperature=config.perplexity_temperature,
@@ -58,16 +58,16 @@
             api_key=api_key,
             timeout=config.perplexity_timeout,
         )
-        
+
         response = await chat.ainvoke([("user", query_text)])
         raw_response = response.content.strip()
-        
+
         if parse_json and raw_response:
             try:
                 parsed_response = parse_llm_json_response(raw_response)
             except Exception:
                 parsed_response = None
-        
+
         query_id = None
         try:
             query_id = await insert_perplexity_result(
@@ -82,20 +82,20 @@
             )
         except Exception:
             pass
-        
+
         result = {
-            'raw_response': raw_response,
-            'parsed_response': parsed_response,
-            'query_id': query_id,
-            'error': None,
+            "raw_response": raw_response,
+            "parsed_response": parsed_response,
+            "query_id": query_id,
+            "error": None,
         }
-        if not result.get('error'):
+        if not result.get("error"):
             await cache.set(firm_name, person_name, person_email, section, result)
         return result
-        
+
     except Exception as e:
         error_message = str(e)
-        
+
         query_id = await insert_perplexity_result(
             user_id=user_id,
             meeting_id=meeting_id,
@@ -106,12 +106,12 @@
             parsed_response=None,
             error_message=error_message,
         )
-        
+
         return {
-            'raw_response': None,
-            'parsed_response': None,
-            'query_id': query_id,
-            'error': error_message,
+            "raw_response": None,
+            "parsed_response": None,
+            "query_id": query_id,
+            "error": error_message,
         }
 
 
@@ -122,41 +122,41 @@
     additional_context: Optional[Dict[str, Any]] = None,
 ) -> Dict[str, Any]:
     from app.meeting_prep.prompts.prompts import get_firm_intelligence_prompt
-    
+
     query_text = get_firm_intelligence_prompt(firm_name, additional_context or {})
-    
+
     result = await query_perplexity(
-        section='firm_intel',
+        section="firm_intel",
         query_text=query_text,
         user_id=user_id,
         meeting_id=meeting_id,
         parse_json=True,
         firm_name=firm_name,
     )
-    
-    if result['error']:
+
+    if result["error"]:
         return {
-            'aum': {'total_aum': None},
-            'activity_sentiment_signals': {'recent_investment_news': []},
-            'recent_hires': {'recent_hires_investment_and_executive_roles_only': []},
-            'new_offices': {'new_offices': []},
-            'awards_recognition': {'awards_recognition': []},
-            'events_conferences': {'events_conferences': []},
-            'firm_strategy_commentary': {'firm_strategy_statements': []},
+            "aum": {"total_aum": None},
+            "activity_sentiment_signals": {"recent_investment_news": []},
+            "recent_hires": {"recent_hires_investment_and_executive_roles_only": []},
+            "new_offices": {"new_offices": []},
+            "awards_recognition": {"awards_recognition": []},
+            "events_conferences": {"events_conferences": []},
+            "firm_strategy_commentary": {"firm_strategy_statements": []},
         }
-    
-    parsed = result.get('parsed_response')
+
+    parsed = result.get("parsed_response")
     if parsed and isinstance(parsed, dict):
         return parsed
-    
+
     return {
-        'aum': {'total_aum': None},
-        'activity_sentiment_signals': {'recent_investment_news': []},
-        'recent_hires': {'recent_hires_investment_and_executive_roles_only': []},
-        'new_offices': {'new_offices': []},
-        'awards_recognition': {'awards_recognition': []},
-        'events_conferences': {'events_conferences': []},
-        'firm_strategy_commentary': {'firm_strategy_statements': []},
+        "aum": {"total_aum": None},
+        "activity_sentiment_signals": {"recent_investment_news": []},
+        "recent_hires": {"recent_hires_investment_and_executive_roles_only": []},
+        "new_offices": {"new_offices": []},
+        "awards_recognition": {"awards_recognition": []},
+        "events_conferences": {"events_conferences": []},
+        "firm_strategy_commentary": {"firm_strategy_statements": []},
     }
 
 
@@ -168,11 +168,11 @@
     additional_context: Optional[Dict[str, Any]] = None,
 ) -> Dict[str, Any]:
     from app.meeting_prep.prompts.prompts import get_person_intelligence_prompt
-    
+
     query_text = get_person_intelligence_prompt(person_name, email, additional_context or {})
-    
+
     result = await query_perplexity(
-        section='person_intel',
+        section="person_intel",
         query_text=query_text,
         user_id=user_id,
         meeting_id=meeting_id,
@@ -180,39 +180,38 @@
         person_name=person_name,
         person_email=email,
     )
-    
-    if result['error']:
+
+    if result["error"]:
         return {
-            'basic_info': {
-                'full_name': person_name,
-                'email': email,
-                'linkedin_url': None,
+            "basic_info": {
+                "full_name": person_name,
+                "email": email,
+                "linkedin_url": None,
             },
-            'conferences_public_speaking': {'speaking_engagements': []},
-            'recent_news': {'recent_news_mentions': []},
-            'recent_social_posts': None,
-            'media': None,
+            "conferences_public_speaking": {"speaking_engagements": []},
+            "recent_news": {"recent_news_mentions": []},
+            "recent_social_posts": None,
+            "media": None,
         }
-    
-    parsed = result.get('parsed_response')
+
+    parsed = result.get("parsed_response")
     if parsed and isinstance(parsed, dict):
-        if 'basic_info' not in parsed:
-            parsed['basic_info'] = {
-                'full_name': person_name,
-                'email': email,
-                'linkedin_url': None,
+        if "basic_info" not in parsed:
+            parsed["basic_info"] = {
+                "full_name": person_name,
+                "email": email,
+                "linkedin_url": None,
             }
         return parsed
-    
+
     return {
-        'basic_info': {
-            'full_name': person_name,
-            'email': email,
-            'linkedin_url': None,
+        "basic_info": {
+            "full_name": person_name,
+            "email": email,
+            "linkedin_url": None,
         },
-        'conferences_public_speaking': {'speaking_engagements': []},
-        'recent_news': {'recent_news_mentions': []},
-        'recent_social_posts': None,
-        'media': None,
+        "conferences_public_speaking": {"speaking_engagements": []},
+        "recent_news": {"recent_news_mentions": []},
+        "recent_social_posts": None,
+        "media": None,
     }
-

--- app/meeting_prep/models.py
+++ app/meeting_prep/models.py
@@ -9,8 +9,9 @@
 
 class ParticipantModel(BaseModel):
     """Participant information in a meeting."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     name: str = Field(..., description="Participant's full name", json_schema_extra={"example": "John Doe"})
     email: str = Field(..., description="Participant's email address", json_schema_extra={"example": "john.doe@example.com"})
     is_external: bool = Field(..., description="Whether the participant is from outside the user's organization", json_schema_extra={"example": True})
@@ -18,8 +19,9 @@
 
 class AttendeeModel(BaseModel):
     """Attendee information with role."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     name: str = Field(..., description="Attendee's full name")
     email: str = Field(..., description="Attendee's email address")
     role: Optional[str] = Field(None, description="Attendee's role if available")
@@ -27,6 +29,7 @@
 
 class MeetingReportResponse(BaseModel):
     """Response model for a meeting preparation report."""
+
     model_config = ConfigDict(
         from_attributes=True,
         json_schema_extra={
@@ -37,18 +40,12 @@
                 "meeting_title": "Q4 Investor Meeting",
                 "meeting_time": "2025-12-04T09:00:00Z",
                 "integration_type": "google",
-                "participants": [
-                    {
-                        "name": "John Doe",
-                        "email": "john.doe@example.com",
-                        "is_external": True
-                    }
-                ],
-                "report_content": "# Meeting Preparation Report\n\n## Overview\n..."
+                "participants": [{"name": "John Doe", "email": "john.doe@example.com", "is_external": True}],
+                "report_content": "# Meeting Preparation Report\n\n## Overview\n...",
             }
-        }
+        },
     )
-    
+
     id: int = Field(..., description="Unique report ID")
     user_id: str = Field(..., description="User ID who owns this report")
     meeting_id: str = Field(..., description="Unique meeting identifier from calendar")
@@ -65,18 +62,9 @@
 
 class MeetingReportListResponse(BaseModel):
     """Response model for paginated list of meeting reports."""
-    model_config = ConfigDict(
-        json_schema_extra={
-            "example": {
-                "reports": [],
-                "total": 25,
-                "limit": 20,
-                "offset": 0,
-                "has_more": True
-            }
-        }
-    )
-    
+
+    model_config = ConfigDict(json_schema_extra={"example": {"reports": [], "total": 25, "limit": 20, "offset": 0, "has_more": True}})
+
     reports: List[MeetingReportResponse] = Field(..., description="List of meeting reports")
     total: int = Field(..., description="Total number of reports matching the filters")
     limit: int = Field(..., description="Maximum number of results per page")
@@ -86,36 +74,20 @@
 
 class GenerateReportRequest(BaseModel):
     """Request model for generating a meeting report manually."""
-    model_config = ConfigDict(
-        json_schema_extra={
-            "example": {
-                "integration_type": "google",
-                "meeting_id": None
-            }
-        }
-    )
-    
-    integration_type: str = Field(
-        ...,
-        description="Calendar integration type to use"
-    )
+
+    model_config = ConfigDict(json_schema_extra={"example": {"integration_type": "google", "meeting_id": None}})
+
+    integration_type: str = Field(..., description="Calendar integration type to use")
     meeting_id: Optional[str] = Field(
-        None,
-        description="Optional: Generate report for specific meeting ID. If not provided, generates for all important events found."
+        None, description="Optional: Generate report for specific meeting ID. If not provided, generates for all important events found."
     )
 
 
 class GenerateReportResponse(BaseModel):
     """Response model for manual report generation."""
-    model_config = ConfigDict(
-        json_schema_extra={
-            "example": {
-                "reports": [],
-                "message": "Generated 3 report(s) successfully"
-            }
-        }
-    )
-    
+
+    model_config = ConfigDict(json_schema_extra={"example": {"reports": [], "message": "Generated 3 report(s) successfully"}})
+
     reports: List[MeetingReportResponse] = Field(..., description="List of generated reports")
     message: str = Field(..., description="Status message")
 
@@ -124,10 +96,12 @@
 # Structured Meeting Report Models
 # ============================================================================
 
+
 class MeetingMetadataModel(BaseModel):
     """Meeting metadata from email thread and calendar invite."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     meeting_title: Optional[str] = Field(None, description="Title of the meeting")
     meeting_date: Optional[str] = Field(None, description="Meeting date (ISO format)")
     meeting_time: Optional[str] = Field(None, description="Meeting time (ISO format)")
@@ -138,8 +112,9 @@
 
 class ExecutiveSummaryModel(BaseModel):
     """Executive summary at the top of the report."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     tldr: Optional[str] = Field(None, description="2-3 sentence summary")
     key_opportunity: Optional[str] = Field(None, description="Why this meeting matters")
     recommended_talking_points: List[str] = Field(default_factory=list, description="3-5 recommended talking points")
@@ -148,8 +123,9 @@
 
 class RelationshipHistoryModel(BaseModel):
     """Relationship history from email thread and calendar."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     last_contact_date: Optional[str] = Field(None, description="Last contact date (ISO format)")
     days_since_last_contact: Optional[int] = Field(None, description="Days since last contact")
     relationship_temperature: Optional[str] = Field(None, description="hot, warm, cool, or cold")
@@ -159,8 +135,9 @@
 
 class RecentHireModel(BaseModel):
     """Recent hire information."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     name: str
     role: str
     start_date: Optional[str] = None
@@ -170,8 +147,9 @@
 
 class RecentInvestmentNewsModel(BaseModel):
     """Recent investment news item."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     date: Optional[str] = None
     headline: str
     source: Optional[str] = None
@@ -180,8 +158,9 @@
 
 class NewOfficeModel(BaseModel):
     """New office/expansion information."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     location: str
     opening_date: Optional[str] = None
     purpose: Optional[str] = None
@@ -189,8 +168,9 @@
 
 class AwardRecognitionModel(BaseModel):
     """Award or recognition information."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     date: Optional[str] = None
     award_name: str
     significance: Optional[str] = None
@@ -198,8 +178,9 @@
 
 class EventConferenceModel(BaseModel):
     """Event or conference participation."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     event_name: str
     date: Optional[str] = None
     role: Optional[str] = None
@@ -207,8 +188,9 @@
 
 class FirmStrategyStatementModel(BaseModel):
     """Firm strategy statement."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     date: Optional[str] = None
     source: Optional[str] = None
     quote: Optional[str] = None
@@ -217,31 +199,23 @@
 
 class FirmIntelligenceModel(BaseModel):
     """Firm intelligence section."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     aum: Dict[str, Optional[str]] = Field(default_factory=lambda: {"total_aum": None})
-    activity_sentiment_signals: Dict[str, List[RecentInvestmentNewsModel]] = Field(
-        default_factory=lambda: {"recent_investment_news": []}
-    )
-    recent_hires: Dict[str, List[RecentHireModel]] = Field(
-        default_factory=lambda: {"recent_hires_investment_and_executive_roles_only": []}
-    )
+    activity_sentiment_signals: Dict[str, List[RecentInvestmentNewsModel]] = Field(default_factory=lambda: {"recent_investment_news": []})
+    recent_hires: Dict[str, List[RecentHireModel]] = Field(default_factory=lambda: {"recent_hires_investment_and_executive_roles_only": []})
     new_offices: Dict[str, List[NewOfficeModel]] = Field(default_factory=lambda: {"new_offices": []})
-    awards_recognition: Dict[str, List[AwardRecognitionModel]] = Field(
-        default_factory=lambda: {"awards_recognition": []}
-    )
-    events_conferences: Dict[str, List[EventConferenceModel]] = Field(
-        default_factory=lambda: {"events_conferences": []}
-    )
-    firm_strategy_commentary: Dict[str, List[FirmStrategyStatementModel]] = Field(
-        default_factory=lambda: {"firm_strategy_statements": []}
-    )
+    awards_recognition: Dict[str, List[AwardRecognitionModel]] = Field(default_factory=lambda: {"awards_recognition": []})
+    events_conferences: Dict[str, List[EventConferenceModel]] = Field(default_factory=lambda: {"events_conferences": []})
+    firm_strategy_commentary: Dict[str, List[FirmStrategyStatementModel]] = Field(default_factory=lambda: {"firm_strategy_statements": []})
 
 
 class SpeakingEngagementModel(BaseModel):
     """Speaking engagement information."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     date: Optional[str] = None
     event_name: str
     topic: Optional[str] = None
@@ -251,8 +225,9 @@
 
 class RecentNewsMentionModel(BaseModel):
     """Recent news mention about the person."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     date: Optional[str] = None
     headline: str
     source: Optional[str] = None
@@ -262,17 +237,12 @@
 
 class PersonIntelligenceModel(BaseModel):
     """Person intelligence section."""
-    model_config = ConfigDict(from_attributes=True, extra='allow')  # Allow extra fields from CoreSignal
-    
-    basic_info: Dict[str, Optional[str]] = Field(
-        default_factory=lambda: {"full_name": None, "email": None, "linkedin_url": None}
-    )
-    conferences_public_speaking: Dict[str, List[SpeakingEngagementModel]] = Field(
-        default_factory=lambda: {"speaking_engagements": []}
-    )
-    recent_news: Dict[str, List[RecentNewsMentionModel]] = Field(
-        default_factory=lambda: {"recent_news_mentions": []}
-    )
+
+    model_config = ConfigDict(from_attributes=True, extra="allow")  # Allow extra fields from CoreSignal
+
+    basic_info: Dict[str, Optional[str]] = Field(default_factory=lambda: {"full_name": None, "email": None, "linkedin_url": None})
+    conferences_public_speaking: Dict[str, List[SpeakingEngagementModel]] = Field(default_factory=lambda: {"speaking_engagements": []})
+    recent_news: Dict[str, List[RecentNewsMentionModel]] = Field(default_factory=lambda: {"recent_news_mentions": []})
     recent_social_posts: Optional[Dict[str, Any]] = Field(None, description="Placeholder for future Coresignal integration")
     media: Optional[Dict[str, Any]] = Field(None, description="Placeholder for future YouTube/Podcast integration")
     # CoreSignal-specific fields
@@ -286,8 +256,9 @@
 
 class ConversationStarterModel(BaseModel):
     """Conversation starter topic."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     topic: str
     why_relevant: Optional[str] = None
     talking_point: Optional[str] = None
@@ -295,8 +266,9 @@
 
 class MeetingObjectivesModel(BaseModel):
     """Meeting objectives section."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     primary_goal: Optional[str] = None
     key_questions_to_ask: List[str] = Field(default_factory=list)
     next_steps_to_propose: List[str] = Field(default_factory=list)
@@ -305,8 +277,9 @@
 
 class CommunicationStyleModel(BaseModel):
     """Communication style analysis from email thread."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     tone_of_typical_reply: Optional[str] = None
     how_they_intro_and_sign_off: Optional[str] = None
     response_behaviour: Optional[str] = None
@@ -315,8 +288,9 @@
 
 class StructuredMeetingReportModel(BaseModel):
     """Complete structured meeting preparation report."""
+
     model_config = ConfigDict(from_attributes=True)
-    
+
     meeting_metadata: Optional[MeetingMetadataModel] = None
     executive_summary: Optional[ExecutiveSummaryModel] = None
     relationship_history: Optional[RelationshipHistoryModel] = None
@@ -325,4 +299,3 @@
     conversation_starters: List[ConversationStarterModel] = Field(default_factory=list)
     meeting_objectives: Optional[MeetingObjectivesModel] = None
     communication_style: Optional[CommunicationStyleModel] = None
-

--- app/meeting_prep/prompts/prompts.py
+++ app/meeting_prep/prompts/prompts.py
@@ -12,15 +12,15 @@
 def get_important_events_prompt(events: List[Dict[str, Any]]) -> str:
     """
     Generate prompt for ChatGPT to identify important calendar events.
-    
+
     Args:
         events: List of calendar event dictionaries from Google/Outlook
-        
+
     Returns:
         Prompt string for ChatGPT
     """
     events_json = json.dumps(events, indent=2, default=str)
-    
+
     prompt = f"""You are analyzing calendar events to identify important meetings that require preparation.
 
 IMPORTANCE CRITERIA:
@@ -65,24 +65,24 @@
 {events_json}
 
 Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
 def get_email_matching_prompt(event: Dict[str, Any], emails: List[Dict[str, Any]]) -> str:
     """
     Generate prompt for ChatGPT to find emails related to a specific calendar event.
-    
+
     Args:
         event: Calendar event dictionary
         emails: List of email dictionaries from Google/Outlook
-        
+
     Returns:
         Prompt string for ChatGPT
     """
     event_json = json.dumps(event, indent=2, default=str)
     emails_json = json.dumps(emails, indent=2, default=str)
-    
+
     prompt = f"""You are finding emails related to a specific calendar event.
 
 EVENT DETAILS:
@@ -108,27 +108,27 @@
 {emails_json}
 
 Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
 def get_meeting_categorization_prompt(emails: List[Dict[str, Any]], events: List[Dict[str, Any]]) -> str:
     """
     Generate prompt for ChatGPT to identify important meetings and find related emails.
-    
+
     DEPRECATED: Use get_important_events_prompt() and get_email_matching_prompt() instead.
     Kept for backward compatibility.
-    
+
     Args:
         emails: List of email dictionaries from Google/Outlook
         events: List of calendar event dictionaries from Google/Outlook
-        
+
     Returns:
         Prompt string for ChatGPT
     """
     emails_json = json.dumps(emails, indent=2, default=str)
     events_json = json.dumps(events, indent=2, default=str)
-    
+
     prompt = f"""You are analyzing calendar events and emails to identify important meetings that require preparation.
 
 IMPORTANCE CRITERIA:
@@ -184,34 +184,30 @@
 {emails_json}
 
 Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
-def get_participant_research_prompt(
-    participant_name: str,
-    participant_email: str,
-    meeting_context: Dict[str, Any]
-) -> str:
+def get_participant_research_prompt(participant_name: str, participant_email: str, meeting_context: Dict[str, Any]) -> str:
     """
     Generate Perplexity query for researching a meeting participant.
-    
+
     Args:
         participant_name: Participant's name
         participant_email: Participant's email address
         meeting_context: Dictionary with meeting title, other participants, etc.
-        
+
     Returns:
         Perplexity search query string
     """
-    meeting_title = meeting_context.get('title', 'Unknown meeting')
-    other_participants = meeting_context.get('other_participants', [])
-    
+    meeting_title = meeting_context.get("title", "Unknown meeting")
+    other_participants = meeting_context.get("other_participants", [])
+
     query = f"""Research {participant_name} ({participant_email}) for a business meeting preparation.
 
 Meeting context:
 - Meeting title: {meeting_title}
-- Other participants: {', '.join(other_participants) if other_participants else 'None'}
+- Other participants: {", ".join(other_participants) if other_participants else "None"}
 
 Please provide:
 1. Current role and company
@@ -222,7 +218,7 @@
 6. Any relevant business context that would be useful for this meeting
 
 Focus on professional and business-relevant information that would help prepare for this meeting."""
-    
+
     return query
 
 
@@ -239,7 +235,7 @@
 ) -> str:
     """
     Generate prompt for ChatGPT to create a comprehensive meeting preparation report in Markdown format.
-    
+
     Args:
         meeting_metadata: Meeting metadata (title, date, time, duration, location, attendees)
         executive_summary: Executive summary (tldr, key_opportunity, recommended_talking_points, recommended_asks)
@@ -250,7 +246,7 @@
         meeting_objectives: Meeting objectives (primary_goal, key_questions_to_ask, next_steps_to_propose, potential_blockers)
         communication_style: Communication style analysis (tone, response_behaviour, etc.)
         related_emails: List of related emails for context
-        
+
     Returns:
         Prompt string for ChatGPT
     """
@@ -263,7 +259,7 @@
     objectives_json = json.dumps(meeting_objectives, indent=2, default=str) if meeting_objectives else "{}"
     communication_json = json.dumps(communication_style, indent=2, default=str) if communication_style else "{}"
     emails_json = json.dumps(related_emails, indent=2, default=str) if related_emails else "[]"
-    
+
     prompt = f"""You are creating a comprehensive meeting preparation report in Markdown format.
 
 Generate a professional, well-structured report that includes ALL of the following sections:
@@ -383,23 +379,23 @@
 {emails_json}
 
 Generate the complete report now in Markdown format:"""
-    
+
     return prompt
 
 
 def get_firm_intelligence_prompt(firm_name: str, meeting_context: Dict[str, Any]) -> str:
     """
     Generate Perplexity query for comprehensive firm intelligence.
-    
+
     Args:
         firm_name: Name of the firm/company
         meeting_context: Dictionary with meeting title, participants, etc.
-        
+
     Returns:
         Perplexity query string requesting structured JSON
     """
-    meeting_title = meeting_context.get('title', 'Unknown meeting')
-    
+    meeting_title = meeting_context.get("title", "Unknown meeting")
+
     prompt = f"""Research {firm_name} comprehensively for a business meeting preparation.
 
 Meeting context:
@@ -500,28 +496,24 @@
 }}
 
 Focus on recent, relevant, and business-critical information. Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
-def get_person_intelligence_prompt(
-    person_name: str,
-    email: str,
-    meeting_context: Dict[str, Any]
-) -> str:
+def get_person_intelligence_prompt(person_name: str, email: str, meeting_context: Dict[str, Any]) -> str:
     """
     Generate Perplexity query for comprehensive person intelligence.
-    
+
     Args:
         person_name: Person's full name
         email: Person's email address
         meeting_context: Dictionary with meeting title, other participants, etc.
-        
+
     Returns:
         Perplexity query string requesting structured JSON
     """
-    meeting_title = meeting_context.get('title', 'Unknown meeting')
-    
+    meeting_title = meeting_context.get("title", "Unknown meeting")
+
     prompt = f"""Research {person_name} ({email}) comprehensively for a business meeting preparation.
 
 Meeting context:
@@ -572,25 +564,22 @@
 }}
 
 Focus on professional and business-relevant information. Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
 def get_executive_summary_prompt(
-    email_thread: List[Dict[str, Any]],
-    meeting_context: Dict[str, Any],
-    firm_intel: Optional[Dict[str, Any]],
-    person_intel: Optional[Dict[str, Any]]
+    email_thread: List[Dict[str, Any]], meeting_context: Dict[str, Any], firm_intel: Optional[Dict[str, Any]], person_intel: Optional[Dict[str, Any]]
 ) -> str:
     """
     Generate prompt for OpenAI to create executive summary section.
-    
+
     Args:
         email_thread: List of related emails
         meeting_context: Meeting details
         firm_intel: Firm intelligence data
         person_intel: Person intelligence data
-        
+
     Returns:
         Prompt string for OpenAI
     """
@@ -598,7 +587,7 @@
     meeting_json = json.dumps(meeting_context, indent=2, default=str)
     firm_json = json.dumps(firm_intel, indent=2, default=str) if firm_intel else "{}"
     person_json = json.dumps(person_intel, indent=2, default=str) if person_intel else "{}"
-    
+
     prompt = f"""You are creating an executive summary for a meeting preparation report.
 
 Based on the email thread, meeting context, and intelligence gathered, generate a concise executive summary.
@@ -628,27 +617,24 @@
 {person_json}
 
 Generate the executive summary now. Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
-def get_conversation_starters_prompt(
-    all_data: Dict[str, Any],
-    user_profile: Optional[Dict[str, Any]] = None
-) -> str:
+def get_conversation_starters_prompt(all_data: Dict[str, Any], user_profile: Optional[Dict[str, Any]] = None) -> str:
     """
     Generate prompt for OpenAI to create conversation starters.
-    
+
     Args:
         all_data: All collected meeting data (metadata, intel, emails, etc.)
         user_profile: Optional user profile from user_profiles table
-        
+
     Returns:
         Prompt string for OpenAI
     """
     all_data_json = json.dumps(all_data, indent=2, default=str)
     user_profile_json = json.dumps(user_profile, indent=2, default=str) if user_profile else "{}"
-    
+
     prompt = f"""You are creating conversation starters for a meeting preparation report.
 
 Based on the firm intelligence, person intelligence, email thread, and user profile, generate timely conversation starters that tie everything together.
@@ -677,22 +663,22 @@
 - Natural conversation openers
 
 Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
 def get_meeting_objectives_prompt(all_data: Dict[str, Any]) -> str:
     """
     Generate prompt for OpenAI to create meeting objectives.
-    
+
     Args:
         all_data: All collected meeting data
-        
+
     Returns:
         Prompt string for OpenAI
     """
     all_data_json = json.dumps(all_data, indent=2, default=str)
-    
+
     prompt = f"""You are creating meeting objectives for a meeting preparation report.
 
 Based on all the collected data, generate forward-looking meeting objectives.
@@ -719,22 +705,22 @@
 - Realistic and achievable
 
 Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
 def get_communication_style_prompt(email_thread: List[Dict[str, Any]]) -> str:
     """
     Generate prompt for OpenAI to analyze communication style.
-    
+
     Args:
         email_thread: List of related emails
-        
+
     Returns:
         Prompt string for OpenAI
     """
     emails_json = json.dumps(email_thread, indent=2, default=str)
-    
+
     prompt = f"""You are analyzing communication style from an email thread.
 
 Analyze the email exchanges and extract communication patterns.
@@ -751,101 +737,97 @@
 {emails_json}
 
 Analyze the communication patterns carefully. Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
 def get_coresignal_profile_matching_prompt(
-    person_name: str,
-    person_email: str,
-    candidates: List[Dict[str, Any]],
-    meeting_context: Dict[str, Any],
-    company_abbreviation: Optional[str] = None
+    person_name: str, person_email: str, candidates: List[Dict[str, Any]], meeting_context: Dict[str, Any], company_abbreviation: Optional[str] = None
 ) -> str:
     """
     Generate prompt for GPT to identify the correct person profile from CoreSignal candidates.
-    
+
     Args:
         person_name: Target person's name
         person_email: Target person's email
         candidates: List of candidate profiles from CoreSignal search
         meeting_context: Context about the meeting (title, company, etc.)
         company_abbreviation: Optional company abbreviation (e.g., "EGPS")
-        
+
     Returns:
         Prompt string for GPT
     """
     # Extract email domain for matching
-    email_domain = person_email.split('@')[1] if '@' in person_email else None
-    meeting_title = meeting_context.get('title', '')
+    email_domain = person_email.split("@")[1] if "@" in person_email else None
+    meeting_title = meeting_context.get("title", "")
     # Try to extract company name from meeting title or context
-    company_from_context = meeting_context.get('company') or meeting_title
-    
+    company_from_context = meeting_context.get("company") or meeting_title
+
     # Create a simplified candidates list with only relevant fields for matching
     # Include emails from _source if available for better matching
     simplified_candidates = []
     for cand in candidates:
         # Try to get email from candidate dict or from _source
-        candidate_email = cand.get('email', '')
+        candidate_email = cand.get("email", "")
         all_emails = []
-        
+
         # First check candidate dict
         if candidate_email:
             all_emails.append(candidate_email)
-        
+
         # Then check _source for more emails
-        if cand.get('_source'):
-            source = cand['_source']
-            emails_field = source.get('emails') or source.get('email')
+        if cand.get("_source"):
+            source = cand["_source"]
+            emails_field = source.get("emails") or source.get("email")
             if emails_field:
                 if isinstance(emails_field, list):
                     for e in emails_field:
-                        if isinstance(e, str) and '@' in e and e not in all_emails:
+                        if isinstance(e, str) and "@" in e and e not in all_emails:
                             all_emails.append(e)
-                elif isinstance(emails_field, str) and '@' in emails_field and emails_field not in all_emails:
+                elif isinstance(emails_field, str) and "@" in emails_field and emails_field not in all_emails:
                     all_emails.append(emails_field)
-        
+
         # Use first email found, or 'Not available'
-        primary_email = all_emails[0] if all_emails else 'Not available'
-        
+        primary_email = all_emails[0] if all_emails else "Not available"
+
         simplified_cand = {
-            'parent_id': cand.get('parent_id'),
-            'full_name': cand.get('full_name', ''),
-            'email': primary_email,
-            'all_emails': all_emails if all_emails else [],  # Include all emails for better matching
-            'company_name': cand.get('company_name', ''),
-            'job_title': cand.get('job_title', ''),
+            "parent_id": cand.get("parent_id"),
+            "full_name": cand.get("full_name", ""),
+            "email": primary_email,
+            "all_emails": all_emails if all_emails else [],  # Include all emails for better matching
+            "company_name": cand.get("company_name", ""),
+            "job_title": cand.get("job_title", ""),
         }
-        
+
         # Check for exact email match in any of the emails
         email_exact_match = any(e.lower() == person_email.lower() for e in all_emails)
-        email_domain_match = any('@' in e and e.split('@')[1].lower() == email_domain.lower() for e in all_emails) if email_domain else False
-        
-        simplified_cand['email_exact_match'] = email_exact_match
-        simplified_cand['email_domain_match'] = email_domain_match
-        
-        if primary_email != 'Not available' and '@' in primary_email:
-            simplified_cand['email_domain'] = primary_email.split('@')[1]
-        
+        email_domain_match = any("@" in e and e.split("@")[1].lower() == email_domain.lower() for e in all_emails) if email_domain else False
+
+        simplified_cand["email_exact_match"] = email_exact_match
+        simplified_cand["email_domain_match"] = email_domain_match
+
+        if primary_email != "Not available" and "@" in primary_email:
+            simplified_cand["email_domain"] = primary_email.split("@")[1]
+
         simplified_candidates.append(simplified_cand)
-    
+
     candidates_json = json.dumps(simplified_candidates, indent=2, default=str)
-    
+
     company_info = company_from_context
     if company_abbreviation:
         company_info = f"{company_from_context} (also known as '{company_abbreviation}')"
-    
+
     prompt = f"""You are matching a person to the correct profile from CoreSignal search results.
 
 TARGET PERSON TO MATCH:
 - Full Name: {person_name}
 - Email: {person_email}
-- Email Domain: {email_domain if email_domain else 'N/A'}
+- Email Domain: {email_domain if email_domain else "N/A"}
 
 MEETING CONTEXT:
 - Meeting Title: {meeting_title}
 - Company/Organization: {company_info}
-- Other Participants: {', '.join(meeting_context.get('other_participants', []))}
+- Other Participants: {", ".join(meeting_context.get("other_participants", []))}
 
 CANDIDATES FROM CORESIGNAL ({len(simplified_candidates)} candidates):
 {candidates_json}
@@ -917,31 +899,28 @@
 - **Only return null** if: (1) No candidate has name "{person_name}", OR (2) Company provided but no candidate works there (check both full name and abbreviation)
 
 Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
+
     return prompt
 
 
 def get_coresignal_data_extraction_prompt(
-    person_name: str,
-    person_email: str,
-    coresignal_profile: Dict[str, Any],
-    meeting_context: Dict[str, Any]
+    person_name: str, person_email: str, coresignal_profile: Dict[str, Any], meeting_context: Dict[str, Any]
 ) -> str:
     """
     Generate prompt for GPT to extract relevant information from CoreSignal profile.
-    
+
     Args:
         person_name: Person's name
         person_email: Person's email
         coresignal_profile: Full CoreSignal employee profile data
         meeting_context: Context about the meeting
-        
+
     Returns:
         Prompt string for GPT
     """
     profile_json = json.dumps(coresignal_profile, indent=2, default=str)
     meeting_json = json.dumps(meeting_context, indent=2, default=str)
-    
+
     prompt = f"""You are extracting relevant information from a CoreSignal profile for a meeting preparation report.
 
 TARGET PERSON (the person you're meeting with, NOT the user):
@@ -1000,6 +979,5 @@
 }}
 
 Focus on information that would be valuable for understanding this person's professional background and how to engage with them effectively. Return ONLY valid JSON - no markdown code blocks, no explanations."""
-    
-    return prompt
 
+    return prompt

--- app/meeting_prep/repositories/meeting_reports_repository.py
+++ app/meeting_prep/repositories/meeting_reports_repository.py
@@ -14,18 +14,18 @@
     """
     if value is None:
         return None
-    
+
     if isinstance(value, (dict, list)):
         return value
-    
+
     if isinstance(value, str):
-        if value == 'null':
+        if value == "null":
             return None
         try:
             return json.loads(value)
         except (json.JSONDecodeError, TypeError, ValueError):
             return value
-    
+
     return value
 
 
@@ -46,13 +46,13 @@
     db = await get_global_db()
     if not db:
         raise ValueError("Database not available")
-    
+
     # Convert empty lists/dicts to None for JSONB fields to avoid asyncpg serialization issues
     # Empty lists/dicts will be stored as NULL in the database
     participants_json = participants if participants else None
     related_emails_json = related_emails if related_emails else None
     research_data_json = research_data if research_data else None
-    
+
     async with db.pool.acquire() as conn:
         await conn.execute(
             """
@@ -103,7 +103,7 @@
     db = await get_global_db()
     if not db:
         return []
-    
+
     async with db.pool.acquire() as conn:
         query = """
             SELECT id,
@@ -122,7 +122,7 @@
             WHERE user_id = $1
         """
         params = [user_id]
-        
+
         if date_start:
             query += " AND meeting_time >= $2"
             params.append(date_start)
@@ -132,29 +132,29 @@
         elif date_end:
             query += " AND meeting_time <= $2"
             params.append(date_end)
-        
+
         query += " ORDER BY meeting_time DESC"
-        
+
         rows = await conn.fetch(query, *params)
-        
+
     reports = []
     for row in rows:
         report_dict = {
-            'id': row['id'],
-            'user_id': row['user_id'],
-            'meeting_id': row['meeting_id'],
-            'meeting_title': row['meeting_title'],
-            'meeting_time': row['meeting_time'],
-            'integration_type': row['integration_type'],
-            'participants': _deserialize_jsonb_field(row.get('participants')),
-            'related_emails': _deserialize_jsonb_field(row.get('related_emails')),
-            'research_data': _deserialize_jsonb_field(row.get('research_data')),
-            'report_content': row['report_content'],
-            'created_at': row['created_at'],
-            'updated_at': row['updated_at'],
+            "id": row["id"],
+            "user_id": row["user_id"],
+            "meeting_id": row["meeting_id"],
+            "meeting_title": row["meeting_title"],
+            "meeting_time": row["meeting_time"],
+            "integration_type": row["integration_type"],
+            "participants": _deserialize_jsonb_field(row.get("participants")),
+            "related_emails": _deserialize_jsonb_field(row.get("related_emails")),
+            "research_data": _deserialize_jsonb_field(row.get("research_data")),
+            "report_content": row["report_content"],
+            "created_at": row["created_at"],
+            "updated_at": row["updated_at"],
         }
         reports.append(report_dict)
-    
+
     return reports
 
 
@@ -172,41 +172,41 @@
     db = await get_global_db()
     if not db:
         return {
-            'reports': [],
-            'total': 0,
-            'limit': limit,
-            'offset': offset,
-            'has_more': False,
+            "reports": [],
+            "total": 0,
+            "limit": limit,
+            "offset": offset,
+            "has_more": False,
         }
-    
+
     async with db.pool.acquire() as conn:
         # Build WHERE clause
         where_clauses = ["user_id = $1"]
         params = [user_id]
         param_index = 2
-        
+
         if start_date:
             where_clauses.append(f"meeting_time >= ${param_index}")
             params.append(start_date)
             param_index += 1
-        
+
         if end_date:
             where_clauses.append(f"meeting_time <= ${param_index}")
             params.append(end_date)
             param_index += 1
-        
+
         if integration_type:
             where_clauses.append(f"integration_type = ${param_index}")
             params.append(integration_type)
             param_index += 1
-        
+
         where_clause = " AND ".join(where_clauses)
-        
+
         # Get total count
         count_query = f"SELECT COUNT(*) FROM meeting_preparation_reports WHERE {where_clause}"
         total_row = await conn.fetchrow(count_query, *params)
-        total = total_row['count'] if total_row else 0
-        
+        total = total_row["count"] if total_row else 0
+
         # Get paginated results
         # Cast to text first, then to jsonb to handle both text and jsonb stored values
         query = f"""
@@ -228,35 +228,35 @@
             LIMIT ${param_index} OFFSET ${param_index + 1}
         """
         params.extend([limit, offset])
-        
+
         rows = await conn.fetch(query, *params)
-    
+
     reports = []
     for row in rows:
         report_dict = {
-            'id': row['id'],
-            'user_id': row['user_id'],
-            'meeting_id': row['meeting_id'],
-            'meeting_title': row['meeting_title'],
-            'meeting_time': row['meeting_time'],
-            'integration_type': row['integration_type'],
-            'participants': _deserialize_jsonb_field(row.get('participants')),
-            'related_emails': _deserialize_jsonb_field(row.get('related_emails')),
-            'research_data': _deserialize_jsonb_field(row.get('research_data')),
-            'report_content': row['report_content'],
-            'created_at': row['created_at'],
-            'updated_at': row['updated_at'],
+            "id": row["id"],
+            "user_id": row["user_id"],
+            "meeting_id": row["meeting_id"],
+            "meeting_title": row["meeting_title"],
+            "meeting_time": row["meeting_time"],
+            "integration_type": row["integration_type"],
+            "participants": _deserialize_jsonb_field(row.get("participants")),
+            "related_emails": _deserialize_jsonb_field(row.get("related_emails")),
+            "research_data": _deserialize_jsonb_field(row.get("research_data")),
+            "report_content": row["report_content"],
+            "created_at": row["created_at"],
+            "updated_at": row["updated_at"],
         }
         reports.append(report_dict)
-    
+
     has_more = (offset + len(reports)) < total
-    
+
     return {
-        'reports': reports,
-        'total': total,
-        'limit': limit,
-        'offset': offset,
-        'has_more': has_more,
+        "reports": reports,
+        "total": total,
+        "limit": limit,
+        "offset": offset,
+        "has_more": has_more,
     }
 
 
@@ -264,12 +264,12 @@
     """
     Get a specific meeting preparation report by ID.
     Verifies that the report belongs to the user.
-    
+
     """
     db = await get_global_db()
     if not db:
         return None
-    
+
     async with db.pool.acquire() as conn:
         row = await conn.fetchrow(
             """
@@ -291,34 +291,30 @@
             report_id,
             user_id,
         )
-    
+
     if row:
         return {
-            'id': row['id'],
-            'user_id': row['user_id'],
-            'meeting_id': row['meeting_id'],
-            'meeting_title': row['meeting_title'],
-            'meeting_time': row['meeting_time'],
-            'integration_type': row['integration_type'],
-            'participants': _deserialize_jsonb_field(row.get('participants')),
-            'related_emails': _deserialize_jsonb_field(row.get('related_emails')),
-            'research_data': _deserialize_jsonb_field(row.get('research_data')),
-            'report_content': row['report_content'],
-            'created_at': row['created_at'],
-            'updated_at': row['updated_at'],
+            "id": row["id"],
+            "user_id": row["user_id"],
+            "meeting_id": row["meeting_id"],
+            "meeting_title": row["meeting_title"],
+            "meeting_time": row["meeting_time"],
+            "integration_type": row["integration_type"],
+            "participants": _deserialize_jsonb_field(row.get("participants")),
+            "related_emails": _deserialize_jsonb_field(row.get("related_emails")),
+            "research_data": _deserialize_jsonb_field(row.get("research_data")),
+            "report_content": row["report_content"],
+            "created_at": row["created_at"],
+            "updated_at": row["updated_at"],
         }
     return None
 
 
-async def get_report_by_meeting_id(
-    user_id: str,
-    meeting_id: str,
-    integration_type: str
-) -> Optional[Dict[str, Any]]:
+async def get_report_by_meeting_id(user_id: str, meeting_id: str, integration_type: str) -> Optional[Dict[str, Any]]:
     db = await get_global_db()
     if not db:
         return None
-    
+
     async with db.pool.acquire() as conn:
         row = await conn.fetchrow(
             """
@@ -343,21 +339,21 @@
             meeting_id,
             integration_type,
         )
-    
+
     if row:
         return {
-            'id': row['id'],
-            'user_id': row['user_id'],
-            'meeting_id': row['meeting_id'],
-            'meeting_title': row['meeting_title'],
-            'meeting_time': row['meeting_time'],
-            'integration_type': row['integration_type'],
-            'participants': _deserialize_jsonb_field(row.get('participants')),
-            'related_emails': _deserialize_jsonb_field(row.get('related_emails')),
-            'research_data': _deserialize_jsonb_field(row.get('research_data')),
-            'report_content': row['report_content'],
-            'created_at': row['created_at'],
-            'updated_at': row['updated_at'],
+            "id": row["id"],
+            "user_id": row["user_id"],
+            "meeting_id": row["meeting_id"],
+            "meeting_title": row["meeting_title"],
+            "meeting_time": row["meeting_time"],
+            "integration_type": row["integration_type"],
+            "participants": _deserialize_jsonb_field(row.get("participants")),
+            "related_emails": _deserialize_jsonb_field(row.get("related_emails")),
+            "research_data": _deserialize_jsonb_field(row.get("research_data")),
+            "report_content": row["report_content"],
+            "created_at": row["created_at"],
+            "updated_at": row["updated_at"],
         }
     return None
 
@@ -369,7 +365,7 @@
     db = await get_global_db()
     if not db:
         return []
-    
+
     async with db.pool.acquire() as conn:
         google_rows = await conn.fetch(
             """
@@ -377,26 +373,19 @@
             FROM google_oauth_tokens
             """
         )
-        
+
         outlook_rows = await conn.fetch(
             """
             SELECT DISTINCT user_id
             FROM outlook_oauth_tokens
             """
         )
-    
+
     users = []
     for row in google_rows:
-        users.append({
-            'user_id': row['user_id'],
-            'integration_type': 'google'
-        })
-    
+        users.append({"user_id": row["user_id"], "integration_type": "google"})
+
     for row in outlook_rows:
-        users.append({
-            'user_id': row['user_id'],
-            'integration_type': 'outlook'
-        })
-    
+        users.append({"user_id": row["user_id"], "integration_type": "outlook"})
+
     return users
-

--- app/meeting_prep/repositories/perplexity_repository.py
+++ app/meeting_prep/repositories/perplexity_repository.py
@@ -19,18 +19,18 @@
     """
     if value is None:
         return None
-    
+
     if isinstance(value, (dict, list)):
         return value
-    
+
     if isinstance(value, str):
-        if value == 'null':
+        if value == "null":
             return None
         try:
             return json.loads(value)
         except (json.JSONDecodeError, TypeError, ValueError):
             return value
-    
+
     return value
 
 
@@ -46,7 +46,7 @@
 ) -> int:
     """
     Insert a Perplexity query result into the database.
-    
+
     Args:
         user_id: User ID
         meeting_id: Meeting ID (optional)
@@ -56,20 +56,20 @@
         raw_response: Raw response text from Perplexity
         parsed_response: Parsed JSON response (if applicable)
         error_message: Error message if query failed
-        
+
     Returns:
         The ID of the inserted record
     """
     db = await get_global_db()
     if not db:
         raise ValueError("Database not available")
-    
+
     async with db.pool.acquire() as conn:
         try:
             # Serialize parsed_response to JSON string for JSONB column
             # asyncpg requires JSON strings for JSONB columns, not Python dicts
             parsed_response_json = json.dumps(parsed_response) if parsed_response is not None else None
-            
+
             row = await conn.fetchrow(
                 """
                 INSERT INTO perplexity_queries (
@@ -95,21 +95,22 @@
                 parsed_response_json,  # Now it's a JSON string, asyncpg will convert to JSONB
                 error_message,
             )
-            
-            return row['id']
+
+            return row["id"]
         except Exception as e:
             # In test mode, if foreign key constraint fails (user doesn't exist),
             # or if there's a type error (shouldn't happen now with JSON serialization),
             # we can still continue - the query was executed, just not persisted
             import os
+
             error_str = str(e).lower()
             is_test_mode = (
-                os.getenv("TESTING") == "true" or 
-                "test" in error_str or
-                "foreign key" in error_str or
-                "violates foreign key constraint" in error_str or
-                "expected str, got dict" in error_str or
-                "invalid input" in error_str
+                os.getenv("TESTING") == "true"
+                or "test" in error_str
+                or "foreign key" in error_str
+                or "violates foreign key constraint" in error_str
+                or "expected str, got dict" in error_str
+                or "invalid input" in error_str
             )
             if is_test_mode:
                 return -1
@@ -123,19 +124,19 @@
 ) -> List[Dict[str, Any]]:
     """
     Get all Perplexity query results for a specific meeting.
-    
+
     Args:
         user_id: User ID
         meeting_id: Meeting ID
         section: Optional section filter
-        
+
     Returns:
         List of query result dictionaries
     """
     db = await get_global_db()
     if not db:
         return []
-    
+
     async with db.pool.acquire() as conn:
         query = """
             SELECT id,
@@ -152,31 +153,31 @@
             WHERE user_id = $1 AND meeting_id = $2
         """
         params = [user_id, meeting_id]
-        
+
         if section:
             query += " AND section = $3"
             params.append(section)
-        
+
         query += " ORDER BY created_at DESC"
-        
+
         rows = await conn.fetch(query, *params)
-    
+
     results = []
     for row in rows:
         result_dict = {
-            'id': row['id'],
-            'user_id': row['user_id'],
-            'meeting_id': row['meeting_id'],
-            'section': row['section'],
-            'query_text': row['query_text'],
-            'model': row['model'],
-            'raw_response': row['raw_response'],
-            'parsed_response': _deserialize_jsonb_field(row.get('parsed_response')),
-            'error_message': row['error_message'],
-            'created_at': row['created_at'],
+            "id": row["id"],
+            "user_id": row["user_id"],
+            "meeting_id": row["meeting_id"],
+            "section": row["section"],
+            "query_text": row["query_text"],
+            "model": row["model"],
+            "raw_response": row["raw_response"],
+            "parsed_response": _deserialize_jsonb_field(row.get("parsed_response")),
+            "error_message": row["error_message"],
+            "created_at": row["created_at"],
         }
         results.append(result_dict)
-    
+
     return results
 
 
@@ -187,19 +188,19 @@
 ) -> List[Dict[str, Any]]:
     """
     Get Perplexity query results by section for analytics/debugging.
-    
+
     Args:
         user_id: User ID
         section: Section identifier
         limit: Maximum number of results to return
-        
+
     Returns:
         List of query result dictionaries
     """
     db = await get_global_db()
     if not db:
         return []
-    
+
     async with db.pool.acquire() as conn:
         rows = await conn.fetch(
             """
@@ -222,22 +223,21 @@
             section,
             limit,
         )
-    
+
     results = []
     for row in rows:
         result_dict = {
-            'id': row['id'],
-            'user_id': row['user_id'],
-            'meeting_id': row['meeting_id'],
-            'section': row['section'],
-            'query_text': row['query_text'],
-            'model': row['model'],
-            'raw_response': row['raw_response'],
-            'parsed_response': _deserialize_jsonb_field(row.get('parsed_response')),
-            'error_message': row['error_message'],
-            'created_at': row['created_at'],
+            "id": row["id"],
+            "user_id": row["user_id"],
+            "meeting_id": row["meeting_id"],
+            "section": row["section"],
+            "query_text": row["query_text"],
+            "model": row["model"],
+            "raw_response": row["raw_response"],
+            "parsed_response": _deserialize_jsonb_field(row.get("parsed_response")),
+            "error_message": row["error_message"],
+            "created_at": row["created_at"],
         }
         results.append(result_dict)
-    
-    return results
 
+    return results

--- app/meeting_prep/router.py
+++ app/meeting_prep/router.py
@@ -40,41 +40,17 @@
 )
 async def list_reports(
     current_user: CurrentUser = Depends(require_user),
-    start_date: Optional[datetime] = Query(
-        None,
-        description="Start date for filtering reports (ISO 8601 format)",
-        examples=["2025-12-01T00:00:00Z"]
-    ),
-    end_date: Optional[datetime] = Query(
-        None,
-        description="End date for filtering reports (ISO 8601 format)",
-        examples=["2025-12-31T23:59:59Z"]
-    ),
-    integration_type: Optional[str] = Query(
-        None,
-        description="Filter by integration type",
-        examples=["google"],
-        enum=["google", "outlook"]
-    ),
-    limit: int = Query(
-        20,
-        ge=1,
-        le=100,
-        description="Maximum number of results per page",
-        examples=[20]
-    ),
-    offset: int = Query(
-        0,
-        ge=0,
-        description="Number of results to skip",
-        examples=[0]
-    ),
+    start_date: Optional[datetime] = Query(None, description="Start date for filtering reports (ISO 8601 format)", examples=["2025-12-01T00:00:00Z"]),
+    end_date: Optional[datetime] = Query(None, description="End date for filtering reports (ISO 8601 format)", examples=["2025-12-31T23:59:59Z"]),
+    integration_type: Optional[str] = Query(None, description="Filter by integration type", examples=["google"], enum=["google", "outlook"]),
+    limit: int = Query(20, ge=1, le=100, description="Maximum number of results per page", examples=[20]),
+    offset: int = Query(0, ge=0, description="Number of results to skip", examples=[0]),
 ):
     user_id = current_user.user_id
-    
+
     try:
         await GlobalDB.initialize()
-        
+
         result = await get_user_reports_paginated(
             user_id=user_id,
             start_date=start_date,
@@ -83,24 +59,19 @@
             limit=limit,
             offset=offset,
         )
-        
-        reports = [
-            MeetingReportResponse(**report) for report in result['reports']
-        ]
-        
+
+        reports = [MeetingReportResponse(**report) for report in result["reports"]]
+
         return MeetingReportListResponse(
             reports=reports,
-            total=result['total'],
-            limit=result['limit'],
-            offset=result['offset'],
-            has_more=result['has_more'],
+            total=result["total"],
+            limit=result["limit"],
+            offset=result["offset"],
+            has_more=result["has_more"],
         )
-        
+
     except Exception as e:
-        raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to retrieve reports"
-        )
+        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve reports")
 
 
 @router.get(
@@ -113,27 +84,21 @@
     current_user: CurrentUser = Depends(require_user),
 ):
     user_id = current_user.user_id
-    
+
     try:
         await GlobalDB.initialize()
-        
+
         report = await get_report_by_id(report_id, user_id)
-        
+
         if not report:
-            raise HTTPException(
-                status_code=status.HTTP_404_NOT_FOUND,
-                detail="Report not found or access denied"
-            )
-        
+            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Report not found or access denied")
+
         return MeetingReportResponse(**report)
-        
+
     except HTTPException:
         raise
     except Exception as e:
-        raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to retrieve report"
-        )
+        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve report")
 
 
 @router.post(
@@ -157,46 +122,28 @@
                                 "integration_type": "google",
                                 "report_content": "# Meeting Preparation Report\n\n## Overview\n...",
                                 "created_at": "2025-12-04T11:00:00Z",
-                                "updated_at": "2025-12-04T11:00:00Z"
+                                "updated_at": "2025-12-04T11:00:00Z",
                             }
                         ],
-                        "message": "Generated 1 report(s) successfully"
+                        "message": "Generated 1 report(s) successfully",
                     }
                 }
-            }
+            },
         },
         400: {
             "description": "Bad request - Invalid integration_type or missing required fields",
-            "content": {
-                "application/json": {
-                    "example": {
-                        "detail": "integration_type must be 'google' or 'outlook'"
-                    }
-                }
-            }
+            "content": {"application/json": {"example": {"detail": "integration_type must be 'google' or 'outlook'"}}},
         },
         401: {"description": "Unauthorized - Authentication required"},
         404: {
             "description": "Meeting not found - The specified meeting_id was not found or is not marked as important",
-            "content": {
-                "application/json": {
-                    "example": {
-                        "detail": "Meeting meeting_456 not found or not important"
-                    }
-                }
-            }
+            "content": {"application/json": {"example": {"detail": "Meeting meeting_456 not found or not important"}}},
         },
         500: {
             "description": "Internal server error - Failed to generate report",
-            "content": {
-                "application/json": {
-                    "example": {
-                        "detail": "Failed to generate report: <error message>"
-                    }
-                }
-            }
-        }
-    }
+            "content": {"application/json": {"example": {"detail": "Failed to generate report: <error message>"}}},
+        },
+    },
 )
 async def generate_report_manual(
     request: GenerateReportRequest,
@@ -204,158 +151,136 @@
 ):
     user_id = current_user.user_id
     integration_type = request.integration_type
-    
-    if integration_type not in ['google', 'outlook']:
-        raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST,
-            detail="integration_type must be 'google' or 'outlook'"
-        )
-    
+
+    if integration_type not in ["google", "outlook"]:
+        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="integration_type must be 'google' or 'outlook'")
+
     try:
         await GlobalDB.initialize()
-        
+
         # If meeting_id is provided, use a wider date range to find the event
         # Otherwise use default 7 days ahead for cronjob
         if request.meeting_id:
             events = await collect_calendar_events(user_id, integration_type, days_ahead=30, days_back=7)
         else:
             events = await collect_calendar_events(user_id, integration_type)
-        
+
         if not events:
-            return GenerateReportResponse(
-                reports=[],
-                message="No calendar events found"
-            )
-        
+            return GenerateReportResponse(reports=[], message="No calendar events found")
+
         # If meeting_id is provided, find the event directly without filtering by importance
         # Importance filtering is only for cronjob/week organization
         if request.meeting_id:
             print(f"[MEETING_PREP] Looking for specific meeting_id: {request.meeting_id} in {len(events)} events")
             target_event = None
             for event in events:
-                event_id = event.get('event_id') or event.get('id', '')
+                event_id = event.get("event_id") or event.get("id", "")
                 print(f"[MEETING_PREP] Checking event_id: {event_id} against {request.meeting_id}")
                 if event_id == request.meeting_id:
                     target_event = event
                     print(f"[MEETING_PREP] ‚úÖ Found event: {event.get('summary') or event.get('title', 'Untitled')}")
                     break
-            
+
             if not target_event:
                 print(f"[MEETING_PREP] ‚ùå Meeting {request.meeting_id} not found in {len(events)} events")
                 # Print all event IDs for debugging
-                event_ids = [e.get('event_id') or e.get('id', 'N/A') for e in events[:10]]
+                event_ids = [e.get("event_id") or e.get("id", "N/A") for e in events[:10]]
                 print(f"[MEETING_PREP] Available event IDs (first 10): {event_ids}")
-                raise HTTPException(
-                    status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Meeting {request.meeting_id} not found"
-                )
-            
+                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Meeting {request.meeting_id} not found")
+
             events_to_process = [target_event]
         else:
             # Only filter by importance when no specific meeting_id is provided (for cronjob)
             important_events = await identify_important_events(events)
-            
+
             if not important_events:
-                return GenerateReportResponse(
-                    reports=[],
-                    message="No important events found"
-                )
-            
+                return GenerateReportResponse(reports=[], message="No important events found")
+
             events_to_process = important_events
-        
+
         generated_reports = []
-        
+
         for event in events_to_process:
             try:
-                meeting_id = event.get('event_id', event.get('id', ''))
-                event_title = event.get('summary') or event.get('title', 'Untitled')
-                
+                meeting_id = event.get("event_id", event.get("id", ""))
+                event_title = event.get("summary") or event.get("title", "Untitled")
+
                 print(f"[MEETING_PREP] Generating report for meeting_id: {meeting_id}, title: {event_title}")
-                
-                report_content = await prepare_meeting_report(
-                    user_id=user_id,
-                    meeting_id=meeting_id,
-                    integration_type=integration_type
-                )
-                
+
+                report_content = await prepare_meeting_report(user_id=user_id, meeting_id=meeting_id, integration_type=integration_type)
+
                 if not report_content:
                     print(f"[MEETING_PREP] ‚ö†Ô∏è prepare_meeting_report returned empty content for meeting_id: {meeting_id}")
                     continue
-                
+
                 print(f"[MEETING_PREP] ‚úÖ Successfully generated report for meeting_id: {meeting_id}, content length: {len(report_content)}")
-                
-                meeting_time_str = event.get('time', '')
+
+                meeting_time_str = event.get("time", "")
                 try:
                     if isinstance(meeting_time_str, str):
-                        meeting_time = datetime.fromisoformat(meeting_time_str.replace('Z', '+00:00'))
+                        meeting_time = datetime.fromisoformat(meeting_time_str.replace("Z", "+00:00"))
                     else:
                         meeting_time = datetime.now()
                 except Exception:
                     meeting_time = datetime.now()
-                
+
                 await save_meeting_report(
                     user_id=user_id,
                     meeting_id=meeting_id,
-                    meeting_title=event.get('title') or event.get('summary'),
+                    meeting_title=event.get("title") or event.get("summary"),
                     meeting_time=meeting_time,
                     integration_type=integration_type,
-                    participants=event.get('participants'),
+                    participants=event.get("participants"),
                     related_emails=None,  # Will be populated by prepare_meeting_report if available
                     research_data=None,  # Will be populated by prepare_meeting_report if available
                     report_content=report_content,
                 )
-                
-                generated_reports.append({
-                    'id': 0,
-                    'user_id': user_id,
-                    'meeting_id': meeting_id,
-                    'meeting_title': event.get('title'),
-                    'meeting_time': meeting_time,
-                    'integration_type': integration_type,
-                    'participants': event.get('participants'),
-                    'related_emails': [],
-                    'research_data': {},
-                    'report_content': report_content,
-                    'created_at': datetime.now(),
-                    'updated_at': datetime.now(),
-                })
-                
+
+                generated_reports.append(
+                    {
+                        "id": 0,
+                        "user_id": user_id,
+                        "meeting_id": meeting_id,
+                        "meeting_title": event.get("title"),
+                        "meeting_time": meeting_time,
+                        "integration_type": integration_type,
+                        "participants": event.get("participants"),
+                        "related_emails": [],
+                        "research_data": {},
+                        "report_content": report_content,
+                        "created_at": datetime.now(),
+                        "updated_at": datetime.now(),
+                    }
+                )
+
             except Exception as e:
                 print(f"[MEETING_PREP] ‚ùå Error generating report for meeting_id {meeting_id}: {e}")
                 import traceback
+
                 traceback.print_exc()
                 continue
-        
+
         if generated_reports:
-            meeting_ids = [r['meeting_id'] for r in generated_reports]
+            meeting_ids = [r["meeting_id"] for r in generated_reports]
             result = await get_user_reports_paginated(
                 user_id=user_id,
                 integration_type=integration_type,
                 limit=len(meeting_ids),
                 offset=0,
             )
-            
-            saved_reports = [
-                r for r in result['reports']
-                if r['meeting_id'] in meeting_ids
-            ]
-            
-            saved_reports.sort(key=lambda x: x.get('created_at', datetime.min), reverse=True)
-            saved_reports = saved_reports[:len(generated_reports)]
-            
+
+            saved_reports = [r for r in result["reports"] if r["meeting_id"] in meeting_ids]
+
+            saved_reports.sort(key=lambda x: x.get("created_at", datetime.min), reverse=True)
+            saved_reports = saved_reports[: len(generated_reports)]
+
             reports = [MeetingReportResponse(**report) for report in saved_reports]
         else:
             reports = []
-        
-        return GenerateReportResponse(
-            reports=reports,
-            message=f"Generated {len(reports)} report(s) successfully"
-        )
-        
+
+        return GenerateReportResponse(reports=reports, message=f"Generated {len(reports)} report(s) successfully")
+
     except HTTPException:
         raise
     except Exception as e:
-        raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to generate report: {str(e)}"
-        )
+        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to generate report: {str(e)}")

--- app/meeting_prep/test/conftest.py
+++ app/meeting_prep/test/conftest.py
@@ -12,34 +12,17 @@
 def mock_calendar_event_google():
     meeting_time = datetime.now(timezone.utc) + timedelta(days=2)
     meeting_time_str = meeting_time.isoformat()
-    
+
     return {
         "id": "4vf3225cstqd85g676cf2ge52o",
         "summary": "Vision Edge One X Economic Group Pension Services",
-        "start": {
-            "dateTime": meeting_time_str,
-            "timeZone": "UTC"
-        },
-        "end": {
-            "dateTime": (meeting_time + timedelta(hours=1)).isoformat(),
-            "timeZone": "UTC"
-        },
+        "start": {"dateTime": meeting_time_str, "timeZone": "UTC"},
+        "end": {"dateTime": (meeting_time + timedelta(hours=1)).isoformat(), "timeZone": "UTC"},
         "attendees": [
-            {
-                "email": "ismael@visionedgeone.com",
-                "displayName": "Ismael",
-                "self": True
-            },
-            {
-                "email": "daniel.liss@economicgroup.com",
-                "displayName": "Daniel Liss",
-                "self": False
-            }
+            {"email": "ismael@visionedgeone.com", "displayName": "Ismael", "self": True},
+            {"email": "daniel.liss@economicgroup.com", "displayName": "Daniel Liss", "self": False},
         ],
-        "organizer": {
-            "email": "ismael@visionedgeone.com",
-            "displayName": "Ismael"
-        }
+        "organizer": {"email": "ismael@visionedgeone.com", "displayName": "Ismael"},
     }
 
 
@@ -47,51 +30,23 @@
 def mock_calendar_event_outlook():
     meeting_time = datetime.now(timezone.utc) + timedelta(days=2)
     meeting_time_str = meeting_time.isoformat()
-    
+
     return {
         "id": "4vf3225cstqd85g676cf2ge52o",
         "subject": "Vision Edge One X Economic Group Pension Services",
-        "start": {
-            "dateTime": meeting_time_str,
-            "timeZone": "UTC"
-        },
-        "end": {
-            "dateTime": (meeting_time + timedelta(hours=1)).isoformat(),
-            "timeZone": "UTC"
-        },
+        "start": {"dateTime": meeting_time_str, "timeZone": "UTC"},
+        "end": {"dateTime": (meeting_time + timedelta(hours=1)).isoformat(), "timeZone": "UTC"},
         "attendees": [
-            {
-                "emailAddress": {
-                    "address": "ismael@visionedgeone.com",
-                    "name": "Ismael"
-                },
-                "type": "organizer"
-            },
-            {
-                "emailAddress": {
-                    "address": "daniel.liss@economicgroup.com",
-                    "name": "Daniel Liss"
-                },
-                "type": "required"
-            }
+            {"emailAddress": {"address": "ismael@visionedgeone.com", "name": "Ismael"}, "type": "organizer"},
+            {"emailAddress": {"address": "daniel.liss@economicgroup.com", "name": "Daniel Liss"}, "type": "required"},
         ],
-        "organizer": {
-            "emailAddress": {
-                "address": "ismael@visionedgeone.com",
-                "name": "Ismael"
-            }
-        }
+        "organizer": {"emailAddress": {"address": "ismael@visionedgeone.com", "name": "Ismael"}},
     }
 
 
 @pytest.fixture
 def mock_gmail_threads():
-    return [
-        {
-            "id": "thread_123",
-            "snippet": "Introduction & Alignment with Your New Real Estate Mandate"
-        }
-    ]
+    return [{"id": "thread_123", "snippet": "Introduction & Alignment with Your New Real Estate Mandate"}]
 
 
 @pytest.fixture
@@ -109,12 +64,12 @@
                         {"name": "Subject", "value": "Introduction & Alignment with Your New Real Estate Mandate"},
                         {"name": "From", "value": "Ismael <ismael@visionedgeone.com>"},
                         {"name": "To", "value": "Daniel Liss <daniel.liss@economicgroup.com>"},
-                        {"name": "Date", "value": (now - timedelta(hours=20)).strftime("%a, %d %b %Y %H:%M:%S %z")}
+                        {"name": "Date", "value": (now - timedelta(hours=20)).strftime("%a, %d %b %Y %H:%M:%S %z")},
                     ],
                     "body": {
                         "data": "SGkgRGFuaWVsLA0KDQpJIGhvcGUgeW91J3JlIHdlbGwuDQoNCkknbSByZWFjaGluZyBvdXQgZnJvbSBWaXNpb24gRWRnZSBPbmUgYWZ0ZXIgcmVhZGluZyBhYm91dCBFY29ub21pYyBHcm91cCBQZW5zaW9uIFNlcnZpY2VzJyB1cGRhdGVkIG1hbmRhdGUgYW5kIGluY3JlYXNlZCBhbGxvY2F0aW9uIGZvY3VzIG9uIHJlYWwgZXN0YXRlIHN0cmF0ZWdpZXMuIEdpdmVuIHRoZSBzaGlmdCB0b3dhcmQgdmFsdWUtYWRkIGFuZCBpbmNvbWUtb3JpZW50ZWQgYXNzZXRzLCBJIGJlbGlldmUgb3VyIGN1cnJlbnQgZnVuZC1WaXNpb24gRWRnZSBSZWFsIEVzdGF0ZSBGdW5kIElJSS1tYXkgYWxpZ24gY2xvc2VseSB3aXRoIHdoYXQgeW91IGFyZSB0YXJnZXRpbmcgZm9yIDIwMjUtMjAyNyBkZXBsb3ltZW50Lg0KDQpXZSBhcmUgYWN0aXZlbHkgcmFpc2luZyBhbmQgYWxyZWFkeSBoYXZlIHN0cm9uZyBlYXJseSBjb21taXRtZW50cyBmcm9tIGEgbWl4IG9mIFVLIEFuZCBFdXJvcGVhbiBMUHMuIEknZCB3ZWxjb21lIHRoZSBvcHBvcnR1bml0eSB0byBpbnRyb2R1Y2Ugb3VyIHN0cmF0ZWd5LCBwaXBlbGluZSwgYW5kIHRyYWNrIHJlY29yZCB0byBzZWUgd2hldGhlciBpdCBjb3VsZCBiZSBhIGZpdC4NCg0KV291bGQgeW91IGJlIG9wZW4gdG8gYW4gaW5pdGlhbCBjb252ZXJzYXRpb24/DQoNCkJlc3QgcmVnYXJkcywNCg0KIEltYWVsDQogUGFydG5lciwgVmlzaW9uIEVkZ2UgT25l"
-                    }
-                }
+                    },
+                },
             },
             {
                 "id": "msg_2",
@@ -125,12 +80,12 @@
                         {"name": "Subject", "value": "Re: Introduction & Alignment with Your New Real Estate Mandate"},
                         {"name": "From", "value": "Daniel Liss <daniel.liss@economicgroup.com>"},
                         {"name": "To", "value": "Ismael <ismael@visionedgeone.com>"},
-                        {"name": "Date", "value": (now - timedelta(hours=18)).strftime("%a, %d %b %Y %H:%M:%S %z")}
+                        {"name": "Date", "value": (now - timedelta(hours=18)).strftime("%a, %d %b %Y %H:%M:%S %z")},
                     ],
                     "body": {
                         "data": "SGkgSXNtYWVsLA0KDQpUaGFua3MgZm9yIGdldHRpbmcgaW4gdG91Y2ggYW5kIGZvciB0aGUgdGhvdWdodGZ1bCBub3RlLg0KDQpZb3UncmUgcmlnaHQtLW91ciBuZXdseSB1cGRhdGVkIG1hbmRhdGUgcGxhY2VzIGdyZWF0ZXIgZW1waGFzaXMgb24gcmVhbCBlc3RhdGUsIGVzcGVjaWFsbHkgd2hlcmUgdGhlcmUncyBhIGNsZWFyIG9wZXJhdGlvbmFsIGltcHJvdmVtZW50IG9yIHN0YWJpbGlzZWQgY2FzaC1mbG93IGFuZ2xlLiBJbSBoYXBweSB0byBleHBsb3JlIHdoYXQgeW91J3JlIGRvaW5nIHdpdGggRnVuZCBJSUkgYW5kIHVuZGVyc3RhbmQgd2hlcmUgaXQgZGlmZmVycyBmcm9tIG1vcmUgdHJhZGl0aW9uYWwgdmFsdWUtYWRkIHN0cmF0ZWdpZXMgd2UndmUgc2VlbiBpbiB0aGUgbWFya2V0Lg0KDQpJZiB5b3UgY291bGQgc2hhcmUgYSBicmllZiBvdmVydmlldyBkZWNrIG9yIGtleSBoaWdobGlnaHRzLCBJIGNhbiByZXZpZXcgYWhlYWQgb2YgYSBkaXNjdXNzaW9uLg0KDQpCZXN0LA0KDQogRGFuaWVsDQogIEVjb25vbWljIEdyb3VwIFBlbnNpb24gU2VydmljZXM="
-                    }
-                }
+                    },
+                },
             },
             {
                 "id": "msg_3",
@@ -141,12 +96,12 @@
                         {"name": "Subject", "value": "Re: Introduction & Alignment with Your New Real Estate Mandate"},
                         {"name": "From", "value": "Ismael <ismael@visionedgeone.com>"},
                         {"name": "To", "value": "Daniel Liss <daniel.liss@economicgroup.com>"},
-                        {"name": "Date", "value": (now - timedelta(hours=16)).strftime("%a, %d %b %Y %H:%M:%S %z")}
+                        {"name": "Date", "value": (now - timedelta(hours=16)).strftime("%a, %d %b %Y %H:%M:%S %z")},
                     ],
                     "body": {
                         "data": "SGkgRGFuaWVsLA0KDQpBYnNvbHV0ZWx5LS1hdHRhY2hlZCBpcyBhIHNob3J0IG92ZXJ2aWV3IG9mIEZ1bmQgSUlJLCBpbmNsdWRpbmcgb3VyIGdlb2dyYXBoaWMgZm9jdXMsIGZvcndhcmQgcGlwZWxpbmUsIHVuZGVyd3JpdGluZyBhcHByb2FjaCwgYW5kIHJlYWxpc2VkIHJldHVybnMgZnJvbSBGdW5kcyBJICYgSUkuDQoNCkluIHN1bW1hcnksIG91ciBzdHJhdGVneSBsZWFucyBoZWF2aWx5IHRvd2FyZCBtaWQtbWFya2V0IGFzc2V0cyB3aXRoIHN0cm9uZyByZXBvc2l0aW9uaW5nIHBvdGVudGlhbCwgYW5kIHdlJ3ZlIGJlZW4gcGFydGljdWxhcmx5IGVmZmVjdGl2ZSBpbiB0aGUgbG9naXN0aWNzIGFuZCByZXNpZGVudGlhbCBzdWItc2VjdG9ycy4gSSB0aGluayB0aGUgY2FzaC1mbG93IHN0YWJpbGlzYXRpb24gcHJvZmlsZSBtYXkgYWxpZ24gd2VsbCB3aXRoIHdoYXQgeW91ciBtYW5kYXRlIG91dGxpbmVzLg0KDQpIYXBweSB0byB3YWxrIHlvdSB0aHJvdWdoIGFueXRoaW5nIGluIG1vcmUgZGV0YWlsLiBXb3VsZCBuZXh0IHdlZWsgc3VpdCBmb3IgYSBjYWxsPw0KDQpCZXN0IHJlZ2FyZHMsDQoNCiBJc21hZWw="
-                    }
-                }
+                    },
+                },
             },
             {
                 "id": "msg_4",
@@ -157,12 +112,12 @@
                         {"name": "Subject", "value": "Re: Introduction & Alignment with Your New Real Estate Mandate"},
                         {"name": "From", "value": "Daniel Liss <daniel.liss@economicgroup.com>"},
                         {"name": "To", "value": "Ismael <ismael@visionedgeone.com>"},
-                        {"name": "Date", "value": (now - timedelta(hours=12)).strftime("%a, %d %b %Y %H:%M:%S %z")}
+                        {"name": "Date", "value": (now - timedelta(hours=12)).strftime("%a, %d %b %Y %H:%M:%S %z")},
                     ],
                     "body": {
                         "data": "SGkgSXNtYWVsLA0KDQpUaGFua3MgZm9yIHNlbmRpbmcgdGhpcyBhY3Jvc3MtLXRoZSBvdmVydmlldyBpcyBoZWxwZnVsLCBhbmQgdGhlIHRyYWNrIHJlY29yZCBpcyBjbGVhci4NCg0KQSBjYWxsIG5leHQgd2VlayB3b3JrcyB3ZWxsIG9uIG15IGVuZC4gSSdtIGZyZWUgVHVlc2RheSBhZnRlcm5vb24gb3IgV2VkbmVzZGF5IG1vcm5pbmcgaWYgZWl0aGVyIHN1aXRzIHlvdXIgc2NoZWR1bGUuIExldCdzIHBsYW4gdG8gcnVuIHRocm91Z2ggRnVuZCBJSUksIHlvdXIgZGVwbG95bWVudCBwYWNpbmcsIGFuZCBob3cgeW91IHNlZSB0aGUgY3VycmVudCByZWFsIGVzdGF0ZSBsYW5kc2NhcGUgZXZvbHZpbmcuDQoNCkxldCBtZSBrbm93IHdoYXQgdGltZSB3b3JrcyBiZXN0Lg0KDQpCZXN0LA0KDQogRGFuaWVs"
-                    }
-                }
+                    },
+                },
             },
             {
                 "id": "msg_5",
@@ -173,14 +128,14 @@
                         {"name": "Subject", "value": "Re: Introduction & Alignment with Your New Real Estate Mandate"},
                         {"name": "From", "value": "Ismael <ismael@visionedgeone.com>"},
                         {"name": "To", "value": "Daniel Liss <daniel.liss@economicgroup.com>"},
-                        {"name": "Date", "value": (now - timedelta(hours=6)).strftime("%a, %d %b %Y %H:%M:%S %z")}
+                        {"name": "Date", "value": (now - timedelta(hours=6)).strftime("%a, %d %b %Y %H:%M:%S %z")},
                     ],
                     "body": {
                         "data": "SGkgRGFuaWVsLA0KDQpHcmVhdC0tV2VkbmVzZGF5IG1vcm5pbmcgd29ya3Mgd2VsbC4gSG93IGRvZXMgMTA6MDAgR01UIHNvdW5kPw0KDQpJJ2xsIHNlbmQgb3ZlciBhIGNhbGVuZGFyIGludml0ZSBzaG9ydGx5LiBMb29raW5nIGZvcndhcmQgdG8gdGhlIGRpc2N1c3Npb24gYW5kIGdpdmluZyB5b3UgYSBmdWxsIHRvdXIgb2YgRnVuZCBJSUkuDQoNCkJlc3QgcmVnYXJkcywNCg0KIEltYWVs"
-                    }
-                }
-            }
-        ]
+                    },
+                },
+            },
+        ],
     }
 
 
@@ -191,156 +146,90 @@
         {
             "id": "msg_1",
             "subject": "Introduction & Alignment with Your New Real Estate Mandate",
-            "from": {
-                "emailAddress": {
-                    "address": "ismael@visionedgeone.com",
-                    "name": "Ismael"
-                }
-            },
-            "toRecipients": [
-                {
-                    "emailAddress": {
-                        "address": "daniel.liss@economicgroup.com",
-                        "name": "Daniel Liss"
-                    }
-                }
-            ],
+            "from": {"emailAddress": {"address": "ismael@visionedgeone.com", "name": "Ismael"}},
+            "toRecipients": [{"emailAddress": {"address": "daniel.liss@economicgroup.com", "name": "Daniel Liss"}}],
             "bodyPreview": "I'm reaching out from Vision Edge One after reading about Economic Group Pension Services' updated mandate and increased allocation focus on real estate strategies. Given the shift toward value-add and income-oriented assets, I believe our current fund‚ÄîVision Edge Real Estate Fund III‚Äîmay align closely with what you are targeting for 2025‚Äì2027 deployment.",
-            "receivedDateTime": (now - timedelta(hours=20)).isoformat() + "Z"
+            "receivedDateTime": (now - timedelta(hours=20)).isoformat() + "Z",
         },
         {
             "id": "msg_2",
             "subject": "Re: Introduction & Alignment with Your New Real Estate Mandate",
-            "from": {
-                "emailAddress": {
-                    "address": "daniel.liss@economicgroup.com",
-                    "name": "Daniel Liss"
-                }
-            },
-            "toRecipients": [
-                {
-                    "emailAddress": {
-                        "address": "ismael@visionedgeone.com",
-                        "name": "Ismael"
-                    }
-                }
-            ],
+            "from": {"emailAddress": {"address": "daniel.liss@economicgroup.com", "name": "Daniel Liss"}},
+            "toRecipients": [{"emailAddress": {"address": "ismael@visionedgeone.com", "name": "Ismael"}}],
             "bodyPreview": "Thanks for getting in touch and for the thoughtful note. You're right‚Äîour newly updated mandate places greater emphasis on real estate, especially where there's a clear operational improvement or stabilised cash-flow angle. I'm happy to explore what you're doing with Fund III and understand where it differs from more traditional value-add strategies we've seen in the market.",
-            "receivedDateTime": (now - timedelta(hours=18)).isoformat() + "Z"
+            "receivedDateTime": (now - timedelta(hours=18)).isoformat() + "Z",
         },
         {
             "id": "msg_3",
             "subject": "Re: Introduction & Alignment with Your New Real Estate Mandate",
-            "from": {
-                "emailAddress": {
-                    "address": "ismael@visionedgeone.com",
-                    "name": "Ismael"
-                }
-            },
-            "toRecipients": [
-                {
-                    "emailAddress": {
-                        "address": "daniel.liss@economicgroup.com",
-                        "name": "Daniel Liss"
-                    }
-                }
-            ],
+            "from": {"emailAddress": {"address": "ismael@visionedgeone.com", "name": "Ismael"}},
+            "toRecipients": [{"emailAddress": {"address": "daniel.liss@economicgroup.com", "name": "Daniel Liss"}}],
             "bodyPreview": "Absolutely‚Äîattached is a short overview of Fund III, including our geographic focus, forward pipeline, underwriting approach, and realised returns from Funds I & II. In summary, our strategy leans heavily toward mid-market assets with strong repositioning potential, and we've been particularly effective in the logistics and residential sub-sectors.",
-            "receivedDateTime": (now - timedelta(hours=16)).isoformat() + "Z"
+            "receivedDateTime": (now - timedelta(hours=16)).isoformat() + "Z",
         },
         {
             "id": "msg_4",
             "subject": "Re: Introduction & Alignment with Your New Real Estate Mandate",
-            "from": {
-                "emailAddress": {
-                    "address": "daniel.liss@economicgroup.com",
-                    "name": "Daniel Liss"
-                }
-            },
-            "toRecipients": [
-                {
-                    "emailAddress": {
-                        "address": "ismael@visionedgeone.com",
-                        "name": "Ismael"
-                    }
-                }
-            ],
+            "from": {"emailAddress": {"address": "daniel.liss@economicgroup.com", "name": "Daniel Liss"}},
+            "toRecipients": [{"emailAddress": {"address": "ismael@visionedgeone.com", "name": "Ismael"}}],
             "bodyPreview": "Thanks for sending this across‚Äîthe overview is helpful, and the track record is clear. A call next week works well on my end. I'm free Tuesday afternoon or Wednesday morning if either suits your schedule. Let's plan to run through Fund III, your deployment pacing, and how you see the current real estate landscape evolving.",
-            "receivedDateTime": (now - timedelta(hours=12)).isoformat() + "Z"
+            "receivedDateTime": (now - timedelta(hours=12)).isoformat() + "Z",
         },
         {
             "id": "msg_5",
             "subject": "Re: Introduction & Alignment with Your New Real Estate Mandate",
-            "from": {
-                "emailAddress": {
-                    "address": "ismael@visionedgeone.com",
-                    "name": "Ismael"
-                }
-            },
-            "toRecipients": [
-                {
-                    "emailAddress": {
-                        "address": "daniel.liss@economicgroup.com",
-                        "name": "Daniel Liss"
-                    }
-                }
-            ],
+            "from": {"emailAddress": {"address": "ismael@visionedgeone.com", "name": "Ismael"}},
+            "toRecipients": [{"emailAddress": {"address": "daniel.liss@economicgroup.com", "name": "Daniel Liss"}}],
             "bodyPreview": "Great‚ÄîWednesday morning works well. How does 10:00 GMT sound? I'll send over a calendar invite shortly. Looking forward to the discussion and giving you a full tour of Fund III.",
-            "receivedDateTime": (now - timedelta(hours=6)).isoformat() + "Z"
-        }
+            "receivedDateTime": (now - timedelta(hours=6)).isoformat() + "Z",
+        },
     ]
-    
 
 
 @pytest.fixture
 def mock_google_email_calendar(mock_calendar_event_google, mock_gmail_threads, mock_gmail_thread):
     async def mock_calendar_func(*args, **kwargs):
         return [mock_calendar_event_google]
-    
+
     async def mock_threads_func(*args, **kwargs):
         return mock_gmail_threads
-    
+
     async def mock_thread_func(*args, **kwargs):
         return mock_gmail_thread
-    
+
     mock_token = AsyncMock(return_value="mock_token")
-    
-    with patch("app.google.oauth.get_valid_google_access_token", mock_token), \
-         patch("app.google.calendar.list_google_calendar_events", side_effect=mock_calendar_func) as mock_calendar, \
-         patch("app.google.email.list_gmail_threads", side_effect=mock_threads_func) as mock_threads, \
-         patch("app.google.email.get_gmail_thread", side_effect=mock_thread_func) as mock_thread, \
-         patch("app.meeting_prep.core.data_collector.list_google_calendar_events", side_effect=mock_calendar_func), \
-         patch("app.meeting_prep.email.collector.list_gmail_threads", side_effect=mock_threads_func), \
-         patch("app.meeting_prep.email.collector.get_gmail_thread", side_effect=mock_thread_func):
-        
-        yield {
-            "calendar": mock_calendar,
-            "threads": mock_threads,
-            "thread": mock_thread
-        }
 
+    with (
+        patch("app.google.oauth.get_valid_google_access_token", mock_token),
+        patch("app.google.calendar.list_google_calendar_events", side_effect=mock_calendar_func) as mock_calendar,
+        patch("app.google.email.list_gmail_threads", side_effect=mock_threads_func) as mock_threads,
+        patch("app.google.email.get_gmail_thread", side_effect=mock_thread_func) as mock_thread,
+        patch("app.meeting_prep.core.data_collector.list_google_calendar_events", side_effect=mock_calendar_func),
+        patch("app.meeting_prep.email.collector.list_gmail_threads", side_effect=mock_threads_func),
+        patch("app.meeting_prep.email.collector.get_gmail_thread", side_effect=mock_thread_func),
+    ):
+        yield {"calendar": mock_calendar, "threads": mock_threads, "thread": mock_thread}
+
 
 @pytest.fixture
 def mock_outlook_email_calendar(mock_calendar_event_outlook, mock_outlook_messages):
     async def mock_calendar_func(*args, **kwargs):
         return [mock_calendar_event_outlook]
-    
+
     async def mock_messages_func(*args, **kwargs):
         return mock_outlook_messages
-    
+
     mock_token = AsyncMock(return_value="mock_token")
-    
-    with patch("app.outlook.oauth.get_valid_outlook_access_token", mock_token), \
-         patch("app.outlook.calendar.list_outlook_calendar_events", side_effect=mock_calendar_func) as mock_calendar, \
-         patch("app.outlook.email.list_outlook_messages", side_effect=mock_messages_func) as mock_messages, \
-         patch("app.meeting_prep.core.data_collector.list_outlook_calendar_events", side_effect=mock_calendar_func), \
-         patch("app.meeting_prep.email.collector.list_outlook_messages", side_effect=mock_messages_func) as mock_service_messages:
-        
-        yield {
-            "calendar": mock_calendar,
-            "messages": mock_messages
-        }
 
+    with (
+        patch("app.outlook.oauth.get_valid_outlook_access_token", mock_token),
+        patch("app.outlook.calendar.list_outlook_calendar_events", side_effect=mock_calendar_func) as mock_calendar,
+        patch("app.outlook.email.list_outlook_messages", side_effect=mock_messages_func) as mock_messages,
+        patch("app.meeting_prep.core.data_collector.list_outlook_calendar_events", side_effect=mock_calendar_func),
+        patch("app.meeting_prep.email.collector.list_outlook_messages", side_effect=mock_messages_func) as mock_service_messages,
+    ):
+        yield {"calendar": mock_calendar, "messages": mock_messages}
+
 
 @pytest_asyncio.fixture
 async def test_user_id():
@@ -349,7 +238,7 @@
     Returns the user_id that can be used in tests.
     """
     user_id = f"test_user_{uuid.uuid4().hex[:8]}"
-    
+
     try:
         db = await get_global_db()
         if db:
@@ -368,14 +257,14 @@
                     "Test Firm",  # Required field
                 )
     except Exception as e:
-        # If database is not available or user creation fails, 
+        # If database is not available or user creation fails,
         # tests that don't require DB will still work
         import logging
+
         logger = logging.getLogger(__name__)
         logger.warning(f"Failed to create test user {user_id}: {e}")
-    
+
     yield user_id
-    
+
     # Cleanup: optionally delete test user (but not required for test isolation)
     # We leave it for now to avoid cleanup complexity
-

--- app/meeting_prep/test/test_meeting_prep_service.py
+++ app/meeting_prep/test/test_meeting_prep_service.py
@@ -17,10 +17,13 @@
 
 
 @pytest.mark.asyncio
-@pytest.mark.parametrize("integration_type,event_fixture,email_fixture", [
-    ("google", "mock_calendar_event_google", "mock_gmail_thread"),
-    ("outlook", "mock_calendar_event_outlook", "mock_outlook_messages"),
-])
+@pytest.mark.parametrize(
+    "integration_type,event_fixture,email_fixture",
+    [
+        ("google", "mock_calendar_event_google", "mock_gmail_thread"),
+        ("outlook", "mock_calendar_event_outlook", "mock_outlook_messages"),
+    ],
+)
 async def test_structured_report_e2e_integration(
     integration_type,
     event_fixture,
@@ -32,9 +35,9 @@
 ):
     event = request.getfixturevalue(event_fixture)
     email_data = request.getfixturevalue(email_fixture)
-    
+
     events = [event]
-    
+
     emails = []
     if isinstance(email_data, dict):
         messages = email_data.get("messages", [])
@@ -45,37 +48,33 @@
         emails = email_data
     else:
         emails = []
-    
+
     assert len(events) == 1
     assert len(emails) > 0
-    
+
     important_events = await identify_important_events(events)
     assert len(important_events) > 0
-    
+
     event = important_events[0]
     meeting_id = event.get("event_id", event.get("id", ""))
     assert meeting_id
-    
-    report_content = await prepare_meeting_report(
-        user_id=test_user_id,
-        meeting_id=meeting_id,
-        integration_type=integration_type
-    )
-    
+
+    report_content = await prepare_meeting_report(user_id=test_user_id, meeting_id=meeting_id, integration_type=integration_type)
+
     assert report_content is not None
     assert isinstance(report_content, str)
     assert len(report_content) > 0
-    
+
     assert "##" in report_content or "#" in report_content
-    
-    print("\n" + "="*80)
+
+    print("\n" + "=" * 80)
     print(f"MEETING PREP REPORT (MARKDOWN) - {integration_type.upper()} INTEGRATION (E2E)")
-    print("="*80)
+    print("=" * 80)
     print(report_content)
-    print("="*80 + "\n")
-    
+    print("=" * 80 + "\n")
+
     report_lower = report_content.lower()
-    
+
     section_indicators = [
         "meeting",
         "overview",
@@ -88,7 +87,7 @@
         "objectives",
         "communication",
     ]
-    
+
     found_sections = sum(1 for indicator in section_indicators if indicator in report_lower)
     assert found_sections >= 3, f"Report should contain at least 3 key sections, found: {found_sections}"
 
@@ -96,35 +95,29 @@
 @pytest.mark.asyncio
 async def test_identify_important_events(mock_calendar_event_google):
     events = [mock_calendar_event_google]
-    
+
     important_events = await identify_important_events(events)
-    
+
     assert len(important_events) > 0
-    assert important_events[0].get('event_id') or important_events[0].get('id')
-    assert important_events[0].get('title') or important_events[0].get('summary')
-    assert 'participants' in important_events[0]
+    assert important_events[0].get("event_id") or important_events[0].get("id")
+    assert important_events[0].get("title") or important_events[0].get("summary")
+    assert "participants" in important_events[0]
 
 
 @pytest.mark.asyncio
 async def test_research_participants(test_user_id):
     meeting_data = {
-        'meeting_id': 'test_meeting_123',
-        'title': 'Test Meeting',
-        'participants': [
-            {
-                'name': 'John Doe',
-                'email': 'john.doe@example.com',
-                'is_external': True
-            }
-        ],
-        'organizer_email': 'user@example.com',
+        "meeting_id": "test_meeting_123",
+        "title": "Test Meeting",
+        "participants": [{"name": "John Doe", "email": "john.doe@example.com", "is_external": True}],
+        "organizer_email": "user@example.com",
     }
-    
+
     participant_research = await collect_intelligence_data(meeting_data, test_user_id)
-    
-    assert 'firm_intel' in participant_research
-    assert 'person_intel' in participant_research
 
+    assert "firm_intel" in participant_research
+    assert "person_intel" in participant_research
+
 
 @pytest.mark.asyncio
 async def test_generate_report():
@@ -138,24 +131,24 @@
         MeetingObjectivesModel,
         CommunicationStyleModel,
     )
-    
+
     meeting_metadata = MeetingMetadataModel(
         meeting_title="Test Meeting",
         meeting_date="2025-12-26",
         meeting_time="2025-12-26T10:00:00Z",
         meeting_duration_minutes=30,
     )
-    
+
     executive_summary = ExecutiveSummaryModel(
         tldr="Test meeting for verification",
         key_opportunity="Testing the report generation",
         recommended_talking_points=["Point 1", "Point 2"],
     )
-    
+
     relationship_history = RelationshipHistoryModel(
         relationship_temperature="warm",
     )
-    
+
     report_content = await generate_report(
         meeting_metadata=meeting_metadata,
         executive_summary=executive_summary,
@@ -167,35 +160,35 @@
         communication_style=None,
         related_emails=None,
     )
-    
+
     assert report_content
     assert len(report_content) > 0
     assert "#" in report_content or "Meeting" in report_content or "Test" in report_content
-    
-    print("\n" + "="*80)
+
+    print("\n" + "=" * 80)
     print("GENERATED REPORT - INDIVIDUAL FUNCTION TEST")
-    print("="*80)
+    print("=" * 80)
     print(report_content)
-    print("="*80 + "\n")
+    print("=" * 80 + "\n")
 
 
 @pytest.mark.asyncio
 async def test_error_handling_chatgpt_fails(mock_calendar_event_google):
     events = [mock_calendar_event_google]
-    
-    with patch('app.meeting_prep.core.event_processor.ChatOpenAI') as mock_chat:
+
+    with patch("app.meeting_prep.core.event_processor.ChatOpenAI") as mock_chat:
         mock_chat.return_value.ainvoke.side_effect = Exception("API Error")
-        
+
         important_events = await identify_important_events(events)
-        
+
         assert isinstance(important_events, list)
 
 
 @pytest.mark.asyncio
 async def test_error_handling_email_calendar_access_fails():
-    with patch('app.meeting_prep.core.data_collector.list_google_calendar_events') as mock_calendar:
+    with patch("app.meeting_prep.core.data_collector.list_google_calendar_events") as mock_calendar:
         mock_calendar.side_effect = Exception("Access denied")
-        
+
         try:
             events = await collect_calendar_events("test_user", "google")
             assert events == []
@@ -216,25 +209,21 @@
     test_user_id,
 ):
     event = mock_calendar_event_google if integration_type == "google" else mock_calendar_event_outlook
-    
+
     meeting_id = event.get("id", "")
     assert meeting_id
-    
-    report_content = await prepare_meeting_report(
-        user_id=test_user_id,
-        meeting_id=meeting_id,
-        integration_type=integration_type
-    )
-    
+
+    report_content = await prepare_meeting_report(user_id=test_user_id, meeting_id=meeting_id, integration_type=integration_type)
+
     assert report_content is not None
     assert isinstance(report_content, str)
     assert len(report_content) > 0
-    
-    print("\n" + "="*80)
+
+    print("\n" + "=" * 80)
     print(f"COMPLETE MEETING PREP REPORT (MARKDOWN) - FULL ORCHESTRATION ({integration_type.upper()})")
-    print("="*80)
+    print("=" * 80)
     print(report_content)
-    print("="*80 + "\n")
-    
+    print("=" * 80 + "\n")
+
     assert "#" in report_content or "##" in report_content
     assert len(report_content.strip()) > 100

--- app/meeting_prep/test/test_perplexity_repository.py
+++ app/meeting_prep/test/test_perplexity_repository.py
@@ -12,8 +12,10 @@
 class MockAsyncContextManager:
     def __init__(self, return_value):
         self.return_value = return_value
+
     async def __aenter__(self):
         return self.return_value
+
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         return None
 
@@ -22,27 +24,27 @@
 async def test_insert_perplexity_result():
     mock_db = Mock()
     mock_conn = Mock()
-    mock_row = {'id': 1}
+    mock_row = {"id": 1}
     mock_conn.fetchrow = AsyncMock(return_value=mock_row)
-    
+
     mock_pool = Mock()
     mock_pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
     mock_db.pool = mock_pool
-    
+
     async def mock_get_db():
         return mock_db
-    
-    with patch('app.meeting_prep.repositories.perplexity_repository.get_global_db', side_effect=mock_get_db):
+
+    with patch("app.meeting_prep.repositories.perplexity_repository.get_global_db", side_effect=mock_get_db):
         query_id = await insert_perplexity_result(
-            user_id='test_user',
-            meeting_id='test_meeting',
-            section='firm_intel',
-            query_text='Test query',
-            model='sonar-pro',
-            raw_response='Test response',
-            parsed_response={'key': 'value'},
+            user_id="test_user",
+            meeting_id="test_meeting",
+            section="firm_intel",
+            query_text="Test query",
+            model="sonar-pro",
+            raw_response="Test response",
+            parsed_response={"key": "value"},
         )
-        
+
         assert query_id == 1
         mock_conn.fetchrow.assert_called_once()
 
@@ -53,35 +55,35 @@
     mock_conn = Mock()
     mock_rows = [
         {
-            'id': 1,
-            'user_id': 'test_user',
-            'meeting_id': 'test_meeting',
-            'section': 'firm_intel',
-            'query_text': 'Test query',
-            'model': 'sonar-pro',
-            'raw_response': 'Test response',
-            'parsed_response': {'key': 'value'},
-            'error_message': None,
-            'created_at': datetime.now(),
+            "id": 1,
+            "user_id": "test_user",
+            "meeting_id": "test_meeting",
+            "section": "firm_intel",
+            "query_text": "Test query",
+            "model": "sonar-pro",
+            "raw_response": "Test response",
+            "parsed_response": {"key": "value"},
+            "error_message": None,
+            "created_at": datetime.now(),
         }
     ]
     mock_conn.fetch = AsyncMock(return_value=mock_rows)
-    
+
     mock_pool = Mock()
     mock_pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
     mock_db.pool = mock_pool
-    
+
     async def mock_get_db():
         return mock_db
-    
-    with patch('app.meeting_prep.repositories.perplexity_repository.get_global_db', side_effect=mock_get_db):
+
+    with patch("app.meeting_prep.repositories.perplexity_repository.get_global_db", side_effect=mock_get_db):
         results = await get_perplexity_results_for_meeting(
-            user_id='test_user',
-            meeting_id='test_meeting',
+            user_id="test_user",
+            meeting_id="test_meeting",
         )
-        
+
         assert len(results) == 1
-        assert results[0]['section'] == 'firm_intel'
+        assert results[0]["section"] == "firm_intel"
 
 
 @pytest.mark.asyncio
@@ -90,19 +92,19 @@
     mock_conn = Mock()
     mock_rows = []
     mock_conn.fetch = AsyncMock(return_value=mock_rows)
-    
+
     mock_pool = Mock()
     mock_pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
     mock_db.pool = mock_pool
-    
+
     async def mock_get_db():
         return mock_db
-    
-    with patch('app.meeting_prep.repositories.perplexity_repository.get_global_db', side_effect=mock_get_db):
+
+    with patch("app.meeting_prep.repositories.perplexity_repository.get_global_db", side_effect=mock_get_db):
         results = await get_perplexity_results_by_section(
-            user_id='test_user',
-            section='firm_intel',
+            user_id="test_user",
+            section="firm_intel",
             limit=10,
         )
-        
+
         assert isinstance(results, list)

--- app/meeting_prep/test/test_perplexity_service.py
+++ app/meeting_prep/test/test_perplexity_service.py
@@ -13,82 +13,84 @@
 async def test_query_perplexity_success():
     mock_response = MagicMock()
     mock_response.content = '{"result": "test"}'
-    
+
     mock_chat = AsyncMock()
     mock_chat.ainvoke = AsyncMock(return_value=mock_response)
-    
-    with patch('app.meeting_prep.intelligence.perplexity_client.get_perplexity_api_key', return_value='test_key'), \
-         patch('app.meeting_prep.intelligence.perplexity_client.ChatPerplexity', return_value=mock_chat), \
-         patch('app.meeting_prep.intelligence.perplexity_client.insert_perplexity_result', new_callable=AsyncMock) as mock_insert:
-        
+
+    with (
+        patch("app.meeting_prep.intelligence.perplexity_client.get_perplexity_api_key", return_value="test_key"),
+        patch("app.meeting_prep.intelligence.perplexity_client.ChatPerplexity", return_value=mock_chat),
+        patch("app.meeting_prep.intelligence.perplexity_client.insert_perplexity_result", new_callable=AsyncMock) as mock_insert,
+    ):
         mock_insert.return_value = 1
-        
+
         result = await query_perplexity(
-            section='test_section',
-            query_text='Test query',
-            user_id='test_user',
-            meeting_id='test_meeting',
+            section="test_section",
+            query_text="Test query",
+            user_id="test_user",
+            meeting_id="test_meeting",
             parse_json=True,
         )
-        
-        assert result['raw_response'] == '{"result": "test"}'
-        assert result['parsed_response'] == {'result': 'test'}
-        assert result['query_id'] == 1
-        assert result['error'] is None
 
+        assert result["raw_response"] == '{"result": "test"}'
+        assert result["parsed_response"] == {"result": "test"}
+        assert result["query_id"] == 1
+        assert result["error"] is None
+
 
 @pytest.mark.asyncio
 async def test_query_perplexity_failure():
-    with patch('app.meeting_prep.intelligence.perplexity_client.get_perplexity_api_key', return_value='test_key'), \
-         patch('app.meeting_prep.intelligence.perplexity_client.ChatPerplexity', side_effect=Exception('API error')), \
-         patch('app.meeting_prep.intelligence.perplexity_client.insert_perplexity_result', new_callable=AsyncMock) as mock_insert:
-        
+    with (
+        patch("app.meeting_prep.intelligence.perplexity_client.get_perplexity_api_key", return_value="test_key"),
+        patch("app.meeting_prep.intelligence.perplexity_client.ChatPerplexity", side_effect=Exception("API error")),
+        patch("app.meeting_prep.intelligence.perplexity_client.insert_perplexity_result", new_callable=AsyncMock) as mock_insert,
+    ):
         mock_insert.return_value = 1
-        
+
         result = await query_perplexity(
-            section='test_section',
-            query_text='Test query',
-            user_id='test_user',
+            section="test_section",
+            query_text="Test query",
+            user_id="test_user",
         )
-        
-        assert result['error'] == 'API error'
-        assert result['raw_response'] is None
 
+        assert result["error"] == "API error"
+        assert result["raw_response"] is None
+
 
 @pytest.mark.asyncio
 async def test_query_firm_intelligence():
     mock_result = {
-        'raw_response': '{"aum": {"total_aum": "100M"}}',
-        'parsed_response': {'aum': {'total_aum': '100M'}},
-        'query_id': 1,
-        'error': None,
+        "raw_response": '{"aum": {"total_aum": "100M"}}',
+        "parsed_response": {"aum": {"total_aum": "100M"}},
+        "query_id": 1,
+        "error": None,
     }
-    
-    with patch('app.meeting_prep.intelligence.perplexity_client.query_perplexity', new_callable=AsyncMock, return_value=mock_result):
+
+    with patch("app.meeting_prep.intelligence.perplexity_client.query_perplexity", new_callable=AsyncMock, return_value=mock_result):
         result = await query_firm_intelligence(
-            firm_name='Test Firm',
-            user_id='test_user',
-            meeting_id='test_meeting',
+            firm_name="Test Firm",
+            user_id="test_user",
+            meeting_id="test_meeting",
         )
-        
-        assert 'aum' in result
 
+        assert "aum" in result
+
 
 @pytest.mark.asyncio
 async def test_query_person_intelligence():
     mock_result = {
-        'raw_response': '{"basic_info": {"full_name": "Test Person"}}',
-        'parsed_response': {'basic_info': {'full_name': 'Test Person', 'email': 'test@example.com'}},
-        'query_id': 1,
-        'error': None,
+        "raw_response": '{"basic_info": {"full_name": "Test Person"}}',
+        "parsed_response": {"basic_info": {"full_name": "Test Person", "email": "test@example.com"}},
+        "query_id": 1,
+        "error": None,
     }
-    
-    with patch('app.meeting_prep.intelligence.perplexity_client.query_perplexity', new_callable=AsyncMock, return_value=mock_result):
+
+    with patch("app.meeting_prep.intelligence.perplexity_client.query_perplexity", new_callable=AsyncMock, return_value=mock_result):
         result = await query_person_intelligence(
-            person_name='Test Person',
-            email='test@example.com',
-            user_id='test_user',
-            meeting_id='test_meeting',
+            person_name="Test Person",
+            email="test@example.com",
+            user_id="test_user",
+            meeting_id="test_meeting",
         )
-        
-        assert 'basic_info' in result
+
+        assert "basic_info" in result

--- app/meeting_prep/validation/validators.py
+++ app/meeting_prep/validation/validators.py
@@ -4,7 +4,7 @@
 def validate_calendar_event(event: Dict[str, Any]) -> bool:
     if not isinstance(event, dict):
         return False
-    if not event.get('id') and not event.get('event_id'):
+    if not event.get("id") and not event.get("event_id"):
         return False
     return True
 
@@ -12,7 +12,7 @@
 def validate_email_message(email: Dict[str, Any]) -> bool:
     if not isinstance(email, dict):
         return False
-    if not email.get('id') and not email.get('msg_id'):
+    if not email.get("id") and not email.get("msg_id"):
         return False
     return True
 
@@ -20,7 +20,6 @@
 def validate_meeting_data(meeting_data: Dict[str, Any]) -> bool:
     if not isinstance(meeting_data, dict):
         return False
-    if not meeting_data.get('meeting_id'):
+    if not meeting_data.get("meeting_id"):
         return False
     return True
-

--- app/middleware/request_logging_middleware.py
+++ app/middleware/request_logging_middleware.py
@@ -16,94 +16,73 @@
 
 logger = get_logger(__name__)
 
+
 class RequestLoggingMiddleware(BaseHTTPMiddleware):
     """
     Middleware to log HTTP requests and responses with structured data.
     Captures timing, status codes, user context, and security-relevant information.
     """
-    
+
     def __init__(self, app, log_request_body: bool = False, log_response_body: bool = False):
         super().__init__(app)
         self.log_request_body = log_request_body
         self.log_response_body = log_response_body
-        
+
         # Paths to exclude from detailed logging (health checks, static files, etc.)
-        self.exclude_paths = {
-            "/",
-            "/health",
-            "/metrics",
-            "/favicon.ico"
-        }
-        
+        self.exclude_paths = {"/", "/health", "/metrics", "/favicon.ico"}
+
         # Sensitive headers to redact
-        self.sensitive_headers = {
-            "authorization",
-            "cookie",
-            "x-api-key",
-            "x-auth-token"
-        }
-    
+        self.sensitive_headers = {"authorization", "cookie", "x-api-key", "x-auth-token"}
+
     async def dispatch(self, request: Request, call_next):
         """Process request and response with logging."""
         # Generate request ID for tracing
         request_id = str(uuid.uuid4())[:8]
-        
+
         # Skip detailed logging for excluded paths
         if request.url.path in self.exclude_paths:
             return await call_next(request)
-        
+
         # Start timing
         start_time = time.time()
-        
+
         # Extract request context
         request_context = await self._extract_request_context(request, request_id)
-        
+
         # Log request start
         start_log_level = "info"
         if request.method.upper() == "GET":
             start_log_level = "debug"
-        log_with_context(
-            logger,
-            start_log_level,
-            "Request started",
-            **request_context,
-            event_type="request_start"
-        )
-        
+        log_with_context(logger, start_log_level, "Request started", **request_context, event_type="request_start")
+
         # Process request
         try:
             response = await call_next(request)
-            
+
             # Calculate processing time
             process_time = time.time() - start_time
-            
+
             # Extract response context
             response_context = await self._extract_response_context(response, process_time)
-            
+
             # Combine contexts
             full_context = {**request_context, **response_context}
-            
+
             # Determine log level based on status code
             log_level = self._get_log_level(response.status_code)
-            
+
             # Log request completion
-            log_with_context(
-                logger,
-                log_level,
-                "Request completed",
-                **full_context,
-                event_type="request_complete"
-            )
-            
+            log_with_context(logger, log_level, "Request completed", **full_context, event_type="request_complete")
+
             # Log security events if applicable
             await self._log_security_events(request, response, full_context)
-            
+
             return response
-            
+
         except Exception as e:
             # Calculate processing time for failed requests
             process_time = time.time() - start_time
-            
+
             # Log request failure
             log_with_context(
                 logger,
@@ -113,11 +92,11 @@
                 error_type=type(e).__name__,
                 error_message=str(e),
                 process_time_seconds=process_time,
-                event_type="request_error"
+                event_type="request_error",
             )
-            
+
             raise
-    
+
     async def _extract_request_context(self, request: Request, request_id: str) -> Dict[str, Any]:
         """Extract structured context from the request."""
         # Get user context if available
@@ -126,18 +105,18 @@
             user_context = {
                 "user_id": getattr(request.state.user, "user_id", None),
                 "email": getattr(request.state.user, "email", None),
-                "is_admin": getattr(request.state.user, "is_admin", False)
+                "is_admin": getattr(request.state.user, "is_admin", False),
             }
-        
+
         # Get client information
         client_ip = self._get_client_ip(request)
-        
+
         # Redact sensitive headers
         headers = self._redact_sensitive_headers(dict(request.headers))
-        
+
         # Extract query parameters
         query_params = dict(request.query_params) if request.query_params else {}
-        
+
         context = {
             "request_id": request_id,
             "method": request.method,
@@ -147,9 +126,9 @@
             "user_agent": request.headers.get("user-agent"),
             "headers": headers,
             "query_params": query_params,
-            **user_context
+            **user_context,
         }
-        
+
         # Add request body if enabled (be careful with sensitive data)
         if self.log_request_body and request.method in ["POST", "PUT", "PATCH"]:
             try:
@@ -168,22 +147,19 @@
             except Exception:
                 # Don't fail request if body logging fails
                 pass
-        
+
         return context
-    
+
     async def _extract_response_context(self, response: Response, process_time: float) -> Dict[str, Any]:
         """Extract structured context from the response."""
-        context = {
-            "status_code": response.status_code,
-            "process_time_seconds": round(process_time, 4)
-        }
-        
+        context = {"status_code": response.status_code, "process_time_seconds": round(process_time, 4)}
+
         # Add response headers (excluding sensitive ones)
         if hasattr(response, "headers"):
             context["response_headers"] = self._redact_sensitive_headers(dict(response.headers))
-        
+
         return context
-    
+
     def _get_client_ip(self, request: Request) -> str:
         """Extract client IP address, considering proxies."""
         # Check for forwarded headers (common in load balancers/proxies)
@@ -191,17 +167,17 @@
         if forwarded_for:
             # Take the first IP in the chain
             return forwarded_for.split(",")[0].strip()
-        
+
         real_ip = request.headers.get("x-real-ip")
         if real_ip:
             return real_ip
-        
+
         # Fallback to direct client IP
         if hasattr(request, "client") and request.client:
             return request.client.host
-        
+
         return "unknown"
-    
+
     def _redact_sensitive_headers(self, headers: Dict[str, str]) -> Dict[str, str]:
         """Redact sensitive header values."""
         redacted = {}
@@ -215,14 +191,11 @@
             else:
                 redacted[key] = value
         return redacted
-    
+
     def _redact_sensitive_body_fields(self, body: Dict[str, Any]) -> Dict[str, Any]:
         """Redact sensitive fields from request body."""
-        sensitive_fields = {
-            "password", "token", "secret", "key", "authorization",
-            "current_password", "new_password", "confirm_password"
-        }
-        
+        sensitive_fields = {"password", "token", "secret", "key", "authorization", "current_password", "new_password", "confirm_password"}
+
         redacted = {}
         for key, value in body.items():
             if key.lower() in sensitive_fields:
@@ -231,9 +204,9 @@
                 redacted[key] = self._redact_sensitive_body_fields(value)
             else:
                 redacted[key] = value
-        
+
         return redacted
-    
+
     def _get_log_level(self, status_code: int) -> str:
         """Determine appropriate log level based on HTTP status code."""
         if status_code < 400:
@@ -242,51 +215,23 @@
             return "warning"
         else:
             return "error"
-    
+
     async def _log_security_events(self, request: Request, response: Response, context: Dict[str, Any]):
         """Log security-relevant events."""
         status_code = response.status_code
-        
+
         # Log authentication failures
         if status_code == 401:
-            log_with_context(
-                logger,
-                "warning",
-                "Authentication failure",
-                **context,
-                security_event=True,
-                event_type="auth_failure"
-            )
-        
+            log_with_context(logger, "warning", "Authentication failure", **context, security_event=True, event_type="auth_failure")
+
         # Log authorization failures
         elif status_code == 403:
-            log_with_context(
-                logger,
-                "warning",
-                "Authorization failure",
-                **context,
-                security_event=True,
-                event_type="authz_failure"
-            )
-        
+            log_with_context(logger, "warning", "Authorization failure", **context, security_event=True, event_type="authz_failure")
+
         # Log rate limiting
         elif status_code == 429:
-            log_with_context(
-                logger,
-                "warning",
-                "Rate limit exceeded",
-                **context,
-                security_event=True,
-                event_type="rate_limit_exceeded"
-            )
-        
+            log_with_context(logger, "warning", "Rate limit exceeded", **context, security_event=True, event_type="rate_limit_exceeded")
+
         # Log server errors (potential security issues)
         elif status_code >= 500:
-            log_with_context(
-                logger,
-                "error",
-                "Server error occurred",
-                **context,
-                security_event=True,
-                event_type="server_error"
-            ) 
+            log_with_context(logger, "error", "Server error occurred", **context, security_event=True, event_type="server_error")

--- app/models/api_models.py
+++ app/models/api_models.py
@@ -7,13 +7,13 @@
    - Client sends search criteria via CompanySearchRequest
    - Backend executes search using CompanySearchAgent
    - Returns CompanySearchResponse with list of matching companies
-   
+
 2. Company Selection:
    - Client selects a specific company from search results
    - Sends CompanySelectionRequest with company index
    - Backend initiates enrichment process
    - Returns EnrichmentInitiationResponse
-   
+
 3. Enrichment Status Tracking:
    - Client polls status using EnrichmentStatusRequest
    - Backend checks progress via ProcessingStatusResponse
@@ -80,26 +80,31 @@
 from typing import Optional, List, Dict
 from datetime import datetime
 
+
 # Session Models
 class SessionStartRequest(BaseModel):
     user_id: str
 
+
 class SessionStartResponse(BaseModel):
     user_id: str
     session_id: str
     start_time: datetime
 
+
 # Processing Models
 class ProcessingRequest(BaseModel):
     user_id: str
     session_id: str  # Required for all runs
     prompt: str
 
+
 class RIAProcessingRequest(BaseModel):
     user_id: str
     session_id: str
     crd_number: str
 
+
 # This is returned immediately when a processing request is initiated
 class ProcessingResponse(BaseModel):
     user_id: str
@@ -107,6 +112,7 @@
     status: str
     workflow_type: str  # 'specific_company', 'general_search', or 'off_topic'
 
+
 # This is used for status updates and contains more detailed information:
 class ProcessingStatusResponse(BaseModel):
     user_id: str
@@ -130,6 +136,7 @@
     ria_summary: Optional[Dict] = None
     riaSummary: Optional[Dict] = None
 
+
 # Search Results Models
 class CompanySearchResult(BaseModel):
     company_id: str
@@ -151,6 +158,7 @@
     preferred_geography: Optional[str] = None
     investor_type: Optional[str] = None
 
+
 class SearchResultsResponse(BaseModel):
     user_id: str
     run_id: str
@@ -158,6 +166,7 @@
     total_count: int
     original_prompt: Optional[str] = None
 
+
 # Enrichment Results Models
 class CompanyEnrichmentResponse(BaseModel):
     user_id: str
@@ -172,6 +181,7 @@
     youtube_media: Optional[Dict] = None
     youtubeMedia: Optional[Dict] = None
 
+
 class PersonEnrichmentResponse(BaseModel):
     user_id: str
     run_id: str
@@ -180,8 +190,10 @@
     execution_time_ms: Optional[int]
     narrative: Optional[Dict] = None
 
+
 class CompanyFeedbackRequest(BaseModel):
     """Request model for company search feedback."""
+
     run_id: str
     user_id: str
     feedback_type: str  # 'thumbs_up', 'thumbs_down', 'saved', 'unsaved'
@@ -196,4 +208,4 @@
     category: Optional[str] = None
     session_id: Optional[str] = None
     run_id: Optional[str] = None
-    metadata: Optional[dict] = None 
+    metadata: Optional[dict] = None

--- app/models/auth.py
+++ app/models/auth.py
@@ -10,6 +10,7 @@
 
 class UserBase(BaseModel):
     """Base user model with common fields."""
+
     email: EmailStr
     firm_name: str
     user_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
@@ -17,6 +18,7 @@
 
 class UserResponse(UserBase):
     """User data returned to frontend (excludes password)."""
+
     first_name: Optional[str] = None
     last_name: Optional[str] = None
     city: Optional[str] = None
@@ -28,11 +30,13 @@
     registration_attempts: Optional[int] = None
     is_admin: Optional[bool] = None
 
+
 MAX_PROFILE_DOCUMENT_BYTES = 15 * 1024 * 1024
 
 
 class ProfileDocumentMetadata(BaseModel):
     """Metadata describing an uploaded profile document stored in S3."""
+
     bucket: str
     key: str
     filename: str
@@ -46,12 +50,14 @@
 
 class ProfileDocumentUploadResponse(BaseModel):
     """Response payload returned after the backend uploads a document to S3."""
+
     document: ProfileDocumentMetadata
     max_size_bytes: int = MAX_PROFILE_DOCUMENT_BYTES
 
 
 class UserProfile(BaseModel):
     """User profile model for creation/updates."""
+
     firm_name: Optional[str] = None
     first_name: Optional[str] = None
     last_name: Optional[str] = None
@@ -63,24 +69,31 @@
     # Optional investor profile metadata captured/updated via profile page
     investor_profile: Optional[Dict[str, Any]] = None
 
+
 class UserProfileResponse(UserProfile):
     """User profile response model with metadata."""
+
     user_id: str
     created_at: str
     updated_at: str
 
+
 class UserProfileHistory(UserProfileResponse):
     """User profile history model with version information."""
+
     version: int
-    version_date: str 
+    version_date: str
+
 
 class ForgotPasswordRequest(BaseModel):
     email: EmailStr
 
+
 class ResetPasswordRequest(BaseModel):
     email: EmailStr
     code: str
     new_password: str
 
+
 class PasswordResetResponse(BaseModel):
-    message: str 
+    message: str

--- app/models/current_user.py
+++ app/models/current_user.py
@@ -36,4 +36,3 @@
     class Config:
         arbitrary_types_allowed = True
         validate_assignment = True
-

--- app/models/enrichment_models.py
+++ app/models/enrichment_models.py
@@ -31,7 +31,9 @@
 
 class LeadershipChangeEvent(BaseModel):
     person_name: str
-    change_type: Optional[str] = Field(None, description="One of arrival|departure|promotion|role_change|unknown; map appointed/named/joined‚Üíarrival; resigned/left‚Üídeparture.")
+    change_type: Optional[str] = Field(
+        None, description="One of arrival|departure|promotion|role_change|unknown; map appointed/named/joined‚Üíarrival; resigned/left‚Üídeparture."
+    )
     previous_role: Optional[str] = Field(None, description="Role before departure/role change.")
     new_role: Optional[str] = Field(None, description="Role after arrival/promotion/role change.")
     effective_date: Optional[str] = Field(None, description="ISO date: YYYY-MM-DD if day known, else YYYY-MM or YYYY")
@@ -72,7 +74,6 @@
 class MAFundSpinouts(BaseModel):
     value: List[CorporateEvent]
     citation: Optional[List[str]] = None
-
 
 
 def get_enrichment_json_schemas() -> Dict[str, Dict]:
@@ -90,4 +91,3 @@
         "activity_sentiment_signals.recent_hires": RecentHires.model_json_schema(),
         "activity_sentiment_signals.M&A_fund_spinouts": MAFundSpinouts.model_json_schema(),
     }
-

--- app/models/lp_registration_models.py
+++ app/models/lp_registration_models.py
@@ -24,4 +24,3 @@
 
     class Config:
         extra = "ignore"
-

--- app/models/tamradar_models.py
+++ app/models/tamradar_models.py
@@ -14,8 +14,10 @@
 # Webhook Payload Models (Exact match to TAMradar documentation)
 # ============================================================================
 
+
 class TAMradarWebhookData(BaseModel):
     """The 'data' object in webhook payloads."""
+
     radar_id: str
     radar_type: str
     domain: Optional[str] = None  # Present for company/contact radars
@@ -28,24 +30,18 @@
     Base webhook payload structure - matches TAMradar documentation exactly.
     Reference: https://tamradar.readme.io/reference/webhook-payload
     """
+
     event_id: str = Field(..., description="Unique identifier for this webhook event")
-    event_type: Literal["radar_finding", "radar_failure"] = Field(
-        ..., description="Type of event"
-    )
-    record_id: Optional[str] = Field(
-        None, description="Unique identifier for detected record (null for failures)"
-    )
-    discovered_at: str = Field(
-        ..., description="ISO timestamp (UTC) when update was detected"
-    )
+    event_type: Literal["radar_finding", "radar_failure"] = Field(..., description="Type of event")
+    record_id: Optional[str] = Field(None, description="Unique identifier for detected record (null for failures)")
+    discovered_at: str = Field(..., description="ISO timestamp (UTC) when update was detected")
     data: TAMradarWebhookData = Field(..., description="Radar metadata")
-    content: Dict[str, Any] = Field(
-        ..., description="Event-specific data (varies by radar_type)"
-    )
+    content: Dict[str, Any] = Field(..., description="Event-specific data (varies by radar_type)")
 
 
 class RadarFailureContent(BaseModel):
     """Content structure for radar_failure events."""
+
     failure_reason: Literal["missing_prerequisites", "insufficient_funds", "setup_failure"]
     refund_amount_usd: float
     message: str
@@ -55,18 +51,21 @@
 # Radar Creation Request Models
 # ============================================================================
 
+
 class CompanyRadarRequest(BaseModel):
     domain: str
-    radar_type: Optional[Literal[
-        "company_mentions",
-        "company_social_posts",
-        "company_social_posts_cxo",
-        "company_job_openings",
-        "company_new_hires",
-        "company_promotions",
-        "company_reviews",
-        "company_social_engagements"
-    ]] = None  # Optional - if not provided, creates all 4 default types
+    radar_type: Optional[
+        Literal[
+            "company_mentions",
+            "company_social_posts",
+            "company_social_posts_cxo",
+            "company_job_openings",
+            "company_new_hires",
+            "company_promotions",
+            "company_reviews",
+            "company_social_engagements",
+        ]
+    ] = None  # Optional - if not provided, creates all 4 default types
     webhook_url: Optional[str] = None
     filters: Optional[Dict[str, Any]] = None  # Persona-based filters
     custom_fields: Optional[Dict[str, Any]] = None
@@ -105,8 +104,10 @@
 # TAMradar API Response Models
 # ============================================================================
 
+
 class TAMradarAccountUsage(BaseModel):
     """Usage metrics for a specific month."""
+
     month: str  # Format: "YYYY-MM"
     radars_activated: int
     radars_deactivated: int
@@ -115,6 +116,7 @@
 
 class TAMradarAccountStatus(BaseModel):
     """Current radar status counts (all time)."""
+
     total_radars: int
     active_radars: int
     inactive_radars: int
@@ -125,15 +127,15 @@
     Account summary response from GET /v1/account
     Reference: https://tamradar.readme.io/reference/getaccountsummary
     """
+
     balance_remaining_usd: float
     account: TAMradarAccountStatus
     usage: TAMradarAccountUsage
-
 
 
-
 class TAMradarRadarResponse(BaseModel):
     """Response from creating/getting a radar."""
+
     radar_id: str
     radar_category: str
     radar_type: str
@@ -152,8 +154,10 @@
 # Internal Models for Database Operations
 # ============================================================================
 
+
 class UserRadarSummary(BaseModel):
     """Summary of a radar with user subscription info."""
+
     radar_id: str
     radar_category: str
     radar_type: str
@@ -173,6 +177,7 @@
 
 class FindingSummary(BaseModel):
     """Summary of a finding for user display."""
+
     finding_id: str
     radar_id: str
     radar_type: str
@@ -180,4 +185,3 @@
     discovered_at: str
     notified: bool
     notified_at: Optional[str] = None
-

--- app/outlook/__init__.py
+++ app/outlook/__init__.py
@@ -1,4 +1 @@
 
-
-
-

--- app/outlook/calendar.py
+++ app/outlook/calendar.py
@@ -55,5 +55,3 @@
     if value.tzinfo is None:
         value = value.replace(tzinfo=timezone.utc)
     return value.isoformat()
-
-

--- app/outlook/email.py
+++ app/outlook/email.py
@@ -42,5 +42,3 @@
     if isinstance(messages, list):
         return messages
     return []
-
-

--- app/outlook/oauth.py
+++ app/outlook/oauth.py
@@ -184,5 +184,3 @@
     if value.tzinfo is None:
         value = value.replace(tzinfo=timezone.utc)
     return value <= datetime.now(timezone.utc)
-
-

--- app/outlook/repositories/__init__.py
+++ app/outlook/repositories/__init__.py
@@ -1,4 +1 @@
 
-
-
-

--- app/outlook/repositories/outlook_tokens_repository.py
+++ app/outlook/repositories/outlook_tokens_repository.py
@@ -94,8 +94,3 @@
             expires_at,
             token_id,
         )
-
-
-
-
-

--- app/outlook/router.py
+++ app/outlook/router.py
@@ -85,4 +85,3 @@
     if not current_user.user_id:
         raise HTTPException(status_code=400, detail="user_id_missing")
     return current_user.user_id
-

--- app/prompts/data_process_prompts.py
+++ app/prompts/data_process_prompts.py
@@ -1,5 +1,6 @@
 import json
 from typing import Optional, List, Dict, Any
+
 """
 Data processing prompts for agents that handle data fusion and consolidation.
 """
@@ -277,62 +278,146 @@
 # Pre-filtering logic for executives
 INVESTMENT_COMPANY_TITLE_PRIORITIES = {
     # Tier 1: Absolute Priority (CEO level and equivalents)
-    'tier_1': [
+    "tier_1": [
         # CEO variations
-        'ceo', 'chief executive officer', 'chief executive', 'group ceo', 'executive chairman',
-        'managing partner', 'senior partner', 'founder', 'co-founder', 'group managing director',
-        'managing director uk', 'managing director emea', 'managing director apac', 'managing director europe',
-        'managing director asia', 'managing director americas', 'executive director', 'group executive director'
+        "ceo",
+        "chief executive officer",
+        "chief executive",
+        "group ceo",
+        "executive chairman",
+        "managing partner",
+        "senior partner",
+        "founder",
+        "co-founder",
+        "group managing director",
+        "managing director uk",
+        "managing director emea",
+        "managing director apac",
+        "managing director europe",
+        "managing director asia",
+        "managing director americas",
+        "executive director",
+        "group executive director",
     ],
-    
     # Tier 2: Investment Leadership (CIO and equivalents)
-    'tier_2': [
+    "tier_2": [
         # CIO variations
-        'cio', 'chief investment officer', 'head of investments', 'investment director', 
-        'chief investment director', 'global head of investments', 'head of investment strategy',
-        'investment committee chair', 'investment committee head', 'head of investment committee',
+        "cio",
+        "chief investment officer",
+        "head of investments",
+        "investment director",
+        "chief investment director",
+        "global head of investments",
+        "head of investment strategy",
+        "investment committee chair",
+        "investment committee head",
+        "head of investment committee",
         # Portfolio/Fund Management
-        'portfolio manager', 'investment manager', 'investment committee', 'investment partner',
-        'head of portfolio management', 'senior portfolio manager', 'fund manager', 'head of fund management',
-        'head of private equity', 'head of venture capital', 'head of asset management',
-        'chief portfolio officer', 'global investment director', 'director of investments'
+        "portfolio manager",
+        "investment manager",
+        "investment committee",
+        "investment partner",
+        "head of portfolio management",
+        "senior portfolio manager",
+        "fund manager",
+        "head of fund management",
+        "head of private equity",
+        "head of venture capital",
+        "head of asset management",
+        "chief portfolio officer",
+        "global investment director",
+        "director of investments",
     ],
-    
     # Tier 3: Operations & Finance Leadership (COO, CFO and equivalents)
-    'tier_3': [
+    "tier_3": [
         # COO variations
-        'coo', 'chief operating officer', 'chief operations officer', 'operations director',
-        'head of operations', 'global head of operations', 'group operations director',
-        'director of operations', 'operating partner', 'operational director',
+        "coo",
+        "chief operating officer",
+        "chief operations officer",
+        "operations director",
+        "head of operations",
+        "global head of operations",
+        "group operations director",
+        "director of operations",
+        "operating partner",
+        "operational director",
         # CFO variations
-        'cfo', 'chief financial officer', 'chief finance officer', 'finance director',
-        'head of finance', 'group finance director', 'group cfo', 'financial director',
-        'director of finance', 'global head of finance', 'treasury director'
+        "cfo",
+        "chief financial officer",
+        "chief finance officer",
+        "finance director",
+        "head of finance",
+        "group finance director",
+        "group cfo",
+        "financial director",
+        "director of finance",
+        "global head of finance",
+        "treasury director",
     ],
-    
     # Tier 4: Senior Management
-    'tier_4': [
-        'president', 'managing director', 'partner', 'principal', 'vice president', 'vp',
-        'director', 'senior director', 'head of', 'senior vice president', 'svp',
-        'general partner', 'gp', 'limited partner', 'lp', 'chief strategy officer',
-        'head of capital markets', 'principal investor', 'deal partner', 'investment banking head',
-        'head of m&a', 'head of corporate development'
+    "tier_4": [
+        "president",
+        "managing director",
+        "partner",
+        "principal",
+        "vice president",
+        "vp",
+        "director",
+        "senior director",
+        "head of",
+        "senior vice president",
+        "svp",
+        "general partner",
+        "gp",
+        "limited partner",
+        "lp",
+        "chief strategy officer",
+        "head of capital markets",
+        "principal investor",
+        "deal partner",
+        "investment banking head",
+        "head of m&a",
+        "head of corporate development",
     ],
-    
     # Tier 5: Other Investment-Related
-    'tier_5': [
-        'analyst', 'senior analyst', 'associate', 'senior associate', 'investment associate',
-        'business development', 'investor relations', 'compliance', 'risk management',
-        'investment strategist', 'portfolio analyst', 'investment banking associate',
-        'private equity associate', 'venture capital associate'
-    ]
+    "tier_5": [
+        "analyst",
+        "senior analyst",
+        "associate",
+        "senior associate",
+        "investment associate",
+        "business development",
+        "investor relations",
+        "compliance",
+        "risk management",
+        "investment strategist",
+        "portfolio analyst",
+        "investment banking associate",
+        "private equity associate",
+        "venture capital associate",
+    ],
 }
 
 # Titles to exclude for investment companies
 INVESTMENT_COMPANY_EXCLUDE_TITLES = [
-    'hr', 'human resources', 'marketing', 'communications', 'legal counsel', 'administrative',
-    'assistant', 'coordinator', 'intern', 'receptionist', 'office manager', 'it support',
-    'facilities', 'security', 'maintenance', 'junior', 'trainee', 'contractor'
+    "hr",
+    "human resources",
+    "marketing",
+    "communications",
+    "legal counsel",
+    "administrative",
+    "assistant",
+    "coordinator",
+    "intern",
+    "receptionist",
+    "office manager",
+    "it support",
+    "facilities",
+    "security",
+    "maintenance",
+    "junior",
+    "trainee",
+    "contractor",
 ]
 
 # Executive Filtering User Prompt Template
@@ -693,31 +778,29 @@
 # Helper Functions for Prompts
 # ============================================================================
 
+
 def get_base_classification_prompt(finding_text: str, company_name: str = "the company") -> str:
     """
     Get the formatted base classification prompt.
-    
+
     Args:
         finding_text: The finding text to classify
         company_name: The name of the company being tracked (default: "the company")
-        
+
     Returns:
         Formatted prompt string
     """
     from app.prompts.data_process_prompts import TIER_DESCRIPTIONS
+
     # Format TIER_DESCRIPTIONS with company_name
     tier_descriptions_formatted = TIER_DESCRIPTIONS.format(company_name=company_name)
-    return BASE_CLASSIFICATION_PROMPT.format(
-        TIER_DESCRIPTIONS=tier_descriptions_formatted,
-        finding_text=finding_text,
-        company_name=company_name
-    )
+    return BASE_CLASSIFICATION_PROMPT.format(TIER_DESCRIPTIONS=tier_descriptions_formatted, finding_text=finding_text, company_name=company_name)
 
 
 def get_user_relevance_prompt(finding_text: str, user_context: str, company_name: str) -> str:
     """
     Get the formatted user relevance prompt.
-    
+
     Args:
         finding_text: The finding text to evaluate
         user_context: User's radar context narrative (profile + watchlist notes)
@@ -726,40 +809,28 @@
                      - User profile (fund_type, sector_focus, geographic_focus, key_objectives)
                      - Watchlist notes (user_interests field from tamradar_user_radars)
         company_name: The name of the company being tracked in this radar
-        
+
     Returns:
         Formatted prompt string
     """
-    return USER_RELEVANCE_PROMPT.format(
-        finding_text=finding_text,
-        user_context=user_context,
-        company_name=company_name
-    )
+    return USER_RELEVANCE_PROMPT.format(finding_text=finding_text, user_context=user_context, company_name=company_name)
 
 
-def get_tier1_verification_prompt(
-    finding_text: str,
-    company_name: str,
-    category: str,
-    initial_confidence: float
-) -> str:
+def get_tier1_verification_prompt(finding_text: str, company_name: str, category: str, initial_confidence: float) -> str:
     """
     Get the formatted Tier 1 verification prompt.
-    
+
     Args:
         finding_text: The finding text to verify
         company_name: The name of the company being tracked
         category: The category from initial classification (e.g., "Direct Allocation Announcements")
         initial_confidence: The confidence score from initial classification (0.0-1.0)
-        
+
     Returns:
         Formatted prompt string
     """
     return TIER1_VERIFICATION_PROMPT.format(
-        finding_text=finding_text,
-        company_name=company_name,
-        category=category,
-        initial_confidence=initial_confidence
+        finding_text=finding_text, company_name=company_name, category=category, initial_confidence=initial_confidence
     )
 
 
@@ -767,30 +838,31 @@
     """Get the LLM data consolidation system prompt."""
     return LLM_DATA_CONSOLIDATION_SYSTEM_PROMPT
 
+
 def get_llm_data_consolidation_user_prompt(data_json: str, schemas: dict | None = None) -> str:
     """Get the LLM data consolidation user prompt with formatted data and optional schemas."""
     schemas_json = json.dumps(schemas, indent=2, ensure_ascii=False) if schemas else "{}"
     return LLM_DATA_CONSOLIDATION_USER_PROMPT_TEMPLATE.format(data_json=data_json, schemas_json=schemas_json)
 
+
 def get_linkedin_mentions_summary_system_prompt() -> str:
     """Get the LinkedIn mentions summary system prompt."""
     return LINKEDIN_MENTIONS_SUMMARY_SYSTEM_PROMPT
 
+
 def get_linkedin_mentions_summary_user_prompt(person_name: str, company_name: str, posts_text: str) -> str:
     """Get the LinkedIn mentions summary user prompt with formatted parameters."""
-    return LINKEDIN_MENTIONS_SUMMARY_USER_PROMPT_TEMPLATE.format(
-        person_name=person_name,
-        company_name=company_name,
-        posts_text=posts_text
-    )
+    return LINKEDIN_MENTIONS_SUMMARY_USER_PROMPT_TEMPLATE.format(person_name=person_name, company_name=company_name, posts_text=posts_text)
+
 
 def get_person_company_verification_system_prompt(search_type: str) -> str:
     """Get the person-company verification system prompt with formatted search type."""
     return PERSON_COMPANY_VERIFICATION_SYSTEM_PROMPT_TEMPLATE.format(search_type=search_type.upper())
 
-def get_person_company_verification_user_prompt(target_person_name: str, target_company_name: str, 
-                                               sector_type: str, search_type: str, person_name: str, 
-                                               location: str, company_context_text: str) -> str:
+
+def get_person_company_verification_user_prompt(
+    target_person_name: str, target_company_name: str, sector_type: str, search_type: str, person_name: str, location: str, company_context_text: str
+) -> str:
     """Get the person-company verification user prompt with formatted parameters."""
     return PERSON_COMPANY_VERIFICATION_USER_PROMPT_TEMPLATE.format(
         target_person_name=target_person_name,
@@ -799,19 +871,28 @@
         search_type=search_type,
         person_name=person_name,
         location=location,
-        company_context_text=company_context_text
+        company_context_text=company_context_text,
     )
 
+
 def get_narrative_generation_system_prompt() -> str:
     """Get the narrative generation system prompt."""
     return NARRATIVE_GENERATION_SYSTEM_PROMPT
 
-def get_narrative_generation_user_prompt(narrative_instructions: str, firm_description: str, 
-                                        key_differentiators: str, key_objectives: str, 
-                                        company_data: str, search_context: str = "",
-                                        fund_type: str = "", fundraising_status: str = "",
-                                        user_geographic_focus: str = "", user_sector_focus: str = "",
-                                        user_lp_target_types: str = "") -> str:
+
+def get_narrative_generation_user_prompt(
+    narrative_instructions: str,
+    firm_description: str,
+    key_differentiators: str,
+    key_objectives: str,
+    company_data: str,
+    search_context: str = "",
+    fund_type: str = "",
+    fundraising_status: str = "",
+    user_geographic_focus: str = "",
+    user_sector_focus: str = "",
+    user_lp_target_types: str = "",
+) -> str:
     """Get the narrative generation user prompt with formatted parameters."""
     return NARRATIVE_GENERATION_USER_PROMPT_TEMPLATE.format(
         narrative_instructions=narrative_instructions,
@@ -824,104 +905,104 @@
         user_sector_focus=user_sector_focus,
         user_lp_target_types=user_lp_target_types,
         company_data=company_data,
-        search_context=search_context
+        search_context=search_context,
     )
 
+
 def get_executive_filtering_system_prompt() -> str:
     """Get the executive filtering system prompt."""
     return EXECUTIVE_FILTERING_SYSTEM_PROMPT
 
+
 def pre_filter_executives_for_investment_company(executives_list, max_output=30):
     """
     Pre-filter executives for investment companies to reduce large lists before LLM processing.
-    
+
     Args:
         executives_list: List of executive dictionaries with title/position information
         max_output: Maximum number of executives to return (default 30)
-    
+
     Returns:
         Tuple of (filtered_executives, original_indices)
     """
     import re
-    
+
     def get_title_score(title_text):
         """Score executive titles based on investment company priorities."""
         if not title_text:
             return 0
-            
+
         title_lower = title_text.lower()
-        
+
         # Check for exclusions first
         for exclude_term in INVESTMENT_COMPANY_EXCLUDE_TITLES:
             if exclude_term in title_lower:
                 return -1  # Exclude
-        
+
         # Score based on tier priority
         for tier, terms in INVESTMENT_COMPANY_TITLE_PRIORITIES.items():
             for term in terms:
                 if term in title_lower:
-                    if tier == 'tier_1':
+                    if tier == "tier_1":
                         return 100
-                    elif tier == 'tier_2':
+                    elif tier == "tier_2":
                         return 90
-                    elif tier == 'tier_3':
+                    elif tier == "tier_3":
                         return 80
-                    elif tier == 'tier_4':
+                    elif tier == "tier_4":
                         return 70
-                    elif tier == 'tier_5':
+                    elif tier == "tier_5":
                         return 60
-        
+
         return 30  # Default score for unmatched titles
-    
+
     # Score all executives
     scored_executives = []
     for i, exec_data in enumerate(executives_list):
         # Try different fields for title information
-        title = (exec_data.get('title') or 
-                exec_data.get('position') or 
-                exec_data.get('job_title') or 
-                exec_data.get('current_position') or 
-                str(exec_data))
-        
+        title = (
+            exec_data.get("title") or exec_data.get("position") or exec_data.get("job_title") or exec_data.get("current_position") or str(exec_data)
+        )
+
         score = get_title_score(title)
-        
+
         if score > 0:  # Include if not excluded
             scored_executives.append((score, i, exec_data))
-    
+
     # Sort by score (highest first) and take top N
     scored_executives.sort(key=lambda x: x[0], reverse=True)
     top_executives = scored_executives[:max_output]
-    
+
     # Extract the filtered list and original indices
     filtered_executives = [exec_data for _, _, exec_data in top_executives]
     original_indices = [original_idx for _, original_idx, _ in top_executives]
-    
+
     return filtered_executives, original_indices
 
+
 def get_executive_filtering_user_prompt(people_json: str, count: int = None, company_context: str = ""):
     """Get the executive filtering user prompt with formatted people data, count, and company context."""
     if count is None:
         # Try to extract count from the JSON if not provided
         try:
             import json
+
             people_list = json.loads(people_json)
             count = len(people_list) if isinstance(people_list, list) else 0
         except:
             count = 0
-    
+
     if not company_context:
         company_context = "Investment company (specific sector and focus not provided)"
-    
-    return EXECUTIVE_FILTERING_USER_PROMPT_TEMPLATE.format(
-        count=count,
-        company_context=company_context,
-        people_json=people_json
-    ) 
 
+    return EXECUTIVE_FILTERING_USER_PROMPT_TEMPLATE.format(count=count, company_context=company_context, people_json=people_json)
+
+
 def get_prompt_analysis_system_prompt() -> str:
     """Get the prompt analysis system prompt."""
     return PROMPT_ANALYSIS_SYSTEM_PROMPT
 
+
 # CoreSignal LLM Company Selection Prompts
 CORESIGNAL_LLM_COMPANY_SELECTION_SYSTEM_PROMPT = """
 You are an expert at matching company records. Select the best matching company from a list of candidates.
@@ -945,26 +1026,25 @@
 
 Select the best matching company ID."""
 
+
 def get_coresignal_llm_company_selection_system_prompt() -> str:
     """Get the CoreSignal LLM company selection system prompt."""
     return CORESIGNAL_LLM_COMPANY_SELECTION_SYSTEM_PROMPT
 
+
 def get_coresignal_llm_company_selection_user_prompt(
-    target_company_name: str,
-    target_hq_city: str,
-    target_hq_country: str,
-    target_investor_type: str,
-    candidates_text: str
+    target_company_name: str, target_hq_city: str, target_hq_country: str, target_investor_type: str, candidates_text: str
 ) -> str:
     """Get the CoreSignal LLM company selection user prompt."""
     return CORESIGNAL_LLM_COMPANY_SELECTION_USER_PROMPT_TEMPLATE.format(
         target_company_name=target_company_name,
-        target_hq_city=target_hq_city or 'N/A',
-        target_hq_country=target_hq_country or 'N/A',
-        target_investor_type=target_investor_type or 'N/A',
-        candidates_text=candidates_text
+        target_hq_city=target_hq_city or "N/A",
+        target_hq_country=target_hq_country or "N/A",
+        target_investor_type=target_investor_type or "N/A",
+        candidates_text=candidates_text,
     )
 
+
 # ============================================================================
 # Weekly Wrap-Up Email Prompts
 # ============================================================================
@@ -1011,27 +1091,27 @@
 ) -> str:
     """
     Generate prompt for LLM to create weekly summaries for multiple companies in one call.
-    
+
     Args:
         companies_data: List of company dicts, each with:
             - company_name: str
             - user_interests: Optional[str] - from user_interests field in tamradar_user_radars
             - findings_summary: str - formatted findings for this company
-        
+
     Returns:
         Formatted prompt string
     """
     company_count = len(companies_data)
-    
+
     # Format companies data for prompt
     companies_text = ""
     for i, company in enumerate(companies_data, 1):
         company_name = company.get("company_name", f"Company {i}")
         user_interests = company.get("user_interests")
         findings_summary = company.get("findings_summary", "")
-        
+
         interests_text = user_interests if user_interests else "General investment tracking (no specific interests noted)"
-        
+
         companies_text += f"""
 COMPANY {i}: {company_name}
 User's Specific Interests: {interests_text}
@@ -1041,7 +1121,7 @@
 
 ---
 """
-    
+
     return WEEKLY_WRAPUP_COMPANY_SUMMARIES_PROMPT.format(
         company_count=company_count,
         companies_data=companies_text.strip(),
@@ -1051,10 +1131,10 @@
 def format_findings_for_summary(findings: list) -> str:
     """
     Format findings list into a summary string for LLM prompt.
-    
+
     Args:
         findings: List of finding dictionaries
-        
+
     Returns:
         Formatted string with finding details
     """
@@ -1063,33 +1143,30 @@
         tier = finding.get("priority_tier", "?")
         category = finding.get("category", "Update")
         relevance = finding.get("relevance_score", 0.0)
-        
+
         # Extract preview from finding_data
         finding_data = finding.get("finding_data", {})
         preview = _extract_finding_preview(finding_data, finding.get("radar_type", ""))
-        
-        lines.append(
-            f"{i}. Tier {tier} - {category} (Relevance: {relevance:.2f})\n"
-            f"   {preview}"
-        )
-    
+
+        lines.append(f"{i}. Tier {tier} - {category} (Relevance: {relevance:.2f})\n   {preview}")
+
     return "\n\n".join(lines)
 
 
 def _extract_finding_preview(finding_data: dict, radar_type: str) -> str:
     """
     Extract preview text from finding_data based on radar_type.
-    
+
     Args:
         finding_data: Finding data dictionary
         radar_type: Type of radar
-        
+
     Returns:
         Preview text string
     """
     if not finding_data:
         return "View finding details on watchlist."
-    
+
     # Extract based on radar type
     if radar_type == "company_mentions":
         return finding_data.get("mention_content", finding_data.get("content", "Company mention"))
@@ -1112,5 +1189,5 @@
         for key in ["content", "description", "text", "message", "headline"]:
             if key in finding_data and finding_data[key]:
                 return str(finding_data[key])[:200]  # Limit length
-        
-        return "View finding details on watchlist." 
\ No newline at end of file
+
+        return "View finding details on watchlist."

--- app/prompts/data_retrieval_prompts.py
+++ app/prompts/data_retrieval_prompts.py
@@ -238,7 +238,7 @@
 }}"""
 
 # OSINT system prompt for company website
-OSINT_SYSTEM_PROMPT_WEBSITE = '''
+OSINT_SYSTEM_PROMPT_WEBSITE = """
 You are an OSINT specialist researching {company_name} (website: {website_url}, location: {location}). Focus heavily on {key_area_of_focus}.
 
 **CRITICAL: Your response must be ONLY a valid JSON object with no additional text before or after.**
@@ -317,10 +317,10 @@
 - Ensure valid JSON syntax
 - Provide detailed, comprehensive content in each "content" field
 - Focus extensively on portfolio details and {key_area_of_focus} information
-'''
+"""
 
 # OSINT system prompt for external sources
-OSINT_SYSTEM_PROMPT_EXTERNAL = '''
+OSINT_SYSTEM_PROMPT_EXTERNAL = """
 Research {company_name} from EXTERNAL news sources only. EXCLUDE site:{website_url}. Focus on {key_area_of_focus}. Location: {location}.
 
 **RESPOND WITH ONLY JSON - NO THINKING OR EXPLANATIONS**
@@ -406,10 +406,10 @@
 - Verify source credibility (established publications only)
 - Extract ALL relevant information, don't summarize
 - Ensure valid JSON syntax
-'''
+"""
 
 # OSINT system prompt for company website - V2
-OSINT_SYSTEM_PROMPT_WEBSITE_V2 = '''
+OSINT_SYSTEM_PROMPT_WEBSITE_V2 = """
 You are an OSINT specialist researching {company_name} (website: {website_url}, location: {location}). Focus on {key_area_of_focus}.
 
 **CRITICAL: Your response must be ONLY a valid JSON object with no additional text before or after.**
@@ -632,10 +632,10 @@
 - Prioritize recent information (past 12 months)
 - For fields that can have multiple entries, provide as many results as possible
 - For fields requesting examples, events, activities, or recognition: aim to return multiple recent instances when available rather than single examples - comprehensive coverage is preferred over brevity
-'''
+"""
 
 # OSINT system prompt for external sources - V2
-OSINT_SYSTEM_PROMPT_EXTERNAL_V2 = '''
+OSINT_SYSTEM_PROMPT_EXTERNAL_V2 = """
 Research {company_name} from EXTERNAL news sources only. EXCLUDE site:{website_url}. Focus on {key_area_of_focus}. Location: {location}.
 
 **RESPOND WITH ONLY JSON - NO THINKING OR EXPLANATIONS**
@@ -869,19 +869,19 @@
 - Prioritize recent information (past 12 months)
 - For fields that can have multiple entries, provide as many results as possible
 - For fields requesting examples, events, activities, or recognition: aim to return multiple recent instances when available rather than single examples - comprehensive coverage is preferred over brevity
-'''
+"""
 
 # OSINT user prompt template (V1 format)
-OSINT_USER_PROMPT_TEMPLATE = '''
+OSINT_USER_PROMPT_TEMPLATE = """
 I want to gather the most recent and relevant information about {company_name}, 
 focusing on {key_area_of_focus}. Please search and extract news, press releases, business updates, and key personnel changes 
 from their official website ({website_url}) and related sources, with a focus on their activities in {location}. 
 For each finding, include the date, headline, a short paragraph with all relevant details, and a direct URL citation. 
 Do not include unrelated content or summaries without sources.
-'''
+"""
 
 # OSINT user prompt template (V2 structured format)
-OSINT_USER_PROMPT_TEMPLATE_V2 = '''
+OSINT_USER_PROMPT_TEMPLATE_V2 = """
 Research {company_name} comprehensively and extract information into the structured JSON format specified in the system prompt.
 
 **Company Context:**
@@ -898,7 +898,7 @@
 - Focus on recent information (past 12 months) where applicable
 
 **Output:** Return ONLY the structured JSON object as specified in the system prompt, with no additional text.
-'''
+"""
 
 # Person enrichment prompt for identifying investment outreach targets (configurable number)
 PERSON_ENRICHMENT_PROMPT = """
@@ -990,7 +990,7 @@
 - Do not add any other information to the output.
 """
 # OSINT Person Research System Prompt
-OSINT_PERSON_SYSTEM_PROMPT = '''
+OSINT_PERSON_SYSTEM_PROMPT = """
 You are an OSINT specialist conducting comprehensive research on a specific person for investment outreach purposes. 
 
 **CRITICAL: Your response must be ONLY a valid JSON object with no additional text before or after.**
@@ -1059,10 +1059,10 @@
 - Provide detailed, comprehensive content in each "value" field
 - Include full URLs for all citations
 - Focus on recent and relevant professional activities
-'''
+"""
 
 # OSINT Person Research User Prompt Template
-OSINT_PERSON_USER_PROMPT_TEMPLATE = '''
+OSINT_PERSON_USER_PROMPT_TEMPLATE = """
 Research the following person for investment outreach purposes:
 
 **Person:** {person_name}
@@ -1080,7 +1080,7 @@
 - Public speaking and thought leadership
 
 Provide comprehensive, detailed information with full URLs for all sources.
-'''
+"""
 
 # --- C-SUITE MEMBERS EXTRACT PROMPT (for focused C-suite search) ---
 c_suite_members_extract_system_prompt = """
@@ -1190,6 +1190,7 @@
 Return only the JSON structure requested with no explanations.
 """
 
+
 # Retrieval function for the C-suite members extract prompt
 def get_c_suite_members_extract_prompt(company_name, company_domain):
     """
@@ -1197,9 +1198,10 @@
     """
     return (
         c_suite_members_extract_system_prompt,
-        c_suite_members_extract_user_prompt.format(company_name=company_name, company_domain=company_domain)
+        c_suite_members_extract_user_prompt.format(company_name=company_name, company_domain=company_domain),
     )
 
+
 # --- CORESIGNAL FALLBACK EXTRACTION PROMPT (when CoreSignal returns 0 matches) ---
 CORESIGNAL_FALLBACK_EXTRACT_PROMPT = """
 You are an expert at extracting comprehensive company information from web sources.
@@ -1275,17 +1277,18 @@
 - First location must have "is_primary": 1
 """
 
+
 # Retrieval function for the CoreSignal fallback extract prompt
 def get_coresignal_fallback_extract_prompt(company_name, website_url, location):
     """
     Returns the prompt for CoreSignal fallback data extraction.
     Used when CoreSignal API returns 0 matches and we need to extract data from Perplexity.
-    
+
     Args:
         company_name: Company name to search for
         website_url: Company website URL (domain will be extracted)
         location: Company location (city, country)
-    
+
     Returns:
         Formatted prompt string
     """
@@ -1293,100 +1296,89 @@
     domain = website_url
     if website_url:
         # Remove protocol and path
-        domain = website_url.replace('https://', '').replace('http://', '').split('/')[0]
-    
-    return CORESIGNAL_FALLBACK_EXTRACT_PROMPT.format(
-        company_name=company_name,
-        website_url=website_url or domain,
-        location=location
-    )
+        domain = website_url.replace("https://", "").replace("http://", "").split("/")[0]
+
+    return CORESIGNAL_FALLBACK_EXTRACT_PROMPT.format(company_name=company_name, website_url=website_url or domain, location=location)
+
 
 def get_company_info_prompt() -> str:
     """Get the company info extraction prompt template."""
     return EXTRACT_COMPANY_INFO_PROMPT
 
+
 def get_search_prompt() -> str:
     """Get the search prompt template."""
     return SEARCH_PROMPT
 
+
 def get_extract_urls_prompt() -> str:
     """Get the URL extraction prompt template."""
     return EXTRACT_URLS_PROMPT_TEMPLATE
 
+
 def get_linkedin_prompt() -> str:
     """Get the LinkedIn profile extraction prompt template."""
     return LINKEDIN_PROMPT_TEMPLATE
 
+
 def get_osint_system_prompt_website(company_name, website_url, location, key_area_of_focus):
     return OSINT_SYSTEM_PROMPT_WEBSITE.format(
-        company_name=company_name,
-        website_url=website_url,
-        location=location,
-        key_area_of_focus=key_area_of_focus
+        company_name=company_name, website_url=website_url, location=location, key_area_of_focus=key_area_of_focus
     )
 
+
 def get_osint_system_prompt_external(company_name, website_url, location, key_area_of_focus):
     return OSINT_SYSTEM_PROMPT_EXTERNAL.format(
-        company_name=company_name,
-        website_url=website_url,
-        location=location,
-        key_area_of_focus=key_area_of_focus
+        company_name=company_name, website_url=website_url, location=location, key_area_of_focus=key_area_of_focus
     )
 
+
 def get_osint_user_prompt(company_name, website_url, location, key_area_of_focus):
     return OSINT_USER_PROMPT_TEMPLATE.format(
-        company_name=company_name,
-        website_url=website_url,
-        location=location,
-        key_area_of_focus=key_area_of_focus
+        company_name=company_name, website_url=website_url, location=location, key_area_of_focus=key_area_of_focus
     )
 
+
 def get_osint_user_prompt_v2(company_name, website_url, location, key_area_of_focus):
     """Get the V2 user prompt that works with V2 structured system prompts."""
     return OSINT_USER_PROMPT_TEMPLATE_V2.format(
-        company_name=company_name,
-        website_url=website_url,
-        location=location,
-        key_area_of_focus=key_area_of_focus
+        company_name=company_name, website_url=website_url, location=location, key_area_of_focus=key_area_of_focus
     )
 
-def get_scrape_prompt_template(tool: str = 'scrape_as_markdown') -> str:
+
+def get_scrape_prompt_template(tool: str = "scrape_as_markdown") -> str:
     """Get the scrape tool prompt template, filling in the tool name."""
     return SCRAPE_PROMPT_TEMPLATE.format(tool=tool)
 
+
 def get_osint_system_prompt_website_v2(company_name, website_url, location, key_area_of_focus):
     """Get the V2 website OSINT system prompt template with fields formatted for structured JSON output."""
     return OSINT_SYSTEM_PROMPT_WEBSITE_V2.format(
-        company_name=company_name,
-        website_url=website_url,
-        location=location,
-        key_area_of_focus=key_area_of_focus
+        company_name=company_name, website_url=website_url, location=location, key_area_of_focus=key_area_of_focus
     )
 
+
 def get_osint_system_prompt_external_v2(company_name, website_url, location, key_area_of_focus):
     """Get the V2 external sources OSINT system prompt template with fields formatted for structured JSON output."""
     return OSINT_SYSTEM_PROMPT_EXTERNAL_V2.format(
-        company_name=company_name,
-        website_url=website_url,
-        location=location,
-        key_area_of_focus=key_area_of_focus
+        company_name=company_name, website_url=website_url, location=location, key_area_of_focus=key_area_of_focus
     )
 
+
 def get_person_enrichment_prompt(num_people: int = 1) -> str:
     """Get the person enrichment prompt for identifying investment outreach targets (configurable number)."""
     return PERSON_ENRICHMENT_PROMPT.format(num_people=num_people)
 
+
 def get_osint_person_system_prompt() -> str:
     """Get the OSINT person research system prompt."""
     return OSINT_PERSON_SYSTEM_PROMPT
 
+
 def get_osint_person_user_prompt(person_name: str, company_name: str, company_website: str) -> str:
     """Get the OSINT person research user prompt with formatted parameters."""
-    return OSINT_PERSON_USER_PROMPT_TEMPLATE.format(
-        person_name=person_name,
-        company_name=company_name,
-        company_website=company_website
-    )
+    return OSINT_PERSON_USER_PROMPT_TEMPLATE.format(person_name=person_name, company_name=company_name, company_website=company_website)
+
 
 def get_multi_domain_prompt(company_name: str, location: str, investor_type: str = "") -> str:
     """Get the optimized multi-domain discovery prompt for company homepage search."""
@@ -1397,25 +1389,21 @@
         IMPORTANT - INVESTOR TYPE IDENTIFICATION:
         The company "{company_name}" that you are searching for is classified as an {investor_type} (note: this classification may not always be 100% accurate, but indicates the company operates as an investor in this general category).
         """
-    
+
     return MULTI_DOMAIN_PROMPT_TEMPLATE.format(
-        company_name=company_name,
-        location=location,
-        investor_type=investor_type or "",
-        investor_type_context=investor_type_context
+        company_name=company_name, location=location, investor_type=investor_type or "", investor_type_context=investor_type_context
     )
 
+
 def get_company_search_prompt(base_query: str) -> str:
     """Get the company search prompt template with formatted base query."""
     return COMPANY_SEARCH_PROMPT_TEMPLATE.format(base_query=base_query)
 
+
 def get_linkedin_analysis_prompt(company_name: str, posts_text: str, posts_count: int) -> str:
     """Get the LinkedIn analysis prompt template with formatted company name, posts text, and posts count."""
-    return LINKEDIN_ANALYSIS_PROMPT_TEMPLATE.format(
-        company_name=company_name,
-        posts_text=posts_text,
-        posts_count=posts_count
-    )
+    return LINKEDIN_ANALYSIS_PROMPT_TEMPLATE.format(company_name=company_name, posts_text=posts_text, posts_count=posts_count)
+
 
 # RIA Detection Prompts
 RIA_DETECTION_USER_PROMPT_TEMPLATE = """
@@ -1446,11 +1434,11 @@
 }
 """
 
+
 def get_ria_detection_user_prompt(company_name: str) -> str:
     """Get the RIA detection user prompt with formatted parameters."""
-    return RIA_DETECTION_USER_PROMPT_TEMPLATE.format(
-        company_name=company_name
-    )
+    return RIA_DETECTION_USER_PROMPT_TEMPLATE.format(company_name=company_name)
+
 
 def get_location_ria_detection_prompt(location: str) -> str:
     """Get the location-based RIA detection prompt."""

--- app/routers/__init__.py
+++ app/routers/__init__.py
@@ -1 +0,0 @@
- 
\ No newline at end of file

--- app/routers/admin.py
+++ app/routers/admin.py
@@ -32,11 +32,13 @@
 )
 rate_limiter = RateLimiter()
 
+
 # Models
 class RateLimitUpdate(BaseModel):
     user_id: str
     limit: int
-    
+
+
 class RateLimitInfo(BaseModel):
     user_id: str
     limit: int
@@ -44,6 +46,7 @@
     remaining: int
     reset: int
 
+
 class AdminUserSummary(BaseModel):
     user_id: str
     email: EmailStr
@@ -58,6 +61,7 @@
     # Optional nested profile for include_profile=true
     profile: Optional["AdminUserProfileSummary"] = None
 
+
 class AdminUserProfileSummary(BaseModel):
     firm_description: Optional[str] = None
     key_differentiators: Optional[str] = None
@@ -66,23 +70,27 @@
     profile_created_at: Optional[str] = None
     profile_updated_at: Optional[str] = None
 
+
 class AdminUsersListResponse(BaseModel):
     users: List[AdminUserSummary]
     total: int
 
+
 class DailyRunCount(BaseModel):
     day: str
     count: int
 
+
 class UserRunHistoryResponse(BaseModel):
     user_id: str
     series: List[DailyRunCount]
     total_days: int
 
+
 class ApprovedUserAdd(BaseModel):
     email: EmailStr
 
-    @field_validator('email', mode='before')
+    @field_validator("email", mode="before")
     @classmethod
     def normalize_email(cls, v: str) -> str:
         """Normalize email to lowercase."""
@@ -90,12 +98,13 @@
             return v.lower().strip()
         return v
 
+
 class DailyLimitUpdate(BaseModel):
     email: EmailStr
     daily_agent_run_limit: int
     tamradar_radar_limit: Optional[int] = None  # Optional TAMradar radar limit
 
-    @field_validator('email', mode='before')
+    @field_validator("email", mode="before")
     @classmethod
     def normalize_email(cls, v: str) -> str:
         """Normalize email to lowercase."""
@@ -103,17 +112,21 @@
             return v.lower().strip()
         return v
 
+
 class DailyLimitUpdateByUserId(BaseModel):
     """Model for updating daily limit by user_id (email not required)."""
+
     daily_agent_run_limit: int
     tamradar_radar_limit: Optional[int] = None  # Optional TAMradar radar limit
 
+
 class DeletionLimitUpdate(BaseModel):
     """Model for updating deletion limit by email."""
+
     email: EmailStr
     tamradar_deletion_limit: int
 
-    @field_validator('email', mode='before')
+    @field_validator("email", mode="before")
     @classmethod
     def normalize_email(cls, v: str) -> str:
         """Normalize email to lowercase."""
@@ -121,18 +134,19 @@
             return v.lower().strip()
         return v
 
+
 class DeletionLimitUpdateByUserId(BaseModel):
     """Model for updating deletion limit by user_id."""
+
     tamradar_deletion_limit: int
 
+
 class UserStatusUpdate(BaseModel):
     is_active: bool
 
+
 # Helper function to check admin status
-async def get_admin_user(
-    current_user: CurrentUser = Depends(require_user),
-    db=Depends(get_db)
-):
+async def get_admin_user(current_user: CurrentUser = Depends(require_user), db=Depends(get_db)):
     """Verify that the current user is an admin by checking claims & DB."""
     if current_user.is_admin:
         return current_user
@@ -152,6 +166,7 @@
         detail="Admin privileges required",
     )
 
+
 @router.get("/rate-limits/{user_id}", response_model=RateLimitInfo)
 async def get_rate_limit(user_id: str, _: CurrentUser = Depends(get_admin_user)):
     """Get rate limit information for a user."""
@@ -160,26 +175,18 @@
         count = await rate_limiter.get_request_count(user_id)
         remaining = max(0, limit - count)
         limit_info = await rate_limiter.check_rate_limit(user_id)
-        return RateLimitInfo(
-            user_id=user_id,
-            limit=limit,
-            used=count,
-            remaining=remaining,
-            reset=limit_info["reset"]
-        )
+        return RateLimitInfo(user_id=user_id, limit=limit, used=count, remaining=remaining, reset=limit_info["reset"])
     except Exception as e:
         logger.exception("Failed to get rate limit info")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.put("/rate-limits", response_model=RateLimitInfo)
 async def update_rate_limit(update: RateLimitUpdate, _: CurrentUser = Depends(get_admin_user)):
     """Update the rate limit for a user."""
     try:
         if update.limit < 1:
-            raise HTTPException(
-                status_code=status.HTTP_400_BAD_REQUEST,
-                detail="Rate limit must be at least 1"
-            )
+            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Rate limit must be at least 1")
         # Write-through: set DB limit via limiter API (now writes to users table) and keep cache in sync
         await rate_limiter.set_user_limit(update.user_id, update.limit)
         return await get_rate_limit(update.user_id, _)
@@ -189,9 +196,11 @@
         logger.exception("Failed to update rate limit")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 class RateLimitBatchRequest(BaseModel):
     user_ids: List[str]
 
+
 @router.post("/rate-limits/batch", response_model=List[RateLimitInfo])
 async def get_rate_limits_batch(req: RateLimitBatchRequest, _: CurrentUser = Depends(get_admin_user)):
     """Batch fetch rate limit info for a list of users."""
@@ -209,6 +218,7 @@
         logger.exception("Failed to fetch batch rate limits")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.get("/users/{user_id}/runs/history", response_model=UserRunHistoryResponse)
 async def get_user_runs_history_admin(
     user_id: str,
@@ -216,7 +226,7 @@
     limit: int = Query(100, ge=1, le=366),
     offset: int = Query(0, ge=0),
     db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
+    _: CurrentUser = Depends(get_admin_user),
 ):
     """Admin: Get daily counts of runs for a user over a period, grouped by day.
 
@@ -239,9 +249,11 @@
                 WHERE {where_sql}
                 GROUP BY 1
                 ORDER BY day DESC
-                LIMIT ${len(params)+1} OFFSET ${len(params)+2}
+                LIMIT ${len(params) + 1} OFFSET ${len(params) + 2}
                 """,
-                *params, limit, offset
+                *params,
+                limit,
+                offset,
             )
             total_days = await conn.fetchval(
                 f"""
@@ -252,7 +264,7 @@
                     GROUP BY date_trunc('day', pr.start_time)
                 ) t
                 """,
-                *params
+                *params,
             )
         series = [DailyRunCount(day=row["day"].date().isoformat(), count=row["count"]) for row in rows]
         return UserRunHistoryResponse(user_id=user_id, series=series, total_days=int(total_days))
@@ -260,6 +272,7 @@
         logger.exception("Failed to get user run history (admin)")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.post("/rate-limits/reset", status_code=status.HTTP_204_NO_CONTENT)
 async def reset_rate_limits(_: CurrentUser = Depends(get_admin_user)):
     """Reset all rate limit counters."""
@@ -270,6 +283,7 @@
         logger.exception("Failed to reset rate limits")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 # Users listing for admin dashboard
 @router.get("/users", response_model=AdminUsersListResponse)
 async def list_users(
@@ -280,19 +294,19 @@
     offset: int = Query(0, ge=0),
     include_profile: bool = Query(False, description="Include full user profile object in results"),
     db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
+    _: CurrentUser = Depends(get_admin_user),
 ):
     try:
         where = ["1=1"]
         params: list = []
         if query:
-            where.append("(u.email ILIKE $%d OR up.firm_description ILIKE $%d)" % (len(params)+1, len(params)+2))
+            where.append("(u.email ILIKE $%d OR up.firm_description ILIKE $%d)" % (len(params) + 1, len(params) + 2))
             params.extend([f"%{query}%", f"%{query}%"])
         if registration_status:
-            where.append("u.registration_status = $%d" % (len(params)+1))
+            where.append("u.registration_status = $%d" % (len(params) + 1))
             params.append(registration_status)
         if is_active is not None:
-            where.append("COALESCE(u.is_active, TRUE) = $%d" % (len(params)+1))
+            where.append("COALESCE(u.is_active, TRUE) = $%d" % (len(params) + 1))
             params.append(is_active)
         where_sql = " AND ".join(where)
         async with db.pool.acquire() as conn:
@@ -319,13 +333,14 @@
                 LEFT JOIN user_profiles up ON up.user_id = u.user_id
                 WHERE {where_sql}
                 ORDER BY u.created_at DESC
-                LIMIT ${len(params)+1} OFFSET ${len(params)+2}
+                LIMIT ${len(params) + 1} OFFSET ${len(params) + 2}
                 """,
-                *params, limit, offset
+                *params,
+                limit,
+                offset,
             )
             total = await conn.fetchval(
-                f"SELECT COUNT(*) FROM users u LEFT JOIN user_profiles up ON up.user_id = u.user_id WHERE {where_sql}",
-                *params
+                f"SELECT COUNT(*) FROM users u LEFT JOIN user_profiles up ON up.user_id = u.user_id WHERE {where_sql}", *params
             )
         users: List[AdminUserSummary] = []
         for r in rows:
@@ -342,12 +357,14 @@
                     elif isinstance(raw_inv, (dict, list)):
                         inv_obj = raw_inv
 
-                if any([
-                    r["p_firm_description"],
-                    r["p_key_differentiators"],
-                    r["p_key_objectives"],
-                    inv_obj is not None,
-                ]):
+                if any(
+                    [
+                        r["p_firm_description"],
+                        r["p_key_differentiators"],
+                        r["p_key_objectives"],
+                        inv_obj is not None,
+                    ]
+                ):
                     profile_obj = AdminUserProfileSummary(
                         firm_description=r["p_firm_description"],
                         key_differentiators=r["p_key_differentiators"],
@@ -357,24 +374,27 @@
                         profile_updated_at=r["p_updated_at"].isoformat() if r["p_updated_at"] else None,
                     )
 
-            users.append(AdminUserSummary(
-                user_id=r["user_id"],
-                email=r["email"],
-                firm_name=r["firm_name"],
-                is_active=r["is_active"],
-                is_admin=r["is_admin"],
-                registration_status=r["registration_status"],
-                daily_agent_run_limit=r["daily_agent_run_limit"],
-                tamradar_radar_limit=r.get("tamradar_radar_limit"),
-                tamradar_deletion_limit=r.get("tamradar_deletion_limit"),
-                created_at=r["created_at"].isoformat() if r["created_at"] else None,
-                profile=profile_obj
-            ))
+            users.append(
+                AdminUserSummary(
+                    user_id=r["user_id"],
+                    email=r["email"],
+                    firm_name=r["firm_name"],
+                    is_active=r["is_active"],
+                    is_admin=r["is_admin"],
+                    registration_status=r["registration_status"],
+                    daily_agent_run_limit=r["daily_agent_run_limit"],
+                    tamradar_radar_limit=r.get("tamradar_radar_limit"),
+                    tamradar_deletion_limit=r.get("tamradar_deletion_limit"),
+                    created_at=r["created_at"].isoformat() if r["created_at"] else None,
+                    profile=profile_obj,
+                )
+            )
         return AdminUsersListResponse(users=users, total=int(total))
     except Exception:
         logger.exception("Failed to list users")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.post("/approved-users", status_code=201)
 async def add_approved_user(data: ApprovedUserAdd, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)):
     """Add an email to the approved users list."""
@@ -388,19 +408,16 @@
                 VALUES ($1, NOW(), $2)
                 ON CONFLICT (email) DO NOTHING
                 """,
-                normalized_email, _.email.lower().strip() if _.email else "admin"
+                normalized_email,
+                _.email.lower().strip() if _.email else "admin",
             )
-        await db.log_admin_action(
-            admin_user_id=_.user_id,
-            action="add_approved_user",
-            target_resource=data.email,
-            details={"approved_by": _.email}
-        )
+        await db.log_admin_action(admin_user_id=_.user_id, action="add_approved_user", target_resource=data.email, details={"approved_by": _.email})
         return {"status": "success", "email": data.email}
     except Exception as e:
         logger.exception("Failed to add approved user")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.delete("/approved-users/{email}", status_code=204)
 async def remove_approved_user(email: str, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)):
     """Remove an email from the approved users list."""
@@ -408,21 +425,14 @@
         # Normalize email from path parameter
         normalized_email = email.lower().strip()
         async with db.pool.acquire() as conn:
-            await conn.execute(
-                "DELETE FROM approved_users WHERE LOWER(email) = LOWER($1)",
-                normalized_email
-            )
-        await db.log_admin_action(
-            admin_user_id=_.user_id,
-            action="remove_approved_user",
-            target_resource=email,
-            details=None
-        )
+            await conn.execute("DELETE FROM approved_users WHERE LOWER(email) = LOWER($1)", normalized_email)
+        await db.log_admin_action(admin_user_id=_.user_id, action="remove_approved_user", target_resource=email, details=None)
         return None
     except Exception as e:
         logger.exception("Failed to remove approved user")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.get("/approved-users", response_model=List[EmailStr])
 async def list_approved_users(db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)):
     """List all approved user emails."""
@@ -434,12 +444,9 @@
         logger.exception("Failed to list approved users")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.patch("/users/set-daily-limit")
-async def set_daily_agent_run_limit(
-    data: DailyLimitUpdate,
-    db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def set_daily_agent_run_limit(data: DailyLimitUpdate, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)):
     """Set the daily agent run limit for a user by email (admin only)."""
     try:
         # Email is already normalized by Pydantic validator, but ensure it's lowercase
@@ -450,36 +457,26 @@
                 raise HTTPException(status_code=404, detail="User not found")
             old_limit = user["daily_agent_run_limit"]
             await conn.execute(
-                "UPDATE users SET daily_agent_run_limit = $1 WHERE LOWER(email) = LOWER($2)",
-                data.daily_agent_run_limit, normalized_email
+                "UPDATE users SET daily_agent_run_limit = $1 WHERE LOWER(email) = LOWER($2)", data.daily_agent_run_limit, normalized_email
             )
         await db.log_admin_action(
             admin_user_id=_.user_id,
             action="set_daily_agent_run_limit",
             target_resource=data.email,
-            details={"old_limit": old_limit, "new_limit": data.daily_agent_run_limit}
+            details={"old_limit": old_limit, "new_limit": data.daily_agent_run_limit},
         )
-        return {
-            "email": data.email,
-            "old_limit": old_limit,
-            "new_limit": data.daily_agent_run_limit
-        }
+        return {"email": data.email, "old_limit": old_limit, "new_limit": data.daily_agent_run_limit}
     except HTTPException:
         raise
     except Exception as e:
         error_msg = str(e) if e else "An unexpected error occurred"
         logger.exception(f"Failed to set daily agent run limit for email {data.email}: {error_msg}")
-        raise HTTPException(
-            status_code=500, 
-            detail=f"An unexpected error occurred: {error_msg}"
-        )
+        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {error_msg}")
+
 
 @router.patch("/users/{user_id}/daily-limit")
 async def set_daily_agent_run_limit_by_user_id(
-    user_id: str,
-    data: DailyLimitUpdateByUserId,
-    db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
+    user_id: str, data: DailyLimitUpdateByUserId, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)
 ):
     """Set the daily agent run limit for a user by user_id (admin only)."""
     try:
@@ -487,185 +484,121 @@
         if data.daily_agent_run_limit < 0:
             raise HTTPException(status_code=400, detail="daily_agent_run_limit must be non-negative")
         async with db.pool.acquire() as conn:
-            user = await conn.fetchrow(
-                "SELECT email, daily_agent_run_limit, tamradar_radar_limit FROM users WHERE user_id = $1", 
-                user_id
-            )
+            user = await conn.fetchrow("SELECT email, daily_agent_run_limit, tamradar_radar_limit FROM users WHERE user_id = $1", user_id)
             if not user:
                 raise HTTPException(status_code=404, detail="User not found")
             old_limit = user["daily_agent_run_limit"]
             old_radar_limit = user.get("tamradar_radar_limit")
-            
+
             # Update daily agent run limit
-            await conn.execute(
-                "UPDATE users SET daily_agent_run_limit = $1 WHERE user_id = $2",
-                data.daily_agent_run_limit, user_id
-            )
-            
+            await conn.execute("UPDATE users SET daily_agent_run_limit = $1 WHERE user_id = $2", data.daily_agent_run_limit, user_id)
+
             # Update TAMradar radar limit if provided
             if data.tamradar_radar_limit is not None:
                 if data.tamradar_radar_limit < 0:
                     raise HTTPException(status_code=400, detail="TAMradar radar limit must be non-negative")
-                await conn.execute(
-                    "UPDATE users SET tamradar_radar_limit = $1 WHERE user_id = $2",
-                    data.tamradar_radar_limit, user_id
-            )
+                await conn.execute("UPDATE users SET tamradar_radar_limit = $1 WHERE user_id = $2", data.tamradar_radar_limit, user_id)
         # Also update the rate limiter to keep cache/enforcement in sync with DB
         limiter = RateLimiter()
         await limiter.set_user_limit(user_id, data.daily_agent_run_limit)
-        
+
         log_details = {"old_limit": old_limit, "new_limit": data.daily_agent_run_limit}
         if data.tamradar_radar_limit is not None:
             log_details["old_radar_limit"] = old_radar_limit
             log_details["new_radar_limit"] = data.tamradar_radar_limit
-        
-        await db.log_admin_action(
-            admin_user_id=_.user_id,
-            action="set_daily_agent_run_limit",
-            target_resource=user_id,
-            details=log_details
-        )
-        
-        response = {
-            "user_id": user_id,
-            "old_limit": old_limit,
-            "new_limit": data.daily_agent_run_limit
-        }
+
+        await db.log_admin_action(admin_user_id=_.user_id, action="set_daily_agent_run_limit", target_resource=user_id, details=log_details)
+
+        response = {"user_id": user_id, "old_limit": old_limit, "new_limit": data.daily_agent_run_limit}
         if data.tamradar_radar_limit is not None:
             response["old_radar_limit"] = old_radar_limit
             response["new_radar_limit"] = data.tamradar_radar_limit
-        
-        return {
-            "status": "success",
-            **response
-        }
+
+        return {"status": "success", **response}
     except HTTPException:
         raise
     except ValueError as e:
         error_msg = str(e) if e else "Invalid input"
         logger.warning(f"Validation error setting daily limit for user {user_id}: {error_msg}")
-        raise HTTPException(
-            status_code=400, 
-            detail=f"Invalid input: {error_msg}"
-        )
+        raise HTTPException(status_code=400, detail=f"Invalid input: {error_msg}")
     except Exception as e:
         error_msg = str(e) if e else "An unexpected error occurred"
         logger.exception(f"Failed to set daily agent run limit for user {user_id}: {error_msg}")
-        raise HTTPException(
-            status_code=500, 
-            detail=f"An unexpected error occurred: {error_msg}"
-        )
+        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {error_msg}")
+
 
 @router.patch("/users/set-deletion-limit")
-async def set_deletion_limit(
-    data: DeletionLimitUpdate,
-    db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def set_deletion_limit(data: DeletionLimitUpdate, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)):
     """Set the TAMradar deletion limit for a user by email (admin only)."""
     try:
         # Email is already normalized by Pydantic validator, but ensure it's lowercase
         normalized_email = data.email.lower().strip()
         if data.tamradar_deletion_limit < 0:
             raise HTTPException(status_code=400, detail="tamradar_deletion_limit must be non-negative")
-        
+
         async with db.pool.acquire() as conn:
-            user = await conn.fetchrow(
-                "SELECT user_id, email, tamradar_deletion_limit FROM users WHERE LOWER(email) = LOWER($1)", 
-                normalized_email
-            )
+            user = await conn.fetchrow("SELECT user_id, email, tamradar_deletion_limit FROM users WHERE LOWER(email) = LOWER($1)", normalized_email)
             if not user:
                 raise HTTPException(status_code=404, detail="User not found")
             old_limit = user.get("tamradar_deletion_limit")
             await conn.execute(
-                "UPDATE users SET tamradar_deletion_limit = $1 WHERE LOWER(email) = LOWER($2)",
-                data.tamradar_deletion_limit, normalized_email
+                "UPDATE users SET tamradar_deletion_limit = $1 WHERE LOWER(email) = LOWER($2)", data.tamradar_deletion_limit, normalized_email
             )
         await db.log_admin_action(
             admin_user_id=_.user_id,
             action="set_deletion_limit",
             target_resource=data.email,
-            details={"old_limit": old_limit, "new_limit": data.tamradar_deletion_limit}
+            details={"old_limit": old_limit, "new_limit": data.tamradar_deletion_limit},
         )
-        return {
-            "email": data.email,
-            "old_limit": old_limit,
-            "new_limit": data.tamradar_deletion_limit
-        }
+        return {"email": data.email, "old_limit": old_limit, "new_limit": data.tamradar_deletion_limit}
     except HTTPException:
         raise
     except Exception as e:
         error_msg = str(e) if e else "An unexpected error occurred"
         logger.exception(f"Failed to set deletion limit for email {data.email}: {error_msg}")
-        raise HTTPException(
-            status_code=500, 
-            detail=f"An unexpected error occurred: {error_msg}"
-        )
+        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {error_msg}")
+
 
 @router.patch("/users/{user_id}/deletion-limit")
 async def set_deletion_limit_by_user_id(
-    user_id: str,
-    data: DeletionLimitUpdateByUserId,
-    db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
+    user_id: str, data: DeletionLimitUpdateByUserId, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)
 ):
     """Set the TAMradar deletion limit for a user by user_id (admin only)."""
     try:
         # Validate input
         if data.tamradar_deletion_limit < 0:
             raise HTTPException(status_code=400, detail="tamradar_deletion_limit must be non-negative")
-        
+
         async with db.pool.acquire() as conn:
-            user = await conn.fetchrow(
-                "SELECT email, tamradar_deletion_limit FROM users WHERE user_id = $1", 
-                user_id
-            )
+            user = await conn.fetchrow("SELECT email, tamradar_deletion_limit FROM users WHERE user_id = $1", user_id)
             if not user:
                 raise HTTPException(status_code=404, detail="User not found")
             old_limit = user.get("tamradar_deletion_limit")
-            
-            await conn.execute(
-                "UPDATE users SET tamradar_deletion_limit = $1 WHERE user_id = $2",
-                data.tamradar_deletion_limit, user_id
-            )
-        
+
+            await conn.execute("UPDATE users SET tamradar_deletion_limit = $1 WHERE user_id = $2", data.tamradar_deletion_limit, user_id)
+
         await db.log_admin_action(
             admin_user_id=_.user_id,
             action="set_deletion_limit",
             target_resource=user_id,
-            details={"old_limit": old_limit, "new_limit": data.tamradar_deletion_limit}
+            details={"old_limit": old_limit, "new_limit": data.tamradar_deletion_limit},
         )
-        
-        return {
-            "status": "success",
-            "user_id": user_id,
-            "old_limit": old_limit,
-            "new_limit": data.tamradar_deletion_limit
-        }
+
+        return {"status": "success", "user_id": user_id, "old_limit": old_limit, "new_limit": data.tamradar_deletion_limit}
     except HTTPException:
         raise
     except ValueError as e:
         error_msg = str(e) if e else "Invalid input"
         logger.warning(f"Validation error setting deletion limit for user {user_id}: {error_msg}")
-        raise HTTPException(
-            status_code=400, 
-            detail=f"Invalid input: {error_msg}"
-        )
+        raise HTTPException(status_code=400, detail=f"Invalid input: {error_msg}")
     except Exception as e:
         error_msg = str(e) if e else "An unexpected error occurred"
         logger.exception(f"Failed to set deletion limit for user {user_id}: {error_msg}")
-        raise HTTPException(
-            status_code=500, 
-            detail=f"An unexpected error occurred: {error_msg}"
-        )
+        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {error_msg}")
+
 
 @router.patch("/users/{user_id}/status")
-async def update_user_status(
-    user_id: str,
-    data: UserStatusUpdate,
-    db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def update_user_status(user_id: str, data: UserStatusUpdate, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)):
     """Enable/disable a user account (admin only)."""
     try:
         async with db.pool.acquire() as conn:
@@ -678,7 +611,7 @@
             admin_user_id=_.user_id,
             action="set_user_status",
             target_resource=user_id,
-            details={"old_status": old_status, "new_status": data.is_active}
+            details={"old_status": old_status, "new_status": data.is_active},
         )
         return {"user_id": user_id, "old_status": old_status, "new_status": data.is_active}
     except HTTPException:
@@ -687,6 +620,7 @@
         logger.exception("Failed to update user status")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.get("/feedback")
 async def get_feedback(
     user_id: str = Query(None),
@@ -697,24 +631,19 @@
     limit: int = Query(50, ge=1, le=200),
     offset: int = Query(0, ge=0),
     db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
+    _: CurrentUser = Depends(get_admin_user),
 ):
     """Admin: Retrieve user feedback with filters."""
     try:
         feedback = await db.get_feedback_admin(
-            user_id=user_id,
-            session_id=session_id,
-            category=category,
-            from_date=from_date,
-            to_date=to_date,
-            limit=limit,
-            offset=offset
+            user_id=user_id, session_id=session_id, category=category, from_date=from_date, to_date=to_date, limit=limit, offset=offset
         )
         return feedback
     except Exception as e:
         logger.exception("Failed to get feedback")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.get("/company-searches")
 async def get_company_searches(
     user_id: str = Query(None),
@@ -723,7 +652,7 @@
     limit: int = Query(50, ge=1, le=200),
     offset: int = Query(0, ge=0),
     db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
+    _: CurrentUser = Depends(get_admin_user),
 ):
     """Admin: Retrieve company search runs for users with optional filters.
 
@@ -759,9 +688,10 @@
                 LEFT JOIN prospecting_runs pr ON pr.run_id LIKE csr.run_id || '_sel_%' AND pr.user_id = csr.user_id
                 WHERE {where_sql}
                 ORDER BY csr.created_at DESC
-                LIMIT ${len(params)+1}
+                LIMIT ${len(params) + 1}
                 """,
-                *params, fetch_count
+                *params,
+                fetch_count,
             )
 
         general_results = []
@@ -782,17 +712,19 @@
                 if isinstance(comps, list):
                     companies = comps
 
-            general_results.append({
-                "run_id": r["run_id"],
-                "user_id": r["user_id"],
-                "session_id": r["session_id"],
-                "created_at": r["created_at"].isoformat() if r["created_at"] else None,
-                "search_query": r["search_query"],
-                "companies": companies,
-                "companies_count": len(companies),
-                "selected_company_id": r["selected_company_id"],
-                "selected_company_name": r["selected_company_name"],
-            })
+            general_results.append(
+                {
+                    "run_id": r["run_id"],
+                    "user_id": r["user_id"],
+                    "session_id": r["session_id"],
+                    "created_at": r["created_at"].isoformat() if r["created_at"] else None,
+                    "search_query": r["search_query"],
+                    "companies": companies,
+                    "companies_count": len(companies),
+                    "selected_company_id": r["selected_company_id"],
+                    "selected_company_name": r["selected_company_name"],
+                }
+            )
 
         # 2) Specific-company selections from prospecting_runs (+ companies data if available)
         specific_where = ["pr.workflow_type = 'specific_company'"]
@@ -817,9 +749,10 @@
                 LEFT JOIN companies c ON c.run_id = pr.run_id AND c.user_id = pr.user_id
                 WHERE {specific_where_sql}
                 ORDER BY pr.start_time DESC
-                LIMIT ${len(specific_params)+1}
+                LIMIT ${len(specific_params) + 1}
                 """,
-                *specific_params, fetch_count
+                *specific_params,
+                fetch_count,
             )
 
         specific_results = []
@@ -828,32 +761,31 @@
             comp_domain = r["c_domain"]
             comp_location = r["c_location"]
             # Normalize a single-company array
-            companies = [{
-                "company_name": comp_name,
-                "company_domain": comp_domain,
-                "location": comp_location
-            }]
-            specific_results.append({
-                "run_id": r["run_id"],
-                "user_id": r["user_id"],
-                "session_id": r["session_id"],
-                "created_at": r["created_at"].isoformat() if r["created_at"] else None,
-                "search_query": f"Specific company: {comp_name}" if comp_name else None,
-                "companies": companies,
-                "companies_count": 1,
-                "selected_company_id": r["company_id"],
-                "selected_company_name": comp_name,
-            })
+            companies = [{"company_name": comp_name, "company_domain": comp_domain, "location": comp_location}]
+            specific_results.append(
+                {
+                    "run_id": r["run_id"],
+                    "user_id": r["user_id"],
+                    "session_id": r["session_id"],
+                    "created_at": r["created_at"].isoformat() if r["created_at"] else None,
+                    "search_query": f"Specific company: {comp_name}" if comp_name else None,
+                    "companies": companies,
+                    "companies_count": 1,
+                    "selected_company_id": r["company_id"],
+                    "selected_company_name": comp_name,
+                }
+            )
 
         # Merge, sort, and paginate
         combined = general_results + specific_results
         combined.sort(key=lambda x: x.get("created_at") or "", reverse=True)
-        sliced = combined[offset:offset + limit]
+        sliced = combined[offset : offset + limit]
         return sliced
     except Exception:
         logger.exception("Failed to get company searches (admin)")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.get("/users/bios")
 async def get_user_bios(
     user_id: str = Query(None),
@@ -861,27 +793,19 @@
     limit: int = Query(50, ge=1, le=200),
     offset: int = Query(0, ge=0),
     db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
+    _: CurrentUser = Depends(get_admin_user),
 ):
     """Admin: Retrieve user bios with filters."""
     try:
-        bios = await db.get_user_bios_admin(
-            user_id=user_id,
-            firm_name=firm_name,
-            limit=limit,
-            offset=offset
-        )
+        bios = await db.get_user_bios_admin(user_id=user_id, firm_name=firm_name, limit=limit, offset=offset)
         return bios
     except Exception as e:
         logger.exception("Failed to get user bios")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.get("/users/{user_id}/profile/history")
-async def get_user_profile_history_admin(
-    user_id: str,
-    db=Depends(get_db),
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def get_user_profile_history_admin(user_id: str, db=Depends(get_db), _: CurrentUser = Depends(get_admin_user)):
     """Admin: Retrieve full bio version history for a user."""
     try:
         history = await db.get_user_profile_history_admin(user_id)
@@ -895,23 +819,21 @@
 # TAMradar Admin Endpoints
 # ============================================================================
 
+
 @router.get("/tamradar/users/{user_id}/radars")
-async def get_user_tamradar_radars(
-    user_id: str,
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def get_user_tamradar_radars(user_id: str, _: CurrentUser = Depends(get_admin_user)):
     """
     Admin: Get all TAMradar radars for a specific user.
-    
+
     Returns watchlist with company name, domain, and subscription date.
     """
     try:
         from app.services.tamradar_service import tamradar_service
         from app.utils.global_db import get_global_db
         import json
-        
+
         radars = await tamradar_service.get_user_radars(user_id)
-        
+
         # Get user's radar limit
         db = await get_global_db()
         user_limit = 5  # default
@@ -923,13 +845,13 @@
                 )
                 if user_row and user_row["tamradar_radar_limit"] is not None:
                     user_limit = user_row["tamradar_radar_limit"]
-        
+
         # Process radars to extract company info
         watchlist = []
         categories_used = set()
         newsletter_subscriptions = 0
         hot_alert_subscriptions = 0
-        
+
         for r in radars:
             radar_config = r.get("radar_config", {})
             # Handle case where radar_config might be a JSON string
@@ -938,47 +860,49 @@
                     radar_config = json.loads(radar_config)
                 except:
                     radar_config = {}
-            
+
             # Extract company name and domain
             company_name = radar_config.get("company_name") or radar_config.get("domain") or "N/A"
             domain = radar_config.get("domain") or "N/A"
-            
+
             # Track categories and newsletter subscriptions
             watchlist_category = r.get("watchlist_category")
             if watchlist_category:
                 categories_used.add(watchlist_category)
-            
+
             if r.get("weekly_wrapup_email", False):
                 newsletter_subscriptions += 1
-            
+
             if r.get("tier1_email_alerts", False):
                 hot_alert_subscriptions += 1
-            
+
             # Handle grouped structure (group_by_company=True returns radar_ids and radar_types as arrays)
             radar_ids = r.get("radar_ids", [])
             radar_types = r.get("radar_types", [])
-            
+
             # Use first radar_id and radar_type if available (for backward compatibility)
             # Or join them if multiple radars per company
             radar_id = radar_ids[0] if radar_ids else None
             radar_type = radar_types[0] if radar_types else None
-            
-            watchlist.append({
-                "radar_id": radar_id,  # Primary radar_id (first one if multiple)
-                "radar_ids": radar_ids,  # All radar IDs for this company
-                "radar_category": r.get("radar_category"),
-                "radar_type": radar_type,  # Primary radar_type (first one if multiple)
-                "radar_types": radar_types,  # All radar types for this company
-                "company_name": company_name,
-                "domain": domain,
-                "status": r.get("status"),
-                "subscribed_at": r.get("subscribed_at").isoformat() if r.get("subscribed_at") else None,
-                "is_owner": r.get("is_owner", False),
-                "watchlist_category": watchlist_category,
-                "weekly_wrapup_email": r.get("weekly_wrapup_email", False),
-                "tier1_email_alerts": r.get("tier1_email_alerts", False),  # Hot radar alert email subscription
-            })
-        
+
+            watchlist.append(
+                {
+                    "radar_id": radar_id,  # Primary radar_id (first one if multiple)
+                    "radar_ids": radar_ids,  # All radar IDs for this company
+                    "radar_category": r.get("radar_category"),
+                    "radar_type": radar_type,  # Primary radar_type (first one if multiple)
+                    "radar_types": radar_types,  # All radar types for this company
+                    "company_name": company_name,
+                    "domain": domain,
+                    "status": r.get("status"),
+                    "subscribed_at": r.get("subscribed_at").isoformat() if r.get("subscribed_at") else None,
+                    "is_owner": r.get("is_owner", False),
+                    "watchlist_category": watchlist_category,
+                    "weekly_wrapup_email": r.get("weekly_wrapup_email", False),
+                    "tier1_email_alerts": r.get("tier1_email_alerts", False),  # Hot radar alert email subscription
+                }
+            )
+
         return {
             "status": "success",
             "user_id": user_id,
@@ -995,17 +919,15 @@
 
 
 @router.get("/tamradar/radars/{radar_id}/users")
-async def get_radar_subscribers(
-    radar_id: str,
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def get_radar_subscribers(radar_id: str, _: CurrentUser = Depends(get_admin_user)):
     """Admin: Get all users subscribed to a specific radar."""
     try:
         from app.utils.global_db import get_global_db
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         async with db.pool.acquire() as conn:
             rows = await conn.fetch(
                 """
@@ -1045,15 +967,16 @@
     radar_category: Optional[str] = Query(None, description="Filter by category: company, contact, industry"),
     limit: int = Query(100, ge=1, le=1000),
     offset: int = Query(0, ge=0),
-    _: CurrentUser = Depends(get_admin_user)
+    _: CurrentUser = Depends(get_admin_user),
 ):
     """Admin: List all TAMradar radars with user counts."""
     try:
         from app.utils.global_db import get_global_db
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         async with db.pool.acquire() as conn:
             where_clauses = []
             params = []
@@ -1085,7 +1008,9 @@
                 ORDER BY r.created_at DESC
                 LIMIT ${param_count + 1} OFFSET ${param_count + 2}
                 """,
-                *params, limit, offset
+                *params,
+                limit,
+                offset,
             )
 
             return {
@@ -1117,11 +1042,9 @@
     """Admin: Sync all radars from TAMradar API to local database."""
     try:
         from app.services.tamradar_service import tamradar_service
+
         result = await tamradar_service.sync_radars_from_tamradar()
-        return {
-            "status": "success",
-            "data": result
-        }
+        return {"status": "success", "data": result}
     except Exception as e:
         logger.exception("Failed to sync TAMradar radars")
         raise HTTPException(status_code=500, detail="Failed to sync radars")
@@ -1134,15 +1057,16 @@
     radar_type: Optional[str] = Query(None, description="Filter by radar_type"),
     limit: int = Query(100, ge=1, le=1000),
     offset: int = Query(0, ge=0),
-    _: CurrentUser = Depends(get_admin_user)
+    _: CurrentUser = Depends(get_admin_user),
 ):
     """Admin: List TAMradar findings with optional filters."""
     try:
         from app.utils.global_db import get_global_db
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         async with db.pool.acquire() as conn:
             where_clauses = []
             params = []
@@ -1216,41 +1140,38 @@
 
 
 @router.get("/tamradar/balance")
-async def get_tamradar_balance(
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def get_tamradar_balance(_: CurrentUser = Depends(get_admin_user)):
     """Admin: Get current TAMradar account balance and stats."""
     try:
         from app.services.tamradar_service import tamradar_service, TAMradarServiceError
+
         logger.info("Admin requesting TAMradar account balance")
-        
+
         # Get account summary from TAMradar API (primary source)
         # According to docs: https://tamradar.readme.io/reference/getaccountsummary
         # This returns balance, active/inactive radar counts, and usage metrics
         summary = await tamradar_service.get_account_summary()
-        
+
         if not summary:
             logger.warning("TAMradar account summary returned None - checking API key and configuration")
             # Check if it's a configuration issue
             if not tamradar_service.api_key:
                 logger.error("TAMradar API key not configured")
-                raise HTTPException(
-                    status_code=500, 
-                    detail="TAMradar API key not configured. Please check environment variables."
-                )
+                raise HTTPException(status_code=500, detail="TAMradar API key not configured. Please check environment variables.")
             # TAMradar API is returning errors - this is a TAMradar service issue
             logger.error("TAMradar account summary returned None - TAMradar API returned error")
             raise HTTPException(
-                status_code=503, 
-                detail="Service API is currently unavailable. This appears to be a temporary issue. Please try again later or contact support."
+                status_code=503,
+                detail="Service API is currently unavailable. This appears to be a temporary issue. Please try again later or contact support.",
             )
 
         # Use TAMradar API data as the source of truth for balance
         # But also query our database for complete status breakdown
         summary_dict = summary.model_dump()
-        
+
         # Get complete breakdown from our database (includes pending, active, failed)
         from app.utils.global_db import get_global_db
+
         db = await get_global_db()
         db_breakdown = {}
         if db:
@@ -1265,45 +1186,41 @@
                     """
                 )
                 db_breakdown = {row["status"]: row["count"] for row in status_counts}
-                
+
                 # Get total count
                 total_from_db = await conn.fetchval("SELECT COUNT(*) FROM tamradar_radars")
                 db_breakdown["total"] = total_from_db
-        
+
         # Add database breakdown to response
         summary_dict["account"]["db_breakdown"] = db_breakdown
-        
+
         # Log the exact response structure being sent to frontend
         import json
+
         logger.info(f"Response being sent to frontend: {json.dumps({'status': 'success', 'data': summary_dict}, indent=2, default=str)}")
-        logger.info(f"Successfully retrieved account summary from TAMradar: balance=${summary.balance_remaining_usd}, active_radars={summary.account.active_radars}, total_radars={summary.account.total_radars}")
+        logger.info(
+            f"Successfully retrieved account summary from TAMradar: balance=${summary.balance_remaining_usd}, active_radars={summary.account.active_radars}, total_radars={summary.account.total_radars}"
+        )
         logger.info(f"Database breakdown: {db_breakdown}")
         return {"status": "success", "data": summary_dict}
     except HTTPException:
         raise
     except TAMradarServiceError as e:
         logger.exception(f"TAMradarServiceError getting balance: {e}")
-        raise HTTPException(
-            status_code=503,
-            detail=f"Service API error: {str(e)}"
-        )
+        raise HTTPException(status_code=503, detail=f"Service API error: {str(e)}")
     except Exception as e:
         logger.exception(f"Failed to get account summary: {e}")
-        raise HTTPException(
-            status_code=500, 
-            detail=f"An unexpected error occurred: {str(e)}"
-        )
+        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")
 
 
 @router.post("/tamradar/balance/check")
-async def check_balance_manual(
-    _: CurrentUser = Depends(get_admin_user)
-):
+async def check_balance_manual(_: CurrentUser = Depends(get_admin_user)):
     """Admin: Manually trigger balance check and alert."""
     try:
         from app.services.tamradar_balance_monitor import tamradar_balance_monitor
+
         await tamradar_balance_monitor.check_balance_and_alert()
         return {"status": "success", "message": "Balance check completed"}
     except Exception as e:
         logger.exception("Failed to check balance")
-        raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.") 
+        raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")

--- app/routers/auth.py
+++ app/routers/auth.py
@@ -46,6 +46,7 @@
     cleaned_stem = _FILENAME_SANITIZE_PATTERN.sub("_", stem).strip("_") or "document"
     return f"{cleaned_stem[:150]}.pdf"
 
+
 class ApprovalStatusResponse(BaseModel):
     approved: bool
 
@@ -145,9 +146,7 @@
 
     logger.info("User profile creation attempt", extra={"user_id": current_user.user_id})
     try:
-        result = await auth_service.create_user_profile(
-            current_user.user_id, profile, db, background_tasks=background_tasks
-        )
+        result = await auth_service.create_user_profile(current_user.user_id, profile, db, background_tasks=background_tasks)
         logger.info("User profile created successfully", extra={"user_id": current_user.user_id})
         return result
     except ValueError as exc:
@@ -176,9 +175,7 @@
 
     logger.info("User profile update attempt", extra={"user_id": current_user.user_id})
     try:
-        result = await auth_service.update_user_profile(
-            current_user.user_id, profile, db, background_tasks=background_tasks
-        )
+        result = await auth_service.update_user_profile(current_user.user_id, profile, db, background_tasks=background_tasks)
         logger.info("User profile updated successfully", extra={"user_id": current_user.user_id})
         return result
     except ValueError as exc:

--- app/routers/prospecting_api.py
+++ app/routers/prospecting_api.py
@@ -45,29 +45,33 @@
 
 router = APIRouter(prefix="/api/prospecting", dependencies=[Depends(rate_limit_dependency(60, 60))])
 
+
 # RIA Activity Tracking Helper Functions
 def generate_search_id() -> str:
     """Generate a unique search ID for tracking related activities."""
     return f"search_{int(time.time() * 1000)}_{uuid.uuid4().hex[:9]}"
 
+
 def extract_search_context(request: Request) -> Optional[dict]:
     """Extract search context from X-Search-Context header."""
     search_context_header = request.headers.get("X-Search-Context")
     if not search_context_header:
         return None
-    
+
     try:
         return json.loads(search_context_header)
     except json.JSONDecodeError as e:
         logger.warning(f"Invalid search context header: {e}")
         return None
 
+
 def get_session_id_from_request(request: Request) -> str:
     """Extract session ID from request headers or generate a new one."""
     # Try to get from Authorization header or other sources
     # For now, generate a session ID based on user and timestamp
     return f"session_{int(time.time())}_{uuid.uuid4().hex[:8]}"
 
+
 def format_search_params_for_storage(request_params: dict) -> dict:
     """Format search parameters for database storage."""
     # Remove None values and convert to proper types
@@ -84,6 +88,7 @@
 
 class StructuredSearchRequest(BaseModel):
     """Request model for structured form-based investor search."""
+
     user_id: str
     session_id: Optional[str] = None
     hq_city: Optional[str] = None
@@ -118,8 +123,7 @@
     # Verify run belongs to user and is in a selectable state
     async with db.pool.acquire() as conn:
         run_row = await conn.fetchrow(
-            "SELECT run_id, user_id, session_id, status, workflow_type FROM prospecting_runs WHERE run_id = $1",
-            request.run_id
+            "SELECT run_id, user_id, session_id, status, workflow_type FROM prospecting_runs WHERE run_id = $1", request.run_id
         )
     if not run_row:
         raise HTTPException(status_code=404, detail="Run not found")
@@ -138,7 +142,8 @@
             ORDER BY created_at DESC
             LIMIT 1
             """,
-            request.run_id, request.user_id
+            request.run_id,
+            request.user_id,
         )
     if results_rows:
         raw = results_rows[0]["search_results"]
@@ -179,11 +184,12 @@
             ORDER BY interaction_number DESC
             LIMIT 1
             """,
-            request.run_id, run_row["session_id"]
+            request.run_id,
+            run_row["session_id"],
         )
         if interaction and interaction.get("prompt"):
             original_prompt = interaction["prompt"]
-    
+
     # Extract Qdrant filters from search results metadata (for feedback collection)
     # The actual Qdrant filter fields are: hq_city, hq_country_region, hq_global_region,
     # primary_investor_types, aum, is_foundation, is_pension, is_endowment, is_insurance, is_charity
@@ -191,7 +197,7 @@
     if isinstance(raw, dict) and raw.get("search_metadata"):
         # Get qdrant_filters from metadata (stored during search)
         qdrant_filters_applied = raw["search_metadata"].get("qdrant_filters")
-        
+
         # If not found, qdrant_filters_applied will be None (filters weren't applied)
 
     # Generate new run_id for this company selection
@@ -203,7 +209,7 @@
         "parent_search_run_id": request.run_id,
         "original_prompt": original_prompt,
         "selection_index": selection_index,
-        "workflow_type": "company_selection"
+        "workflow_type": "company_selection",
     }
 
     async with db.pool.acquire() as conn:
@@ -212,18 +218,22 @@
             INSERT INTO prospecting_runs (run_id, user_id, session_id, search_params, company_name, company_id, status, workflow_type, start_time, created_at, updated_at)
             VALUES ($1, $2, $3, $4, $5, $6, 'processing', 'company_selection', NOW(), NOW(), NOW())
             """,
-            new_run_id, request.user_id, run_row["session_id"], json.dumps(search_params), company_name, company_id_to_set
+            new_run_id,
+            request.user_id,
+            run_row["session_id"],
+            json.dumps(search_params),
+            company_name,
+            company_id_to_set,
         )
-        
+
         # Store the prompt in session_interactions for the new run_id (so all prompts are in one place)
         if original_prompt:
             # Get current interaction count for the session
             session_info = await conn.fetchrow(
-                "SELECT interaction_count FROM sessions WHERE session_id = $1 AND user_id = $2",
-                run_row["session_id"], request.user_id
+                "SELECT interaction_count FROM sessions WHERE session_id = $1 AND user_id = $2", run_row["session_id"], request.user_id
             )
             interaction_number = (session_info.get("interaction_count") or 0) if session_info else 0
-            
+
             # Store the prompt in session_interactions with the new run_id
             interaction_id = f"interaction_{uuid.uuid4().hex[:12]}"
             await conn.execute(
@@ -232,13 +242,17 @@
                 (interaction_id, session_id, run_id, interaction_number, prompt, workflow_type, start_time)
                 VALUES ($1, $2, $3, $4, $5, $6, NOW())
                 """,
-                interaction_id, run_row["session_id"], new_run_id, interaction_number, original_prompt, "company_selection"
+                interaction_id,
+                run_row["session_id"],
+                new_run_id,
+                interaction_number,
+                original_prompt,
+                "company_selection",
             )
-            
+
             # Update session interaction count
             await conn.execute(
-                "UPDATE sessions SET interaction_count = interaction_count + 1, last_activity = NOW() WHERE session_id = $1",
-                run_row["session_id"]
+                "UPDATE sessions SET interaction_count = interaction_count + 1, last_activity = NOW() WHERE session_id = $1", run_row["session_id"]
             )
 
     # Reset progress and smoothing state for enrichment phase
@@ -250,10 +264,10 @@
     # Collect feedback data in background (only for company_search or general_search workflow)
     if run_row.get("workflow_type") in ("company_search", "general_search") and selected_company and original_prompt:
         from app.services.feedback_collection_service import collect_company_search_feedback
-        
+
         ranking_position = request.selected_company_index if request.selected_company_index else None
         company_domain = selected_company.get("company_domain") or selected_company.get("website_url") or "unknown"
-        
+
         if background_tasks is not None:
             background_tasks.add_task(
                 collect_company_search_feedback,
@@ -266,23 +280,24 @@
                 company_domain=company_domain,
                 ranking_position=ranking_position,
                 qdrant_filters_applied=qdrant_filters_applied,
-                db=db
+                db=db,
             )
-    
+
     # Kick off enrichment in background by reusing the orchestrator selection path
     session_context = SessionContext(session_id=run_row["session_id"], user_id=request.user_id)
     orchestrator = ProspectingOrchestrator(db)
     if background_tasks is not None:
         background_tasks.add_task(
-        orchestrator.handle_user_company_selection,
-        run_id=new_run_id,
-        user_id=request.user_id,
-        selected_company_index=(request.selected_company_index or 1),
-        session_context=session_context
-    )
+            orchestrator.handle_user_company_selection,
+            run_id=new_run_id,
+            user_id=request.user_id,
+            selected_company_index=(request.selected_company_index or 1),
+            session_context=session_context,
+        )
     else:
         # Fallback: schedule task directly if BackgroundTasks injection failed
         import asyncio
+
         asyncio.create_task(
             orchestrator.handle_user_company_selection(
                 run_id=new_run_id,
@@ -300,6 +315,7 @@
     )
     return resp
 
+
 @router.post("/search/structured")
 async def structured_investor_search(
     request: StructuredSearchRequest,
@@ -307,35 +323,35 @@
     current_user=Depends(require_user),
 ):
     """Search investors using structured form filters (HQ location, AUM range).
-    
+
     This endpoint bypasses LLM extraction and uses direct filter mapping,
     then performs vector search with user profile personalization.
     """
     if request.user_id != current_user.user_id:
         raise HTTPException(status_code=403, detail="Forbidden: user_id does not match authenticated user.")
-    
+
     start_time = time.time()
-    
+
     try:
         from app.services.vector_search.fund_lp_engine import FundLPMatchingEngine
         from app.agents.sub_agents.company_search_agent import CompanySearchAgent
-        
+
         # Build filters dict from request
         filters = {}
-        
+
         # Geographic filters
         if request.hq_city:
             filters["hq_city"] = [request.hq_city] if isinstance(request.hq_city, str) else request.hq_city
-        
+
         if request.hq_country:
             filters["hq_country_region"] = [request.hq_country] if isinstance(request.hq_country, str) else request.hq_country
-        
+
         if request.hq_region:
             # Map to valid regions if needed
             valid_regions = ["Africa", "Middle East", "Americas", "Europe", "Oceania", "Asia"]
             if request.hq_region in valid_regions:
                 filters["hq_global_region"] = [request.hq_region]
-        
+
         # AUM filters (convert millions to actual USD)
         aum_filters = {}
         if request.aum_min is not None:
@@ -344,49 +360,46 @@
             aum_filters["aum_max"] = request.aum_max * 1_000_000
         if aum_filters:
             filters["aum"] = aum_filters
-        
+
         # Initialize engine for investor type processing and search
         engine = FundLPMatchingEngine()
-        
+
         # Process investor types if provided
         if request.investor_types:
             # Detect flag-based categories (pension, endowment, foundation, insurance, charity)
             categories_or, cleaned_types = engine._detect_flag_categories(request.investor_types)
-            
+
             if categories_or:
                 filters["categories"] = categories_or
-            
+
             # Map remaining types to category IDs
             if cleaned_types:
                 investor_type_ids = await engine.match_investor_types(cleaned_types)
                 if investor_type_ids:
                     filters["primary_investor_types"] = investor_type_ids
-        
+
         # Perform structured search
         limit = request.limit or 50
-        
+
         search_response = await engine.search_with_structured_filters(
-            filters=filters,
-            user_id=request.user_id,
-            limit=limit,
-            use_cached_profile_embedding=True
+            filters=filters, user_id=request.user_id, limit=limit, use_cached_profile_embedding=True
         )
-        
+
         results = search_response.get("results", [])
         qdrant_filters = search_response.get("filters", {})
-        
+
         # Convert to company format (same as _vector_company_search)
         agent = CompanySearchAgent(output_dir="app/data", db=db)
         companies: List[Dict[str, Any]] = []
-        
+
         for r in results:
             payload = r.get("raw_payload", {})
             metadata = payload.get("metadata", {})
             filters_data = payload.get("filters", {})
-            
+
             website = metadata.get("website") or ""
             domain = agent._extract_domain(website) if website else ""
-            
+
             # Build location string
             location_parts = []
             if filters_data.get("hq_city"):
@@ -394,7 +407,7 @@
             if filters_data.get("hq_country_region"):
                 location_parts.append(filters_data.get("hq_country_region"))
             location = ", ".join(location_parts) if location_parts else None
-            
+
             # Extract AUM
             aum = metadata.get("aum")
             if aum is not None:
@@ -402,7 +415,7 @@
                     aum = float(aum)
                 except (ValueError, TypeError):
                     aum = None
-            
+
             company_data = {
                 "company_name": metadata.get("company_name") or r.get("lp_name", "Unknown"),
                 "company_domain": domain,
@@ -414,10 +427,10 @@
                 "company_bio": metadata.get("company_description") or metadata.get("bio") or "",
                 "aum": aum,
                 "score": r.get("score", 0.0),
-                "raw_payload": payload
+                "raw_payload": payload,
             }
             companies.append(company_data)
-        
+
         # Generate run_id following the same pattern as prompt-based workflow
         # Use session-based run_id for consistency
         async with db.pool.acquire() as conn:
@@ -453,11 +466,7 @@
                 )
 
                 # Create prospecting_runs entry (required for select_company to work)
-                search_params = {
-                    "filters": filters,
-                    "qdrant_filters": qdrant_filters,
-                    "source": "structured_search"
-                }
+                search_params = {"filters": filters, "qdrant_filters": qdrant_filters, "source": "structured_search"}
                 await conn.execute(
                     """
                     INSERT INTO prospecting_runs (run_id, user_id, session_id, search_params, start_time, status, workflow_type)
@@ -468,27 +477,26 @@
                     request.session_id,
                     json.dumps(search_params),
                 )
-        
+
         # Store search results using the helper method (uses correct columns)
         try:
             execution_time_ms = int((time.time() - start_time) * 1000)
-            search_query_json = json.dumps({
-                "hq_city": request.hq_city,
-                "hq_country": request.hq_country,
-                "hq_region": request.hq_region,
-                "investor_types": request.investor_types,
-                "aum_min": request.aum_min,
-                "aum_max": request.aum_max,
-                "limit": limit
-            })
+            search_query_json = json.dumps(
+                {
+                    "hq_city": request.hq_city,
+                    "hq_country": request.hq_country,
+                    "hq_region": request.hq_region,
+                    "investor_types": request.investor_types,
+                    "aum_min": request.aum_min,
+                    "aum_max": request.aum_max,
+                    "limit": limit,
+                }
+            )
             search_results_dict = {
                 "companies": companies,
                 "total_count": len(companies),
                 "filters_applied": qdrant_filters,
-                "search_metadata": {
-                    "qdrant_filters": qdrant_filters,
-                    "search_type": "structured_search"
-                }
+                "search_metadata": {"qdrant_filters": qdrant_filters, "search_type": "structured_search"},
             }
             await db.store_company_search_results(
                 run_id=run_id,
@@ -496,31 +504,22 @@
                 session_id=request.session_id,
                 search_query=search_query_json,
                 search_results=search_results_dict,
-                execution_time_ms=execution_time_ms
+                execution_time_ms=execution_time_ms,
             )
         except Exception as e:
             logger.warning(f"Failed to store structured search results: {e}", exc_info=True)
-        
-        return {
-            "status": "success",
-            "run_id": run_id,
-            "results": companies,
-            "total_count": len(companies),
-            "filters_applied": qdrant_filters
-        }
-        
+
+        return {"status": "success", "run_id": run_id, "results": companies, "total_count": len(companies), "filters_applied": qdrant_filters}
+
     except Exception as e:
         logger.exception(f"Structured search failed: {e}")
         raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")
 
+
 @router.post("/feedback/company")
-async def submit_company_feedback(
-    request: CompanyFeedbackRequest,
-    db=Depends(get_db),
-    current_user=Depends(require_user)
-):
+async def submit_company_feedback(request: CompanyFeedbackRequest, db=Depends(get_db), current_user=Depends(require_user)):
     """Submit explicit feedback for a company (thumbs up/down, saved).
-    
+
     Only accepts feedback from the results output page (after enrichment is complete).
     The run_id should be the enrichment run_id (company_selection or specific_company workflow).
     For company_selection workflows: Updates the feedback_types JSONB array in company_search_feedback table.
@@ -529,20 +528,16 @@
     """
     if request.user_id != current_user.user_id:
         raise HTTPException(status_code=403, detail="Forbidden: user_id does not match authenticated user.")
-    
-    if request.feedback_type not in ['thumbs_up', 'thumbs_down', 'saved', 'unsaved']:
+
+    if request.feedback_type not in ["thumbs_up", "thumbs_down", "saved", "unsaved"]:
         raise HTTPException(status_code=400, detail="feedback_type must be 'thumbs_up', 'thumbs_down', 'saved', or 'unsaved'")
-    
+
     if not request.company_id and not request.company_domain:
         raise HTTPException(status_code=400, detail="Provide company_id or company_domain")
-    
+
     try:
-        from app.services.feedback_collection_service import (
-            update_feedback_type, 
-            save_company_to_favorites,
-            archive_saved_company
-        )
-        
+        from app.services.feedback_collection_service import update_feedback_type, save_company_to_favorites, archive_saved_company
+
         # Verify the run_id is an enrichment run and is completed
         async with db.pool.acquire() as conn:
             enrichment_run = await conn.fetchrow(
@@ -551,34 +546,31 @@
                 FROM prospecting_runs
                 WHERE run_id = $1 AND user_id = $2
                 """,
-                request.run_id, request.user_id
+                request.run_id,
+                request.user_id,
             )
-            
+
             if not enrichment_run:
                 raise HTTPException(status_code=404, detail="Enrichment run not found")
-            
+
             # Only allow feedback from completed enrichment runs (company_selection or specific_company)
             if enrichment_run["workflow_type"] not in ("company_selection", "specific_company"):
                 raise HTTPException(
-                    status_code=400,
-                    detail="Feedback can only be submitted from the results output page (after enrichment is complete)"
+                    status_code=400, detail="Feedback can only be submitted from the results output page (after enrichment is complete)"
                 )
-            
+
             if enrichment_run["status"] != "completed":
-                raise HTTPException(
-                    status_code=400,
-                    detail="Feedback can only be submitted after enrichment is completed"
-                )
-            
+                raise HTTPException(status_code=400, detail="Feedback can only be submitted after enrichment is completed")
+
             workflow_type = enrichment_run["workflow_type"]
-            
+
             # Handle different workflow types
             if workflow_type == "specific_company":
                 # For specific_company workflows, get company info directly from the run or request
                 company_id = request.company_id or enrichment_run.get("company_id")
                 company_name = enrichment_run.get("company_name") or "Unknown"
                 company_domain = request.company_domain
-                
+
                 # Get company domain from companies table if not provided
                 if not company_domain and company_id:
                     company_row = await conn.fetchrow(
@@ -587,17 +579,16 @@
                         WHERE run_id = $1 AND user_id = $2 AND company_id = $3
                         LIMIT 1
                         """,
-                        request.run_id, request.user_id, company_id
+                        request.run_id,
+                        request.user_id,
+                        company_id,
                     )
                     if company_row and company_row.get("website_url"):
                         company_domain = company_row["website_url"]
-                
+
                 if not company_id and not company_domain:
-                    raise HTTPException(
-                        status_code=400,
-                        detail="Cannot determine company information for this run"
-                    )
-                
+                    raise HTTPException(status_code=400, detail="Cannot determine company information for this run")
+
                 # Get the original user prompt from session_interactions for "Found via" field
                 saved_from_query_text = None
                 interaction_row = await conn.fetchrow(
@@ -607,14 +598,15 @@
                     ORDER BY interaction_number DESC
                     LIMIT 1
                     """,
-                    request.run_id, enrichment_run.get("session_id")
+                    request.run_id,
+                    enrichment_run.get("session_id"),
                 )
                 if interaction_row and interaction_row.get("prompt"):
                     saved_from_query_text = interaction_row["prompt"]
-                
+
                 # For specific_company, we can directly save/unsave without feedback table lookup
                 # (specific_company workflows don't have feedback records)
-                if request.feedback_type == 'saved':
+                if request.feedback_type == "saved":
                     saved_success = await save_company_to_favorites(
                         user_id=request.user_id,
                         company_id=company_id,
@@ -624,53 +616,40 @@
                         saved_from_query_text=saved_from_query_text,  # Use the user's original prompt
                         category=request.category,
                         notes=request.notes,
-                        db=db
+                        db=db,
                     )
-                    
+
                     if not saved_success:
                         raise HTTPException(status_code=500, detail="Failed to save company to favorites")
-                    
+
                     logger.info(
                         f"Company saved from specific_company workflow",
-                        extra={
-                            "user_id": request.user_id,
-                            "run_id": request.run_id,
-                            "company_id": company_id,
-                            "company_name": company_name
-                        }
+                        extra={"user_id": request.user_id, "run_id": request.run_id, "company_id": company_id, "company_name": company_name},
                     )
-                    
+
                     return {
                         "status": "success",
                         "message": f"Company saved successfully",
-                        "feedback_id": None  # No feedback record for specific_company
+                        "feedback_id": None,  # No feedback record for specific_company
                     }
-                elif request.feedback_type == 'unsaved':
+                elif request.feedback_type == "unsaved":
                     # Archive (unsave) the company
                     archive_success = await archive_saved_company(
-                        user_id=request.user_id,
-                        company_id=company_id,
-                        company_domain=company_domain,
-                        db=db
+                        user_id=request.user_id, company_id=company_id, company_domain=company_domain, db=db
                     )
-                    
+
                     if not archive_success:
                         raise HTTPException(status_code=500, detail="Failed to unsave company")
-                    
+
                     logger.info(
                         f"Company unsaved from specific_company workflow",
-                        extra={
-                            "user_id": request.user_id,
-                            "run_id": request.run_id,
-                            "company_id": company_id,
-                            "company_name": company_name
-                        }
+                        extra={"user_id": request.user_id, "run_id": request.run_id, "company_id": company_id, "company_name": company_name},
                     )
-                    
+
                     return {
                         "status": "success",
                         "message": f"Company unsaved successfully",
-                        "feedback_id": None  # No feedback record for specific_company
+                        "feedback_id": None,  # No feedback record for specific_company
                     }
                 else:
                     # For thumbs_up/thumbs_down on specific_company, we don't track feedback
@@ -678,9 +657,9 @@
                     return {
                         "status": "success",
                         "message": f"Feedback '{request.feedback_type}' acknowledged (not tracked for specific_company workflows)",
-                        "feedback_id": None
+                        "feedback_id": None,
                     }
-            
+
             elif workflow_type == "company_selection":
                 # Get the parent search run_id from search_params
                 search_params = enrichment_run.get("search_params")
@@ -691,14 +670,11 @@
                         search_params = {}
                 elif not isinstance(search_params, dict):
                     search_params = {}
-                
+
                 parent_search_run_id = search_params.get("parent_search_run_id")
                 if not parent_search_run_id:
-                    raise HTTPException(
-                        status_code=400,
-                        detail="Cannot find parent search run for this enrichment"
-                    )
-                
+                    raise HTTPException(status_code=400, detail="Cannot find parent search run for this enrichment")
+
                 # Find the feedback record using the parent search run_id
                 # First try with company_id/domain match, then fall back to just run_id + user_id
                 query = """
@@ -707,18 +683,18 @@
                     WHERE run_id = $1 AND user_id = $2
                 """
                 params = [parent_search_run_id, request.user_id]
-                
+
                 if request.company_id:
                     query += " AND company_id = $3"
                     params.append(request.company_id)
                 elif request.company_domain:
                     query += " AND company_domain = $3"
                     params.append(request.company_domain)
-                
+
                 query += " ORDER BY selected_at DESC LIMIT 1"
-                
+
                 feedback_row = await conn.fetchrow(query, *params)
-                
+
                 # If not found with company match, try without company filter (in case company_id/domain don't match exactly)
                 if not feedback_row:
                     logger.warning(
@@ -728,8 +704,8 @@
                             "enrichment_run_id": request.run_id,
                             "parent_search_run_id": parent_search_run_id,
                             "company_id": request.company_id,
-                            "company_domain": request.company_domain
-                        }
+                            "company_domain": request.company_domain,
+                        },
                     )
                     feedback_row = await conn.fetchrow(
                         """
@@ -739,9 +715,10 @@
                         ORDER BY selected_at DESC
                         LIMIT 1
                         """,
-                        parent_search_run_id, request.user_id
+                        parent_search_run_id,
+                        request.user_id,
                     )
-            
+
                 if not feedback_row:
                     logger.error(
                         f"Feedback record not found for enrichment run",
@@ -750,24 +727,23 @@
                             "enrichment_run_id": request.run_id,
                             "parent_search_run_id": parent_search_run_id,
                             "company_id": request.company_id,
-                            "company_domain": request.company_domain
-                        }
+                            "company_domain": request.company_domain,
+                        },
                     )
                     raise HTTPException(
-                        status_code=404,
-                        detail="Feedback record not found. Feedback can only be submitted after viewing enrichment results."
+                        status_code=404, detail="Feedback record not found. Feedback can only be submitted after viewing enrichment results."
                     )
-                
+
                 feedback_id = feedback_row["id"]
-                
+
                 # Update feedback type
                 success = await update_feedback_type(feedback_id, request.feedback_type, db)
-                
+
                 if not success:
                     raise HTTPException(status_code=500, detail="Failed to update feedback")
-                
+
                 # Handle saved/unsaved actions
-                if request.feedback_type == 'saved':
+                if request.feedback_type == "saved":
                     saved_success = await save_company_to_favorites(
                         user_id=request.user_id,
                         company_id=feedback_row["company_id"],
@@ -777,39 +753,32 @@
                         saved_from_query_text=feedback_row.get("query_text"),
                         category=request.category,
                         notes=request.notes,
-                        db=db
+                        db=db,
                     )
-                    
+
                     if not saved_success:
                         logger.warning(f"Failed to save company to favorites, but feedback was updated")
-                elif request.feedback_type == 'unsaved':
+                elif request.feedback_type == "unsaved":
                     # Archive (unsave) the company
                     archive_success = await archive_saved_company(
-                        user_id=request.user_id,
-                        company_id=feedback_row["company_id"],
-                        company_domain=feedback_row["company_domain"],
-                        db=db
+                        user_id=request.user_id, company_id=feedback_row["company_id"], company_domain=feedback_row["company_domain"], db=db
                     )
-                    
+
                     if not archive_success:
                         logger.warning(f"Failed to unsave company, but feedback was updated")
-                
+
                 logger.info(
                     f"Company feedback submitted: {request.feedback_type}",
                     extra={
                         "user_id": request.user_id,
                         "enrichment_run_id": request.run_id,
                         "parent_search_run_id": parent_search_run_id,
-                        "feedback_type": request.feedback_type
-                    }
+                        "feedback_type": request.feedback_type,
+                    },
                 )
-                
-                return {
-                    "status": "success",
-                    "message": f"Feedback '{request.feedback_type}' submitted successfully",
-                    "feedback_id": feedback_id
-                }
-        
+
+                return {"status": "success", "message": f"Feedback '{request.feedback_type}' submitted successfully", "feedback_id": feedback_id}
+
     except HTTPException:
         raise
     except Exception as e:
@@ -824,12 +793,12 @@
     limit: int = 50,
     offset: int = 0,
     db=Depends(get_db),
-    current_user=Depends(require_user)
+    current_user=Depends(require_user),
 ):
     """Get user's saved companies.
-    
+
     Returns a paginated list of companies the user has saved.
-    
+
     Query Parameters:
         category: Optional filter by category
         is_archived: Include archived companies (default: False)
@@ -837,7 +806,7 @@
         offset: Pagination offset (default: 0)
     """
     user_id = current_user.user_id
-    
+
     try:
         async with db.pool.acquire() as conn:
             # Build query with filters
@@ -860,58 +829,55 @@
             """
             params = [user_id]
             param_index = 2
-            
+
             # Add archive filter
             if not is_archived:
                 query += f" AND is_archived = ${param_index}"
                 params.append(False)
                 param_index += 1
-            
+
             # Add category filter
             if category:
                 query += f" AND category = ${param_index}"
                 params.append(category)
                 param_index += 1
-            
+
             # Get total count for pagination
             count_query = query.replace(
                 "SELECT id, company_id, company_name, company_domain, saved_from_run_id, saved_from_query_text, category, notes, saved_at, updated_at, is_archived, archived_at",
                 "SELECT COUNT(*)",
-                1
+                1,
             )
             total = await conn.fetchval(count_query, *params)
-            
+
             # Add ordering and pagination
             query += " ORDER BY saved_at DESC LIMIT $" + str(param_index) + " OFFSET $" + str(param_index + 1)
             params.extend([limit, offset])
-            
+
             # Fetch results
             rows = await conn.fetch(query, *params)
-        
+
         companies = []
         for row in rows:
-            companies.append({
-                "id": row["id"],
-                "company_id": row["company_id"],
-                "company_name": row["company_name"],
-                "company_domain": row["company_domain"],
-                "saved_from_run_id": row["saved_from_run_id"],
-                "saved_from_query_text": row["saved_from_query_text"],
-                "category": row["category"],
-                "notes": row["notes"],
-                "saved_at": row["saved_at"].isoformat() if row["saved_at"] else None,
-                "updated_at": row["updated_at"].isoformat() if row["updated_at"] else None,
-                "is_archived": row["is_archived"],
-                "archived_at": row["archived_at"].isoformat() if row["archived_at"] else None
-            })
-        
-        return {
-            "companies": companies,
-            "total": total,
-            "limit": limit,
-            "offset": offset
-        }
-        
+            companies.append(
+                {
+                    "id": row["id"],
+                    "company_id": row["company_id"],
+                    "company_name": row["company_name"],
+                    "company_domain": row["company_domain"],
+                    "saved_from_run_id": row["saved_from_run_id"],
+                    "saved_from_query_text": row["saved_from_query_text"],
+                    "category": row["category"],
+                    "notes": row["notes"],
+                    "saved_at": row["saved_at"].isoformat() if row["saved_at"] else None,
+                    "updated_at": row["updated_at"].isoformat() if row["updated_at"] else None,
+                    "is_archived": row["is_archived"],
+                    "archived_at": row["archived_at"].isoformat() if row["archived_at"] else None,
+                }
+            )
+
+        return {"companies": companies, "total": total, "limit": limit, "offset": offset}
+
     except Exception as e:
         logger.exception("Failed to retrieve saved companies")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
@@ -933,7 +899,7 @@
             request.feedback_text,
             request.category,
             request.run_id,
-            metadata_value
+            metadata_value,
         )
         logger.info("User feedback submitted successfully", extra={"user_id": current_user.user_id, "session_id": request.session_id})
         return {"status": "success", "message": "Feedback submitted successfully"}
@@ -941,6 +907,7 @@
         logger.exception("Failed to submit feedback")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 # Session Management
 @router.post("/session/start", response_model=SessionStartResponse)
 async def start_session(request: SessionStartRequest, db=Depends(get_db), current_user=Depends(require_user)):
@@ -956,32 +923,27 @@
             VALUES ($1, $2, NOW(), 'active')
             RETURNING session_id, start_time
             """,
-            session_id, request.user_id
+            session_id,
+            request.user_id,
         )
         if not result or "session_id" not in result or "start_time" not in result:
             raise HTTPException(status_code=500, detail="Failed to create session")
         logger.info("Session started successfully", extra={"user_id": request.user_id, "session_id": result["session_id"]})
-        return SessionStartResponse(
-            user_id=request.user_id,
-            session_id=result["session_id"],
-            start_time=result["start_time"]
-        )
+        return SessionStartResponse(user_id=request.user_id, session_id=result["session_id"], start_time=result["start_time"])
     except HTTPException:
         raise
     except Exception as e:
         logger.exception("Failed to start session")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.post("/session/end")
 async def end_session(session_id: str, db=Depends(get_db), current_user=Depends(require_user)):
     """End an active prospecting session (requires authentication)"""
     logger.info("Session end attempt", extra={"session_id": session_id, "user_id": current_user.user_id})
     # Fetch the session and check ownership
     async with db.pool.acquire() as conn:
-        session = await conn.fetchrow(
-            "SELECT user_id FROM sessions WHERE session_id = $1",
-            session_id
-        )
+        session = await conn.fetchrow("SELECT user_id FROM sessions WHERE session_id = $1", session_id)
     if not session:
         raise HTTPException(status_code=404, detail="Session not found")
     if session["user_id"] != current_user.user_id:
@@ -993,7 +955,7 @@
             SET status = 'completed', end_time = NOW()
             WHERE session_id = $1
             """,
-            session_id
+            session_id,
         )
         logger.info("Session ended successfully", extra={"session_id": session_id, "user_id": current_user.user_id})
         return {"status": "success", "message": "Session ended successfully"}
@@ -1001,36 +963,35 @@
         logger.exception("Failed to end session")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 # Processing Control
 @router.post("/process/start", response_model=ProcessingResponse)
 async def start_processing(request: ProcessingRequest, background_tasks: BackgroundTasks, db=Depends(get_db)):
     """Start processing a prompt"""
-    logger.info("Processing workflow initiation", extra={"user_id": request.user_id, "session_id": request.session_id, "prompt_length": len(request.prompt)})
+    logger.info(
+        "Processing workflow initiation", extra={"user_id": request.user_id, "session_id": request.session_id, "prompt_length": len(request.prompt)}
+    )
     rate_limiter = RateLimiter()
     limit_info = await rate_limiter.check_rate_limit(request.user_id)
     if not limit_info["allowed"]:
         logger.warning("Rate limit exceeded", extra={"user_id": request.user_id, "limit": limit_info["limit"], "reset": limit_info["reset"]})
         raise HTTPException(
-            status_code=429,
-            detail=f"Rate limit exceeded. Limit: {limit_info['limit']} runs per day. Reset in {limit_info['reset']} seconds."
+            status_code=429, detail=f"Rate limit exceeded. Limit: {limit_info['limit']} runs per day. Reset in {limit_info['reset']} seconds."
         )
     try:
         async with db.pool.acquire() as conn:
-            session_row = await conn.fetchrow(
-                "SELECT * FROM sessions WHERE session_id = $1 AND user_id = $2",
-                request.session_id, request.user_id
-            )
+            session_row = await conn.fetchrow("SELECT * FROM sessions WHERE session_id = $1 AND user_id = $2", request.session_id, request.user_id)
         if not session_row:
             raise HTTPException(status_code=404, detail="Session not found or does not belong to user.")
-        
+
         # Analyze prompt to determine workflow type (1-2 seconds)
         logger.info("Analyzing prompt for workflow type", extra={"session_id": request.session_id, "prompt_length": len(request.prompt)})
         orchestrator = ProspectingOrchestrator(db)
         prompt_analysis = await orchestrator.prompt_analyzer.analyze_prompt(request.prompt)
-        workflow_type = prompt_analysis.get('prompt_type', 'unknown')
-        
+        workflow_type = prompt_analysis.get("prompt_type", "unknown")
+
         logger.info("Prompt analysis completed", extra={"session_id": request.session_id, "workflow_type": workflow_type})
-        
+
         # Compute next run index atomically and create the run within a transaction
         async with db.pool.acquire() as conn:
             async with conn.transaction():
@@ -1075,31 +1036,22 @@
                     request.session_id,
                     workflow_type,
                 )
-        
+
         # Add background task for workflow execution
         background_tasks.add_task(
-            execute_workflow_background,
-            run_id=run_id,
-            user_id=request.user_id,
-            session_id=request.session_id,
-            prompt=request.prompt
+            execute_workflow_background, run_id=run_id, user_id=request.user_id, session_id=request.session_id, prompt=request.prompt
         )
-        
+
         # Note: Tally increment moved to execute_workflow_background - only increments on successful completion
         logger.info("Processing workflow started in background", extra={"user_id": request.user_id, "run_id": run_id, "workflow_type": workflow_type})
-        
+
         # Initialize progress to 0 at start
         try:
             await ProgressStore.instance().set_progress(db, run_id, 0)
         except Exception:
             pass
 
-        response = ProcessingResponse(
-            user_id=request.user_id,
-            run_id=run_id,
-            status="started",
-            workflow_type=workflow_type
-        )
+        response = ProcessingResponse(user_id=request.user_id, run_id=run_id, status="started", workflow_type=workflow_type)
         resp = Response(content=response.json(), media_type="application/json", status_code=202)
         resp.headers["X-RateLimit-Limit"] = str(limit_info["limit"])
         # Note: Remaining shows current count - tally only increments on successful completion
@@ -1112,87 +1064,88 @@
         logger.exception("Failed to start processing")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.post("/process/start-from-ria", response_model=ProcessingResponse)
 async def start_processing_from_ria(
-    request: RIAProcessingRequest, 
-    background_tasks: BackgroundTasks, 
-    db=Depends(get_db),
-    current_user=Depends(require_user)
+    request: RIAProcessingRequest, background_tasks: BackgroundTasks, db=Depends(get_db), current_user=Depends(require_user)
 ):
     """
     Start prospecting workflow from RIA CRD.
     Extracts company data internally and begins standard prospecting flow.
     """
     logger = get_logger(__name__)
-    logger.info("üöÄ RIA PROSPECTING ENDPOINT HIT", extra={
-        "user_id": request.user_id, 
-        "session_id": request.session_id, 
-        "crd_number": request.crd_number,
-        "current_user": str(current_user) if current_user else "None"
-    })
+    logger.info(
+        "üöÄ RIA PROSPECTING ENDPOINT HIT",
+        extra={
+            "user_id": request.user_id,
+            "session_id": request.session_id,
+            "crd_number": request.crd_number,
+            "current_user": str(current_user) if current_user else "None",
+        },
+    )
     print(f"üöÄ RIA PROSPECTING ENDPOINT HIT - CRD: {request.crd_number}, User: {request.user_id}")
-    
+
     try:
         logger.info("üîç STEP 1: Extracting company data from RIA cache", extra={"crd_number": request.crd_number})
         print(f"üîç STEP 1: Extracting company data from RIA cache for CRD: {request.crd_number}")
-        
+
         # Extract company data from RIA cache
         company_data = await _extract_company_from_crd(request.crd_number)
-        
-        logger.info("‚úÖ STEP 1 COMPLETE: Company data extracted", extra={
-            "company_name": company_data.get("company_name"),
-            "location": company_data.get("location"),
-            "crd_number": company_data.get("crd_number")
-        })
+
+        logger.info(
+            "‚úÖ STEP 1 COMPLETE: Company data extracted",
+            extra={
+                "company_name": company_data.get("company_name"),
+                "location": company_data.get("location"),
+                "crd_number": company_data.get("crd_number"),
+            },
+        )
         print(f"‚úÖ STEP 1 COMPLETE: Found company '{company_data.get('company_name')}' in '{company_data.get('location')}'")
-        
+
         # Generate prospecting prompt
         company_name = company_data["company_name"]
         location = company_data.get("location", "")
-        
+
         if location:
             prompt = f"Research {company_name} headquartered in {location}"
         else:
             prompt = f"Research {company_name}"
-        
+
         logger.info("üîç STEP 2: Generated prospecting prompt", extra={"prompt": prompt})
         print(f"üîç STEP 2: Generated prospecting prompt: {prompt}")
-        
+
         # Generate run_id
         run_id = f"{request.user_id}_{request.session_id}_{int(time.time())}_ria"
         logger.info("üîç STEP 3: Generated run_id", extra={"run_id": run_id})
         print(f"üîç STEP 3: Generated run_id: {run_id}")
-        
+
         # Check rate limits
         logger.info("üîç STEP 4: Checking rate limits", extra={"user_id": request.user_id})
         print(f"üîç STEP 4: Checking rate limits for user: {request.user_id}")
-        
+
         rate_limiter = RateLimiter()
         limit_info = await rate_limiter.check_rate_limit(request.user_id)
-        
-        logger.info("‚úÖ STEP 4 COMPLETE: Rate limit check", extra={
-            "allowed": limit_info["allowed"],
-            "remaining": limit_info.get("remaining", 0),
-            "limit": limit_info.get("limit", 0)
-        })
+
+        logger.info(
+            "‚úÖ STEP 4 COMPLETE: Rate limit check",
+            extra={"allowed": limit_info["allowed"], "remaining": limit_info.get("remaining", 0), "limit": limit_info.get("limit", 0)},
+        )
         print(f"‚úÖ STEP 4 COMPLETE: Rate limit - Allowed: {limit_info['allowed']}, Remaining: {limit_info.get('remaining', 0)}")
-        
+
         if not limit_info["allowed"]:
-            logger.warning("‚ùå RATE LIMIT EXCEEDED", extra={
-                "user_id": request.user_id, 
-                "limit": limit_info.get("limit", 0), 
-                "reset": limit_info.get("reset", 0)
-            })
+            logger.warning(
+                "‚ùå RATE LIMIT EXCEEDED", extra={"user_id": request.user_id, "limit": limit_info.get("limit", 0), "reset": limit_info.get("reset", 0)}
+            )
             print(f"‚ùå RATE LIMIT EXCEEDED for user {request.user_id}: Limit {limit_info.get('limit', 0)} runs per day")
             raise HTTPException(
-                status_code=429, 
-                detail=f"Rate limit exceeded. Limit: {limit_info.get('limit', 0)} runs per day. Reset in {limit_info.get('reset', 0)} seconds."
+                status_code=429,
+                detail=f"Rate limit exceeded. Limit: {limit_info.get('limit', 0)} runs per day. Reset in {limit_info.get('reset', 0)} seconds.",
             )
-        
+
         # Create database entry
         logger.info("üîç STEP 5: Creating database entry", extra={"run_id": run_id})
         print(f"üîç STEP 5: Creating database entry for run_id: {run_id}")
-        
+
         async with db.pool.acquire() as conn:
             # Prepare search_params with the generated prompt
             search_params = {
@@ -1200,17 +1153,15 @@
                 "company_name": company_name,
                 "location": location,
                 "crd_number": request.crd_number,
-                "source": "ria_prospecting"
+                "source": "ria_prospecting",
             }
-            
-            logger.info("üîç STEP 5: Storing search_params", extra={
-                "run_id": run_id,
-                "prompt": prompt,
-                "company_name": company_name,
-                "crd_number": request.crd_number
-            })
+
+            logger.info(
+                "üîç STEP 5: Storing search_params",
+                extra={"run_id": run_id, "prompt": prompt, "company_name": company_name, "crd_number": request.crd_number},
+            )
             print(f"üîç STEP 5: Storing search_params for run_id: {run_id}, prompt: {prompt}")
-            
+
             await conn.execute(
                 """
                 INSERT INTO prospecting_runs (run_id, user_id, session_id, search_params, company_name, start_time, status, workflow_type)
@@ -1220,37 +1171,29 @@
                 request.user_id,
                 request.session_id,
                 json.dumps(search_params),
-                company_name
+                company_name,
             )
-        
+
         logger.info("‚úÖ STEP 5 COMPLETE: Database entry created", extra={"run_id": run_id})
         print(f"‚úÖ STEP 5 COMPLETE: Database entry created for run_id: {run_id}")
-        
+
         # Add background task for workflow execution
         logger.info("üîç STEP 6: Adding background task", extra={"run_id": run_id, "prompt": prompt})
         print(f"üîç STEP 6: Adding background task for run_id: {run_id}")
-        
-        background_tasks.add_task(
-            execute_workflow_background,
-            run_id=run_id,
-            user_id=request.user_id,
-            session_id=request.session_id,
-            prompt=prompt
-        )
-        
+
+        background_tasks.add_task(execute_workflow_background, run_id=run_id, user_id=request.user_id, session_id=request.session_id, prompt=prompt)
+
         # Note: Tally increment moved to execute_workflow_background - only increments on successful completion
-        logger.info("‚úÖ STEP 6 COMPLETE: Background task added", extra={
-            "user_id": request.user_id, 
-            "run_id": run_id, 
-            "crd_number": request.crd_number,
-            "company_name": company_name
-        })
+        logger.info(
+            "‚úÖ STEP 6 COMPLETE: Background task added",
+            extra={"user_id": request.user_id, "run_id": run_id, "crd_number": request.crd_number, "company_name": company_name},
+        )
         print(f"‚úÖ STEP 6 COMPLETE: Background task added for run_id: {run_id}")
-        
+
         # Initialize progress to 0 at start
         logger.info("üîç STEP 7: Initializing progress", extra={"run_id": run_id})
         print(f"üîç STEP 7: Initializing progress for run_id: {run_id}")
-        
+
         try:
             await ProgressStore.instance().set_progress(db, run_id, 0)
             logger.info("‚úÖ STEP 7 COMPLETE: Progress initialized", extra={"run_id": run_id})
@@ -1259,62 +1202,49 @@
             logger.warning("‚ö†Ô∏è STEP 7 WARNING: Progress initialization failed", extra={"run_id": run_id, "error": str(e)})
             print(f"‚ö†Ô∏è STEP 7 WARNING: Progress initialization failed for run_id: {run_id}, error: {str(e)}")
 
-        response = ProcessingResponse(
-            user_id=request.user_id,
-            run_id=run_id,
-            status="started",
-            workflow_type="specific_company"
+        response = ProcessingResponse(user_id=request.user_id, run_id=run_id, status="started", workflow_type="specific_company")
+
+        logger.info(
+            "üéâ RIA PROSPECTING ENDPOINT SUCCESS",
+            extra={"user_id": request.user_id, "run_id": run_id, "crd_number": request.crd_number, "company_name": company_name, "status": "started"},
         )
-        
-        logger.info("üéâ RIA PROSPECTING ENDPOINT SUCCESS", extra={
-            "user_id": request.user_id,
-            "run_id": run_id,
-            "crd_number": request.crd_number,
-            "company_name": company_name,
-            "status": "started"
-        })
         print(f"üéâ RIA PROSPECTING ENDPOINT SUCCESS - Run ID: {run_id}, Company: {company_name}")
-        
+
         resp = Response(content=response.json(), media_type="application/json", status_code=202)
         resp.headers["X-RateLimit-Limit"] = str(limit_info["limit"])
         resp.headers["X-RateLimit-Remaining"] = str(limit_info["remaining"] - 1)
         resp.headers["X-RateLimit-Reset"] = str(limit_info["reset"])
         return resp
-        
+
     except HTTPException as he:
-        logger.error("‚ùå RIA PROSPECTING ENDPOINT HTTP ERROR", extra={
-            "status_code": he.status_code,
-            "detail": he.detail,
-            "user_id": request.user_id,
-            "crd_number": request.crd_number
-        })
+        logger.error(
+            "‚ùå RIA PROSPECTING ENDPOINT HTTP ERROR",
+            extra={"status_code": he.status_code, "detail": he.detail, "user_id": request.user_id, "crd_number": request.crd_number},
+        )
         print(f"‚ùå RIA PROSPECTING ENDPOINT HTTP ERROR - Status: {he.status_code}, Detail: {he.detail}")
         raise
     except Exception as e:
-        logger.exception("‚ùå RIA PROSPECTING ENDPOINT EXCEPTION", extra={
-            "error": str(e),
-            "user_id": request.user_id,
-            "crd_number": request.crd_number
-        })
+        logger.exception(
+            "‚ùå RIA PROSPECTING ENDPOINT EXCEPTION", extra={"error": str(e), "user_id": request.user_id, "crd_number": request.crd_number}
+        )
         print(f"‚ùå RIA PROSPECTING ENDPOINT EXCEPTION - Error: {str(e)}")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 async def execute_workflow_background(run_id: str, user_id: str, session_id: str, prompt: str):
     """Execute the workflow in the background"""
     try:
         # Update status to processing
         db = await get_db()
         async with db.pool.acquire() as conn:
-            await conn.execute(
-                "UPDATE prospecting_runs SET status = 'processing' WHERE run_id = $1",
-                run_id
-            )
-        
+            await conn.execute("UPDATE prospecting_runs SET status = 'processing' WHERE run_id = $1", run_id)
+
         # Execute the orchestrator
         session_context = SessionContext(session_id=session_id, user_id=user_id)
         # Align session interaction count with provided run_id so orchestrator uses the same run_id
         try:
             import re
+
             m = re.search(r"_run_(\d+)$", run_id)
             if m:
                 idx = int(m.group(1))
@@ -1329,7 +1259,7 @@
             session_context=session_context,
             run_id=run_id,
         )
-        
+
         # If orchestrator returned company info, ensure it's populated on the run
         try:
             if isinstance(result, dict):
@@ -1355,45 +1285,25 @@
 
         # Update status to completed
         async with db.pool.acquire() as conn:
-            await conn.execute(
-                "UPDATE prospecting_runs SET status = 'completed', end_time = NOW() WHERE run_id = $1",
-                run_id
-            )
-            
+            await conn.execute("UPDATE prospecting_runs SET status = 'completed', end_time = NOW() WHERE run_id = $1", run_id)
+
             # Increment agent run tally only for completed specific_company or company_selection workflows
             # Explicitly exclude general_search, company_search, and off_topic workflows
-            run_info = await conn.fetchrow(
-                "SELECT workflow_type, status FROM prospecting_runs WHERE run_id = $1",
-                run_id
-            )
-            
+            run_info = await conn.fetchrow("SELECT workflow_type, status FROM prospecting_runs WHERE run_id = $1", run_id)
+
             if run_info and run_info["status"] == "completed":
                 workflow_type = run_info.get("workflow_type")
-                
+
                 # Only increment for workflows that result in company enrichment
                 # Explicitly exclude: general_search, company_search, off_topic, and any None/empty values
                 if workflow_type and workflow_type in ("specific_company", "company_selection"):
                     try:
                         rate_limiter = RateLimiter()
                         await rate_limiter.increment_request_count(user_id)
-                        logger.info(
-                            "Agent run tally incremented",
-                            extra={
-                                "run_id": run_id,
-                                "user_id": user_id,
-                                "workflow_type": workflow_type
-                            }
-                        )
+                        logger.info("Agent run tally incremented", extra={"run_id": run_id, "user_id": user_id, "workflow_type": workflow_type})
                     except Exception as tally_error:
                         # Non-fatal: log but don't fail the workflow
-                        logger.warning(
-                            "Failed to increment agent run tally",
-                            extra={
-                                "run_id": run_id,
-                                "user_id": user_id,
-                                "error": str(tally_error)
-                            }
-                        )
+                        logger.warning("Failed to increment agent run tally", extra={"run_id": run_id, "user_id": user_id, "error": str(tally_error)})
                 else:
                     # Log when we explicitly skip incrementing (for debugging and troubleshooting)
                     excluded_types = ("general_search", "company_search", "off_topic")
@@ -1404,8 +1314,8 @@
                                 "run_id": run_id,
                                 "user_id": user_id,
                                 "workflow_type": workflow_type or "NULL",
-                                "reason": f"Only specific_company and company_selection workflows increment tally. Excluded types: {excluded_types}"
-                            }
+                                "reason": f"Only specific_company and company_selection workflows increment tally. Excluded types: {excluded_types}",
+                            },
                         )
                     else:
                         # Unknown workflow type - log as warning for investigation
@@ -1415,47 +1325,41 @@
                                 "run_id": run_id,
                                 "user_id": user_id,
                                 "workflow_type": workflow_type,
-                                "reason": "Unknown workflow type - may need to be added to increment logic"
-                            }
+                                "reason": "Unknown workflow type - may need to be added to increment logic",
+                            },
                         )
-        
+
         # Ensure progress reflects completion
         try:
             await ProgressStore.instance().set_progress(db, run_id, 100)
         except Exception:
             pass
-        
+
         logger.info("Background workflow completed", extra={"run_id": run_id, "user_id": user_id})
-        
+
     except Exception as e:
         logger.exception("Background workflow failed", extra={"run_id": run_id, "user_id": user_id})
         # Update status to failed
         try:
             db = await get_db()
             async with db.pool.acquire() as conn:
-                await conn.execute(
-                    "UPDATE prospecting_runs SET status = 'failed' WHERE run_id = $1",
-                    run_id
-                )
+                await conn.execute("UPDATE prospecting_runs SET status = 'failed' WHERE run_id = $1", run_id)
         except Exception as db_error:
             logger.exception("Failed to update run status to failed", extra={"run_id": run_id})
 
+
 @router.get("/process/status/{run_id}", response_model=ProcessingStatusResponse)
 async def get_processing_status(run_id: str, db=Depends(get_db), current_user=Depends(require_user)):
     """Get current status of processing"""
     user_id = current_user.user_id
     logger.info("Processing status request", extra={"run_id": run_id, "user_id": user_id})
     try:
-        
         # Validate ownership of run
         async with db.pool.acquire() as conn:
             owner = await conn.fetchval("SELECT user_id FROM prospecting_runs WHERE run_id = $1", run_id)
         if owner and owner != user_id:
             async with db.pool.acquire() as conn:
-                run_exists = await conn.fetchrow(
-                    "SELECT 1 FROM prospecting_runs WHERE run_id = $1",
-                    run_id
-                )
+                run_exists = await conn.fetchrow("SELECT 1 FROM prospecting_runs WHERE run_id = $1", run_id)
             if run_exists:
                 raise HTTPException(status_code=403, detail="Forbidden: run does not belong to this user.")
             else:
@@ -1467,7 +1371,8 @@
                 FROM prospecting_runs
                 WHERE run_id = $1 AND user_id = $2
                 """,
-                run_id, user_id
+                run_id,
+                user_id,
             )
         if not status:
             raise HTTPException(status_code=404, detail="Run not found")
@@ -1522,9 +1427,8 @@
 
         terminal_states = {"success", "failed"}
         both_terminal = comp_state_company in terminal_states and comp_state_person in terminal_states
-        one_success_other_failed = (
-            (comp_state_company == "success" and comp_state_person == "failed") or
-            (comp_state_company == "failed" and comp_state_person == "success")
+        one_success_other_failed = (comp_state_company == "success" and comp_state_person == "failed") or (
+            comp_state_company == "failed" and comp_state_person == "success"
         )
         should_continue_polling = not (both_terminal or one_success_other_failed or status["status"] in ("completed", "failed"))
 
@@ -1537,7 +1441,8 @@
         ria_summary = None
         try:
             async with db.pool.acquire() as conn:
-                ria_result = await conn.fetchrow("""
+                ria_result = await conn.fetchrow(
+                    """
                     SELECT result_data
                     FROM agent_results 
                     WHERE run_id = $1 
@@ -1545,12 +1450,15 @@
                     AND agent_name = 'RIA Detection'
                     ORDER BY created_at DESC 
                     LIMIT 1
-                """, run_id, user_id)
-                
-                if ria_result and ria_result['result_data']:
-                    result_data = ria_result['result_data']
+                """,
+                    run_id,
+                    user_id,
+                )
+
+                if ria_result and ria_result["result_data"]:
+                    result_data = ria_result["result_data"]
                     # Handle asyncpg.Record or similar
-                    if hasattr(result_data, 'items') and not isinstance(result_data, dict):
+                    if hasattr(result_data, "items") and not isinstance(result_data, dict):
                         result_data = dict(result_data)
                     # Handle string
                     if isinstance(result_data, str):
@@ -1558,12 +1466,12 @@
                             result_data = json.loads(result_data)
                         except Exception:
                             result_data = None
-                    
+
                     if result_data:
                         ria_summary = {
-                            'is_ria': result_data.get('is_ria', False),
-                            'crd_number': result_data.get('crd_number'),
-                            'has_ria_data': bool(result_data.get('crd_number'))
+                            "is_ria": result_data.get("is_ria", False),
+                            "crd_number": result_data.get("crd_number"),
+                            "has_ria_data": bool(result_data.get("crd_number")),
                         }
         except Exception as e:
             logger.warning(f"Failed to fetch RIA detection result: {e}", extra={"run_id": run_id, "user_id": user_id})
@@ -1586,9 +1494,12 @@
             "error": None,
             "workflow_type": status["workflow_type"] or "unknown",
             "ria_summary": ria_summary,
-            "riaSummary": ria_summary
+            "riaSummary": ria_summary,
         }
-        logger.info("Processing status retrieved", extra={"run_id": run_id, "user_id": user_id, "status": status["status"], "workflow_type": status["workflow_type"] or "unknown"})
+        logger.info(
+            "Processing status retrieved",
+            extra={"run_id": run_id, "user_id": user_id, "status": status["status"], "workflow_type": status["workflow_type"] or "unknown"},
+        )
         return ProcessingStatusResponse(**response_data)
     except HTTPException:
         raise
@@ -1596,6 +1507,7 @@
         logger.exception("Failed to get processing status")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 # Results Retrieval
 @router.get("/search/results/{run_id}", response_model=SearchResultsResponse)
 async def get_search_results(run_id: str, db=Depends(get_db), current_user=Depends(require_user)):
@@ -1603,16 +1515,12 @@
     user_id = current_user.user_id
     logger.info("Search results request", extra={"run_id": run_id, "user_id": user_id})
     try:
-        
         # Validate ownership of run
         async with db.pool.acquire() as conn:
             owner = await conn.fetchval("SELECT user_id FROM prospecting_runs WHERE run_id = $1", run_id)
         if owner and owner != user_id:
             async with db.pool.acquire() as conn:
-                result_exists = await conn.fetchrow(
-                    "SELECT 1 FROM company_search_results WHERE run_id = $1",
-                    run_id
-                )
+                result_exists = await conn.fetchrow("SELECT 1 FROM company_search_results WHERE run_id = $1", run_id)
             if result_exists:
                 raise HTTPException(status_code=403, detail="Forbidden: results do not belong to this user.")
             else:
@@ -1647,7 +1555,8 @@
         # Get user's saved companies for cross-reference
         saved_companies = []
         async with db.pool.acquire() as conn:
-            previous_enrichments = await conn.fetch("""
+            previous_enrichments = await conn.fetch(
+                """
                 SELECT pr.company_name, pr.end_time, pr.created_at, pr.run_id
                 FROM prospecting_runs pr
                 WHERE pr.user_id = $1 
@@ -1656,15 +1565,20 @@
                   AND pr.company_name IS NOT NULL
                 ORDER BY pr.end_time DESC NULLS LAST, pr.created_at DESC
                 LIMIT 100
-            """, user_id)
-            
+            """,
+                user_id,
+            )
+
             # Query saved companies for this user
-            saved_companies = await conn.fetch("""
+            saved_companies = await conn.fetch(
+                """
                 SELECT company_domain, company_name
                 FROM user_saved_companies
                 WHERE user_id = $1 
                   AND is_archived = FALSE
-            """, user_id)
+            """,
+                user_id,
+            )
 
         # Map stored company entries to API model shape with cross-reference data
         mapped_results = []
@@ -1674,49 +1588,49 @@
             name = company.get("company_name") or company.get("name") or ""
             if not name:
                 continue
-            
+
             # Cross-reference with previously enriched companies
             company_domain = company.get("company_domain") or company.get("website_url")
             last_seen = None
             previously_seen = False
-            
+
             for prev_enrichment in previous_enrichments:
-                prev_company_name = prev_enrichment['company_name']
-                
+                prev_company_name = prev_enrichment["company_name"]
+
                 # Match by company name (case-insensitive)
                 if name and prev_company_name and name.lower() == prev_company_name.lower():
                     # Use end_time if available, otherwise use created_at
-                    last_seen = prev_enrichment['end_time'] or prev_enrichment['created_at']
+                    last_seen = prev_enrichment["end_time"] or prev_enrichment["created_at"]
                     previously_seen = True
                     break
-            
+
             # Check if company is saved (match by domain first, then name as fallback)
             is_saved = False
             if company_domain or name:
                 for saved_company in saved_companies:
-                    saved_domain = saved_company.get('company_domain', '').lower().strip()
-                    saved_name = saved_company.get('company_name', '').lower().strip()
-                    company_domain_normalized = (company_domain or '').lower().strip()
+                    saved_domain = saved_company.get("company_domain", "").lower().strip()
+                    saved_name = saved_company.get("company_name", "").lower().strip()
+                    company_domain_normalized = (company_domain or "").lower().strip()
                     name_normalized = name.lower().strip()
-                    
+
                     # Primary match: by domain (exact match)
                     if company_domain_normalized and saved_domain:
                         if company_domain_normalized == saved_domain:
                             is_saved = True
                             break
                         # Handle www. prefix variations
-                        domain_without_www = company_domain_normalized.replace('www.', '')
-                        saved_domain_without_www = saved_domain.replace('www.', '')
+                        domain_without_www = company_domain_normalized.replace("www.", "")
+                        saved_domain_without_www = saved_domain.replace("www.", "")
                         if domain_without_www == saved_domain_without_www:
                             is_saved = True
                             break
-                    
+
                     # Fallback match: by name (case-insensitive)
                     if not is_saved and name_normalized and saved_name:
                         if name_normalized == saved_name:
                             is_saved = True
                             break
-            
+
             # Build location from hq_city and hq_country_region (new Qdrant structure)
             location_parts = []
             if company.get("hq_city"):
@@ -1724,36 +1638,35 @@
             if company.get("hq_country_region"):
                 location_parts.append(company.get("hq_country_region"))
             location_str = ", ".join(location_parts) if location_parts else company.get("location")
-            
-            mapped_results.append({
-                "company_id": f"{run_id}_company_{idx+1:03d}",
-                "name": name,
-                "website_url": company_domain,
-                "location": location_str,  # Built from new Qdrant fields
-                "focus_area": company.get("preferred_industry"),  # Map directly from new Qdrant metadata
-                "previously_seen": previously_seen,
-                "last_seen": last_seen.isoformat() if last_seen else None,
-                "is_saved": is_saved,
-                # Additional metadata fields from new Qdrant structure
-                "hq_city": company.get("hq_city"),
-                "hq_country_region": company.get("hq_country_region"),
-                "hq_global_region": company.get("hq_global_region"),
-                "aum": company.get("aum"),
-                "preferred_investment_types": company.get("preferred_investment_types"),
-                "other_investor_types": company.get("other_investor_types"),
-                "preferred_industry": company.get("preferred_industry"),
-                "preferred_geography": company.get("preferred_geography"),
-                "investor_type": company.get("investor_type")
-            })
 
+            mapped_results.append(
+                {
+                    "company_id": f"{run_id}_company_{idx + 1:03d}",
+                    "name": name,
+                    "website_url": company_domain,
+                    "location": location_str,  # Built from new Qdrant fields
+                    "focus_area": company.get("preferred_industry"),  # Map directly from new Qdrant metadata
+                    "previously_seen": previously_seen,
+                    "last_seen": last_seen.isoformat() if last_seen else None,
+                    "is_saved": is_saved,
+                    # Additional metadata fields from new Qdrant structure
+                    "hq_city": company.get("hq_city"),
+                    "hq_country_region": company.get("hq_country_region"),
+                    "hq_global_region": company.get("hq_global_region"),
+                    "aum": company.get("aum"),
+                    "preferred_investment_types": company.get("preferred_investment_types"),
+                    "other_investor_types": company.get("other_investor_types"),
+                    "preferred_industry": company.get("preferred_industry"),
+                    "preferred_geography": company.get("preferred_geography"),
+                    "investor_type": company.get("investor_type"),
+                }
+            )
+
         # Get the original prompt from session interactions
         original_prompt = None
         async with db.pool.acquire() as conn:
             # Get session_id from prospecting_runs
-            run_row = await conn.fetchrow(
-                "SELECT session_id FROM prospecting_runs WHERE run_id = $1 AND user_id = $2",
-                run_id, user_id
-            )
+            run_row = await conn.fetchrow("SELECT session_id FROM prospecting_runs WHERE run_id = $1 AND user_id = $2", run_id, user_id)
             if run_row:
                 interaction = await conn.fetchrow(
                     """
@@ -1762,22 +1675,16 @@
                     ORDER BY interaction_number DESC
                     LIMIT 1
                     """,
-                    run_id, run_row["session_id"]
+                    run_id,
+                    run_row["session_id"],
                 )
                 if interaction and interaction.get("prompt"):
                     original_prompt = interaction["prompt"]
 
-        logger.info(
-            "Search results retrieved",
-            extra={"run_id": run_id, "user_id": user_id, "results_count": len(mapped_results)}
-        )
+        logger.info("Search results retrieved", extra={"run_id": run_id, "user_id": user_id, "results_count": len(mapped_results)})
 
         return SearchResultsResponse(
-            user_id=user_id,
-            run_id=run_id,
-            results=mapped_results,
-            total_count=len(mapped_results),
-            original_prompt=original_prompt
+            user_id=user_id, run_id=run_id, results=mapped_results, total_count=len(mapped_results), original_prompt=original_prompt
         )
     except HTTPException:
         raise
@@ -1796,7 +1703,7 @@
     limit: int = 100,
     offset: int = 0,
     db=Depends(get_db),
-    current_user=Depends(require_user)
+    current_user=Depends(require_user),
 ):
     """Return daily usage stats and historical runs for a user.
 
@@ -1852,9 +1759,11 @@
                 FROM prospecting_runs pr
                 WHERE {where_sql}
                 ORDER BY pr.end_time DESC NULLS LAST, pr.start_time DESC
-                LIMIT ${len(params)+1} OFFSET ${len(params)+2}
+                LIMIT ${len(params) + 1} OFFSET ${len(params) + 2}
                 """,
-                *params, limit, offset
+                *params,
+                limit,
+                offset,
             )
 
         previous_runs: List[dict] = []
@@ -1872,7 +1781,7 @@
             # Optionally attach search context (minimal phrase only)
             if include_search_context:
                 search_phrase = None
-                
+
                 # For all workflow types, get prompt from session_interactions (unified approach)
                 if row["workflow_type"] in ("company_selection", "general_search", "specific_company"):
                     async with db.pool.acquire() as conn:
@@ -1883,7 +1792,8 @@
                             ORDER BY interaction_number DESC
                             LIMIT 1
                             """,
-                            row["run_id"], row["session_id"]
+                            row["run_id"],
+                            row["session_id"],
                         )
                     if interaction and interaction.get("prompt"):
                         search_phrase = interaction["prompt"]
@@ -1897,7 +1807,8 @@
                                 ORDER BY created_at DESC
                                 LIMIT 1
                                 """,
-                                row["run_id"], user_id
+                                row["run_id"],
+                                user_id,
                             )
                         if csr and csr.get("search_query"):
                             query_raw = csr["search_query"]
@@ -1905,9 +1816,12 @@
                                 query_obj = json.loads(query_raw) if isinstance(query_raw, str) else query_raw
                                 # Synthesize a compact phrase if fields present
                                 parts = []
-                                if query_obj.get("investor_type"): parts.append(query_obj["investor_type"])
-                                if query_obj.get("location"): parts.append(query_obj["location"])
-                                if query_obj.get("investor_focus"): parts.append(query_obj["investor_focus"])
+                                if query_obj.get("investor_type"):
+                                    parts.append(query_obj["investor_type"])
+                                if query_obj.get("location"):
+                                    parts.append(query_obj["location"])
+                                if query_obj.get("investor_focus"):
+                                    parts.append(query_obj["investor_focus"])
                                 search_phrase = " ".join(parts) or None
                             except Exception:
                                 search_phrase = None
@@ -1917,12 +1831,7 @@
 
             previous_runs.append(item)
 
-        return {
-            "runs_today": runs_today,
-            "daily_limit": daily_limit,
-            "runs_remaining": runs_remaining,
-            "previous_runs": previous_runs
-        }
+        return {"runs_today": runs_today, "daily_limit": daily_limit, "runs_remaining": runs_remaining, "previous_runs": previous_runs}
     except HTTPException:
         raise
     except Exception:
@@ -1930,28 +1839,26 @@
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
 
-
 @router.get("/enrich/company/{run_id}", response_model=CompanyEnrichmentResponse)
 async def get_company_enrichment(run_id: str, db=Depends(get_db), current_user=Depends(require_user)):
     """Get company enrichment results"""
     user_id = current_user.user_id
     logger.info("Company enrichment results request", extra={"run_id": run_id, "user_id": user_id})
     try:
-        
-        
         # Get company enrichment data
         company_data = await db.get_company_enrichment_by_run(run_id, user_id)
         if not company_data:
             raise HTTPException(status_code=404, detail="Company enrichment results not found")
-        
+
         # Get YouTube media results if available
         youtube_media = await db.get_youtube_media_by_run(run_id, user_id)
-        
+
         # Get narrative data and RIA summary from agent_results
         narrative_data = None
         ria_summary = None
         async with db.pool.acquire() as conn:
-            result = await conn.fetchrow("""
+            result = await conn.fetchrow(
+                """
                 SELECT 
                     result_data->'narrative_result' AS narrative_data,
                     result_data AS raw_result
@@ -1961,24 +1868,29 @@
                 AND agent_name = 'Company Enrichment Agent'
                 ORDER BY created_at DESC 
                 LIMIT 1
-            """, run_id, user_id)
-            
-            if result and result['narrative_data']:
-                narrative_data = result['narrative_data']
+            """,
+                run_id,
+                user_id,
+            )
+
+            if result and result["narrative_data"]:
+                narrative_data = result["narrative_data"]
                 # Handle asyncpg.Record or similar
-                if hasattr(narrative_data, 'items') and not isinstance(narrative_data, dict):
+                if hasattr(narrative_data, "items") and not isinstance(narrative_data, dict):
                     narrative_data = dict(narrative_data)
                 # Handle string
                 if isinstance(narrative_data, str):
                     try:
                         import json
+
                         narrative_data = json.loads(narrative_data)
                     except Exception as e:
                         print(f"‚ùå Error parsing narrative_data as JSON: {e}")
                         narrative_data = None
 
             # Fetch latest RIA detection result
-            ria_result = await conn.fetchrow("""
+            ria_result = await conn.fetchrow(
+                """
                 SELECT result_data
                 FROM agent_results
                 WHERE run_id = $1
@@ -1986,11 +1898,14 @@
                 AND agent_name = 'RIA Detection'
                 ORDER BY created_at DESC
                 LIMIT 1
-            """, run_id, user_id)
+            """,
+                run_id,
+                user_id,
+            )
 
-            if ria_result and ria_result['result_data']:
-                ria_data = ria_result['result_data']
-                if hasattr(ria_data, 'items') and not isinstance(ria_data, dict):
+            if ria_result and ria_result["result_data"]:
+                ria_data = ria_result["result_data"]
+                if hasattr(ria_data, "items") and not isinstance(ria_data, dict):
                     ria_data = dict(ria_data)
                 if isinstance(ria_data, str):
                     try:
@@ -2000,21 +1915,22 @@
 
                 if ria_data:
                     ria_summary = {
-                        'is_ria': ria_data.get('is_ria', False),
-                        'crd_number': ria_data.get('crd_number'),
-                        'has_ria_data': bool(ria_data.get('crd_number'))
+                        "is_ria": ria_data.get("is_ria", False),
+                        "crd_number": ria_data.get("crd_number"),
+                        "has_ria_data": bool(ria_data.get("crd_number")),
                     }
-        
+
         # Get execution time and original prompt from prospecting_runs table
         original_prompt = None
         async with db.pool.acquire() as conn:
             run_info = await conn.fetchrow(
                 "SELECT execution_time_ms, session_id, workflow_type, search_params FROM prospecting_runs WHERE run_id = $1 AND user_id = $2",
-                run_id, user_id
+                run_id,
+                user_id,
             )
-            
+
             execution_time_ms = run_info["execution_time_ms"] if run_info else None
-            
+
             # Get the original prompt from session_interactions (unified approach for all workflow types)
             if run_info and run_info.get("session_id"):
                 interaction = await conn.fetchrow(
@@ -2024,18 +1940,22 @@
                     ORDER BY interaction_number DESC
                     LIMIT 1
                     """,
-                    run_id, run_info["session_id"]
+                    run_id,
+                    run_info["session_id"],
                 )
                 if interaction and interaction.get("prompt"):
                     original_prompt = interaction["prompt"]
-        
-        logger.info("Company enrichment results retrieved", extra={
-            "run_id": run_id,
-            "user_id": user_id,
-            "has_company_data": bool(company_data),
-            "has_narrative_data": bool(narrative_data),
-            "has_youtube_media": bool(youtube_media)
-        })
+
+        logger.info(
+            "Company enrichment results retrieved",
+            extra={
+                "run_id": run_id,
+                "user_id": user_id,
+                "has_company_data": bool(company_data),
+                "has_narrative_data": bool(narrative_data),
+                "has_youtube_media": bool(youtube_media),
+            },
+        )
         return CompanyEnrichmentResponse(
             user_id=user_id,
             run_id=run_id,
@@ -2047,7 +1967,7 @@
             riaSummary=ria_summary,
             original_prompt=original_prompt,
             youtube_media=youtube_media,
-            youtubeMedia=youtube_media
+            youtubeMedia=youtube_media,
         )
     except HTTPException:
         raise
@@ -2055,36 +1975,28 @@
         logger.exception("Failed to get company enrichment results")
         raise HTTPException(status_code=500, detail="An unexpected error occurred. Please try again later.")
 
+
 @router.get("/enrich/person/{run_id}", response_model=PersonEnrichmentResponse)
 async def get_person_enrichment(run_id: str, db=Depends(get_db), current_user=Depends(require_user)):
     """Get person enrichment results"""
     user_id = current_user.user_id
     logger.info("Person enrichment results request", extra={"run_id": run_id, "user_id": user_id})
     try:
-        
-        
         # Get person enrichment data
         person_data = await db.get_person_enrichment_by_run(run_id, user_id)
         if not person_data:
             logger.info("Person enrichment not available", extra={"run_id": run_id, "user_id": user_id})
             return Response(status_code=204)
-        
+
         # Get execution time from prospecting_runs table
         async with db.pool.acquire() as conn:
-            run_info = await conn.fetchrow(
-                "SELECT execution_time_ms FROM prospecting_runs WHERE run_id = $1 AND user_id = $2",
-                run_id, user_id
-            )
-        
+            run_info = await conn.fetchrow("SELECT execution_time_ms FROM prospecting_runs WHERE run_id = $1 AND user_id = $2", run_id, user_id)
+
         execution_time_ms = run_info["execution_time_ms"] if run_info else None
-        
+
         logger.info("Person enrichment results retrieved", extra={"run_id": run_id, "user_id": user_id, "has_person_data": bool(person_data)})
         return PersonEnrichmentResponse(
-            user_id=user_id,
-            run_id=run_id,
-            person_data=person_data,
-            status="completed",
-            execution_time_ms=execution_time_ms
+            user_id=user_id, run_id=run_id, person_data=person_data, status="completed", execution_time_ms=execution_time_ms
         )
     except HTTPException:
         raise
@@ -2104,10 +2016,10 @@
         self._latest_path_cache: Optional[str] = None
         self._path_cache_time: Optional[datetime] = None
         self._data_cache_time: Optional[datetime] = None
-        self._s3_client = boto3.client('s3')
+        self._s3_client = boto3.client("s3")
         self._bucket = get_s3_bucket()
         self._prefix = get_s3_prefix()
-        
+
         # Log configuration for debugging
         logger = get_logger(__name__)
         logger.info(f"IndexCache initialized with bucket: {self._bucket}, prefix: {self._prefix}")
@@ -2117,11 +2029,11 @@
         try:
             # Step 1: Get latest file path
             latest_path = self._get_latest_file_path()
-            
+
             # Step 2: Check if we need to refresh data
             if self._needs_data_refresh():
                 self._download_and_cache_data(latest_path)
-                
+
             return self._df
         except Exception as e:
             logger = get_logger(__name__)
@@ -2131,10 +2043,9 @@
     def _get_latest_file_path(self) -> str:
         """Get the latest index file path, using cache if available."""
         # Use cached path if available and not stale
-        if (self._latest_path_cache and self._path_cache_time and 
-            datetime.now() - self._path_cache_time < timedelta(hours=1)):
+        if self._latest_path_cache and self._path_cache_time and datetime.now() - self._path_cache_time < timedelta(hours=1):
             return self._latest_path_cache
-            
+
         # Find latest by scanning folder structure
         latest_path = self._find_latest_by_folder_structure()
         self._latest_path_cache = latest_path
@@ -2148,67 +2059,59 @@
             # List all year folders
             year_prefix = f"{self._prefix}/processed/index/"
             logger.info(f"Scanning S3 for index data at: s3://{self._bucket}/{year_prefix}")
-            
-            response = self._s3_client.list_objects_v2(
-                Bucket=self._bucket,
-                Prefix=year_prefix,
-                Delimiter='/'
-            )
-            
+
+            response = self._s3_client.list_objects_v2(Bucket=self._bucket, Prefix=year_prefix, Delimiter="/")
+
             logger.info(f"S3 response keys: {list(response.keys())}")
-            if 'CommonPrefixes' in response:
+            if "CommonPrefixes" in response:
                 logger.info(f"Found {len(response['CommonPrefixes'])} year folders")
             else:
                 logger.warning("No CommonPrefixes found in S3 response")
-            
-            if 'CommonPrefixes' not in response:
+
+            if "CommonPrefixes" not in response:
                 raise HTTPException(status_code=503, detail="No index data found in S3")
-            
+
             # Find the latest year
             latest_year = None
-            for prefix in response['CommonPrefixes']:
-                folder_name = prefix['Prefix'].split('/')[-2]  # Get folder name
-                if folder_name.startswith('year='):
-                    year = int(folder_name.split('=')[1])
+            for prefix in response["CommonPrefixes"]:
+                folder_name = prefix["Prefix"].split("/")[-2]  # Get folder name
+                if folder_name.startswith("year="):
+                    year = int(folder_name.split("=")[1])
                     if latest_year is None or year > latest_year:
                         latest_year = year
-            
+
             if latest_year is None:
                 raise HTTPException(status_code=503, detail="No valid year folders found")
-            
+
             # List months in the latest year
             month_prefix = f"{year_prefix}year={latest_year}/"
-            response = self._s3_client.list_objects_v2(
-                Bucket=self._bucket,
-                Prefix=month_prefix,
-                Delimiter='/'
-            )
-            
-            if 'CommonPrefixes' not in response:
+            response = self._s3_client.list_objects_v2(Bucket=self._bucket, Prefix=month_prefix, Delimiter="/")
+
+            if "CommonPrefixes" not in response:
                 raise HTTPException(status_code=503, detail=f"No month folders found for year {latest_year}")
-            
+
             # Find the latest month
             latest_month = None
-            for prefix in response['CommonPrefixes']:
-                folder_name = prefix['Prefix'].split('/')[-2]  # Get folder name
-                if folder_name.startswith('month='):
-                    month = int(folder_name.split('=')[1])
+            for prefix in response["CommonPrefixes"]:
+                folder_name = prefix["Prefix"].split("/")[-2]  # Get folder name
+                if folder_name.startswith("month="):
+                    month = int(folder_name.split("=")[1])
                     if latest_month is None or month > latest_month:
                         latest_month = month
-            
+
             if latest_month is None:
                 raise HTTPException(status_code=503, detail=f"No valid month folders found for year {latest_year}")
-            
+
             # Construct the path to the index file
             latest_path = f"{month_prefix}month={latest_month:02d}/index.csv"
-            
+
             # Verify the file exists
             try:
                 self._s3_client.head_object(Bucket=self._bucket, Key=latest_path)
                 return latest_path
             except ClientError:
                 raise HTTPException(status_code=503, detail=f"Index file not found: {latest_path}")
-                
+
         except ClientError as e:
             raise HTTPException(status_code=503, detail=f"S3 error: {str(e)}")
 
@@ -2217,27 +2120,26 @@
         # No data cached yet
         if self._df is None:
             return True
-            
+
         # Check if cache is stale (1 hour)
-        if (self._data_cache_time and 
-            datetime.now() - self._data_cache_time < timedelta(hours=1)):
+        if self._data_cache_time and datetime.now() - self._data_cache_time < timedelta(hours=1):
             return False  # Cache is still fresh
-        
+
         # Check if a newer month exists
         if self._check_for_newer_months():
             return True  # New month exists, need to refresh
-        
+
         return False
 
     def _check_for_newer_months(self) -> bool:
         """Check if any month newer than current exists."""
         if not self._latest_path_cache:
             return True  # No cached path, need to refresh
-            
+
         try:
             # Parse current cached path
             current_year, current_month = self._parse_cached_path()
-            
+
             # Check for any month in the same year that's newer
             for month in range(current_month + 1, 13):
                 check_path = f"{self._prefix}/processed/index/year={current_year}/month={month:02d}/index.csv"
@@ -2246,7 +2148,7 @@
                     return True  # Found a newer month
                 except ClientError:
                     continue
-            
+
             # Check for any month in the next year
             next_year = current_year + 1
             for month in range(1, 13):
@@ -2256,36 +2158,36 @@
                     return True  # Found a newer month in next year
                 except ClientError:
                     continue
-            
+
             return False  # No newer months found
-            
+
         except Exception:
             return True  # Error parsing, refresh to be safe
 
     def _parse_cached_path(self) -> tuple[int, int]:
         """Parse year and month from cached path."""
         # Path format: output/sec_ingestion/processed/index/year=2025/month=07/index.csv
-        parts = self._latest_path_cache.split('/')
+        parts = self._latest_path_cache.split("/")
         year_part = None
         month_part = None
-        
+
         for part in parts:
-            if part.startswith('year='):
+            if part.startswith("year="):
                 year_part = part
-            elif part.startswith('month='):
+            elif part.startswith("month="):
                 month_part = part
-        
+
         if not year_part or not month_part:
             raise ValueError(f"Invalid path format: {self._latest_path_cache}")
-        
-        year = int(year_part.split('=')[1])
-        month = int(month_part.split('=')[1])
+
+        year = int(year_part.split("=")[1])
+        month = int(month_part.split("=")[1])
         return year, month
 
     def _convert_numeric_columns(self, df: pl.DataFrame) -> pl.DataFrame:
         """Convert known numeric columns from strings to appropriate numeric types."""
         logger = get_logger(__name__)
-        
+
         # Define known numeric columns and their expected types
         numeric_columns = {
             "CRD": pl.Int64,
@@ -2298,69 +2200,64 @@
             "YEAR": pl.Int64,
             "MONTH": pl.Int64,
         }
-        
+
         try:
             for col_name, target_type in numeric_columns.items():
                 if col_name in df.columns:
                     try:
                         if target_type == pl.Int64:
                             # Convert to int, handling nulls and invalid values
-                            df = df.with_columns(
-                                pl.col(col_name).cast(pl.Int64, strict=False).alias(col_name)
-                            )
+                            df = df.with_columns(pl.col(col_name).cast(pl.Int64, strict=False).alias(col_name))
                         elif target_type == pl.Float64:
                             # Convert to float, handling nulls and invalid values
-                            df = df.with_columns(
-                                pl.col(col_name).cast(pl.Float64, strict=False).alias(col_name)
-                            )
+                            df = df.with_columns(pl.col(col_name).cast(pl.Float64, strict=False).alias(col_name))
                         # pl.Utf8 columns are left as strings (no conversion needed)
-                        
+
                         logger.debug(f"Successfully converted column '{col_name}' to {target_type}")
-                        
+
                     except Exception as col_error:
                         logger.warning(f"Failed to convert column '{col_name}' to {target_type}: {str(col_error)}")
                         # Keep as string if conversion fails
                         continue
-                        
+
         except Exception as e:
             logger.warning(f"Error during numeric column conversion: {str(e)}")
             # Return original dataframe if conversion fails
             return df
-            
+
         return df
 
     def _download_and_cache_data(self, s3_path: str) -> None:
         """Download and cache data from S3."""
         logger = get_logger(__name__)
         logger.info(f"Downloading index data from S3: {s3_path}")
-        
+
         try:
             # Download file from S3
             response = self._s3_client.get_object(Bucket=self._bucket, Key=s3_path)
-            content = response['Body'].read()
-            
+            content = response["Body"].read()
+
             # Parse CSV content with error handling
             import io
-            csv_buffer = io.StringIO(content.decode('utf-8'))
-            
+
+            csv_buffer = io.StringIO(content.decode("utf-8"))
+
             try:
                 # Read CSV as strings first to avoid type inference issues with comma-separated numbers
                 csv_buffer.seek(0)  # Reset buffer position
                 self._df = pl.read_csv(
                     csv_buffer,
                     infer_schema_length=0,  # Read all columns as strings initially
-                    null_values=["", "N/A", "NULL", "null", "11-25"]  # Treat range values as null
+                    null_values=["", "N/A", "NULL", "null", "11-25"],  # Treat range values as null
                 )
-                
+
                 # Strip commas from all columns before type conversion (handles values like "696,322,350.00")
                 for col in self._df.columns:
-                    self._df = self._df.with_columns(
-                        pl.col(col).str.replace_all(",", "").alias(col)
-                    )
-                
+                    self._df = self._df.with_columns(pl.col(col).str.replace_all(",", "").alias(col))
+
                 # Smart type conversion for known numeric columns
                 self._df = self._convert_numeric_columns(self._df)
-                
+
             except Exception as parse_error:
                 logger.warning(f"Polars parsing failed, trying with more lenient settings: {str(parse_error)}")
                 # Fallback: read with error handling
@@ -2368,20 +2265,18 @@
                 self._df = pl.read_csv(
                     csv_buffer,
                     infer_schema_length=0,  # Read all columns as strings
-                    ignore_errors=True  # Skip problematic rows
+                    ignore_errors=True,  # Skip problematic rows
                 )
                 # Strip commas from all columns
                 for col in self._df.columns:
-                    self._df = self._df.with_columns(
-                        pl.col(col).str.replace_all(",", "").alias(col)
-                    )
+                    self._df = self._df.with_columns(pl.col(col).str.replace_all(",", "").alias(col))
                 # Try type conversion on fallback data too
                 self._df = self._convert_numeric_columns(self._df)
-            
+
             # Update cache timestamp
             self._data_cache_time = datetime.now()
             logger.info(f"Successfully loaded index data: {len(self._df)} rows, {len(self._df.columns)} columns")
-            
+
         except Exception as e:
             logger.error(f"Failed to download and cache index data from {s3_path}: {str(e)}", exc_info=True)
             raise HTTPException(status_code=503, detail=f"Failed to load index data: {str(e)}")
@@ -2394,10 +2289,10 @@
         self._df: Optional[pd.DataFrame] = None
         self._s3_last_modified: Optional[datetime] = None
         self._local_cache_time: Optional[datetime] = None
-        self._s3_client = boto3.client('s3')
+        self._s3_client = boto3.client("s3")
         self._bucket = get_s3_bucket()
         self._prefix = get_s3_prefix()
-        
+
         # Log configuration for debugging
         logger = get_logger(__name__)
         logger.info(f"LatestPdfCache initialized with bucket: {self._bucket}, prefix: {self._prefix}")
@@ -2407,11 +2302,11 @@
         try:
             # Fixed path - always the same
             s3_path = f"{self._prefix}/processed/adv/latest.csv"
-            
+
             # Check if we need to refresh
             if self._needs_data_refresh(s3_path):
                 self._download_and_cache_data(s3_path)
-                
+
             return self._df
         except Exception as e:
             logger = get_logger(__name__)
@@ -2422,21 +2317,19 @@
         """Check if data needs to be refreshed based on S3 timestamp."""
         if self._df is None:
             return True  # No data cached yet
-            
+
         # Check if cache is stale (1 hour)
-        if (self._local_cache_time and 
-            datetime.now() - self._local_cache_time < timedelta(hours=1)):
+        if self._local_cache_time and datetime.now() - self._local_cache_time < timedelta(hours=1):
             return False  # Cache is still fresh
-            
+
         # Check if S3 file is newer than our cached version
         try:
             response = self._s3_client.head_object(Bucket=self._bucket, Key=s3_path)
-            s3_last_modified = response['LastModified'].replace(tzinfo=None)
-            
-            if (self._s3_last_modified and 
-                s3_last_modified <= self._s3_last_modified):
+            s3_last_modified = response["LastModified"].replace(tzinfo=None)
+
+            if self._s3_last_modified and s3_last_modified <= self._s3_last_modified:
                 return False  # File hasn't changed
-                
+
             return True  # File is newer
         except ClientError:
             return True  # Error checking, refresh to be safe
@@ -2444,7 +2337,7 @@
     def _convert_numeric_columns_pandas(self, df: pd.DataFrame) -> pd.DataFrame:
         """Convert known numeric columns from strings to appropriate numeric types."""
         logger = get_logger(__name__)
-        
+
         # Define known numeric columns and their expected types
         numeric_columns = {
             "CRD": "int64",
@@ -2454,57 +2347,58 @@
             "TOTAL_INVESTMENT_ADVISER_REPRESENTATIVES": "int64",
             "TOTAL_OFFICES": "int64",
         }
-        
+
         try:
             for col_name, target_type in numeric_columns.items():
                 if col_name in df.columns:
                     try:
                         if target_type == "int64":
                             # Convert to int, handling nulls and invalid values
-                            df[col_name] = pd.to_numeric(df[col_name], errors='coerce').astype('Int64')
+                            df[col_name] = pd.to_numeric(df[col_name], errors="coerce").astype("Int64")
                         elif target_type == "float64":
                             # Convert to float, handling nulls and invalid values
-                            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')
-                        
+                            df[col_name] = pd.to_numeric(df[col_name], errors="coerce")
+
                         logger.debug(f"Successfully converted column '{col_name}' to {target_type}")
-                        
+
                     except Exception as col_error:
                         logger.warning(f"Failed to convert column '{col_name}' to {target_type}: {str(col_error)}")
                         # Keep as string if conversion fails
                         continue
-                        
+
         except Exception as e:
             logger.warning(f"Error during numeric column conversion: {str(e)}")
             # Return original dataframe if conversion fails
             return df
-            
+
         return df
 
     def _download_and_cache_data(self, s3_path: str) -> None:
         """Download and cache data from S3."""
         logger = get_logger(__name__)
         logger.info(f"Downloading latest PDFs data from S3: {s3_path}")
-        
+
         try:
             # Download file from S3
             response = self._s3_client.get_object(Bucket=self._bucket, Key=s3_path)
-            content = response['Body'].read()
-            
+            content = response["Body"].read()
+
             # Parse CSV content with error handling
             import io
-            csv_buffer = io.StringIO(content.decode('utf-8'))
-            
+
+            csv_buffer = io.StringIO(content.decode("utf-8"))
+
             try:
                 # Load as strings first (most robust approach)
                 self._df = pd.read_csv(
                     csv_buffer,
                     dtype=str,  # Read all columns as strings to avoid parsing errors
-                    na_values=["", "N/A", "NULL", "null", "11-25"]  # Treat range values as null
+                    na_values=["", "N/A", "NULL", "null", "11-25"],  # Treat range values as null
                 )
-                
+
                 # Convert known numeric columns for better performance
                 self._df = self._convert_numeric_columns_pandas(self._df)
-                
+
             except Exception as parse_error:
                 logger.warning(f"Pandas parsing failed, trying with more lenient settings: {str(parse_error)}")
                 # Fallback: read with error handling
@@ -2512,16 +2406,16 @@
                 self._df = pd.read_csv(
                     csv_buffer,
                     dtype=str,  # Force all columns to string
-                    on_bad_lines='skip'  # Skip problematic lines
+                    on_bad_lines="skip",  # Skip problematic lines
                 )
                 # Try type conversion on fallback data too
                 self._df = self._convert_numeric_columns_pandas(self._df)
-            
+
             # Update cache timestamps
             self._local_cache_time = datetime.now()
-            self._s3_last_modified = response['LastModified'].replace(tzinfo=None)
+            self._s3_last_modified = response["LastModified"].replace(tzinfo=None)
             logger.info(f"Successfully loaded latest PDFs data: {len(self._df)} rows, {len(self._df.columns)} columns")
-            
+
         except ClientError as e:
             logger.error(f"Failed to download latest PDFs data from {s3_path}: {str(e)}", exc_info=True)
             raise HTTPException(status_code=503, detail=f"Failed to load latest PDFs data: {str(e)}")
@@ -2555,19 +2449,19 @@
 
 def _load_trends_json(crd: int) -> Dict[str, Any]:
     """Load trend data for a specific CRD from S3."""
-    s3_client = boto3.client('s3')
+    s3_client = boto3.client("s3")
     bucket = get_s3_bucket()
-    
+
     # S3 path structure: output/sec_ingestion/processed/trends/latest/crd={crd}/trends.v1.json
     s3_path = f"{get_s3_prefix()}/processed/trends/latest/crd={crd}/trends.v1.json"
-    
+
     try:
         # Download trend data from S3
         response = s3_client.get_object(Bucket=bucket, Key=s3_path)
-        content = response['Body'].read().decode('utf-8')
+        content = response["Body"].read().decode("utf-8")
         return json.loads(content)
     except ClientError as e:
-        if e.response['Error']['Code'] == 'NoSuchKey':
+        if e.response["Error"]["Code"] == "NoSuchKey":
             raise HTTPException(status_code=404, detail=f"Trend data not found for CRD {crd}")
         else:
             raise HTTPException(status_code=503, detail=f"Failed to load trend data: {str(e)}")
@@ -2593,13 +2487,9 @@
     if column not in df.columns:
         logger.warning(f"‚ö†Ô∏è Website column '{column}' not found, adding empty domain column")
         return df.with_columns(pl.lit(None).alias("__domain"))
-    
-    return df.with_columns(
-        pl.col(column)
-        .map_elements(_canonicalize_domain, return_dtype=pl.Utf8)
-        .alias("__domain")
-    )
 
+    return df.with_columns(pl.col(column).map_elements(_canonicalize_domain, return_dtype=pl.Utf8).alias("__domain"))
+
 
 def _to_serializable_records(df: pl.DataFrame, limit: int) -> List[Dict[str, Any]]:
     if df.height == 0:
@@ -2611,20 +2501,17 @@
 def _get_top_3_private_funds(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:
     """Get top 3 private fund types by count for a company."""
     fund_counts = []
-    
+
     for field_name, column_name in PRIVATE_FUND_COUNT_COLUMNS.items():
         if column_name in company_record and company_record[column_name] is not None:
             try:
                 count = int(company_record[column_name])
                 if count > 0:
                     display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)
-                    fund_counts.append({
-                        "category": display_name,
-                        "count": count
-                    })
+                    fund_counts.append({"category": display_name, "count": count})
             except (ValueError, TypeError):
                 continue
-    
+
     # Sort by count (highest first) and return top 3
     fund_counts.sort(key=lambda x: x["count"], reverse=True)
     return fund_counts[:3]
@@ -2633,20 +2520,17 @@
 def _get_top_3_allocations(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:
     """Get top 3 SMA allocation categories by percentage for a company."""
     allocations = []
-    
+
     for field_name, column_name in SMA_ALLOCATION_COLUMNS.items():
         if column_name in company_record and company_record[column_name] is not None:
             try:
                 percentage = float(company_record[column_name])
                 if percentage > 0:
                     display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)
-                    allocations.append({
-                        "category": display_name,
-                        "percentage": percentage
-                    })
+                    allocations.append({"category": display_name, "percentage": percentage})
             except (ValueError, TypeError):
                 continue
-    
+
     # Sort by percentage (highest first) and return top 3
     allocations.sort(key=lambda x: x["percentage"], reverse=True)
     return allocations[:3]
@@ -2655,7 +2539,7 @@
 def _get_total_clients(company_record: Dict[str, Any]) -> int:
     """Get total count of all clients for a company by summing all client type columns."""
     total_count = 0
-    
+
     for column_name in ALL_CLIENT_TYPE_COLUMNS:
         if column_name in company_record and company_record[column_name] is not None:
             try:
@@ -2664,7 +2548,7 @@
                     total_count += count
             except (ValueError, TypeError):
                 continue
-    
+
     return total_count
 
 
@@ -2672,32 +2556,32 @@
     """Transform a company record for search preview with limited fields as per schema."""
     logger = get_logger(__name__)
     transformed = {}
-    
+
     # Debug: Log available fields in the raw record
     available_fields = list(company_record.keys())
     logger.info(f"üîç Available fields in company record: {len(available_fields)} total")
     logger.debug(f"üîç Sample fields: {available_fields[:10]}...")
-    
+
     # Map only the specific fields required for search preview (limited subset)
     mapped_count = 0
     for original_field, display_field in COMPANY_PREVIEW_MAPPING.items():
         if original_field in company_record and company_record[original_field] is not None:
             transformed[display_field] = company_record[original_field]
             mapped_count += 1
-    
+
     logger.info(f"üîç Successfully mapped {mapped_count} fields from COMPANY_PREVIEW_MAPPING")
-    
+
     # Add top 3 private funds (calculated from all private fund data)
     transformed["top_3_private_funds"] = _get_top_3_private_funds(company_record)
-    
+
     # Add top 3 allocations (calculated from all SMA allocation data)
     transformed["top_3_allocations"] = _get_top_3_allocations(company_record)
-    
+
     # Add total clients (calculated from all client type columns)
     transformed["total_clients"] = _get_total_clients(company_record)
-    
+
     logger.info(f"üîç Final transformed record has {len(transformed)} fields (limited preview)")
-    
+
     return transformed
 
 
@@ -2709,28 +2593,28 @@
     logger = get_logger(__name__)
     logger.info(f"üîç RIA EXTRACT: Starting extraction for CRD: {crd_number}")
     print(f"üîç RIA EXTRACT: Starting extraction for CRD: {crd_number}")
-    
+
     try:
         logger.info("üîç RIA EXTRACT: Loading RIA index cache")
         print("üîç RIA EXTRACT: Loading RIA index cache")
-        
+
         df = _INDEX_CACHE.load()
         logger.info(f"‚úÖ RIA EXTRACT: Loaded RIA index data: {df.height} rows, {len(df.columns)} columns")
         print(f"‚úÖ RIA EXTRACT: Loaded RIA index data: {df.height} rows, {len(df.columns)} columns")
-        
+
         # Ensure required columns exist
         logger.info("üîç RIA EXTRACT: Ensuring required columns exist")
         print("üîç RIA EXTRACT: Ensuring required columns exist")
-        
+
         _ensure_columns(df, [CRD_COLUMN, NAME_COLUMN, CITY_COLUMN, STATE_COLUMN, COUNTRY_COLUMN])
-        
+
         logger.info("‚úÖ RIA EXTRACT: Required columns ensured")
         print("‚úÖ RIA EXTRACT: Required columns ensured")
-        
+
         # Filter by CRD number
         logger.info(f"üîç RIA EXTRACT: Filtering by CRD: {crd_number}")
         print(f"üîç RIA EXTRACT: Filtering by CRD: {crd_number}")
-        
+
         # Convert string CRD to integer for database comparison
         try:
             crd_int = int(crd_number)
@@ -2740,25 +2624,25 @@
             logger.error(f"‚ùå RIA EXTRACT: Invalid CRD number format: {crd_number}", extra={"error": str(e)})
             print(f"‚ùå RIA EXTRACT: Invalid CRD number format: {crd_number}")
             raise HTTPException(status_code=400, detail=f"Invalid CRD number format: {crd_number}")
-        
+
         company_df = df.filter(pl.col(CRD_COLUMN) == crd_int)
-        
+
         logger.info(f"üîç RIA EXTRACT: Filter result: {company_df.height} rows found")
         print(f"üîç RIA EXTRACT: Filter result: {company_df.height} rows found")
-        
+
         if company_df.is_empty():
             logger.warning(f"‚ùå RIA EXTRACT: CRD {crd_number} not found in RIA index")
             print(f"‚ùå RIA EXTRACT: CRD {crd_number} not found in RIA index")
             raise HTTPException(status_code=404, detail=f"CRD {crd_number} not found in RIA database")
-        
+
         company_data = company_df.to_dicts()[0]
         logger.info(f"‚úÖ RIA EXTRACT: Found company data for CRD {crd_number}: {company_data.get(NAME_COLUMN)}")
         print(f"‚úÖ RIA EXTRACT: Found company data for CRD {crd_number}: {company_data.get(NAME_COLUMN)}")
-        
+
         # Build location string from components
         logger.info("üîç RIA EXTRACT: Building location string")
         print("üîç RIA EXTRACT: Building location string")
-        
+
         location_parts = []
         if company_data.get(CITY_COLUMN):
             location_parts.append(company_data[CITY_COLUMN])
@@ -2766,25 +2650,18 @@
             location_parts.append(company_data[STATE_COLUMN])
         if company_data.get(COUNTRY_COLUMN):
             location_parts.append(company_data[COUNTRY_COLUMN])
-        
+
         location = ", ".join(location_parts) if location_parts else None
-        
-        result = {
-            "crd_number": crd_number,
-            "company_name": company_data.get(NAME_COLUMN),
-            "location": location
-        }
-        
+
+        result = {"crd_number": crd_number, "company_name": company_data.get(NAME_COLUMN), "location": location}
+
         logger.info(f"üéâ RIA EXTRACT: Successfully extracted data", extra=result)
         print(f"üéâ RIA EXTRACT: Successfully extracted data: {result}")
-        
+
         return result
-        
+
     except HTTPException as he:
-        logger.error(f"‚ùå RIA EXTRACT: HTTP Exception for CRD {crd_number}", extra={
-            "status_code": he.status_code,
-            "detail": he.detail
-        })
+        logger.error(f"‚ùå RIA EXTRACT: HTTP Exception for CRD {crd_number}", extra={"status_code": he.status_code, "detail": he.detail})
         print(f"‚ùå RIA EXTRACT: HTTP Exception for CRD {crd_number} - Status: {he.status_code}, Detail: {he.detail}")
         raise
     except Exception as e:
@@ -2792,25 +2669,26 @@
         print(f"‚ùå RIA EXTRACT: Exception for CRD {crd_number}: {str(e)}")
         raise HTTPException(status_code=500, detail=f"Failed to extract company data: {str(e)}")
 
+
 def _pdf_records_for_crd(crd: int) -> List[Dict[str, Any]]:
     logger = get_logger(__name__)
     logger.info(f"Looking up PDF records for CRD: {crd}")
-    
+
     pdf_df = _LATEST_PDFS_CACHE.load().copy()
     logger.info(f"Loaded PDF data: {len(pdf_df)} rows, columns: {list(pdf_df.columns)}")
-    
+
     if "crd" not in pdf_df.columns:
         logger.error("‚ùå RIA latest.csv missing 'crd' column")
         raise HTTPException(status_code=500, detail="RIA latest.csv missing 'crd' column")
-    
+
     pdf_df["crd_numeric"] = pd.to_numeric(pdf_df["crd"], errors="coerce")
     subset = pdf_df[pdf_df["crd_numeric"] == crd]
     logger.info(f"Found {len(subset)} PDF records for CRD {crd}")
-    
+
     if subset.empty:
         logger.warning(f"No PDF records found for CRD {crd}")
         return []
-    
+
     subset = subset.drop(columns=["crd_numeric"], errors="ignore")
     subset = subset.where(pd.notnull(subset), None)
     records = subset.to_dict(orient="records")
@@ -2833,7 +2711,7 @@
 # Client type columns (Y/N Selection)
 CLIENT_TYPE_COLUMNS = {
     "individual_clients": "5D(a)(1)",
-    "high_net_worth_clients": "5D(b)(1)", 
+    "high_net_worth_clients": "5D(b)(1)",
     "banking_clients": "5D(c)(1)",
     "investment_companies_clients": "5D(d)(1)",
     "pension_clients": "5D(e)(1)",
@@ -2847,20 +2725,34 @@
 
 # All client type columns for preview aggregation (both (1) and (2) variants)
 ALL_CLIENT_TYPE_COLUMNS = [
-    "5D(a)(1)", "5D(a)(2)",
-    "5D(b)(1)", "5D(b)(2)", 
-    "5D(c)(1)", "5D(c)(2)",
-    "5D(d)(1)", "5D(d)(2)",
-    "5D(e)(1)", "5D(e)(2)",
-    "5D(f)(1)", "5D(f)(2)",
-    "5D(g)(1)", "5D(g)(2)",
-    "5D(h)(1)", "5D(h)(2)",
-    "5D(i)(1)", "5D(i)(2)",
-    "5D(j)(1)", "5D(j)(2)",
-    "5D(k)(1)", "5D(k)(2)",
-    "5D(l)(1)", "5D(l)(2)",
-    "5D(m)(1)", "5D(m)(2)",
-    "5D(n)(1)", "5D(n)(2)"
+    "5D(a)(1)",
+    "5D(a)(2)",
+    "5D(b)(1)",
+    "5D(b)(2)",
+    "5D(c)(1)",
+    "5D(c)(2)",
+    "5D(d)(1)",
+    "5D(d)(2)",
+    "5D(e)(1)",
+    "5D(e)(2)",
+    "5D(f)(1)",
+    "5D(f)(2)",
+    "5D(g)(1)",
+    "5D(g)(2)",
+    "5D(h)(1)",
+    "5D(h)(2)",
+    "5D(i)(1)",
+    "5D(i)(2)",
+    "5D(j)(1)",
+    "5D(j)(2)",
+    "5D(k)(1)",
+    "5D(k)(2)",
+    "5D(l)(1)",
+    "5D(l)(2)",
+    "5D(m)(1)",
+    "5D(m)(2)",
+    "5D(n)(1)",
+    "5D(n)(2)",
 ]
 
 # Private funds and asset class columns (Y/N Selection)
@@ -2878,7 +2770,7 @@
 # Private fund count columns (for top 3 by count)
 PRIVATE_FUND_COUNT_COLUMNS = {
     "hedge_funds_count": "Total number of Hedge funds",
-    "liquidity_funds_count": "Total number of Liquidity funds", 
+    "liquidity_funds_count": "Total number of Liquidity funds",
     "pe_funds_count": "Total number of PE funds",
     "real_estate_funds_count": "Total number of Real Estate funds",
     "securitized_funds_count": "Total number of Securitized funds",
@@ -2907,16 +2799,14 @@
 COMPANY_PREVIEW_MAPPING = {
     # Core business information (exactly as shown in schema)
     "Primary Business Name": "name",
-    "Organization CRD#": "crd", 
+    "Organization CRD#": "crd",
     "Main Office City": "city",
     "Main Office State": "state",
     "Main Office Country": "country",
-    
     # Financial and size metrics (exactly as shown in schema)
     "5F(2)(c)": "total_aum",
     "5C(1)": "total_employees",
     "Total number of offices, other than your Principal Office and place of business": "total_offices",
-    
     # Note: total_clients is calculated field, not a direct column mapping
 }
 
@@ -2924,7 +2814,7 @@
 DISPLAY_NAME_MAPPING = {
     # Core company fields
     "Primary Business Name": "name",
-    "Organization CRD#": "crd", 
+    "Organization CRD#": "crd",
     "Main Office City": "city",
     "Main Office State": "state",
     "Main Office Country": "country",
@@ -2932,15 +2822,13 @@
     "5C(1)": "total_employees",
     "Website Address": "website",
     "Total number of offices, other than your Principal Office and place of business": "total_offices",
-    
     # AUM fields
     "5F(2)(c)": "total_aum",
     "5F(2)(a)": "discretionary_aum",
     "5F(2)(b)": "non_discretionary_aum",
-    
     # Client type fields
     "5D(1)(a)": "individual_clients",
-    "5D(1)(b)": "high_net_worth_clients", 
+    "5D(1)(b)": "high_net_worth_clients",
     "5D(1)(c)": "banking_clients",
     "5D(1)(d)": "investment_companies_clients",
     "5D(1)(e)": "pension_clients",
@@ -2950,26 +2838,23 @@
     "5D(1)(i)": "insurance_clients",
     "5D(1)(j)": "sovereign_wealth_clients",
     "5D(1)(k)": "corporate_clients",
-    
     # Private fund count fields
     "Total number of Hedge funds": "Total Hedge Funds",
     "Total number of Liquidity funds": "Total Liquidity Funds",
-    "Total number of PE funds": "Total Private Equity Funds", 
+    "Total number of PE funds": "Total Private Equity Funds",
     "Total number of Real Estate funds": "Total Real Estate Funds",
     "Total number of Securitized funds": "Total Securitized Asset Funds",
     "Total number of VC funds": "Total Venture Capital Funds",
     "Total number of Other funds": "Total Other Private Funds",
-    
     # Private fund Y/N fields
     "Count of Private Funds - 7B(1)": "private_funds_count",
     "Any Hedge Funds": "has_hedge_funds",
-    "Any Liquidity Funds": "has_liquidity_funds", 
+    "Any Liquidity Funds": "has_liquidity_funds",
     "Any PE Funds": "has_pe_funds",
     "Any Real Estate Funds": "has_real_estate_funds",
     "Any Securitized Funds": "has_securitized_funds",
     "Any VC Funds": "has_vc_funds",
     "Any Other Funds": "has_other_funds",
-    
     # SMA allocation fields
     "5.K.(1)(a)(i) end year percentage": "SMA Year-End: Exchange-Traded Equities %",
     "5.K.(1)(a)(ii) end year percentage": "SMA Year-End: Non-Exchange-Traded Equities %",
@@ -2989,12 +2874,12 @@
 def _intelligent_name_search(df: pl.DataFrame, search_name: str, max_results: int) -> List[Dict[str, Any]]:
     """
     Intelligent company name search with best-in-class ensemble fuzzy matching.
-    
+
     Strategy:
     1. Try exact match first (case-insensitive)
     2. If no exact match, use ensemble fuzzy matching
     3. Return only high-quality matches (quality over quantity)
-    
+
     Ensemble Algorithm:
     - Uses 4 different similarity algorithms for robustness
     - Weighted combination ensures balanced matching
@@ -3002,33 +2887,31 @@
     """
     logger = get_logger(__name__)
     search_name_lower = search_name.lower().strip()
-    
+
     # Phase 1: Exact match (case-insensitive)
-    exact_matches = df.filter(
-        pl.col(NAME_COLUMN).str.to_lowercase() == search_name_lower
-    )
-    
+    exact_matches = df.filter(pl.col(NAME_COLUMN).str.to_lowercase() == search_name_lower)
+
     if not exact_matches.is_empty():
         logger.info(f"Found {exact_matches.height} exact matches for '{search_name}'")
         return _to_serializable_records(exact_matches, max_results)
-    
+
     # Phase 2: Ensemble fuzzy matching
     logger.info(f"No exact matches found, performing fuzzy search for '{search_name}'")
     fuzzy_matches = _ensemble_fuzzy_search(df, search_name, max_results)
-    
+
     return fuzzy_matches
 
 
 def _ensemble_fuzzy_search(df: pl.DataFrame, search_name: str, max_results: int) -> List[Dict[str, Any]]:
     """
     Best-in-class ensemble fuzzy matching using multiple algorithms.
-    
+
     Ensemble Components:
     1. Ratio (40%): Overall character similarity - handles typos
-    2. Partial Ratio (25%): Best substring match - handles partial names  
+    2. Partial Ratio (25%): Best substring match - handles partial names
     3. Token Sort Ratio (20%): Word order independent - handles "Company American" vs "American Company"
     4. Token Set Ratio (15%): Word overlap - handles extra/missing words
-    
+
     Why this ensemble is robust:
     - Multiple algorithms catch different types of variations
     - Weighted combination balances different matching strategies
@@ -3041,63 +2924,65 @@
         logger = get_logger(__name__)
         logger.warning("rapidfuzz not available, falling back to basic matching")
         return []
-    
+
     logger = get_logger(__name__)
     search_name_lower = search_name.lower().strip()
-    
+
     # Get all company data for comparison (preserve all columns for preview mapping)
     company_data = df.to_dicts()
-    
+
     scored_matches = []
-    
+
     for company in company_data:
         company_name = company[NAME_COLUMN]
         if not company_name:
             continue
-            
+
         company_name_lower = company_name.lower().strip()
-        
+
         # Calculate ensemble score using 4 different algorithms
         scores = {
-            'ratio': fuzz.ratio(search_name_lower, company_name_lower) / 100.0,
-            'partial_ratio': fuzz.partial_ratio(search_name_lower, company_name_lower) / 100.0,
-            'token_sort_ratio': fuzz.token_sort_ratio(search_name_lower, company_name_lower) / 100.0,
-            'token_set_ratio': fuzz.token_set_ratio(search_name_lower, company_name_lower) / 100.0,
+            "ratio": fuzz.ratio(search_name_lower, company_name_lower) / 100.0,
+            "partial_ratio": fuzz.partial_ratio(search_name_lower, company_name_lower) / 100.0,
+            "token_sort_ratio": fuzz.token_sort_ratio(search_name_lower, company_name_lower) / 100.0,
+            "token_set_ratio": fuzz.token_set_ratio(search_name_lower, company_name_lower) / 100.0,
         }
-        
+
         # Weighted ensemble score (tuned for company names)
         ensemble_score = (
-            scores['ratio'] * 0.40 +           # Overall similarity (handles typos)
-            scores['partial_ratio'] * 0.25 +   # Best substring (handles partial names)
-            scores['token_sort_ratio'] * 0.20 + # Word order independent
-            scores['token_set_ratio'] * 0.15   # Word overlap (handles extra/missing words)
+            scores["ratio"] * 0.40  # Overall similarity (handles typos)
+            + scores["partial_ratio"] * 0.25  # Best substring (handles partial names)
+            + scores["token_sort_ratio"] * 0.20  # Word order independent
+            + scores["token_set_ratio"] * 0.15  # Word overlap (handles extra/missing words)
         )
-        
+
         # Quality threshold: only include matches above 60% similarity
         if ensemble_score >= 0.6:
-            company['match_score'] = round(ensemble_score, 3)
-            company['match_details'] = {
-                'ratio': round(scores['ratio'], 3),
-                'partial_ratio': round(scores['partial_ratio'], 3),
-                'token_sort_ratio': round(scores['token_sort_ratio'], 3),
-                'token_set_ratio': round(scores['token_set_ratio'], 3)
+            company["match_score"] = round(ensemble_score, 3)
+            company["match_details"] = {
+                "ratio": round(scores["ratio"], 3),
+                "partial_ratio": round(scores["partial_ratio"], 3),
+                "token_sort_ratio": round(scores["token_sort_ratio"], 3),
+                "token_set_ratio": round(scores["token_set_ratio"], 3),
             }
             scored_matches.append(company)
-    
+
     # Sort by ensemble score (highest first)
-    scored_matches.sort(key=lambda x: x['match_score'], reverse=True)
-    
+    scored_matches.sort(key=lambda x: x["match_score"], reverse=True)
+
     # Quality over quantity: return only strong matches
     # If we have 2 strong matches (0.9+) and 8 weak ones (0.6-0.7), return only the 2 strong ones
-    strong_matches = [m for m in scored_matches if m['match_score'] >= 0.8]
+    strong_matches = [m for m in scored_matches if m["match_score"] >= 0.8]
     if len(strong_matches) >= 2:
         logger.info(f"Found {len(strong_matches)} strong matches (‚â•0.8), returning only strong matches")
         return strong_matches[:max_results]
-    
+
     # Otherwise, return top matches up to the limit
     result_count = min(len(scored_matches), max_results)
-    logger.info(f"Found {len(scored_matches)} total matches, returning top {result_count} (scores: {[m['match_score'] for m in scored_matches[:result_count]]})")
-    
+    logger.info(
+        f"Found {len(scored_matches)} total matches, returning top {result_count} (scores: {[m['match_score'] for m in scored_matches[:result_count]]})"
+    )
+
     return scored_matches[:result_count]
 
 
@@ -3112,7 +2997,7 @@
     """Return up to 100 advisors from the cached latest index after applying optional filters."""
     logger = get_logger(__name__)
     logger.info(f"RIA index request: state={state}, name={name}, min_5f_2c={min_5f_2c}, limit={limit}")
-    
+
     try:
         df = _INDEX_CACHE.load()
         logger.info(f"Loaded index data: {df.height} rows, {len(df.columns)} columns")
@@ -3129,12 +3014,7 @@
     if min_5f_2c is not None:
         if AUM_COLUMN not in df.columns:
             raise HTTPException(status_code=500, detail="RIA index missing 5F(2)(c) column")
-        filtered = filtered.filter(
-            pl.col(AUM_COLUMN)
-            .cast(pl.Float64, strict=False)
-            .fill_null(-1.0)
-            >= float(min_5f_2c)
-        )
+        filtered = filtered.filter(pl.col(AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) >= float(min_5f_2c))
 
     filtered = filtered.sort(NAME_COLUMN)
     total_matches = filtered.height
@@ -3148,12 +3028,10 @@
     # Text searches
     name: Optional[str] = Query(None, description="Primary Business Name"),
     domain: Optional[str] = Query(None, description="Website domain"),
-    
     # Location filters
     state: Optional[str] = Query(None, description="Main Office State (exact match)"),
     city: Optional[str] = Query(None, description="Main Office City (exact match)"),
     country: Optional[str] = Query(None, description="Main Office Country (exact match)"),
-    
     # AUM filters
     min_aum: Optional[float] = Query(None, description="Minimum AUM (greater than or equal)"),
     max_aum: Optional[float] = Query(None, description="Maximum AUM (less than or equal)"),
@@ -3161,13 +3039,11 @@
     max_discretionary_aum: Optional[float] = Query(None, description="Maximum Discretionary AUM"),
     min_non_discretionary_aum: Optional[float] = Query(None, description="Minimum Non-Discretionary AUM"),
     max_non_discretionary_aum: Optional[float] = Query(None, description="Maximum Non-Discretionary AUM"),
-    
     # Size filters
     min_total_employees: Optional[int] = Query(None, description="Minimum Total Employees"),
     max_total_employees: Optional[int] = Query(None, description="Maximum Total Employees"),
     min_total_offices: Optional[int] = Query(None, description="Minimum Total Offices"),
     max_total_offices: Optional[int] = Query(None, description="Maximum Total Offices"),
-    
     # Client type filters (Y/N Selection)
     individual_clients: Optional[str] = Query(None, description="Individual Clients (Y/N)"),
     high_net_worth_clients: Optional[str] = Query(None, description="High-Net-Worth Clients (Y/N)"),
@@ -3180,10 +3056,8 @@
     insurance_clients: Optional[str] = Query(None, description="Insurance Clients (Y/N)"),
     sovereign_wealth_clients: Optional[str] = Query(None, description="Sovereign Wealth Fund Clients (Y/N)"),
     corporate_clients: Optional[str] = Query(None, description="Corporate Clients (Y/N)"),
-    
     # Private funds filter
     private_funds: Optional[str] = Query(None, description="Private Funds (Y/N)"),
-    
     # Asset class filters (Y/N Selection)
     has_hedge_funds: Optional[str] = Query(None, description="Has Hedge Funds (Y/N)"),
     has_liquidity_funds: Optional[str] = Query(None, description="Has Liquidity Funds (Y/N)"),
@@ -3192,7 +3066,6 @@
     has_securitized_funds: Optional[str] = Query(None, description="Has Securitized Funds (Y/N)"),
     has_vc_funds: Optional[str] = Query(None, description="Has VC Funds (Y/N)"),
     has_other_funds: Optional[str] = Query(None, description="Has Other Funds (Y/N)"),
-    
     limit: int = Query(10, ge=1, le=50, description="Number of results to return"),
     offset: int = Query(0, ge=0, description="Number of results to skip (for pagination)"),
     current_user=Depends(get_current_user),
@@ -3200,22 +3073,47 @@
     """Find CRDs using intelligent fuzzy matching with comprehensive filters."""
     logger = get_logger(__name__)
     logger.info(f"RIA company lookup request: name={name}, domain={domain}, state={state}, city={city}, limit={limit}, offset={offset}")
-    
+
     # Check if at least one search parameter is provided
-    has_search_params = any([
-        name, domain, state, city, country,
-        min_aum is not None, max_aum is not None,
-        min_discretionary_aum is not None, max_discretionary_aum is not None,
-        min_non_discretionary_aum is not None, max_non_discretionary_aum is not None,
-        min_total_employees is not None, max_total_employees is not None,
-        min_total_offices is not None, max_total_offices is not None,
-        individual_clients, high_net_worth_clients, banking_clients, investment_companies_clients,
-        pension_clients, charity_clients, state_municipal_clients, investment_advisor_clients,
-        insurance_clients, sovereign_wealth_clients, corporate_clients,
-        private_funds, has_hedge_funds, has_liquidity_funds, has_pe_funds, 
-        has_real_estate_funds, has_securitized_funds, has_vc_funds, has_other_funds
-    ])
-    
+    has_search_params = any(
+        [
+            name,
+            domain,
+            state,
+            city,
+            country,
+            min_aum is not None,
+            max_aum is not None,
+            min_discretionary_aum is not None,
+            max_discretionary_aum is not None,
+            min_non_discretionary_aum is not None,
+            max_non_discretionary_aum is not None,
+            min_total_employees is not None,
+            max_total_employees is not None,
+            min_total_offices is not None,
+            max_total_offices is not None,
+            individual_clients,
+            high_net_worth_clients,
+            banking_clients,
+            investment_companies_clients,
+            pension_clients,
+            charity_clients,
+            state_municipal_clients,
+            investment_advisor_clients,
+            insurance_clients,
+            sovereign_wealth_clients,
+            corporate_clients,
+            private_funds,
+            has_hedge_funds,
+            has_liquidity_funds,
+            has_pe_funds,
+            has_real_estate_funds,
+            has_securitized_funds,
+            has_vc_funds,
+            has_other_funds,
+        ]
+    )
+
     if not has_search_params:
         raise HTTPException(status_code=400, detail="Provide at least one search parameter")
 
@@ -3226,14 +3124,30 @@
     except Exception as e:
         logger.error(f"Failed to load index data for lookup: {str(e)}", exc_info=True)
         raise
-    
+
     # Ensure all required columns exist
-    required_columns = [
-        CRD_COLUMN, NAME_COLUMN, STATE_COLUMN, CITY_COLUMN, COUNTRY_COLUMN, DOMAIN_COLUMN,
-        AUM_COLUMN, DISCRETIONARY_AUM_COLUMN, NON_DISCRETIONARY_AUM_COLUMN,
-        EMPLOYEES_COLUMN, OFFICES_COLUMN
-    ] + list(CLIENT_TYPE_COLUMNS.values()) + list(FUND_COLUMNS.values()) + list(PRIVATE_FUND_COUNT_COLUMNS.values()) + list(SMA_ALLOCATION_COLUMNS.values()) + ["Count of Private Funds - 7B(2)"] + ALL_CLIENT_TYPE_COLUMNS
-    
+    required_columns = (
+        [
+            CRD_COLUMN,
+            NAME_COLUMN,
+            STATE_COLUMN,
+            CITY_COLUMN,
+            COUNTRY_COLUMN,
+            DOMAIN_COLUMN,
+            AUM_COLUMN,
+            DISCRETIONARY_AUM_COLUMN,
+            NON_DISCRETIONARY_AUM_COLUMN,
+            EMPLOYEES_COLUMN,
+            OFFICES_COLUMN,
+        ]
+        + list(CLIENT_TYPE_COLUMNS.values())
+        + list(FUND_COLUMNS.values())
+        + list(PRIVATE_FUND_COUNT_COLUMNS.values())
+        + list(SMA_ALLOCATION_COLUMNS.values())
+        + ["Count of Private Funds - 7B(2)"]
+        + ALL_CLIENT_TYPE_COLUMNS
+    )
+
     logger.info(f"Validating {len(required_columns)} required columns")
     try:
         _ensure_columns(df, required_columns)
@@ -3241,7 +3155,7 @@
     except Exception as e:
         logger.error(f"‚ùå Column validation failed: {str(e)}", exc_info=True)
         raise
-    
+
     try:
         df_domain = _attach_domain_column(df, DOMAIN_COLUMN)
         logger.info("‚úÖ Domain column attached successfully")
@@ -3252,12 +3166,12 @@
     # Apply filters in order of selectivity (most selective first)
     filtered_df = df_domain
     logger.info(f"Starting with {filtered_df.height} rows before filtering")
-    
+
     # Track filter status for frontend
     applied_filters = {}
     ignored_filters = {}
     available_filters = []
-    
+
     # Location filters (exact match)
     if state:
         if STATE_COLUMN in df.columns:
@@ -3271,7 +3185,7 @@
             ignored_filters["state"] = f"Column '{STATE_COLUMN}' not available in dataset"
     else:
         available_filters.append("state")
-    
+
     if city:
         if CITY_COLUMN in df.columns:
             logger.info(f"Applying city filter: {city}")
@@ -3284,7 +3198,7 @@
             ignored_filters["city"] = f"Column '{CITY_COLUMN}' not available in dataset"
     else:
         available_filters.append("city")
-    
+
     if country:
         if COUNTRY_COLUMN in df.columns:
             logger.info(f"Applying country filter: {country}")
@@ -3297,14 +3211,12 @@
             ignored_filters["country"] = f"Column '{COUNTRY_COLUMN}' not available in dataset"
     else:
         available_filters.append("country")
-    
+
     # AUM filters (greater than/less than)
     if min_aum is not None:
         if AUM_COLUMN in df.columns:
             logger.info(f"Applying min AUM filter: {min_aum}")
-            filtered_df = filtered_df.filter(
-                pl.col(AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) >= min_aum
-            )
+            filtered_df = filtered_df.filter(pl.col(AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) >= min_aum)
             logger.info(f"‚úÖ Applied min AUM filter: {min_aum}, remaining rows: {filtered_df.height}")
             applied_filters["min_aum"] = min_aum
             available_filters.append("min_aum")
@@ -3313,13 +3225,11 @@
             ignored_filters["min_aum"] = f"Column '{AUM_COLUMN}' not available in dataset"
     else:
         available_filters.append("min_aum")
-    
+
     if max_aum is not None:
         if AUM_COLUMN in df.columns:
             logger.info(f"Applying max AUM filter: {max_aum}")
-            filtered_df = filtered_df.filter(
-                pl.col(AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) <= max_aum
-            )
+            filtered_df = filtered_df.filter(pl.col(AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) <= max_aum)
             logger.info(f"‚úÖ Applied max AUM filter: {max_aum}, remaining rows: {filtered_df.height}")
             applied_filters["max_aum"] = max_aum
             available_filters.append("max_aum")
@@ -3328,12 +3238,10 @@
             ignored_filters["max_aum"] = f"Column '{AUM_COLUMN}' not available in dataset"
     else:
         available_filters.append("max_aum")
-    
+
     if min_discretionary_aum is not None:
         if DISCRETIONARY_AUM_COLUMN in df.columns:
-            filtered_df = filtered_df.filter(
-                pl.col(DISCRETIONARY_AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) >= min_discretionary_aum
-            )
+            filtered_df = filtered_df.filter(pl.col(DISCRETIONARY_AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) >= min_discretionary_aum)
             logger.info(f"‚úÖ Applied min_discretionary_aum filter: {min_discretionary_aum}, remaining rows: {filtered_df.height}")
             applied_filters["min_discretionary_aum"] = min_discretionary_aum
             available_filters.append("min_discretionary_aum")
@@ -3342,12 +3250,10 @@
             ignored_filters["min_discretionary_aum"] = f"Column '{DISCRETIONARY_AUM_COLUMN}' not available in dataset"
     else:
         available_filters.append("min_discretionary_aum")
-    
+
     if max_discretionary_aum is not None:
         if DISCRETIONARY_AUM_COLUMN in df.columns:
-            filtered_df = filtered_df.filter(
-                pl.col(DISCRETIONARY_AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) <= max_discretionary_aum
-            )
+            filtered_df = filtered_df.filter(pl.col(DISCRETIONARY_AUM_COLUMN).cast(pl.Float64, strict=False).fill_null(-1.0) <= max_discretionary_aum)
             logger.info(f"‚úÖ Applied max_discretionary_aum filter: {max_discretionary_aum}, remaining rows: {filtered_df.height}")
             applied_filters["max_discretionary_aum"] = max_discretionary_aum
             available_filters.append("max_discretionary_aum")
@@ -3356,7 +3262,7 @@
             ignored_filters["max_discretionary_aum"] = f"Column '{DISCRETIONARY_AUM_COLUMN}' not available in dataset"
     else:
         available_filters.append("max_discretionary_aum")
-    
+
     if min_non_discretionary_aum is not None:
         if NON_DISCRETIONARY_AUM_COLUMN in df.columns:
             filtered_df = filtered_df.filter(
@@ -3370,7 +3276,7 @@
             ignored_filters["min_non_discretionary_aum"] = f"Column '{NON_DISCRETIONARY_AUM_COLUMN}' not available in dataset"
     else:
         available_filters.append("min_non_discretionary_aum")
-    
+
     if max_non_discretionary_aum is not None:
         if NON_DISCRETIONARY_AUM_COLUMN in df.columns:
             filtered_df = filtered_df.filter(
@@ -3384,13 +3290,11 @@
             ignored_filters["max_non_discretionary_aum"] = f"Column '{NON_DISCRETIONARY_AUM_COLUMN}' not available in dataset"
     else:
         available_filters.append("max_non_discretionary_aum")
-    
+
     # Size filters (greater than/less than)
     if min_total_employees is not None:
         if EMPLOYEES_COLUMN in df.columns:
-            filtered_df = filtered_df.filter(
-                pl.col(EMPLOYEES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) >= min_total_employees
-            )
+            filtered_df = filtered_df.filter(pl.col(EMPLOYEES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) >= min_total_employees)
             logger.info(f"‚úÖ Applied min_total_employees filter: {min_total_employees}, remaining rows: {filtered_df.height}")
             applied_filters["min_total_employees"] = min_total_employees
             available_filters.append("min_total_employees")
@@ -3399,12 +3303,10 @@
             ignored_filters["min_total_employees"] = f"Column '{EMPLOYEES_COLUMN}' not available in dataset"
     else:
         available_filters.append("min_total_employees")
-    
+
     if max_total_employees is not None:
         if EMPLOYEES_COLUMN in df.columns:
-            filtered_df = filtered_df.filter(
-                pl.col(EMPLOYEES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) <= max_total_employees
-            )
+            filtered_df = filtered_df.filter(pl.col(EMPLOYEES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) <= max_total_employees)
             logger.info(f"‚úÖ Applied max_total_employees filter: {max_total_employees}, remaining rows: {filtered_df.height}")
             applied_filters["max_total_employees"] = max_total_employees
             available_filters.append("max_total_employees")
@@ -3413,13 +3315,11 @@
             ignored_filters["max_total_employees"] = f"Column '{EMPLOYEES_COLUMN}' not available in dataset"
     else:
         available_filters.append("max_total_employees")
-    
+
     # Office filters (conditional - only if column exists)
     if min_total_offices is not None:
         if OFFICES_COLUMN in df.columns:
-            filtered_df = filtered_df.filter(
-                pl.col(OFFICES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) >= min_total_offices
-            )
+            filtered_df = filtered_df.filter(pl.col(OFFICES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) >= min_total_offices)
             logger.info(f"‚úÖ Applied min_total_offices filter: {min_total_offices}, remaining rows: {filtered_df.height}")
             applied_filters["min_total_offices"] = min_total_offices
             available_filters.append("min_total_offices")
@@ -3428,12 +3328,10 @@
             ignored_filters["min_total_offices"] = f"Column '{OFFICES_COLUMN}' not available in dataset"
     else:
         available_filters.append("min_total_offices")
-    
+
     if max_total_offices is not None:
         if OFFICES_COLUMN in df.columns:
-            filtered_df = filtered_df.filter(
-                pl.col(OFFICES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) <= max_total_offices
-            )
+            filtered_df = filtered_df.filter(pl.col(OFFICES_COLUMN).cast(pl.Int64, strict=False).fill_null(-1) <= max_total_offices)
             logger.info(f"‚úÖ Applied max_total_offices filter: {max_total_offices}, remaining rows: {filtered_df.height}")
             applied_filters["max_total_offices"] = max_total_offices
             available_filters.append("max_total_offices")
@@ -3442,7 +3340,7 @@
             ignored_filters["max_total_offices"] = f"Column '{OFFICES_COLUMN}' not available in dataset"
     else:
         available_filters.append("max_total_offices")
-    
+
     # Client type filters (Y/N Selection)
     client_type_filters = {
         "individual_clients": individual_clients,
@@ -3457,23 +3355,19 @@
         "sovereign_wealth_clients": sovereign_wealth_clients,
         "corporate_clients": corporate_clients,
     }
-    
+
     for filter_name, filter_value in client_type_filters.items():
         if filter_value is not None:
             yn_value = filter_value.strip().upper()
-            if yn_value in ['Y', 'N']:
+            if yn_value in ["Y", "N"]:
                 column_name = CLIENT_TYPE_COLUMNS[filter_name]
                 if column_name in df.columns:
                     if yn_value == "Y":
                         # Check for non-null, non-zero numeric value
-                        filtered_df = filtered_df.filter(
-                            pl.col(column_name).is_not_null() & (pl.col(column_name) > 0)
-                        )
+                        filtered_df = filtered_df.filter(pl.col(column_name).is_not_null() & (pl.col(column_name) > 0))
                     elif yn_value == "N":
                         # Check for null or zero numeric value
-                        filtered_df = filtered_df.filter(
-                            pl.col(column_name).is_null() | (pl.col(column_name) == 0)
-                        )
+                        filtered_df = filtered_df.filter(pl.col(column_name).is_null() | (pl.col(column_name) == 0))
                     logger.info(f"‚úÖ Applied {filter_name} filter: {yn_value}, remaining rows: {filtered_df.height}")
                     applied_filters[filter_name] = yn_value
                     available_filters.append(filter_name)
@@ -3482,7 +3376,7 @@
                     ignored_filters[filter_name] = f"Column '{column_name}' not available in dataset"
         else:
             available_filters.append(filter_name)
-    
+
     # Private funds and asset class filters (Y/N Selection)
     fund_filters = {
         "private_funds": private_funds,
@@ -3494,11 +3388,11 @@
         "has_vc_funds": has_vc_funds,
         "has_other_funds": has_other_funds,
     }
-    
+
     for filter_name, filter_value in fund_filters.items():
         if filter_value is not None:
             yn_value = filter_value.strip().upper()
-            if yn_value in ['Y', 'N']:
+            if yn_value in ["Y", "N"]:
                 if filter_name == "private_funds":
                     # Special handling for private funds - check both 7B(1) and 7B(2) columns
                     column_7b1 = "Count of Private Funds - 7B(1)"
@@ -3507,14 +3401,14 @@
                         if yn_value == "Y":
                             # Check if either column has a non-null, non-zero value
                             filtered_df = filtered_df.filter(
-                                (pl.col(column_7b1).is_not_null() & (pl.col(column_7b1) > 0)) |
-                                (pl.col(column_7b2).is_not_null() & (pl.col(column_7b2) > 0))
+                                (pl.col(column_7b1).is_not_null() & (pl.col(column_7b1) > 0))
+                                | (pl.col(column_7b2).is_not_null() & (pl.col(column_7b2) > 0))
                             )
                         elif yn_value == "N":
                             # Check if both columns are null or zero
                             filtered_df = filtered_df.filter(
-                                (pl.col(column_7b1).is_null() | (pl.col(column_7b1) == 0)) &
-                                (pl.col(column_7b2).is_null() | (pl.col(column_7b2) == 0))
+                                (pl.col(column_7b1).is_null() | (pl.col(column_7b1) == 0))
+                                & (pl.col(column_7b2).is_null() | (pl.col(column_7b2) == 0))
                             )
                         logger.info(f"‚úÖ Applied {filter_name} filter: {yn_value}, remaining rows: {filtered_df.height}")
                         applied_filters[filter_name] = yn_value
@@ -3531,9 +3425,7 @@
                     # Standard fund filter logic for asset class filters (Y/N strings)
                     column_name = FUND_COLUMNS[filter_name]
                     if column_name in df.columns:
-                        filtered_df = filtered_df.filter(
-                            pl.col(column_name).str.strip_chars().str.to_uppercase() == yn_value
-                        )
+                        filtered_df = filtered_df.filter(pl.col(column_name).str.strip_chars().str.to_uppercase() == yn_value)
                         logger.info(f"‚úÖ Applied {filter_name} filter: {yn_value}, remaining rows: {filtered_df.height}")
                         applied_filters[filter_name] = yn_value
                         available_filters.append(filter_name)
@@ -3542,7 +3434,7 @@
                         ignored_filters[filter_name] = f"Column '{column_name}' not available in dataset"
         else:
             available_filters.append(filter_name)
-    
+
     # Domain filter (exact match)
     if domain:
         domain_norm = _canonicalize_domain(domain)
@@ -3555,7 +3447,7 @@
             ignored_filters["domain"] = "Invalid domain format"
     else:
         available_filters.append("domain")
-    
+
     # Check if any rows remain after filtering
     if filtered_df.is_empty():
         raise HTTPException(status_code=404, detail="No matching advisors found")
@@ -3567,7 +3459,7 @@
             # Get ALL matches first (no limit), then apply pagination
             raw_matches = _intelligent_name_search(filtered_df, name, 10000)  # Large number to get all matches
             logger.info(f"‚úÖ Name search '{name}' returned {len(raw_matches)} total matches")
-            
+
             applied_filters["name"] = name
             available_filters.append("name")
             # Name search results are already sorted by similarity score (highest first)
@@ -3581,7 +3473,7 @@
     except Exception as e:
         logger.error(f"‚ùå Main lookup logic failed: {str(e)}", exc_info=True)
         raise
-    
+
     # Transform all matches to include display names and top 3 calculations
     all_transformed_matches = []
     for match in raw_matches:
@@ -3590,70 +3482,75 @@
         if "match_score" in match:
             transformed_match["match_score"] = match["match_score"]
         all_transformed_matches.append(transformed_match)
-    
+
     # Apply pagination to transformed results
     total_matches = len(all_transformed_matches)
     start_idx = offset
     end_idx = offset + limit
     matches = all_transformed_matches[start_idx:end_idx]
-    
+
     # Calculate pagination metadata
     has_more = end_idx < total_matches
     next_offset = end_idx if has_more else None
-    
+
     logger.info(f"‚úÖ Pagination: showing {len(matches)} of {total_matches} total matches (offset={offset}, limit={limit})")
-    
+
     # Remove duplicates from available_filters
     available_filters = list(set(available_filters))
-    
+
     # Store RIA search activity
     try:
         search_id = generate_search_id()
-        search_params = format_search_params_for_storage({
-            "name": name,
-            "domain": domain,
-            "state": state,
-            "city": city,
-            "country": country,
-            "min_aum": min_aum,
-            "max_aum": max_aum,
-            "min_discretionary_aum": min_discretionary_aum,
-            "max_discretionary_aum": max_discretionary_aum,
-            "min_non_discretionary_aum": min_non_discretionary_aum,
-            "max_non_discretionary_aum": max_non_discretionary_aum,
-            "min_total_employees": min_total_employees,
-            "max_total_employees": max_total_employees,
-            "min_total_offices": min_total_offices,
-            "max_total_offices": max_total_offices,
-            "individual_clients": individual_clients,
-            "high_net_worth_clients": high_net_worth_clients,
-            "banking_clients": banking_clients,
-            "investment_companies_clients": investment_companies_clients,
-            "pension_clients": pension_clients,
-            "charity_clients": charity_clients,
-            "state_municipal_clients": state_municipal_clients,
-            "investment_advisor_clients": investment_advisor_clients,
-            "insurance_clients": insurance_clients,
-            "sovereign_wealth_clients": sovereign_wealth_clients,
-            "corporate_clients": corporate_clients,
-            "private_funds": private_funds,
-            "has_hedge_funds": has_hedge_funds,
-            "has_liquidity_funds": has_liquidity_funds,
-            "has_pe_funds": has_pe_funds,
-            "has_real_estate_funds": has_real_estate_funds,
-            "has_securitized_funds": has_securitized_funds,
-            "has_vc_funds": has_vc_funds,
-            "has_other_funds": has_other_funds,
-            "limit": limit,
-            "offset": offset
-        })
-        
+        search_params = format_search_params_for_storage(
+            {
+                "name": name,
+                "domain": domain,
+                "state": state,
+                "city": city,
+                "country": country,
+                "min_aum": min_aum,
+                "max_aum": max_aum,
+                "min_discretionary_aum": min_discretionary_aum,
+                "max_discretionary_aum": max_discretionary_aum,
+                "min_non_discretionary_aum": min_non_discretionary_aum,
+                "max_non_discretionary_aum": max_non_discretionary_aum,
+                "min_total_employees": min_total_employees,
+                "max_total_employees": max_total_employees,
+                "min_total_offices": min_total_offices,
+                "max_total_offices": max_total_offices,
+                "individual_clients": individual_clients,
+                "high_net_worth_clients": high_net_worth_clients,
+                "banking_clients": banking_clients,
+                "investment_companies_clients": investment_companies_clients,
+                "pension_clients": pension_clients,
+                "charity_clients": charity_clients,
+                "state_municipal_clients": state_municipal_clients,
+                "investment_advisor_clients": investment_advisor_clients,
+                "insurance_clients": insurance_clients,
+                "sovereign_wealth_clients": sovereign_wealth_clients,
+                "corporate_clients": corporate_clients,
+                "private_funds": private_funds,
+                "has_hedge_funds": has_hedge_funds,
+                "has_liquidity_funds": has_liquidity_funds,
+                "has_pe_funds": has_pe_funds,
+                "has_real_estate_funds": has_real_estate_funds,
+                "has_securitized_funds": has_securitized_funds,
+                "has_vc_funds": has_vc_funds,
+                "has_other_funds": has_other_funds,
+                "limit": limit,
+                "offset": offset,
+            }
+        )
+
         # Store search activity
         try:
             from app.utils.global_db import GlobalDB
             import json as json_module
+
             db = await GlobalDB.get_instance()
-            logger.info(f"üìù RIA tracking params: user_id={current_user.user_id}, session_id={get_session_id_from_request(request)}, search_id={search_id}, search_params={type(search_params).__name__}: {search_params}")
+            logger.info(
+                f"üìù RIA tracking params: user_id={current_user.user_id}, session_id={get_session_id_from_request(request)}, search_id={search_id}, search_params={type(search_params).__name__}: {search_params}"
+            )
             await db.store_ria_activity(
                 user_id=current_user.user_id,
                 session_id=get_session_id_from_request(request),
@@ -3661,7 +3558,7 @@
                 search_id=search_id,
                 search_params=json_module.dumps(search_params),
                 search_timestamp=datetime.now(),
-                ip_address=request.client.host
+                ip_address=request.client.host,
             )
             logger.info(f"‚úÖ Stored RIA search activity: {search_id}")
         except Exception as e:
@@ -3669,7 +3566,7 @@
             # Don't fail the request if tracking fails
     except Exception as e:
         logger.error(f"‚ùå Failed to prepare RIA search parameters: {e}")
-    
+
     return {
         "total_matches": total_matches,
         "matches": matches,
@@ -3678,13 +3575,9 @@
             "limit": limit,
             "has_more": has_more,
             "next_offset": next_offset,
-            "showing": f"{len(matches)} of {total_matches}"
+            "showing": f"{len(matches)} of {total_matches}",
         },
-        "filter_status": {
-            "applied_filters": applied_filters,
-            "ignored_filters": ignored_filters,
-            "available_filters": available_filters
-        }
+        "filter_status": {"applied_filters": applied_filters, "ignored_filters": ignored_filters, "available_filters": available_filters},
     }
 
 
@@ -3693,14 +3586,14 @@
     """Return the index row, trend JSON and PDF metadata for a specific CRD."""
     logger = get_logger(__name__)
     logger.info(f"RIA company detail request for CRD: {crd}")
-    
+
     try:
         df = _INDEX_CACHE.load()
         logger.info(f"Loaded index data for CRD {crd}: {df.height} rows")
     except Exception as e:
         logger.error(f"Failed to load index data for CRD {crd}: {str(e)}", exc_info=True)
         raise
-        
+
     _ensure_columns(df, [CRD_COLUMN])
     # Cast CRD_COLUMN to int for comparison (column is string, crd parameter is int)
     company_df = df.filter(pl.col(CRD_COLUMN).cast(pl.Int64, strict=False) == crd)
@@ -3715,38 +3608,38 @@
     except Exception as e:
         logger.error(f"Failed to load trends for CRD {crd}: {str(e)}", exc_info=True)
         raise
-        
+
     try:
         pdfs = _pdf_records_for_crd(crd)
         logger.info(f"Loaded {len(pdfs)} PDF records for CRD {crd}")
-        
+
         # Debug: Log the structure of PDF records before adding download URLs
         logger.info(f"üîç PDF records structure before adding download URLs:")
         for i, pdf_record in enumerate(pdfs):
-            logger.info(f"  PDF {i+1}: {pdf_record}")
-        
+            logger.info(f"  PDF {i + 1}: {pdf_record}")
+
         # Add download URLs to each PDF record
         # Force HTTPS for download URLs to prevent mixed content issues
         base_url = f"https://{request.url.netloc}"
         logger.info(f"üîç Base URL for download URLs: {base_url}")
-        
+
         for i, pdf_record in enumerate(pdfs):
             # Extract filename from rel_path (always available in latest.csv)
-            if 'rel_path' in pdf_record and pdf_record['rel_path']:
+            if "rel_path" in pdf_record and pdf_record["rel_path"]:
                 # Extract filename from rel_path (e.g., "raw/pdfs/crd=79/year=2025/month=03/79_409063_1_20250313.pdf" -> "79_409063_1_20250313.pdf")
-                filename = pdf_record['rel_path'].split('/')[-1]
+                filename = pdf_record["rel_path"].split("/")[-1]
                 download_url = f"{base_url}/api/prospecting/ria/company/{crd}/pdf/{filename}"
-                pdf_record['download_url'] = download_url
-                logger.info(f"üîç PDF {i+1} - Filename: {filename}, Download URL: {download_url}")
+                pdf_record["download_url"] = download_url
+                logger.info(f"üîç PDF {i + 1} - Filename: {filename}, Download URL: {download_url}")
             else:
-                pdf_record['download_url'] = None
-                logger.warning(f"‚ö†Ô∏è PDF {i+1} - No rel_path found: {pdf_record}")
-        
+                pdf_record["download_url"] = None
+                logger.warning(f"‚ö†Ô∏è PDF {i + 1} - No rel_path found: {pdf_record}")
+
         # Debug: Log the final structure of PDF records with download URLs
         logger.info(f"üîç PDF records structure after adding download URLs:")
         for i, pdf_record in enumerate(pdfs):
-            logger.info(f"  PDF {i+1}: {pdf_record}")
-                
+            logger.info(f"  PDF {i + 1}: {pdf_record}")
+
     except Exception as e:
         logger.error(f"Failed to load PDF records for CRD {crd}: {str(e)}", exc_info=True)
         raise
@@ -3758,23 +3651,24 @@
         search_id = search_context.get("searchId") if search_context else None
         search_params = search_context.get("searchParams") if search_context else None
         search_timestamp = search_context.get("timestamp") if search_context else None
-        
+
         # Get company name from index row
         company_name = index_row.get("Primary Business Name", f"CRD {crd}")
-        
+
         # Store company view activity
         from app.utils.global_db import GlobalDB
         import json as json_module
         from datetime import datetime
+
         db = await GlobalDB.get_instance()
-        
+
         # Parse search_timestamp if it's a string
         parsed_timestamp = None
         if search_timestamp:
             if isinstance(search_timestamp, str):
                 try:
                     # Parse ISO format timestamp string and convert to timezone-naive
-                    if search_timestamp.endswith('Z'):
+                    if search_timestamp.endswith("Z"):
                         # Remove Z and parse as UTC, then convert to naive
                         dt_str = search_timestamp[:-1]
                         parsed_timestamp = datetime.fromisoformat(dt_str)
@@ -3789,7 +3683,7 @@
                     parsed_timestamp = search_timestamp.replace(tzinfo=None)
                 else:
                     parsed_timestamp = search_timestamp
-        
+
         await db.store_ria_activity(
             user_id=current_user.user_id,
             session_id=get_session_id_from_request(request),
@@ -3800,7 +3694,7 @@
             company_crd=str(crd),
             company_name=company_name,
             company_action="view_details",
-            ip_address=request.client.host
+            ip_address=request.client.host,
         )
         logger.info(f"‚úÖ Stored RIA company view activity: CRD {crd}")
     except Exception as e:
@@ -3816,104 +3710,95 @@
 
 
 @router.get("/ria/company/{crd}/pdf/{pdf_filename}")
-def ria_company_pdf_download(
-    crd: int, 
-    pdf_filename: str
-):
+def ria_company_pdf_download(crd: int, pdf_filename: str):
     """Download the actual PDF file for a specific CRD and filename."""
     logger = get_logger(__name__)
     logger.info(f"PDF download request for CRD {crd}, filename: {pdf_filename}")
-    
+
     try:
         # Get PDF records to find the correct S3 path
         pdfs = _pdf_records_for_crd(crd)
         logger.info(f"Found {len(pdfs)} PDF records for CRD {crd}")
-        
+
         # Find the PDF record that matches the requested filename
         target_pdf = None
         for pdf_record in pdfs:
             # Extract filename from rel_path (always available in latest.csv)
-            if pdf_record.get('rel_path'):
-                record_filename = pdf_record['rel_path'].split('/')[-1]
+            if pdf_record.get("rel_path"):
+                record_filename = pdf_record["rel_path"].split("/")[-1]
                 logger.info(f"Checking PDF record: {record_filename} vs requested: {pdf_filename}")
                 if record_filename == pdf_filename:
                     target_pdf = pdf_record
                     logger.info(f"‚úÖ Found matching PDF record: {pdf_record}")
                     break
-        
+
         if not target_pdf:
             logger.warning(f"‚ùå PDF not found in records: CRD {crd}, filename {pdf_filename}")
             # Show available filenames (extracted from rel_path)
             available_filenames = []
             for p in pdfs:
-                if p.get('rel_path'):
-                    filename = p['rel_path'].split('/')[-1]
+                if p.get("rel_path"):
+                    filename = p["rel_path"].split("/")[-1]
                     available_filenames.append(filename)
             logger.info(f"Available PDFs: {available_filenames}")
             raise HTTPException(status_code=404, detail=f"PDF not found: {pdf_filename}")
-        
+
         # Use the rel_path from latest.csv as the S3 key, but prepend the output directory
-        rel_path = target_pdf.get('rel_path')
+        rel_path = target_pdf.get("rel_path")
         if not rel_path:
             logger.error(f"‚ùå No rel_path found for PDF: CRD {crd}, filename {pdf_filename}")
             logger.info(f"PDF record: {target_pdf}")
             raise HTTPException(status_code=500, detail="PDF path information missing")
-        
+
         # Construct the full S3 path by prepending the output directory
         from app.utils.config import get_s3_prefix
+
         s3_prefix = get_s3_prefix()  # This should return "output/sec_ingestion"
         s3_path = f"{s3_prefix}/{rel_path}"
-        
+
         logger.info(f"üîç Using S3 path: {s3_path}")
-        
+
         # Download PDF from S3 using the correct path
-        s3_client = boto3.client('s3')
+        s3_client = boto3.client("s3")
         bucket = get_s3_bucket()
-        
+
         logger.info(f"üîç S3 Configuration - Bucket: {bucket}, Key: {s3_path}")
         logger.info(f"üîç Attempting to download PDF from S3...")
-        
+
         try:
             response = s3_client.get_object(Bucket=bucket, Key=s3_path)
-            pdf_content = response['Body'].read()
-            
+            pdf_content = response["Body"].read()
+
             # Get content type and metadata
-            content_type = response.get('ContentType', 'application/pdf')
-            content_length = response.get('ContentLength', len(pdf_content))
-            
+            content_type = response.get("ContentType", "application/pdf")
+            content_length = response.get("ContentLength", len(pdf_content))
+
             logger.info(f"‚úÖ Successfully downloaded PDF from S3: {content_length} bytes")
-            
+
         except Exception as s3_error:
             logger.error(f"‚ùå S3 download failed: {str(s3_error)}", exc_info=True)
             if "NoSuchKey" in str(s3_error):
                 logger.warning(f"‚ö†Ô∏è PDF file does not exist in S3: {s3_path}")
                 logger.info(f"üîç This suggests the PDF was referenced in latest.csv but never uploaded to S3")
                 raise HTTPException(
-                    status_code=404, 
-                    detail=f"PDF file not found in storage: {pdf_filename}. The file may not have been uploaded during ingestion."
+                    status_code=404, detail=f"PDF file not found in storage: {pdf_filename}. The file may not have been uploaded during ingestion."
                 )
             else:
-                raise HTTPException(
-                    status_code=503, 
-                    detail=f"Failed to download PDF: {str(s3_error)}"
-                )
-        
+                raise HTTPException(status_code=503, detail=f"Failed to download PDF: {str(s3_error)}")
+
         # Return PDF as streaming response
         from fastapi.responses import StreamingResponse
         import io
-        
+
         return StreamingResponse(
             io.BytesIO(pdf_content),
             media_type=content_type,
-            headers={
-                "Content-Disposition": f"attachment; filename={pdf_filename}",
-                "Content-Length": str(content_length)
-            }
+            headers={"Content-Disposition": f"attachment; filename={pdf_filename}", "Content-Length": str(content_length)},
         )
-        
+
     except ClientError as e:
-        error_code = e.response['Error']['Code']
-        if error_code == 'NoSuchKey':
+        error_code = e.response["Error"]["Code"]
+        if error_code == "NoSuchKey":
             logger.warning(f"PDF not found in S3: CRD {crd}, filename {pdf_filename}")
             raise HTTPException(status_code=404, detail=f"PDF not found: {pdf_filename}")
         else:
@@ -3929,19 +3814,15 @@
     """List all available PDFs for a specific CRD without downloading them."""
     logger = get_logger(__name__)
     logger.info(f"PDF list request for CRD: {crd}")
-    
+
     try:
         # Get PDF records (metadata only)
         pdfs = _pdf_records_for_crd(crd)
         logger.info(f"Found {len(pdfs)} PDF records for CRD {crd}")
-        
+
         # Return just the metadata without download URLs
-        return {
-            "crd": crd,
-            "pdf_count": len(pdfs),
-            "pdfs": pdfs
-        }
-        
+        return {"crd": crd, "pdf_count": len(pdfs), "pdfs": pdfs}
+
     except Exception as e:
         logger.error(f"Failed to list PDFs for CRD {crd}: {str(e)}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"Failed to list PDFs: {str(e)}")
@@ -3949,7 +3830,7 @@
 
 class _LocationMetadataCache:
     """Cached location metadata (countries and cities) with 1-hour refresh cycle."""
-    
+
     def __init__(self) -> None:
         self._countries_cache: Optional[List[str]] = None
         self._cities_cache: Optional[List[str]] = None
@@ -3957,45 +3838,40 @@
         self._cache_duration = timedelta(hours=1)  # 1-hour cache
         logger = get_logger(__name__)
         logger.info("LocationMetadataCache initialized with 1-hour refresh cycle")
-    
+
     def get_location_data(self) -> Dict[str, List[str]]:
         """Get both countries and cities, refreshing cache if needed."""
         if self._needs_refresh():
             self._refresh_cache()
-        
-        return {
-            "countries": self._countries_cache or [],
-            "cities": self._cities_cache or []
-        }
-    
+
+        return {"countries": self._countries_cache or [], "cities": self._cities_cache or []}
+
     def _needs_refresh(self) -> bool:
         """Check if cache needs refresh based on time and data availability."""
         if self._countries_cache is None or self._cities_cache is None or self._cache_time is None:
             return True
-        
+
         return datetime.now() - self._cache_time > self._cache_duration
-    
+
     def _refresh_cache(self) -> None:
         """Extract countries and cities from index data and cache them."""
         logger = get_logger(__name__)
         logger.info("Refreshing location metadata cache")
-        
+
         try:
             df = _INDEX_CACHE.load()
-            
+
             # Extract and cache countries
             countries_df = df.select(COUNTRY_COLUMN).unique().sort(COUNTRY_COLUMN)
-            self._countries_cache = [row[COUNTRY_COLUMN] for row in countries_df.to_dicts() 
-                                   if row[COUNTRY_COLUMN]]
-            
+            self._countries_cache = [row[COUNTRY_COLUMN] for row in countries_df.to_dicts() if row[COUNTRY_COLUMN]]
+
             # Extract and cache cities
             cities_df = df.select(CITY_COLUMN).unique().sort(CITY_COLUMN)
-            self._cities_cache = [row[CITY_COLUMN] for row in cities_df.to_dicts() 
-                                 if row[CITY_COLUMN]]
-            
+            self._cities_cache = [row[CITY_COLUMN] for row in cities_df.to_dicts() if row[CITY_COLUMN]]
+
             self._cache_time = datetime.now()
             logger.info(f"Location cache refreshed: {len(self._countries_cache)} countries, {len(self._cities_cache)} cities")
-            
+
         except Exception as e:
             logger.error(f"Failed to refresh location cache: {str(e)}", exc_info=True)
             raise
@@ -4010,7 +3886,7 @@
     """Get all distinct countries for dropdown."""
     logger = get_logger(__name__)
     logger.info("RIA countries metadata request")
-    
+
     try:
         location_data = _LOCATION_CACHE.get_location_data()
         logger.info(f"Returning {len(location_data['countries'])} distinct countries")
@@ -4025,7 +3901,7 @@
     """Get all distinct cities for dropdown."""
     logger = get_logger(__name__)
     logger.info("RIA cities metadata request")
-    
+
     try:
         location_data = _LOCATION_CACHE.get_location_data()
         logger.info(f"Returning {len(location_data['cities'])} distinct cities")
@@ -4040,7 +3916,7 @@
     """Get all distinct countries and cities for dropdowns in a single call."""
     logger = get_logger(__name__)
     logger.info("RIA location metadata request")
-    
+
     try:
         location_data = _LOCATION_CACHE.get_location_data()
         logger.info(f"Returning {len(location_data['countries'])} countries and {len(location_data['cities'])} cities")
@@ -4054,39 +3930,35 @@
 def ria_debug_s3(current_user=Depends(get_current_user)):
     """Debug endpoint to test S3 access and configuration."""
     logger = get_logger(__name__)
-    
+
     try:
         # Test configuration
         bucket = get_s3_bucket()
         prefix = get_s3_prefix()
-        
+
         # Test S3 client
-        s3_client = boto3.client('s3')
-        
+        s3_client = boto3.client("s3")
+
         # Test listing index folders
         year_prefix = f"{prefix}/processed/index/"
-        response = s3_client.list_objects_v2(
-            Bucket=bucket,
-            Prefix=year_prefix,
-            Delimiter='/'
-        )
-        
+        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=year_prefix, Delimiter="/")
+
         year_folders = []
-        if 'CommonPrefixes' in response:
-            for prefix_obj in response['CommonPrefixes']:
-                folder_name = prefix_obj['Prefix'].split('/')[-2]
+        if "CommonPrefixes" in response:
+            for prefix_obj in response["CommonPrefixes"]:
+                folder_name = prefix_obj["Prefix"].split("/")[-2]
                 year_folders.append(folder_name)
-        
+
         # Test latest.csv access
         latest_path = f"{prefix}/processed/adv/latest.csv"
         try:
             latest_response = s3_client.head_object(Bucket=bucket, Key=latest_path)
             latest_exists = True
-            latest_size = latest_response['ContentLength']
+            latest_size = latest_response["ContentLength"]
         except ClientError:
             latest_exists = False
             latest_size = 0
-        
+
         # Test data loading and column types
         data_info = {}
         try:
@@ -4096,11 +3968,11 @@
                 "rows": len(index_df),
                 "columns": len(index_df.columns),
                 "column_types": {col: str(dtype) for col, dtype in zip(index_df.columns, index_df.dtypes)},
-                "sample_columns": list(index_df.columns[:10])  # First 10 columns
+                "sample_columns": list(index_df.columns[:10]),  # First 10 columns
             }
         except Exception as e:
             data_info["index_data"] = {"error": str(e)}
-        
+
         try:
             # Test latest PDFs cache
             pdfs_df = _LATEST_PDFS_CACHE.load()
@@ -4108,52 +3980,45 @@
                 "rows": len(pdfs_df),
                 "columns": len(pdfs_df.columns),
                 "column_types": {col: str(dtype) for col, dtype in zip(pdfs_df.columns, pdfs_df.dtypes)},
-                "sample_columns": list(pdfs_df.columns[:10])  # First 10 columns
+                "sample_columns": list(pdfs_df.columns[:10]),  # First 10 columns
             }
         except Exception as e:
             data_info["latest_pdfs"] = {"error": str(e)}
-        
+
         # Check which COMPANY_PREVIEW_MAPPING fields are actually available
         mapping_analysis = {}
         if "index_data" in data_info and "error" not in data_info["index_data"]:
             available_columns = data_info["index_data"]["column_types"].keys()
             available_preview_mappings = {}
             missing_preview_mappings = []
-            
+
             for original_field, display_field in COMPANY_PREVIEW_MAPPING.items():
                 if original_field in available_columns:
                     available_preview_mappings[original_field] = display_field
                 else:
                     missing_preview_mappings.append(original_field)
-            
+
             mapping_analysis = {
                 "preview_mapping": {
                     "total_mappings": len(COMPANY_PREVIEW_MAPPING),
                     "available_mappings": len(available_preview_mappings),
                     "missing_mappings": len(missing_preview_mappings),
                     "available_fields": available_preview_mappings,
-                    "missing_fields": missing_preview_mappings
+                    "missing_fields": missing_preview_mappings,
                 },
-                "full_mapping": {
-                    "total_mappings": len(DISPLAY_NAME_MAPPING),
-                    "note": "Full mapping available for detailed company view"
-                }
+                "full_mapping": {"total_mappings": len(DISPLAY_NAME_MAPPING), "note": "Full mapping available for detailed company view"},
             }
-        
+
         return {
             "bucket": bucket,
             "prefix": prefix,
             "year_folders": year_folders,
-            "latest_csv": {
-                "path": latest_path,
-                "exists": latest_exists,
-                "size": latest_size
-            },
+            "latest_csv": {"path": latest_path, "exists": latest_exists, "size": latest_size},
             "s3_response_keys": list(response.keys()),
             "data_loading": data_info,
-            "mapping_analysis": mapping_analysis
+            "mapping_analysis": mapping_analysis,
         }
-        
+
     except Exception as e:
         logger.error(f"S3 debug failed: {str(e)}", exc_info=True)
         return {"error": str(e), "type": type(e).__name__}
@@ -4169,7 +4034,7 @@
     limit: int = Query(50, ge=1, le=200, description="Number of records to return"),
     offset: int = Query(0, ge=0, description="Number of records to skip"),
     current_user=Depends(get_current_user),
-    db=Depends(get_db)
+    db=Depends(get_db),
 ):
     """
     Admin endpoint to retrieve RIA activity history for all users.
@@ -4177,23 +4042,17 @@
     """
     logger = get_logger(__name__)
     logger.info(f"Admin RIA activity request: user_id={user_id}, search_id={search_id}, activity_type={activity_type}")
-    
+
     # TODO: Add admin role check here
     # if not current_user.is_admin:
     #     raise HTTPException(status_code=403, detail="Admin access required")
-    
+
     try:
         # Get RIA activity from database
         activity_data = await db.get_ria_activity(
-            user_id=user_id,
-            search_id=search_id,
-            activity_type=activity_type,
-            from_date=from_date,
-            to_date=to_date,
-            limit=limit,
-            offset=offset
+            user_id=user_id, search_id=search_id, activity_type=activity_type, from_date=from_date, to_date=to_date, limit=limit, offset=offset
         )
-        
+
         # Format the response for admin display
         formatted_records = []
         for record in activity_data["records"]:
@@ -4209,29 +4068,23 @@
                 "company_name": record["company_name"],
                 "company_action": record["company_action"],
                 "ip_address": record["ip_address"],
-                "created_at": record["created_at"].isoformat() if record["created_at"] else None
+                "created_at": record["created_at"].isoformat() if record["created_at"] else None,
             }
             formatted_records.append(formatted_record)
-        
+
         logger.info(f"‚úÖ Retrieved {len(formatted_records)} RIA activity records")
-        
+
         return {
             "records": formatted_records,
             "pagination": {
                 "total_count": activity_data["total_count"],
                 "limit": activity_data["limit"],
                 "offset": activity_data["offset"],
-                "has_more": (activity_data["offset"] + activity_data["limit"]) < activity_data["total_count"]
+                "has_more": (activity_data["offset"] + activity_data["limit"]) < activity_data["total_count"],
             },
-            "filters": {
-                "user_id": user_id,
-                "search_id": search_id,
-                "activity_type": activity_type,
-                "from_date": from_date,
-                "to_date": to_date
-            }
+            "filters": {"user_id": user_id, "search_id": search_id, "activity_type": activity_type, "from_date": from_date, "to_date": to_date},
         }
-        
+
     except Exception as e:
         logger.error(f"‚ùå Failed to retrieve RIA activity: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"Failed to retrieve RIA activity: {str(e)}")
@@ -4242,112 +4095,115 @@
     from_date: Optional[str] = Query(None, description="Filter from date (ISO format)"),
     to_date: Optional[str] = Query(None, description="Filter to date (ISO format)"),
     current_user=Depends(get_current_user),
-    db=Depends(get_db)
+    db=Depends(get_db),
 ):
     """
     Admin endpoint to get summary statistics of RIA activity.
     """
     logger = get_logger(__name__)
     logger.info(f"Admin RIA activity summary request: from_date={from_date}, to_date={to_date}")
-    
+
     # TODO: Add admin role check here
     # if not current_user.is_admin:
     #     raise HTTPException(status_code=403, detail="Admin access required")
-    
+
     try:
         # Get summary data from database
         if not db.pool:
             await db.init_pool()
-        
+
         async with db.pool.acquire() as conn:
             # Build WHERE clause for date filtering
             where_conditions = []
             params = []
             param_count = 0
-            
+
             if from_date:
                 param_count += 1
                 where_conditions.append(f"created_at >= ${param_count}")
                 params.append(from_date)
-                
+
             if to_date:
                 param_count += 1
                 where_conditions.append(f"created_at <= ${param_count}")
                 params.append(to_date)
-            
+
             where_clause = " AND ".join(where_conditions) if where_conditions else "TRUE"
-            
+
             # Get activity type counts
-            activity_counts = await conn.fetch(f"""
+            activity_counts = await conn.fetch(
+                f"""
                 SELECT activity_type, COUNT(*) as count
                 FROM ria_user_activity
                 WHERE {where_clause}
                 GROUP BY activity_type
                 ORDER BY count DESC
-            """, *params)
-            
+            """,
+                *params,
+            )
+
             # Get unique users count
-            unique_users = await conn.fetchrow(f"""
+            unique_users = await conn.fetchrow(
+                f"""
                 SELECT COUNT(DISTINCT user_id) as unique_users
                 FROM ria_user_activity
                 WHERE {where_clause}
-            """, *params)
-            
+            """,
+                *params,
+            )
+
             # Get unique searches count
-            unique_searches = await conn.fetchrow(f"""
+            unique_searches = await conn.fetchrow(
+                f"""
                 SELECT COUNT(DISTINCT search_id) as unique_searches
                 FROM ria_user_activity
                 WHERE {where_clause} AND search_id IS NOT NULL
-            """, *params)
-            
+            """,
+                *params,
+            )
+
             # Get top search parameters
-            top_search_params = await conn.fetch(f"""
+            top_search_params = await conn.fetch(
+                f"""
                 SELECT search_params, COUNT(*) as count
                 FROM ria_user_activity
                 WHERE {where_clause} AND activity_type = 'search' AND search_params IS NOT NULL
                 GROUP BY search_params
                 ORDER BY count DESC
                 LIMIT 10
-            """, *params)
-            
+            """,
+                *params,
+            )
+
             # Get top companies viewed
-            top_companies = await conn.fetch(f"""
+            top_companies = await conn.fetch(
+                f"""
                 SELECT company_name, company_crd, COUNT(*) as view_count
                 FROM ria_user_activity
                 WHERE {where_clause} AND activity_type = 'company_view' AND company_name IS NOT NULL
                 GROUP BY company_name, company_crd
                 ORDER BY view_count DESC
                 LIMIT 10
-            """, *params)
-        
+            """,
+                *params,
+            )
+
         logger.info(f"‚úÖ Generated RIA activity summary")
-        
+
         return {
             "summary": {
                 "total_activities": sum(row["count"] for row in activity_counts),
                 "unique_users": unique_users["unique_users"],
                 "unique_searches": unique_searches["unique_searches"],
-                "activity_type_breakdown": {row["activity_type"]: row["count"] for row in activity_counts}
+                "activity_type_breakdown": {row["activity_type"]: row["count"] for row in activity_counts},
             },
-            "top_search_parameters": [
-                {
-                    "search_params": row["search_params"],
-                    "count": row["count"]
-                } for row in top_search_params
-            ],
+            "top_search_parameters": [{"search_params": row["search_params"], "count": row["count"]} for row in top_search_params],
             "top_companies_viewed": [
-                {
-                    "company_name": row["company_name"],
-                    "company_crd": row["company_crd"],
-                    "view_count": row["view_count"]
-                } for row in top_companies
+                {"company_name": row["company_name"], "company_crd": row["company_crd"], "view_count": row["view_count"]} for row in top_companies
             ],
-            "date_range": {
-                "from_date": from_date,
-                "to_date": to_date
-            }
+            "date_range": {"from_date": from_date, "to_date": to_date},
         }
-        
+
     except Exception as e:
         logger.error(f"‚ùå Failed to generate RIA activity summary: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"Failed to generate RIA activity summary: {str(e)}")

--- app/routers/tamradar.py
+++ app/routers/tamradar.py
@@ -51,9 +51,10 @@
                 return bool(tier1_values[0])
             # If values differ, use the most common one
             from collections import Counter
+
             most_common = Counter(tier1_values).most_common(1)[0][0]
             return bool(most_common)
-    
+
     # Fallback to top-level value
     tier1_value = radar_dict.get("tier1_email_alerts")
     return bool(tier1_value) if tier1_value is not None else False
@@ -62,12 +63,12 @@
 def extract_radar_ids_from_user_radars(user_radars: List[Dict[str, Any]]) -> List[str]:
     """
     Extract all radar_ids from user_radars list.
-    
+
     Handles both grouped format (with radar_ids as a list) and ungrouped format (with radar_id as a string).
-    
+
     Args:
         user_radars: List of radar dictionaries from get_user_radars
-        
+
     Returns:
         List of radar_id strings
     """
@@ -92,17 +93,17 @@
 ):
     """
     Receive webhook events from Lambda fan-out (internal routing).
-    
+
     This endpoint is called by the Lambda function that fans out webhooks
     from TAMradar to all backend environments (dev, stage, prod, local).
-    
+
     Security:
     - Validates internal webhook secret token (INTERNAL_WEBHOOK_SECRET)
     - Validates payload structure using Pydantic models
     - Verifies radar_id exists in database
     - Checks idempotency (event_id)
     - Rate limited: 100 requests per minute per IP
-    
+
     The payload is automatically validated by FastAPI using TAMradarWebhookPayload model.
     """
     # Log incoming webhook request for debugging
@@ -114,9 +115,9 @@
             "query_params": dict(request.query_params),
             "has_secret_header": "X-Internal-Webhook-Secret" in request.headers,
             "has_secret_query": "secret" in request.query_params,
-        }
+        },
     )
-    
+
     # 1. Verify internal secret token
     if not tamradar_webhook_service.verify_internal_webhook_secret(request):
         logger.warning(
@@ -125,8 +126,8 @@
                 "ip": request.client.host if request.client else "unknown",
                 "has_secret_header": "X-Internal-Webhook-Secret" in request.headers,
                 "has_secret_query": "secret" in request.query_params,
-                "headers": {k: v for k, v in request.headers.items() if k.lower() not in ['authorization', 'cookie']},
-            }
+                "headers": {k: v for k, v in request.headers.items() if k.lower() not in ["authorization", "cookie"]},
+            },
         )
         raise HTTPException(status_code=403, detail="Invalid internal webhook secret")
 
@@ -154,6 +155,7 @@
     # Pass webhook_event_id to avoid double storage
     # Use payload_dict (not Pydantic model) for background processing
     from app.tasks.tamradar_tasks import process_webhook_event_task
+
     process_webhook_event_task.delay(payload_dict, webhook_event_id)
 
     logger.info(f"Internal webhook received: {event_id}, radar: {radar_id}, type: {payload.event_type}")
@@ -164,6 +166,7 @@
 # User Endpoints (Authenticated)
 # ============================================================================
 
+
 @router.post("/radars/companies")
 async def create_company_radar(
     request: CompanyRadarRequest,
@@ -175,7 +178,7 @@
     - company_social_posts
     - new_hires
     - company_mentions
-    
+
     Returns list of all created/linked radars.
     """
     logger.info(
@@ -190,18 +193,15 @@
             "tier1_email_alerts": request.tier1_email_alerts,
             "has_filters": bool(request.filters),
             "has_custom_fields": bool(request.custom_fields),
-        }
+        },
     )
     try:
         # Validate domain before creating
-        validation_result = await tamradar_service.validate_domain_before_creation(
-            request.domain,
-            user_id=current_user.user_id
-        )
-        
+        validation_result = await tamradar_service.validate_domain_before_creation(request.domain, user_id=current_user.user_id)
+
         # Create radars (validation warnings don't block creation)
         result = await tamradar_service.create_company_radar(current_user.user_id, request)
-        
+
         # Handle both old format (list) and new format (dict with recent_findings)
         if isinstance(result, dict):
             radars = result["radars"]
@@ -210,7 +210,7 @@
             # Backward compatibility - old format returns list directly
             radars = result
             recent_findings_by_radar = {}
-        
+
         logger.info(
             "Company radar created successfully",
             extra={
@@ -221,21 +221,22 @@
                 "radar_types": [r.radar_type for r in radars],
                 "validation_warnings": len(validation_result.get("warnings", [])),
                 "radars_with_recent_findings": len(recent_findings_by_radar),
-            }
+            },
         )
-        
+
         # Check balance after creating all radars (only once)
         try:
             from app.services.tamradar_balance_monitor import tamradar_balance_monitor
+
             await tamradar_balance_monitor.check_balance_and_alert()
         except Exception as e:
             logger.warning(f"Failed to check balance after radar creation: {e}")
-        
+
         # Determine user message based on radar statuses
         # TAMradar always returns "active" when creating a radar
         # Failures are detected later via webhook (24-96 hours)
         message = "Radar created successfully"
-        
+
         # Build response data with recent_findings attached to each radar
         response_data = []
         for radar in radars:
@@ -244,7 +245,7 @@
             if radar.radar_id in recent_findings_by_radar:
                 radar_dict["recent_findings"] = recent_findings_by_radar[radar.radar_id]
             response_data.append(radar_dict)
-        
+
         return {
             "status": "success",
             "message": message,
@@ -253,7 +254,7 @@
             "validation": {
                 "warnings": validation_result.get("warnings", []),
                 "errors": validation_result.get("errors", []),
-            }
+            },
         }
     except TAMradarServiceError as e:
         logger.error(f"Failed to create company radars: {e}")
@@ -275,11 +276,11 @@
             "contact_email": request.contact_email,
             "contact_name": request.contact_name,
             "watchlist_category": request.watchlist_category,
-        }
+        },
     )
     try:
         result = await tamradar_service.create_contact_radar(current_user.user_id, request)
-        
+
         # Handle both dict (with recent_findings) and TAMradarRadarResponse
         if isinstance(result, dict):
             radar = result["radar"]
@@ -287,7 +288,7 @@
         else:
             radar = result
             recent_findings = []
-        
+
         logger.info(
             "Contact radar created successfully",
             extra={
@@ -296,17 +297,17 @@
                 "domain": request.domain,
                 "radar_type": radar.radar_type,
                 "recent_findings_count": len(recent_findings),
-            }
+            },
         )
-        
+
         # TAMradar always returns "active" when creating a radar
         # Failures are detected later via webhook (24-96 hours)
         message = "Radar created successfully"
-        
+
         radar_dict = radar.model_dump()
         if recent_findings:
             radar_dict["recent_findings"] = recent_findings
-        
+
         return {"status": "success", "message": message, "data": radar_dict}
     except TAMradarServiceError as e:
         logger.error(f"Failed to create contact radar: {e}")
@@ -326,11 +327,11 @@
             "industry_keyword": request.industry_keyword,
             "radar_type": request.radar_type,
             "watchlist_category": request.watchlist_category,
-        }
+        },
     )
     try:
         result = await tamradar_service.create_industry_radar(current_user.user_id, request)
-        
+
         # Handle both dict (with recent_findings) and TAMradarRadarResponse
         if isinstance(result, dict):
             radar = result["radar"]
@@ -338,7 +339,7 @@
         else:
             radar = result
             recent_findings = []
-        
+
         logger.info(
             "Industry radar created successfully",
             extra={
@@ -347,17 +348,17 @@
                 "industry_keyword": request.industry_keyword,
                 "radar_type": radar.radar_type,
                 "recent_findings_count": len(recent_findings),
-            }
+            },
         )
-        
+
         # TAMradar always returns "active" when creating a radar
         # Failures are detected later via webhook (24-96 hours)
         message = "Radar created successfully"
-        
+
         radar_dict = radar.model_dump()
         if recent_findings:
             radar_dict["recent_findings"] = recent_findings
-        
+
         return {"status": "success", "message": message, "data": radar_dict}
     except TAMradarServiceError as e:
         logger.error(f"Failed to create industry radar: {e}")
@@ -371,14 +372,14 @@
 ):
     """
     List all radars for the current user, optionally filtered by category.
-    
+
     Returns radar count information showing how many watchlist companies the user has
     out of their allowed limit.
     """
     try:
         radars = await tamradar_service.get_user_radars(current_user.user_id, category=category)
         count_info = await tamradar_service.get_user_radar_count_and_limit(current_user.user_id)
-        
+
         # Format response - handle both grouped (by company) and ungrouped formats
         formatted_data = []
         for r in radars:
@@ -389,65 +390,70 @@
                 radar_config = r.get("radar_config", {})
                 if not radar_config or not isinstance(radar_config, dict):
                     import json
+
                     radar_config = {
                         "domain": r["domain"],  # r["domain"] is already normalized from service
                         "company_name": r.get("company_name", ""),
                     }
-                
+
                 # Ensure domain in radar_config matches the normalized domain
                 if "domain" in radar_config:
                     radar_config["domain"] = r["domain"]  # Use normalized domain from service
-                
-                formatted_data.append({
-                    "domain": r["domain"],  # Already normalized from service
-                    "company_name": r.get("company_name", ""),
-                    "radar_category": r["radar_category"],
-                    "radar_types": r["radar_types"],  # List of all radar types for this company
-                    "radar_ids": r["radar_ids"],  # List of all radar IDs for this company
-                    "status": r["status"],  # 'active' or 'failed'
-                    "failed_at": r.get("failed_at").isoformat() if r.get("failed_at") else None,
-                    "failure_reason": r.get("failure_reason"),
-                    "radar_config": radar_config,  # Include full radar_config for backward compatibility
-                    "is_owner": r["is_owner"],
-                    "subscribed_at": r["subscribed_at"].isoformat() if r.get("subscribed_at") else None,
-                    "next_charge_at": r["next_charge_at"].isoformat() if r.get("next_charge_at") else None,
-                    "webhook_url": r.get("webhook_url"),
-                    "custom_fields": r.get("custom_fields"),
-                    "filters": r.get("filters"),
-                    "created_at": r["created_at"].isoformat() if r.get("created_at") else None,
-                    "watchlist_category": r.get("watchlist_category"),
-                    "user_interests": r.get("user_interests"),
-                    "weekly_wrapup_email": bool(r.get("weekly_wrapup_email", False)),
-                    # Ensure tier1_email_alerts is explicitly a boolean, not None
-                    # If radar_preferences exists and all radars have the same value, use that for consistency
-                    "tier1_email_alerts": _get_consistent_tier1_value(r),
-                    # Include per-radar preferences map for cases where multiple radars have different settings
-                    "radar_preferences": r.get("_radar_preferences", {}),
-                })
+
+                formatted_data.append(
+                    {
+                        "domain": r["domain"],  # Already normalized from service
+                        "company_name": r.get("company_name", ""),
+                        "radar_category": r["radar_category"],
+                        "radar_types": r["radar_types"],  # List of all radar types for this company
+                        "radar_ids": r["radar_ids"],  # List of all radar IDs for this company
+                        "status": r["status"],  # 'active' or 'failed'
+                        "failed_at": r.get("failed_at").isoformat() if r.get("failed_at") else None,
+                        "failure_reason": r.get("failure_reason"),
+                        "radar_config": radar_config,  # Include full radar_config for backward compatibility
+                        "is_owner": r["is_owner"],
+                        "subscribed_at": r["subscribed_at"].isoformat() if r.get("subscribed_at") else None,
+                        "next_charge_at": r["next_charge_at"].isoformat() if r.get("next_charge_at") else None,
+                        "webhook_url": r.get("webhook_url"),
+                        "custom_fields": r.get("custom_fields"),
+                        "filters": r.get("filters"),
+                        "created_at": r["created_at"].isoformat() if r.get("created_at") else None,
+                        "watchlist_category": r.get("watchlist_category"),
+                        "user_interests": r.get("user_interests"),
+                        "weekly_wrapup_email": bool(r.get("weekly_wrapup_email", False)),
+                        # Ensure tier1_email_alerts is explicitly a boolean, not None
+                        # If radar_preferences exists and all radars have the same value, use that for consistency
+                        "tier1_email_alerts": _get_consistent_tier1_value(r),
+                        # Include per-radar preferences map for cases where multiple radars have different settings
+                        "radar_preferences": r.get("_radar_preferences", {}),
+                    }
+                )
             else:
                 # Individual radar format (for backward compatibility)
-                formatted_data.append({
-                    "radar_id": r["radar_id"],
-                    "radar_category": r["radar_category"],
-                    "radar_type": r["radar_type"],
-                    "status": r["status"],  # 'active', 'pending', or 'failed'
-                    "failed_at": r.get("failed_at").isoformat() if r.get("failed_at") else None,
-                    "failure_reason": r.get("failure_reason"),
-                    "radar_config": r["radar_config"],
-                    "is_owner": r["is_owner"],
-                    "subscribed_at": r["subscribed_at"].isoformat() if r.get("subscribed_at") else None,
-                    "next_charge_at": r["next_charge_at"].isoformat() if r.get("next_charge_at") else None,
-                    "webhook_url": r.get("webhook_url"),
-                    "custom_fields": r.get("custom_fields"),
-                    "filters": r.get("filters"),
-                    "created_at": r["created_at"].isoformat() if r.get("created_at") else None,
-                    "watchlist_category": r.get("watchlist_category"),
-                    "user_interests": r.get("user_interests"),
-                    "weekly_wrapup_email": bool(r.get("weekly_wrapup_email", False)),
-                    # Ensure tier1_email_alerts is explicitly a boolean, not None
-                    "tier1_email_alerts": _get_consistent_tier1_value(r),
-                })
-        
+                formatted_data.append(
+                    {
+                        "radar_id": r["radar_id"],
+                        "radar_category": r["radar_category"],
+                        "radar_type": r["radar_type"],
+                        "status": r["status"],  # 'active', 'pending', or 'failed'
+                        "failed_at": r.get("failed_at").isoformat() if r.get("failed_at") else None,
+                        "failure_reason": r.get("failure_reason"),
+                        "radar_config": r["radar_config"],
+                        "is_owner": r["is_owner"],
+                        "subscribed_at": r["subscribed_at"].isoformat() if r.get("subscribed_at") else None,
+                        "next_charge_at": r["next_charge_at"].isoformat() if r.get("next_charge_at") else None,
+                        "webhook_url": r.get("webhook_url"),
+                        "custom_fields": r.get("custom_fields"),
+                        "filters": r.get("filters"),
+                        "created_at": r["created_at"].isoformat() if r.get("created_at") else None,
+                        "watchlist_category": r.get("watchlist_category"),
+                        "user_interests": r.get("user_interests"),
+                        "weekly_wrapup_email": bool(r.get("weekly_wrapup_email", False)),
+                        # Ensure tier1_email_alerts is explicitly a boolean, not None
+                        "tier1_email_alerts": _get_consistent_tier1_value(r),
+                    }
+                )
+
         return {
             "status": "success",
             "data": formatted_data,
@@ -472,7 +478,7 @@
         # Verify user has access to this radar
         user_radars = await tamradar_service.get_user_radars(current_user.user_id)
         radar_ids = [r["radar_id"] for r in user_radars]
-        
+
         if radar_id not in radar_ids:
             raise HTTPException(status_code=404, detail="Radar not found")
 
@@ -506,7 +512,7 @@
             "user_id": current_user.user_id,
             "radar_id": radar_id,
             "new_category": request.watchlist_category,
-        }
+        },
     )
     try:
         # Verify user has access to this radar
@@ -518,24 +524,20 @@
                 extra={
                     "user_id": current_user.user_id,
                     "radar_id": radar_id,
-                }
+                },
             )
             raise HTTPException(status_code=404, detail="Radar not found or access denied")
-        
+
         # Get current category before update for logging
         current_category = None
         for r in user_radars:
             if radar_id in (r.get("radar_ids", []) if "radar_ids" in r else [r.get("radar_id")]):
                 current_category = r.get("watchlist_category")
                 break
-        
+
         # Update category
-        await tamradar_service.update_radar_category(
-            current_user.user_id, 
-            radar_id, 
-            request.watchlist_category
-        )
-        
+        await tamradar_service.update_radar_category(current_user.user_id, radar_id, request.watchlist_category)
+
         logger.info(
             "Radar category updated successfully",
             extra={
@@ -543,14 +545,10 @@
                 "radar_id": radar_id,
                 "old_category": current_category,
                 "new_category": request.watchlist_category,
-            }
+            },
         )
-        
-        return {
-            "status": "success",
-            "message": "Category updated successfully",
-            "watchlist_category": request.watchlist_category
-        }
+
+        return {"status": "success", "message": "Category updated successfully", "watchlist_category": request.watchlist_category}
     except HTTPException:
         raise
     except Exception as e:
@@ -579,7 +577,7 @@
             "user_interests": request.user_interests,
             "weekly_wrapup_email": request.weekly_wrapup_email,
             "tier1_email_alerts": request.tier1_email_alerts,
-        }
+        },
     )
     try:
         # Verify user has access to this radar
@@ -591,15 +589,15 @@
                 extra={
                     "user_id": current_user.user_id,
                     "radar_id": radar_id,
-                }
+                },
             )
             raise HTTPException(status_code=404, detail="Radar not found or access denied")
-        
+
         # Get current preferences before update for logging
         current_prefs = {}
         target_domain = None
         company_radar_ids = []
-        
+
         # Find the radar and get its domain to find all radars for this company
         for r in user_radars:
             if radar_id in (r.get("radar_ids", []) if "radar_ids" in r else [r.get("radar_id")]):
@@ -612,14 +610,15 @@
                 if r.get("radar_category") == "company":
                     target_domain = r.get("domain")
                 break
-        
+
         # If this is a company radar, find ALL radar_ids for this company/domain
         # This ensures all radar types for the company get updated (matching CREATE behavior)
         if target_domain:
             from app.utils.domain_utils import clean_domain
             from app.utils.global_db import get_global_db
+
             normalized_domain = clean_domain(target_domain)
-            
+
             # Query database directly to find ALL radar_ids for this domain for this user
             # This is more reliable than relying on grouped format
             db = await get_global_db()
@@ -637,7 +636,7 @@
                           AND r.status = 'active'
                         """,
                         current_user.user_id,
-                        normalized_domain
+                        normalized_domain,
                     )
                     company_radar_ids = [row["radar_id"] for row in radar_rows]
             else:
@@ -656,14 +655,14 @@
         else:
             # For non-company radars (contact/industry), just update the single radar
             company_radar_ids = [radar_id]
-        
+
         # Ensure the original radar_id is included (safety check)
         if radar_id not in company_radar_ids:
             company_radar_ids.append(radar_id)
-        
+
         # Remove duplicates
         company_radar_ids = list(set(company_radar_ids))
-        
+
         logger.info(
             "Updating preferences for company radars",
             extra={
@@ -673,18 +672,18 @@
                 "radar_count": len(company_radar_ids),
                 "tier1_email_alerts": request.tier1_email_alerts,
                 "weekly_wrapup_email": request.weekly_wrapup_email,
-            }
+            },
         )
-        
+
         # Update preferences for all radar_ids (all radar types for company, or single radar for contact/industry)
         await tamradar_service.update_radar_user_preferences(
-            current_user.user_id, 
-            company_radar_ids, 
+            current_user.user_id,
+            company_radar_ids,
             user_interests=request.user_interests,
             weekly_wrapup_email=request.weekly_wrapup_email,
-            tier1_email_alerts=request.tier1_email_alerts
+            tier1_email_alerts=request.tier1_email_alerts,
         )
-        
+
         logger.info(
             "Radar preferences updated successfully",
             extra={
@@ -696,15 +695,15 @@
                     "weekly_wrapup_email": request.weekly_wrapup_email,
                     "tier1_email_alerts": request.tier1_email_alerts,
                 },
-            }
+            },
         )
-        
+
         return {
             "status": "success",
             "message": "Preferences updated successfully",
             "user_interests": request.user_interests,
             "weekly_wrapup_email": request.weekly_wrapup_email,
-            "tier1_email_alerts": request.tier1_email_alerts
+            "tier1_email_alerts": request.tier1_email_alerts,
         }
     except HTTPException:
         raise
@@ -720,7 +719,7 @@
 ):
     """
     Deactivate user's subscription to a radar.
-    
+
     Users are limited to deleting maximum 5 radars per month.
     """
     logger.info(
@@ -728,38 +727,39 @@
         extra={
             "user_id": current_user.user_id,
             "radar_id": radar_id,
-        }
+        },
     )
     try:
         # Verify user has access to this radar and get radar info for logging
         user_radars = await tamradar_service.get_user_radars(current_user.user_id, group_by_company=False)
         radar_ids = [r["radar_id"] for r in user_radars]
-        
+
         if radar_id not in radar_ids:
             logger.warning(
                 "User attempted to deactivate radar they don't have access to",
                 extra={
                     "user_id": current_user.user_id,
                     "radar_id": radar_id,
-                }
+                },
             )
             raise HTTPException(status_code=404, detail="Radar not found")
-        
+
         # Get radar info before deactivation for logging and find all radars for this company
         radar_info = None
         target_domain = None
         company_radar_ids = []
-        
+
         for r in user_radars:
             if r.get("radar_id") == radar_id:
                 radar_config = r.get("radar_config", {})
                 if isinstance(radar_config, str):
                     try:
                         import json as json_module
+
                         radar_config = json_module.loads(radar_config)
                     except (JSONDecodeError, TypeError, ValueError):
                         radar_config = {}
-                
+
                 target_domain = radar_config.get("domain") if isinstance(radar_config, dict) else None
                 radar_info = {
                     "radar_category": r.get("radar_category"),
@@ -767,12 +767,13 @@
                     "domain": target_domain,
                 }
                 break
-        
+
         # If this is a company radar, find ALL radars for this company/domain
         if target_domain and radar_info and radar_info.get("radar_category") == "company":
             from app.utils.domain_utils import clean_domain
+
             normalized_domain = clean_domain(target_domain)
-            
+
             # Find all radars for this domain
             for r in user_radars:
                 if r.get("radar_category") == "company":
@@ -780,17 +781,18 @@
                     if isinstance(r_config, str):
                         try:
                             import json as json_module
+
                             r_config = json_module.loads(r_config)
                         except (JSONDecodeError, TypeError, ValueError):
                             r_config = {}
-                    
+
                     r_domain = r_config.get("domain") if isinstance(r_config, dict) else None
                     if r_domain and clean_domain(r_domain) == normalized_domain:
                         company_radar_ids.append(r.get("radar_id"))
         else:
             # For non-company radars (contact/industry), just delete the single radar
             company_radar_ids = [radar_id]
-        
+
         # Check monthly deletion limit (5 companies per month, not individual radars)
         # For company radars, count as 1 deletion regardless of how many radar types
         can_delete, current_count, limit = await tamradar_service.check_deletion_limit(current_user.user_id)
@@ -804,27 +806,23 @@
                     "radars_to_delete": len(company_radar_ids),
                     "current_count": current_count,
                     "limit": limit,
-                }
+                },
             )
             raise HTTPException(
                 status_code=429,
-                detail=f"Monthly deletion limit exceeded. You have deleted {current_count} companies this month (limit: {limit}). Please wait until next month to delete more companies."
+                detail=f"Monthly deletion limit exceeded. You have deleted {current_count} companies this month (limit: {limit}). Please wait until next month to delete more companies.",
             )
-        
+
         # Delete all radars for this company
         # For company radars, we delete all radar types at once
         # Only record deletion once per company (not per radar)
         deleted_count = 0
         failed_count = 0
-        
+
         for idx, r_id in enumerate(company_radar_ids):
             # Only record deletion for the first radar (counts as 1 company deletion)
-            record_deletion = (idx == 0)
-            success = await tamradar_service.deactivate_user_radar(
-                current_user.user_id, 
-                r_id, 
-                record_deletion=record_deletion
-            )
+            record_deletion = idx == 0
+            success = await tamradar_service.deactivate_user_radar(current_user.user_id, r_id, record_deletion=record_deletion)
             if success:
                 deleted_count += 1
             else:
@@ -835,9 +833,9 @@
                         "user_id": current_user.user_id,
                         "radar_id": r_id,
                         "domain": target_domain,
-                    }
+                    },
                 )
-        
+
         if failed_count > 0:
             logger.error(
                 f"Failed to deactivate {failed_count} radar(s) for company {target_domain}",
@@ -847,16 +845,16 @@
                     "deleted_count": deleted_count,
                     "failed_count": failed_count,
                     "total_radars": len(company_radar_ids),
-                }
+                },
             )
             raise HTTPException(
-                status_code=500, 
-                detail=f"Failed to deactivate {failed_count} radar(s) for company. {deleted_count} radar(s) were successfully deleted."
+                status_code=500,
+                detail=f"Failed to deactivate {failed_count} radar(s) for company. {deleted_count} radar(s) were successfully deleted.",
             )
-        
+
         if deleted_count == 0:
             raise HTTPException(status_code=500, detail="Failed to deactivate radar")
-        
+
         logger.info(
             "Company radars deactivated successfully",
             extra={
@@ -868,16 +866,16 @@
                 "radar_ids_deleted": company_radar_ids,
                 "deletions_used": current_count + 1,  # Count as 1 company deletion
                 "deletions_remaining": limit - (current_count + 1),
-            }
+            },
         )
 
         # Return remaining deletions in response
         # Note: For company radars, this counts as 1 deletion (1 company) regardless of radar count
         _, remaining_count, _ = await tamradar_service.check_deletion_limit(current_user.user_id)
         remaining_deletions = limit - remaining_count
-        
+
         message = "Company deactivated" if len(company_radar_ids) > 1 else "Radar deactivated"
-        
+
         return {
             "status": "success",
             "message": message,
@@ -890,8 +888,8 @@
     except HTTPException:
         raise
     except Exception as e:
-
         import traceback
+
         error_traceback = traceback.format_exc()
         logger.error(
             f"Failed to deactivate radar {radar_id}: {e}",
@@ -902,12 +900,9 @@
                 "error_message": str(e),
                 "traceback": error_traceback,
             },
-            exc_info=True
-        )
-        raise HTTPException(
-            status_code=500, 
-            detail=f"Failed to deactivate radar: {str(e)}"
+            exc_info=True,
         )
+        raise HTTPException(status_code=500, detail=f"Failed to deactivate radar: {str(e)}")
 
 
 @router.get("/account")
@@ -958,22 +953,24 @@
 ):
     """
     Get TAMradar findings for the current user.
-    
+
     Returns findings from all radars the user is subscribed to, filtered by optional parameters.
     Findings are returned in reverse chronological order (newest first).
     """
-    logger.info(f"Getting findings for user {current_user.user_id}, radar_id={radar_id}, radar_type={radar_type}, days={days}, limit={limit}, offset={offset}")
+    logger.info(
+        f"Getting findings for user {current_user.user_id}, radar_id={radar_id}, radar_type={radar_type}, days={days}, limit={limit}, offset={offset}"
+    )
     try:
         from app.utils.global_db import get_global_db
         from datetime import datetime, timezone, timedelta
-        
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         # Calculate date threshold (days ago)
         date_threshold = datetime.now(timezone.utc) - timedelta(days=days)
-        
+
         async with db.pool.acquire() as conn:
             where_clauses = ["uf.user_id = $1", "f.discovered_at >= $2"]
             params = [current_user.user_id, date_threshold.replace(tzinfo=None)]
@@ -992,7 +989,7 @@
                         radar_ids.append(r["radar_id"])
                 if radar_id not in radar_ids:
                     raise HTTPException(status_code=404, detail="Radar not found or access denied")
-                
+
                 param_count += 1
                 where_clauses.append(f"f.radar_id = ${param_count}")
                 params.append(radar_id)
@@ -1041,37 +1038,42 @@
                 if isinstance(radar_config, str):
                     try:
                         import json as json_module
+
                         radar_config = json_module.loads(radar_config)
                     except (JSONDecodeError, TypeError, ValueError):
                         radar_config = {}
-                
+
                 # Normalize domain in radar_config if it exists
                 if isinstance(radar_config, dict) and "domain" in radar_config and radar_config["domain"]:
                     from app.utils.domain_utils import clean_domain
+
                     radar_config["domain"] = clean_domain(radar_config["domain"])
-                
+
                 # Parse finding_data if it's a JSON string
                 finding_data = r.get("finding_data")
                 if isinstance(finding_data, str):
                     try:
                         import json as json_module
+
                         finding_data = json_module.loads(finding_data)
                     except (JSONDecodeError, TypeError, ValueError):
                         finding_data = {}
-                
-                findings.append({
-                    "finding_id": r["finding_id"],
-                    "radar_id": r["radar_id"],
-                    "radar_category": r["radar_category"],
-                    "radar_type": r["radar_type"],
-                    "finding_data": finding_data,  # Parsed JSONB from TAMradar
-                    "discovered_at": r["discovered_at"].isoformat() if r.get("discovered_at") else None,
-                    "created_at": r["created_at"].isoformat() if r.get("created_at") else None,
-                    "notified": r.get("notified", False),
-                    "notified_at": r["notified_at"].isoformat() if r.get("notified_at") else None,
-                    "radar_config": radar_config,  # Parsed JSONB - includes normalized domain, contact info, etc.
-                })
 
+                findings.append(
+                    {
+                        "finding_id": r["finding_id"],
+                        "radar_id": r["radar_id"],
+                        "radar_category": r["radar_category"],
+                        "radar_type": r["radar_type"],
+                        "finding_data": finding_data,  # Parsed JSONB from TAMradar
+                        "discovered_at": r["discovered_at"].isoformat() if r.get("discovered_at") else None,
+                        "created_at": r["created_at"].isoformat() if r.get("created_at") else None,
+                        "notified": r.get("notified", False),
+                        "notified_at": r["notified_at"].isoformat() if r.get("notified_at") else None,
+                        "radar_config": radar_config,  # Parsed JSONB - includes normalized domain, contact info, etc.
+                    }
+                )
+
             return {
                 "status": "success",
                 "data": {
@@ -1083,7 +1085,7 @@
                         "has_more": (offset + limit) < total,
                         "days": days,  # Frontend expects days inside pagination
                     },
-                }
+                },
             }
     except HTTPException:
         raise
@@ -1101,11 +1103,11 @@
     """
     Get most relevant findings for dashboard display.
     Returns findings from past 24 hours and past 7 days, ordered by tier and relevance.
-    
+
     Optional filters:
     - company_domain: Filter by specific company domain
     - watchlist_category: Filter by watchlist category
-    
+
     Uses in-memory cache (2-minute TTL) to reduce database load.
     Cache is invalidated when Tier 1, 2, or 3 findings are added.
     """
@@ -1114,53 +1116,48 @@
         from app.utils.dashboard_cache import get_cached_dashboard, set_cached_dashboard
         import json
         from datetime import datetime, timezone, timedelta
-        
+
         user_id = current_user.user_id
-        
+
         # Check cache first (only if no filters, as filtered results are less cacheable)
         if not company_domain and not watchlist_category:
             cached_data = get_cached_dashboard(user_id)
             if cached_data is not None:
                 logger.debug(f"Returning cached dashboard data for user {user_id}")
                 return cached_data
-        
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
         now = datetime.now(timezone.utc).replace(tzinfo=None)
         twenty_four_hours_ago = now - timedelta(hours=24)
         seven_days_ago = now - timedelta(days=7)
-        
+
         async with db.pool.acquire() as conn:
             # Build WHERE clause with optional filters
-            where_clauses = [
-                "uf.user_id = $1",
-                "uf.priority_tier IS NOT NULL",
-                "f.discovered_at >= $2",
-                "r.status = 'active'"
-            ]
+            where_clauses = ["uf.user_id = $1", "uf.priority_tier IS NOT NULL", "f.discovered_at >= $2", "r.status = 'active'"]
             query_params = [user_id, twenty_four_hours_ago]
             param_index = 3
-            
+
             # Add company domain filter if provided
             if company_domain:
                 where_clauses.append(f"r.radar_config->>'domain' = ${param_index}")
                 query_params.append(company_domain)
                 param_index += 1
-            
+
             # Add watchlist category filter if provided
             if watchlist_category:
                 where_clauses.append(f"ur.watchlist_category = ${param_index}")
                 query_params.append(watchlist_category)
                 param_index += 1
-            
+
             where_clause = " AND ".join(where_clauses)
-            
+
             # Add JOIN for watchlist_category if filtering by category
             join_clause = ""
             if watchlist_category:
                 join_clause = "INNER JOIN tamradar_user_radars ur ON uf.radar_id = ur.radar_id AND uf.user_id = ur.user_id"
-            
+
             # Query for past 24 hours - ordered by tier (1,2,3), confidence (highest first), date (newest first)
             query_24h = f"""
                 SELECT 
@@ -1187,13 +1184,13 @@
                     f.discovered_at DESC
                 LIMIT 50
             """
-            
+
             rows_24h = await conn.fetch(query_24h, *query_params)
-            
+
             # Update WHERE clause for 7-day query (replace $2 with seven_days_ago)
             query_params_7d = query_params.copy()
             query_params_7d[1] = seven_days_ago  # Replace the time parameter
-            
+
             # Query for past 7 days - ordered by tier (1,2,3), confidence (highest first), date (newest first)
             query_7d = f"""
                 SELECT 
@@ -1220,9 +1217,9 @@
                     f.discovered_at DESC
                 LIMIT 50
             """
-            
+
             rows_7d = await conn.fetch(query_7d, *query_params_7d)
-            
+
             def format_finding(row):
                 """Format a finding row for response."""
                 # Parse radar_config if it's a JSON string
@@ -1230,24 +1227,27 @@
                 if isinstance(radar_config, str):
                     try:
                         import json as json_module
+
                         radar_config = json_module.loads(radar_config)
                     except (JSONDecodeError, TypeError, ValueError):
                         radar_config = {}
-                
+
                 # Normalize domain in radar_config if it exists
                 if isinstance(radar_config, dict) and "domain" in radar_config and radar_config["domain"]:
                     from app.utils.domain_utils import clean_domain
+
                     radar_config["domain"] = clean_domain(radar_config["domain"])
-                
+
                 # Parse finding_data if it's a JSON string
                 finding_data = row.get("finding_data")
                 if isinstance(finding_data, str):
                     try:
                         import json as json_module
+
                         finding_data = json_module.loads(finding_data)
                     except (JSONDecodeError, TypeError, ValueError):
                         finding_data = {}
-                
+
                 # Extract company name from radar_config
                 company_name = None
                 if radar_config:
@@ -1256,7 +1256,7 @@
                     if not company_name and domain:
                         # Fallback to domain if no company_name
                         company_name = domain.replace(".com", "").replace(".co.uk", "").title()
-                
+
                 return {
                     "finding_id": row["finding_id"],
                     "radar_id": row["radar_id"],
@@ -1272,11 +1272,11 @@
                     "radar_config": radar_config,
                     "company_name": company_name,
                 }
-            
+
             # Format findings
             findings_24h = [format_finding(row) for row in rows_24h]
             findings_7d = [format_finding(row) for row in rows_7d]
-            
+
             # Count by tier for 24h
             tier_counts_24h = {"tier_1": 0, "tier_2": 0, "tier_3": 0}
             for f in findings_24h:
@@ -1287,7 +1287,7 @@
                     tier_counts_24h["tier_2"] += 1
                 elif tier == 3:
                     tier_counts_24h["tier_3"] += 1
-            
+
             # Count by tier for 7d
             tier_counts_7d = {"tier_1": 0, "tier_2": 0, "tier_3": 0}
             for f in findings_7d:
@@ -1298,7 +1298,7 @@
                     tier_counts_7d["tier_2"] += 1
                 elif tier == 3:
                     tier_counts_7d["tier_3"] += 1
-            
+
             result = {
                 "status": "success",
                 "past_24_hours": {
@@ -1312,12 +1312,12 @@
                     "tier_counts": tier_counts_7d,
                 },
             }
-            
+
             # Cache the result
             set_cached_dashboard(user_id, result)
-            
+
             return result
-    
+
     except HTTPException:
         raise
     except Exception as e:
@@ -1331,27 +1331,27 @@
 ):
     """
     Get top 5 most relevant findings for flash display.
-    
+
     Returns top 5 findings from past 24 hours, ordered by tier, confidence, and date.
     If fewer than 5 findings in 24 hours, falls back to 48 hours to fill remaining slots.
-    
+
     Optimized for flash display on dashboard/homepage.
     """
     try:
         from app.utils.global_db import get_global_db
         import json
         from datetime import datetime, timezone, timedelta
-        
+
         user_id = current_user.user_id
-        
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         now = datetime.now(timezone.utc).replace(tzinfo=None)
         twenty_four_hours_ago = now - timedelta(hours=24)
         forty_eight_hours_ago = now - timedelta(hours=48)
-        
+
         async with db.pool.acquire() as conn:
             # Step 1: Get top finding per company from past 24 hours
             # Use window function to get one finding per company (best one)
@@ -1407,14 +1407,14 @@
                     discovered_at DESC
                 LIMIT 5
             """
-            
+
             rows_top_per_company_24h = await conn.fetch(query_top_per_company_24h, user_id, twenty_four_hours_ago)
-            
+
             findings = []
             time_range = "24_hours"
             selected_finding_ids = set()
             selected_company_domains = set()
-            
+
             # Add top finding per company from 24 hours
             for row in rows_top_per_company_24h:
                 findings.append(row)
@@ -1423,6 +1423,7 @@
                 radar_config = row.get("radar_config")
                 if isinstance(radar_config, str):
                     import json
+
                     try:
                         radar_config = json.loads(radar_config)
                     except:
@@ -1431,11 +1432,11 @@
                     domain = radar_config.get("domain")
                     if domain:
                         selected_company_domains.add(domain)
-            
+
             # Step 2: If we have fewer than 5, get next best findings from all companies
             if len(findings) < 5:
                 remaining_slots = 5 - len(findings)
-                
+
                 query_next_best_24h = """
                     SELECT 
                         uf.finding_id,
@@ -1464,24 +1465,20 @@
                         f.discovered_at DESC
                     LIMIT $4
                 """
-                
+
                 rows_next_best_24h = await conn.fetch(
-                    query_next_best_24h,
-                    user_id,
-                    twenty_four_hours_ago,
-                    list(selected_finding_ids),
-                    remaining_slots
+                    query_next_best_24h, user_id, twenty_four_hours_ago, list(selected_finding_ids), remaining_slots
                 )
-                
+
                 # Add next best findings
                 for row in rows_next_best_24h:
                     findings.append(row)
                     selected_finding_ids.add(row["finding_id"])
-            
+
             # Step 3: If still fewer than 5, fall back to 48-hour window
             if len(findings) < 5:
                 remaining_slots = 5 - len(findings)
-                
+
                 # First, get top finding per company from 48-hour window (excluding companies we already have)
                 query_top_per_company_48h = """
                     WITH ranked_findings AS (
@@ -1539,7 +1536,7 @@
                         discovered_at DESC
                     LIMIT $6
                 """
-                
+
                 rows_top_per_company_48h = await conn.fetch(
                     query_top_per_company_48h,
                     user_id,
@@ -1547,9 +1544,9 @@
                     twenty_four_hours_ago,
                     list(selected_finding_ids),
                     list(selected_company_domains),
-                    remaining_slots
+                    remaining_slots,
                 )
-                
+
                 # Add top finding per company from 48 hours
                 for row in rows_top_per_company_48h:
                     findings.append(row)
@@ -1558,6 +1555,7 @@
                     radar_config = row.get("radar_config")
                     if isinstance(radar_config, str):
                         import json
+
                         try:
                             radar_config = json.loads(radar_config)
                         except:
@@ -1566,12 +1564,12 @@
                         domain = radar_config.get("domain")
                         if domain:
                             selected_company_domains.add(domain)
-                
+
                 # If still fewer than 5, get next best from 48-hour window
                 rows_next_best_48h = []
                 if len(findings) < 5:
                     remaining_slots = 5 - len(findings)
-                    
+
                     query_next_best_48h = """
                         SELECT 
                             uf.finding_id,
@@ -1601,33 +1599,30 @@
                             f.discovered_at DESC
                         LIMIT $5
                     """
-                    
+
                     rows_next_best_48h = await conn.fetch(
-                        query_next_best_48h,
-                        user_id,
-                        forty_eight_hours_ago,
-                        twenty_four_hours_ago,
-                        list(selected_finding_ids),
-                        remaining_slots
+                        query_next_best_48h, user_id, forty_eight_hours_ago, twenty_four_hours_ago, list(selected_finding_ids), remaining_slots
                     )
-                    
+
                     # Add next best findings from 48 hours
                     for row in rows_next_best_48h:
                         findings.append(row)
                         selected_finding_ids.add(row["finding_id"])
-                
+
                 # Update time range if we used fallback
                 if len(rows_top_per_company_48h) > 0 or len(rows_next_best_48h) > 0:
                     time_range = "48_hours"
-            
+
             # Final ordering: tier ‚Üí relevance ‚Üí recency
             # (Findings are already mostly ordered, but ensure final order)
-            findings.sort(key=lambda x: (
-                x["priority_tier"] or 999,  # Tier 1, 2, 3
-                -(x["relevance_score"] or 0),  # Higher relevance first
-                -(x["discovered_at"].timestamp() if x["discovered_at"] else 0)  # Newer first
-            ))
-            
+            findings.sort(
+                key=lambda x: (
+                    x["priority_tier"] or 999,  # Tier 1, 2, 3
+                    -(x["relevance_score"] or 0),  # Higher relevance first
+                    -(x["discovered_at"].timestamp() if x["discovered_at"] else 0),  # Newer first
+                )
+            )
+
             def format_finding(row):
                 """Format a finding row for response."""
                 # Parse radar_config if it's a JSON string
@@ -1635,19 +1630,21 @@
                 if isinstance(radar_config, str):
                     try:
                         import json as json_module
+
                         radar_config = json_module.loads(radar_config)
                     except (JSONDecodeError, TypeError, ValueError):
                         radar_config = {}
-                
+
                 # Parse finding_data if it's a JSON string
                 finding_data = row.get("finding_data")
                 if isinstance(finding_data, str):
                     try:
                         import json as json_module
+
                         finding_data = json_module.loads(finding_data)
                     except (JSONDecodeError, TypeError, ValueError):
                         finding_data = {}
-                
+
                 # Extract company name from radar_config
                 company_name = None
                 if radar_config:
@@ -1656,7 +1653,7 @@
                     if not company_name and domain:
                         # Fallback to domain if no company_name
                         company_name = domain.replace(".com", "").replace(".co.uk", "").title()
-                
+
                 return {
                     "finding_id": row["finding_id"],
                     "radar_id": row["radar_id"],
@@ -1672,10 +1669,10 @@
                     "radar_config": radar_config,
                     "company_name": company_name,
                 }
-            
+
             # Format findings
             formatted_findings = [format_finding(row) for row in findings]
-            
+
             # Count by tier
             tier_counts = {"tier_1": 0, "tier_2": 0, "tier_3": 0}
             for f in formatted_findings:
@@ -1686,7 +1683,7 @@
                     tier_counts["tier_2"] += 1
                 elif tier == 3:
                     tier_counts["tier_3"] += 1
-            
+
             result = {
                 "status": "success",
                 "findings": formatted_findings,
@@ -1694,9 +1691,9 @@
                 "time_range": time_range,
                 "tier_counts": tier_counts,
             }
-            
+
             return result
-    
+
     except HTTPException:
         raise
     except Exception as e:
@@ -1707,10 +1704,10 @@
 def get_radar_type_display_name(radar_type: str) -> str:
     """
     Map radar type to display name for frontend.
-    
+
     Args:
         radar_type: Internal radar type (e.g., 'company_social_posts')
-        
+
     Returns:
         Display name (e.g., 'Company - Social Posts')
     """
@@ -1740,13 +1737,13 @@
 ):
     """
     Get dashboard metrics for hot signals (Tier 1 + Tier 2).
-    
+
     Returns:
     - Hot signals count (Tier 1 + Tier 2) with time breakdowns (6h, 12h, 24h)
     - Signal velocity (% change: current 24h vs previous 24h)
     - Radar type breakdown (for hot signals)
     - Company breakdown (for hot signals)
-    
+
     All metrics respect the active filter (company_domain or watchlist_category).
     Uses caching (5-minute TTL) to reduce database load.
     """
@@ -1754,46 +1751,46 @@
         from app.utils.global_db import get_global_db
         import json
         from datetime import datetime, timezone, timedelta
-        
+
         user_id = current_user.user_id
-        
+
         # Check cache first (only if no filters, as filtered results are less cacheable)
         cache_key = f"metrics:{user_id}"
         if company_domain:
             cache_key += f":company:{company_domain}"
         if watchlist_category:
             cache_key += f":category:{watchlist_category}"
-        
+
         # Simple cache check (we'll use a similar pattern to dashboard_cache)
         from app.utils.dashboard_cache import get_cached_dashboard, set_cached_dashboard
         # For now, we'll implement a simple cache check - could be enhanced later
-        
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         now = datetime.now(timezone.utc).replace(tzinfo=None)
         six_hours_ago = now - timedelta(hours=6)
         twelve_hours_ago = now - timedelta(hours=12)
         twenty_four_hours_ago = now - timedelta(hours=24)
         forty_eight_hours_ago = now - timedelta(hours=48)
-        
+
         async with db.pool.acquire() as conn:
             # Build WHERE clause with optional filters
             where_clauses = [
                 "uf.user_id = $1",
                 "uf.priority_tier IN (1, 2)",  # Hot signals only (Tier 1 + Tier 2)
-                "r.status = 'active'"
+                "r.status = 'active'",
             ]
             base_params = [user_id]
             param_index = 2
-            
+
             # Add company domain filter if provided
             if company_domain:
                 where_clauses.append(f"r.radar_config->>'domain' = ${param_index}")
                 base_params.append(company_domain)
                 param_index += 1
-            
+
             # Add watchlist category filter if provided
             join_clause = ""
             if watchlist_category:
@@ -1801,9 +1798,9 @@
                 where_clauses.append(f"ur.watchlist_category = ${param_index}")
                 base_params.append(watchlist_category)
                 param_index += 1
-            
+
             where_clause = " AND ".join(where_clauses)
-            
+
             # 1. Hot signals count with time breakdowns
             # Parameters: base_params + [six_hours_ago, twelve_hours_ago, twenty_four_hours_ago]
             time_param_start = param_index
@@ -1821,10 +1818,10 @@
                 WHERE {where_clause}
                   AND f.discovered_at >= ${time_param_start + 2}
             """
-            
+
             hot_signals_params = base_params + [six_hours_ago, twelve_hours_ago, twenty_four_hours_ago]
             row_hot_signals = await conn.fetchrow(query_hot_signals, *hot_signals_params)
-            
+
             # 2. Signal velocity (current 24h vs previous 24h)
             # Parameters: base_params + [twenty_four_hours_ago, now, forty_eight_hours_ago]
             vel_param_start = param_index
@@ -1858,14 +1855,14 @@
                     END as percent_change
                 FROM current_period, previous_period
             """
-            
+
             velocity_params = base_params + [
                 twenty_four_hours_ago,  # vel_param_start
                 now,  # vel_param_start + 1
                 forty_eight_hours_ago,  # vel_param_start + 2
             ]
             row_velocity = await conn.fetchrow(query_velocity, *velocity_params)
-            
+
             # 3. Radar type breakdown
             # Parameters: base_params + [twenty_four_hours_ago]
             radar_param_start = param_index
@@ -1884,10 +1881,10 @@
                 ORDER BY count DESC
                 LIMIT 15
             """
-            
+
             radar_type_params = base_params + [twenty_four_hours_ago]
             rows_radar_types = await conn.fetch(query_radar_types, *radar_type_params)
-            
+
             # 4. Company breakdown
             # Parameters: base_params + [twenty_four_hours_ago]
             company_param_start = param_index
@@ -1908,17 +1905,17 @@
                 ORDER BY count DESC
                 LIMIT 15
             """
-            
+
             company_params = base_params + [twenty_four_hours_ago]
             rows_companies = await conn.fetch(query_companies, *company_params)
-            
+
             # Format response
             hot_signals_total = (row_hot_signals["tier_1"] or 0) + (row_hot_signals["tier_2"] or 0)
-            
+
             current_24h = row_velocity["current_24h"] or 0
             previous_24h = row_velocity["previous_24h"] or 0
             percent_change = row_velocity["percent_change"]
-            
+
             # Determine trend
             if percent_change is None:
                 trend = "new" if current_24h > 0 else "stable"
@@ -1928,17 +1925,19 @@
                 trend = "decreasing"
             else:
                 trend = "stable"
-            
+
             # Format radar type breakdown
             radar_type_breakdown = []
             for row in rows_radar_types:
-                radar_type_breakdown.append({
-                    "radar_type": row["radar_type"],
-                    "display_name": get_radar_type_display_name(row["radar_type"]),
-                    "count": row["count"],
-                    "percentage": round(float(row["percentage"] or 0), 1)
-                })
-            
+                radar_type_breakdown.append(
+                    {
+                        "radar_type": row["radar_type"],
+                        "display_name": get_radar_type_display_name(row["radar_type"]),
+                        "count": row["count"],
+                        "percentage": round(float(row["percentage"] or 0), 1),
+                    }
+                )
+
             # Format company breakdown
             company_breakdown = []
             for row in rows_companies:
@@ -1947,14 +1946,16 @@
                 if not company_name_val and company_domain_val:
                     # Fallback to domain if no company_name
                     company_name_val = company_domain_val.replace(".com", "").replace(".co.uk", "").title()
-                
-                company_breakdown.append({
-                    "company_domain": company_domain_val,
-                    "company_name": company_name_val or company_domain_val,
-                    "count": row["count"],
-                    "percentage": round(float(row["percentage"] or 0), 1)
-                })
-            
+
+                company_breakdown.append(
+                    {
+                        "company_domain": company_domain_val,
+                        "company_name": company_name_val or company_domain_val,
+                        "count": row["count"],
+                        "percentage": round(float(row["percentage"] or 0), 1),
+                    }
+                )
+
             # Determine filter applied
             filter_applied = None
             if company_domain:
@@ -1962,18 +1963,14 @@
                 company_name = None
                 if company_breakdown:
                     company_name = company_breakdown[0].get("company_name")
-                filter_applied = {
-                    "type": "company_domain",
-                    "value": company_domain,
-                    "display_name": company_name or company_domain
-                }
+                filter_applied = {"type": "company_domain", "value": company_domain, "display_name": company_name or company_domain}
             elif watchlist_category:
                 filter_applied = {
                     "type": "watchlist_category",
                     "value": watchlist_category,
-                    "display_name": watchlist_category.replace("_", " ").title()
+                    "display_name": watchlist_category.replace("_", " ").title(),
                 }
-            
+
             result = {
                 "status": "success",
                 "metrics": {
@@ -1989,7 +1986,7 @@
                         "current_24h": current_24h,
                         "previous_24h": previous_24h,
                         "percent_change": round(percent_change, 1) if percent_change is not None else None,
-                        "trend": trend
+                        "trend": trend,
                     },
                     "radar_type_breakdown": radar_type_breakdown,
                     "company_breakdown": company_breakdown,
@@ -1998,13 +1995,13 @@
                         "last_6_hours": six_hours_ago.isoformat(),
                         "last_12_hours": twelve_hours_ago.isoformat(),
                         "last_24_hours": twenty_four_hours_ago.isoformat(),
-                        "previous_24_hours": f"{forty_eight_hours_ago.isoformat()} to {twenty_four_hours_ago.isoformat()}"
-                    }
-                }
+                        "previous_24_hours": f"{forty_eight_hours_ago.isoformat()} to {twenty_four_hours_ago.isoformat()}",
+                    },
+                },
             }
-            
+
             return result
-    
+
     except HTTPException:
         raise
     except Exception as e:
@@ -2016,28 +2013,29 @@
 # Notifications Settings Endpoints
 # ============================================================================
 
+
 @router.get("/notifications/settings")
 async def get_notifications_settings(
     current_user: CurrentUser = Depends(require_user),
 ):
     """
     Get notification preferences for the user, formatted for the settings page.
-    
+
     Returns all radars (company, contact, industry) with their notification preferences.
     """
     try:
         from app.utils.global_db import get_global_db
-        
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         async with db.pool.acquire() as conn:
             # Get all radars with their notification preferences, grouped by category
             # Company radars: group by domain
             # Contact radars: group by contact_email or contact_name
             # Industry radars: group by industry_keyword
-            
+
             # Company radars
             company_rows = await conn.fetch(
                 """
@@ -2057,9 +2055,9 @@
                 GROUP BY r.radar_category, r.radar_config->>'domain', r.radar_config->>'company_name'
                 ORDER BY subscribed_at DESC
                 """,
-                current_user.user_id
+                current_user.user_id,
             )
-            
+
             # Contact radars
             contact_rows = await conn.fetch(
                 """
@@ -2079,9 +2077,9 @@
                 GROUP BY r.radar_category, r.radar_config->>'contact_email', r.radar_config->>'contact_name'
                 ORDER BY subscribed_at DESC
                 """,
-                current_user.user_id
+                current_user.user_id,
             )
-            
+
             # Industry radars
             industry_rows = await conn.fetch(
                 """
@@ -2100,91 +2098,91 @@
                 GROUP BY r.radar_category, r.radar_config->>'industry_keyword'
                 ORDER BY subscribed_at DESC
                 """,
-                current_user.user_id
+                current_user.user_id,
             )
-            
+
             radars = []
             high_priority_count = 0
             weekly_wrapup_count = 0
-            
+
             # Process company radars
             for row in company_rows:
                 display_name = row["company_name"] or row["domain"] or "Unknown Company"
                 high_priority_enabled = bool(row["high_priority_enabled"])
                 weekly_wrapup_enabled = bool(row["weekly_wrapup_enabled"])
-                
+
                 if high_priority_enabled:
                     high_priority_count += 1
                 if weekly_wrapup_enabled:
                     weekly_wrapup_count += 1
-                
-                radars.append({
-                    "radar_category": "company",
-                    "display_name": display_name,
-                    "domain": row["domain"],
-                    "radar_ids": row["radar_ids"],
-                    "high_priority_alerts_enabled": high_priority_enabled,
-                    "weekly_wrapup_enabled": weekly_wrapup_enabled,
-                    "subscribed_at": row["subscribed_at"].isoformat() if row["subscribed_at"] else None
-                })
-            
+
+                radars.append(
+                    {
+                        "radar_category": "company",
+                        "display_name": display_name,
+                        "domain": row["domain"],
+                        "radar_ids": row["radar_ids"],
+                        "high_priority_alerts_enabled": high_priority_enabled,
+                        "weekly_wrapup_enabled": weekly_wrapup_enabled,
+                        "subscribed_at": row["subscribed_at"].isoformat() if row["subscribed_at"] else None,
+                    }
+                )
+
             # Process contact radars
             for row in contact_rows:
                 display_name = row["contact_name"] or row["contact_email"] or "Unknown Contact"
                 high_priority_enabled = bool(row["high_priority_enabled"])
                 weekly_wrapup_enabled = bool(row["weekly_wrapup_enabled"])
-                
+
                 if high_priority_enabled:
                     high_priority_count += 1
                 if weekly_wrapup_enabled:
                     weekly_wrapup_count += 1
-                
-                radars.append({
-                    "radar_category": "contact",
-                    "display_name": display_name,
-                    "contact_email": row["contact_email"],
-                    "contact_name": row["contact_name"],
-                    "radar_ids": row["radar_ids"],
-                    "high_priority_alerts_enabled": high_priority_enabled,
-                    "weekly_wrapup_enabled": weekly_wrapup_enabled,
-                    "subscribed_at": row["subscribed_at"].isoformat() if row["subscribed_at"] else None
-                })
-            
+
+                radars.append(
+                    {
+                        "radar_category": "contact",
+                        "display_name": display_name,
+                        "contact_email": row["contact_email"],
+                        "contact_name": row["contact_name"],
+                        "radar_ids": row["radar_ids"],
+                        "high_priority_alerts_enabled": high_priority_enabled,
+                        "weekly_wrapup_enabled": weekly_wrapup_enabled,
+                        "subscribed_at": row["subscribed_at"].isoformat() if row["subscribed_at"] else None,
+                    }
+                )
+
             # Process industry radars
             for row in industry_rows:
                 display_name = row["industry_keyword"] or "Unknown Industry"
                 high_priority_enabled = bool(row["high_priority_enabled"])
                 weekly_wrapup_enabled = bool(row["weekly_wrapup_enabled"])
-                
+
                 if high_priority_enabled:
                     high_priority_count += 1
                 if weekly_wrapup_enabled:
                     weekly_wrapup_count += 1
-                
-                radars.append({
-                    "radar_category": "industry",
-                    "display_name": display_name,
-                    "industry_keyword": row["industry_keyword"],
-                    "radar_ids": row["radar_ids"],
-                    "high_priority_alerts_enabled": high_priority_enabled,
-                    "weekly_wrapup_enabled": weekly_wrapup_enabled,
-                    "subscribed_at": row["subscribed_at"].isoformat() if row["subscribed_at"] else None
-                })
-            
+
+                radars.append(
+                    {
+                        "radar_category": "industry",
+                        "display_name": display_name,
+                        "industry_keyword": row["industry_keyword"],
+                        "radar_ids": row["radar_ids"],
+                        "high_priority_alerts_enabled": high_priority_enabled,
+                        "weekly_wrapup_enabled": weekly_wrapup_enabled,
+                        "subscribed_at": row["subscribed_at"].isoformat() if row["subscribed_at"] else None,
+                    }
+                )
+
             summary = {
                 "total_radars": len(radars),
                 "high_priority_alerts_enabled_count": high_priority_count,
-                "weekly_wrapup_enabled_count": weekly_wrapup_count
-            }
-            
-            return {
-                "status": "success",
-                "data": {
-                    "summary": summary,
-                    "radars": radars
-                }
+                "weekly_wrapup_enabled_count": weekly_wrapup_count,
             }
-    
+
+            return {"status": "success", "data": {"summary": summary, "radars": radars}}
+
     except HTTPException:
         raise
     except Exception as e:
@@ -2198,16 +2196,16 @@
 ):
     """
     Disable high-priority alerts for all radars.
-    
+
     Sets tier1_email_alerts to false for all user's radars.
     """
     try:
         from app.utils.global_db import get_global_db
-        
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         async with db.pool.acquire() as conn:
             # Disable all tier1_email_alerts for this user
             result = await conn.execute(
@@ -2216,22 +2214,13 @@
                 SET tier1_email_alerts = false
                 WHERE user_id = $1
                 """,
-                current_user.user_id
+                current_user.user_id,
             )
-            
-            logger.info(
-                f"Disabled all high-priority alerts for user",
-                extra={
-                    "user_id": current_user.user_id,
-                    "rows_affected": result
-                }
-            )
-            
-            return {
-                "status": "success",
-                "message": "High-priority alerts disabled for all radars"
-            }
-    
+
+            logger.info(f"Disabled all high-priority alerts for user", extra={"user_id": current_user.user_id, "rows_affected": result})
+
+            return {"status": "success", "message": "High-priority alerts disabled for all radars"}
+
     except HTTPException:
         raise
     except Exception as e:
@@ -2245,16 +2234,16 @@
 ):
     """
     Disable weekly wrap-up emails for all radars.
-    
+
     Sets weekly_wrapup_email to false for all user's radars.
     """
     try:
         from app.utils.global_db import get_global_db
-        
+
         db = await get_global_db()
         if not db:
             raise HTTPException(status_code=500, detail="Database not available")
-        
+
         async with db.pool.acquire() as conn:
             # Disable all weekly_wrapup_email for this user
             result = await conn.execute(
@@ -2263,25 +2252,15 @@
                 SET weekly_wrapup_email = false
                 WHERE user_id = $1
                 """,
-                current_user.user_id
-            )
-            
-            logger.info(
-                f"Disabled all weekly wrap-up emails for user",
-                extra={
-                    "user_id": current_user.user_id,
-                    "rows_affected": result
-                }
+                current_user.user_id,
             )
-            
-            return {
-                "status": "success",
-                "message": "Weekly wrap-up emails disabled for all radars"
-            }
-    
+
+            logger.info(f"Disabled all weekly wrap-up emails for user", extra={"user_id": current_user.user_id, "rows_affected": result})
+
+            return {"status": "success", "message": "Weekly wrap-up emails disabled for all radars"}
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Failed to disable all weekly wrap-up emails: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="Failed to disable weekly wrap-up emails")
-

--- app/salesforce/crm/__init__.py
+++ app/salesforce/crm/__init__.py
@@ -19,4 +19,3 @@
     "list_users",
     "get_salesforce_context_by_email",
 ]
-

--- app/salesforce/crm/accounts.py
+++ app/salesforce/crm/accounts.py
@@ -17,55 +17,52 @@
 ) -> List[SalesforceAccountSummary]:
     """
     Search for accounts by name or domain.
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL
         name: Account name to search for (LIKE query)
         domain: Domain to search for (extracted from Website field)
-        
+
     Returns:
         List of SalesforceAccountSummary objects
     """
     if not name and not domain:
         raise ValueError("Either name or domain must be provided")
-    
+
     where_clauses = []
-    
+
     if name:
         escaped_name = _escape_soql_string(name)
         where_clauses.append(f"Name LIKE '%{escaped_name}%'")
-    
+
     if domain:
         escaped_domain = _escape_soql_string(domain)
         where_clauses.append(f"Website LIKE '%{escaped_domain}%'")
-    
+
     where_clause = " AND ".join(where_clauses)
-    
-    soql_query = (
-        f"SELECT Id, Name, Industry, NumberOfEmployees, AnnualRevenue, OwnerId, Website "
-        f"FROM Account WHERE {where_clause}"
-    )
-    
+
+    soql_query = f"SELECT Id, Name, Industry, NumberOfEmployees, AnnualRevenue, OwnerId, Website FROM Account WHERE {where_clause}"
+
     records = await _paginate_soql_query(
         access_token=access_token,
         instance_url=instance_url,
         soql_query=soql_query,
     )
-    
+
     results = []
     for record in records:
         website = record.get("Website")
         extracted_domain = None
         if website:
             extracted_domain = extract_domain_from_url(website)
-            
+
             if domain and extracted_domain:
                 search_domain = domain.lower().replace("www.", "").strip()
                 extracted_normalized = extracted_domain.lower().replace("www.", "").strip()
                 if search_domain != extracted_normalized and search_domain not in extracted_normalized and extracted_normalized not in search_domain:
                     continue
-        
+
         account = SalesforceAccountSummary(
             id=record.get("Id", ""),
             name=record.get("Name"),
@@ -74,9 +71,12 @@
             employee_count=record.get("NumberOfEmployees"),
             annual_revenue=record.get("AnnualRevenue"),
             owner_id=record.get("OwnerId"),
-            raw_fields={k: v for k, v in record.items() if k not in ["Id", "Name", "Industry", "NumberOfEmployees", "AnnualRevenue", "OwnerId", "Website", "attributes"]},
+            raw_fields={
+                k: v
+                for k, v in record.items()
+                if k not in ["Id", "Name", "Industry", "NumberOfEmployees", "AnnualRevenue", "OwnerId", "Website", "attributes"]
+            },
         )
         results.append(account)
-    
+
     return results
-

--- app/salesforce/crm/client.py
+++ app/salesforce/crm/client.py
@@ -13,17 +13,20 @@
 
 class SalesforceAPIError(HTTPException):
     """Base exception for Salesforce API errors."""
+
     pass
 
 
 class SalesforceAuthenticationError(SalesforceAPIError):
     """Raised when authentication fails (401/403)."""
+
     def __init__(self, detail: str = "Salesforce authentication failed. Please reconnect your account."):
         super().__init__(status_code=401, detail=detail)
 
 
 class SalesforceRateLimitError(SalesforceAPIError):
     """Raised when rate limit is exceeded (429)."""
+
     def __init__(self, detail: str = "Salesforce rate limit exceeded. Please try again later."):
         super().__init__(status_code=429, detail=detail)
 
@@ -38,7 +41,7 @@
 ) -> Dict:
     """
     Generic helper for making HTTP requests to Salesforce API.
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL (e.g., https://mydomain.my.salesforce.com)
@@ -46,10 +49,10 @@
         path: Relative API path (e.g., /services/data/v62.0/query)
         query_params: Optional query parameters
         body: Optional request body for POST/PUT requests
-        
+
     Returns:
         JSON response as dict
-        
+
     Raises:
         SalesforceAuthenticationError: For 401/403 errors
         SalesforceRateLimitError: For 429 errors
@@ -58,20 +61,20 @@
     api_version = get_salesforce_default_api_version()
     if not path.startswith("/"):
         path = "/" + path
-    
+
     base_path = f"/services/data/{api_version}"
     if not path.startswith(base_path):
         full_path = urljoin(base_path + "/", path.lstrip("/"))
     else:
         full_path = path
-    
+
     url = urljoin(instance_url.rstrip("/") + "/", full_path.lstrip("/"))
-    
+
     headers = {
         "Authorization": f"Bearer {access_token}",
         "Content-Type": "application/json",
     }
-    
+
     async with httpx.AsyncClient(timeout=30.0) as client:
         try:
             if method.upper() == "GET":
@@ -86,7 +89,7 @@
                 response = await client.delete(url, headers=headers, params=query_params)
             else:
                 raise ValueError(f"Unsupported HTTP method: {method}")
-            
+
             if 200 <= response.status_code < 300:
                 return response.json()
             elif response.status_code == 401 or response.status_code == 403:
@@ -139,23 +142,21 @@
             ) from exc
 
 
-async def _paginate_soql_query(
-    access_token: str, instance_url: str, soql_query: str
-) -> List[Dict]:
+async def _paginate_soql_query(access_token: str, instance_url: str, soql_query: str) -> List[Dict]:
     """
     Execute a SOQL query with pagination support.
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL
         soql_query: SOQL query string
-        
+
     Returns:
         List of all records from all pages
     """
     all_records = []
     next_records_url = None
-    
+
     while True:
         if next_records_url:
             response = await _make_salesforce_request(
@@ -172,13 +173,12 @@
                 path="/query",
                 query_params={"q": soql_query},
             )
-        
+
         records = response.get("records", [])
         all_records.extend(records)
-        
+
         next_records_url = response.get("nextRecordsUrl")
         if not next_records_url:
             break
-    
+
     return all_records
-

--- app/salesforce/crm/contacts.py
+++ app/salesforce/crm/contacts.py
@@ -9,42 +9,37 @@
 logger = get_logger(__name__)
 
 
-async def search_contacts_by_email(
-    access_token: str, instance_url: str, emails: List[str]
-) -> List[SalesforceContactSummary]:
+async def search_contacts_by_email(access_token: str, instance_url: str, emails: List[str]) -> List[SalesforceContactSummary]:
     """
     Search for contacts by email address(es).
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL
         emails: List of email addresses to search for
-        
+
     Returns:
         List of SalesforceContactSummary objects
     """
     results = []
-    
+
     batch_size = 100
-    
+
     for i in range(0, len(emails), batch_size):
-        email_batch = emails[i:i + batch_size]
-        
+        email_batch = emails[i : i + batch_size]
+
         escaped_emails = [f"'{_escape_soql_string(email)}'" for email in email_batch]
         email_list = ", ".join(escaped_emails)
-        
-        soql_query = (
-            f"SELECT Id, FirstName, LastName, Email, Title, AccountId, OwnerId "
-            f"FROM Contact WHERE Email IN ({email_list})"
-        )
-        
+
+        soql_query = f"SELECT Id, FirstName, LastName, Email, Title, AccountId, OwnerId FROM Contact WHERE Email IN ({email_list})"
+
         try:
             records = await _paginate_soql_query(
                 access_token=access_token,
                 instance_url=instance_url,
                 soql_query=soql_query,
             )
-            
+
             for record in records:
                 contact = SalesforceContactSummary(
                     id=record.get("Id", ""),
@@ -54,7 +49,11 @@
                     title=record.get("Title"),
                     account_id=record.get("AccountId"),
                     owner_id=record.get("OwnerId"),
-                    raw_fields={k: v for k, v in record.items() if k not in ["Id", "Email", "FirstName", "LastName", "Title", "AccountId", "OwnerId", "attributes"]},
+                    raw_fields={
+                        k: v
+                        for k, v in record.items()
+                        if k not in ["Id", "Email", "FirstName", "LastName", "Title", "AccountId", "OwnerId", "attributes"]
+                    },
                 )
                 results.append(contact)
         except Exception as e:
@@ -62,6 +61,5 @@
                 f"Failed to search contacts for email batch",
                 extra={"email_count": len(email_batch), "error": str(e)},
             )
-    
+
     return results
-

--- app/salesforce/crm/context.py
+++ app/salesforce/crm/context.py
@@ -15,52 +15,47 @@
 logger = get_logger(__name__)
 
 
-async def get_salesforce_context_by_email(
-    access_token: str, instance_url: str, email: str
-) -> SalesforceContextByEmailResponse:
+async def get_salesforce_context_by_email(access_token: str, instance_url: str, email: str) -> SalesforceContextByEmailResponse:
     """
     Get complete context for an email address, including contacts, accounts, opportunities, and users.
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL
         email: Email address to search for
-        
+
     Returns:
         SalesforceContextByEmailResponse with all related data
     """
     contacts = await search_contacts_by_email(access_token, instance_url, [email])
-    
+
     account_ids = set()
     owner_ids = set()
-    
+
     for contact in contacts:
         if contact.account_id:
             account_ids.add(contact.account_id)
         if contact.owner_id:
             owner_ids.add(contact.owner_id)
-    
+
     accounts = []
     if account_ids:
         escaped_account_ids = [f"'{_escape_soql_string(aid)}'" for aid in account_ids]
         account_ids_list = ", ".join(escaped_account_ids)
-        
-        soql_query = (
-            f"SELECT Id, Name, Industry, NumberOfEmployees, AnnualRevenue, OwnerId, Website "
-            f"FROM Account WHERE Id IN ({account_ids_list})"
-        )
-        
+
+        soql_query = f"SELECT Id, Name, Industry, NumberOfEmployees, AnnualRevenue, OwnerId, Website FROM Account WHERE Id IN ({account_ids_list})"
+
         try:
             records = await _paginate_soql_query(
                 access_token=access_token,
                 instance_url=instance_url,
                 soql_query=soql_query,
             )
-            
+
             for record in records:
                 website = record.get("Website")
                 extracted_domain = extract_domain_from_url(website) if website else None
-                
+
                 account = SalesforceAccountSummary(
                     id=record.get("Id", ""),
                     name=record.get("Name"),
@@ -69,15 +64,19 @@
                     employee_count=record.get("NumberOfEmployees"),
                     annual_revenue=record.get("AnnualRevenue"),
                     owner_id=record.get("OwnerId"),
-                    raw_fields={k: v for k, v in record.items() if k not in ["Id", "Name", "Industry", "NumberOfEmployees", "AnnualRevenue", "OwnerId", "Website", "attributes"]},
+                    raw_fields={
+                        k: v
+                        for k, v in record.items()
+                        if k not in ["Id", "Name", "Industry", "NumberOfEmployees", "AnnualRevenue", "OwnerId", "Website", "attributes"]
+                    },
                 )
                 accounts.append(account)
-                
+
                 if account.owner_id:
                     owner_ids.add(account.owner_id)
         except Exception as e:
             logger.warning(f"Failed to fetch accounts for context: {e}", exc_info=True)
-    
+
     opportunities = []
     if account_ids:
         for account_id in account_ids:
@@ -85,30 +84,27 @@
                 opp_filters = SalesforceOpportunitySearchRequest(account_id=account_id, limit=50)
                 account_opps = await search_opportunities(access_token, instance_url, opp_filters)
                 opportunities.extend(account_opps)
-                
+
                 for opp in account_opps:
                     if opp.owner_id:
                         owner_ids.add(opp.owner_id)
             except Exception as e:
                 logger.warning(f"Failed to fetch opportunities for account {account_id}: {e}", exc_info=True)
-    
+
     users = []
     if owner_ids:
         escaped_owner_ids = [f"'{_escape_soql_string(oid)}'" for oid in owner_ids]
         owner_ids_list = ", ".join(escaped_owner_ids)
-        
-        soql_query = (
-            f"SELECT Id, FirstName, LastName, Email, IsActive "
-            f"FROM User WHERE Id IN ({owner_ids_list})"
-        )
-        
+
+        soql_query = f"SELECT Id, FirstName, LastName, Email, IsActive FROM User WHERE Id IN ({owner_ids_list})"
+
         try:
             records = await _paginate_soql_query(
                 access_token=access_token,
                 instance_url=instance_url,
                 soql_query=soql_query,
             )
-            
+
             for record in records:
                 user = SalesforceUserSummary(
                     id=record.get("Id", ""),
@@ -121,11 +117,10 @@
                 users.append(user)
         except Exception as e:
             logger.warning(f"Failed to fetch users for context: {e}", exc_info=True)
-    
+
     return SalesforceContextByEmailResponse(
         contacts=contacts,
         accounts=accounts,
         opportunities=opportunities,
         users=users,
     )
-

--- app/salesforce/crm/helpers.py
+++ app/salesforce/crm/helpers.py
@@ -13,28 +13,27 @@
 def extract_domain_from_url(website: str) -> Optional[str]:
     """
     Extract domain from a website URL.
-    
+
     Args:
         website: Website URL string (may or may not have protocol)
-        
+
     Returns:
         Extracted domain or None if extraction fails
     """
     if not website:
         return None
-    
+
     try:
         url = website if website.startswith("http") else f"https://{website}"
         parsed = urlparse(url)
         domain = parsed.netloc or parsed.path
-        
+
         if ":" in domain:
             domain = domain.split(":")[0]
-        
+
         if domain.startswith("www."):
             domain = domain[4:]
-        
+
         return domain.lower() if domain else None
     except Exception:
         return None
-

--- app/salesforce/crm/opportunities.py
+++ app/salesforce/crm/opportunities.py
@@ -10,26 +10,21 @@
 logger = get_logger(__name__)
 
 
-async def _get_opportunity_ids_by_contact_id(
-    access_token: str, instance_url: str, contact_id: str
-) -> List[str]:
+async def _get_opportunity_ids_by_contact_id(access_token: str, instance_url: str, contact_id: str) -> List[str]:
     """
     Get opportunity IDs associated with a contact via Contact Roles.
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL
         contact_id: Salesforce contact ID
-        
+
     Returns:
         List of opportunity IDs
     """
     escaped_contact_id = _escape_soql_string(contact_id)
-    soql_query = (
-        f"SELECT OpportunityId FROM OpportunityContactRole "
-        f"WHERE ContactId = '{escaped_contact_id}'"
-    )
-    
+    soql_query = f"SELECT OpportunityId FROM OpportunityContactRole WHERE ContactId = '{escaped_contact_id}'"
+
     try:
         response = await _make_salesforce_request(
             access_token=access_token,
@@ -38,13 +33,13 @@
             path="/query",
             query_params={"q": soql_query},
         )
-        
+
         opportunity_ids = []
         for record in response.get("records", []):
             opp_id = record.get("OpportunityId")
             if opp_id:
                 opportunity_ids.append(opp_id)
-        
+
         return opportunity_ids
     except Exception as e:
         logger.warning(
@@ -61,62 +56,57 @@
 ) -> List[SalesforceOpportunitySummary]:
     """
     Search for opportunities with optional filters.
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL
         filters: SalesforceOpportunitySearchRequest with filters
-        
+
     Returns:
         List of SalesforceOpportunitySummary objects
     """
     where_clauses = []
-    
+
     if filters.account_id:
         escaped_account_id = _escape_soql_string(filters.account_id)
         where_clauses.append(f"AccountId = '{escaped_account_id}'")
-    
+
     opportunity_ids_from_contact = []
     if filters.contact_id:
-        opportunity_ids_from_contact = await _get_opportunity_ids_by_contact_id(
-            access_token, instance_url, filters.contact_id
-        )
+        opportunity_ids_from_contact = await _get_opportunity_ids_by_contact_id(access_token, instance_url, filters.contact_id)
         if not opportunity_ids_from_contact:
             return []
-    
+
     if opportunity_ids_from_contact:
         escaped_opp_ids = [f"'{_escape_soql_string(opp_id)}'" for opp_id in opportunity_ids_from_contact]
         opp_ids_list = ", ".join(escaped_opp_ids)
         where_clauses.append(f"Id IN ({opp_ids_list})")
-    
+
     if filters.stages:
         escaped_stages = [f"'{_escape_soql_string(stage)}'" for stage in filters.stages]
         stages_list = ", ".join(escaped_stages)
         where_clauses.append(f"StageName IN ({stages_list})")
-    
+
     if filters.close_date_from:
         date_str = filters.close_date_from.strftime("%Y-%m-%d")
         where_clauses.append(f"CloseDate >= {date_str}")
-    
+
     if filters.close_date_to:
         date_str = filters.close_date_to.strftime("%Y-%m-%d")
         where_clauses.append(f"CloseDate <= {date_str}")
-    
+
     where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
-    
-    soql_query = (
-        f"SELECT Id, Name, Amount, StageName, CloseDate, Probability, AccountId, OwnerId "
-        f"FROM Opportunity WHERE {where_clause}"
-    )
-    
+
+    soql_query = f"SELECT Id, Name, Amount, StageName, CloseDate, Probability, AccountId, OwnerId FROM Opportunity WHERE {where_clause}"
+
     records = await _paginate_soql_query(
         access_token=access_token,
         instance_url=instance_url,
         soql_query=soql_query,
     )
-    
-    records = records[:filters.limit]
-    
+
+    records = records[: filters.limit]
+
     results = []
     for record in records:
         close_date = None
@@ -125,7 +115,7 @@
                 close_date = datetime.strptime(record.get("CloseDate"), "%Y-%m-%d")
             except (ValueError, TypeError):
                 pass
-        
+
         opportunity = SalesforceOpportunitySummary(
             id=record.get("Id", ""),
             name=record.get("Name"),
@@ -135,9 +125,12 @@
             probability=record.get("Probability"),
             account_id=record.get("AccountId"),
             owner_id=record.get("OwnerId"),
-            raw_fields={k: v for k, v in record.items() if k not in ["Id", "Name", "Amount", "StageName", "CloseDate", "Probability", "AccountId", "OwnerId", "attributes"]},
+            raw_fields={
+                k: v
+                for k, v in record.items()
+                if k not in ["Id", "Name", "Amount", "StageName", "CloseDate", "Probability", "AccountId", "OwnerId", "attributes"]
+            },
         )
         results.append(opportunity)
-    
+
     return results
-

--- app/salesforce/crm/users.py
+++ app/salesforce/crm/users.py
@@ -5,33 +5,28 @@
 from .client import _paginate_soql_query
 
 
-async def list_users(
-    access_token: str, instance_url: str, active_only: bool = True
-) -> List[SalesforceUserSummary]:
+async def list_users(access_token: str, instance_url: str, active_only: bool = True) -> List[SalesforceUserSummary]:
     """
     List Salesforce users (owners).
-    
+
     Args:
         access_token: Salesforce access token
         instance_url: Salesforce instance URL
         active_only: Filter to only active users
-        
+
     Returns:
         List of SalesforceUserSummary objects
     """
     where_clause = "IsActive = TRUE" if active_only else "1=1"
-    
-    soql_query = (
-        f"SELECT Id, FirstName, LastName, Email, IsActive "
-        f"FROM User WHERE {where_clause}"
-    )
-    
+
+    soql_query = f"SELECT Id, FirstName, LastName, Email, IsActive FROM User WHERE {where_clause}"
+
     records = await _paginate_soql_query(
         access_token=access_token,
         instance_url=instance_url,
         soql_query=soql_query,
     )
-    
+
     results = []
     for record in records:
         user = SalesforceUserSummary(
@@ -43,6 +38,5 @@
             raw_fields={k: v for k, v in record.items() if k not in ["Id", "Email", "FirstName", "LastName", "IsActive", "attributes"]},
         )
         results.append(user)
-    
+
     return results
-

--- app/salesforce/models.py
+++ app/salesforce/models.py
@@ -58,22 +58,15 @@
 class SalesforceContextByEmailResponse(BaseModel):
     """Complete context for an email address, including contacts, accounts, opportunities, and users."""
 
-    contacts: List[SalesforceContactSummary] = Field(
-        default_factory=list, description="Contacts found for this email"
-    )
-    accounts: List[SalesforceAccountSummary] = Field(
-        default_factory=list, description="Accounts associated with contacts"
-    )
-    opportunities: List[SalesforceOpportunitySummary] = Field(
-        default_factory=list, description="Opportunities associated with accounts"
-    )
-    users: List[SalesforceUserSummary] = Field(
-        default_factory=list, description="Users (owners) used in contacts/accounts/opportunities"
-    )
+    contacts: List[SalesforceContactSummary] = Field(default_factory=list, description="Contacts found for this email")
+    accounts: List[SalesforceAccountSummary] = Field(default_factory=list, description="Accounts associated with contacts")
+    opportunities: List[SalesforceOpportunitySummary] = Field(default_factory=list, description="Opportunities associated with accounts")
+    users: List[SalesforceUserSummary] = Field(default_factory=list, description="Users (owners) used in contacts/accounts/opportunities")
 
 
 # Request Models
 
+
 class SalesforceContactSearchRequest(BaseModel):
     """Request model for searching contacts by email."""
 
@@ -96,4 +89,3 @@
     close_date_from: Optional[datetime] = Field(None, description="Filter by close date from")
     close_date_to: Optional[datetime] = Field(None, description="Filter by close date to")
     limit: int = Field(20, description="Maximum number of results to return", ge=1, le=200)
-

--- app/salesforce/oauth.py
+++ app/salesforce/oauth.py
@@ -38,10 +38,10 @@
 def _parse_identity_url(id_url: str) -> Dict[str, str]:
     """
     Parse Salesforce identity URL to extract org_id and user_id.
-    
+
     Format: https://login.salesforce.com/id/<org_id>/<user_id>
     """
-    match = re.search(r'/id/([^/]+)/([^/]+)', id_url)
+    match = re.search(r"/id/([^/]+)/([^/]+)", id_url)
     if not match:
         raise ValueError(f"Invalid Salesforce identity URL format: {id_url}")
     org_id = match.group(1)
@@ -52,7 +52,7 @@
 async def exchange_code_for_tokens(code: str) -> Dict:
     """
     Exchange authorization code for access token.
-    
+
     Returns dict with:
     - access_token
     - refresh_token (optional)
@@ -65,7 +65,7 @@
     """
     login_base_url = get_salesforce_login_base_url()
     token_url = f"{login_base_url}/services/oauth2/token"
-    
+
     data = {
         "grant_type": "authorization_code",
         "code": code,
@@ -73,20 +73,20 @@
         "client_secret": get_salesforce_client_secret(),
         "redirect_uri": get_salesforce_redirect_uri(),
     }
-    
+
     token_payload = await _request_token(token_url, data)
     access_token = token_payload.get("access_token")
     instance_url = token_payload.get("instance_url")
-    
+
     if not access_token:
         raise HTTPException(status_code=400, detail="salesforce_access_token_missing")
     if not instance_url:
         raise HTTPException(status_code=400, detail="salesforce_instance_url_missing")
-    
+
     id_url = token_payload.get("id")
     if not id_url:
         raise HTTPException(status_code=400, detail="salesforce_identity_url_missing")
-    
+
     try:
         identity_data = _parse_identity_url(id_url)
         org_id = identity_data["org_id"]
@@ -94,11 +94,11 @@
     except (ValueError, KeyError) as e:
         logger.error(f"Failed to parse Salesforce identity URL: {id_url}", exc_info=True)
         raise HTTPException(status_code=400, detail="salesforce_identity_url_invalid") from e
-    
+
     expires_at = None
     issued_at = token_payload.get("issued_at")
     expires_in = token_payload.get("expires_in")
-    
+
     if issued_at and expires_in:
         try:
             issued_datetime = datetime.fromtimestamp(int(issued_at) / 1000, tz=timezone.utc)
@@ -107,7 +107,7 @@
             logger.warning(f"Failed to parse expires_at from issued_at={issued_at}, expires_in={expires_in}", exc_info=True)
     elif expires_in:
         expires_at = _resolve_expiry(expires_in)
-    
+
     return {
         "access_token": access_token,
         "refresh_token": token_payload.get("refresh_token"),
@@ -139,42 +139,42 @@
 async def get_valid_salesforce_access_token(user_id: str) -> Optional[Tuple[str, str]]:
     """
     Get a valid Salesforce access token and instance URL for a user.
-    
+
     Returns:
         Tuple of (access_token, instance_url) or None if not available
     """
     tokens = await get_salesforce_oauth_tokens_by_user_id(user_id)
     if not tokens:
         return None
-    
+
     access_token = tokens.get("access_token")
     instance_url = tokens.get("instance_url")
-    
+
     if not access_token or not instance_url:
         return None
-    
+
     expires_at = tokens.get("expires_at")
     if not _is_expired(expires_at):
         return (access_token, instance_url)
-    
+
     refresh_token = tokens.get("refresh_token")
     if not refresh_token:
         logger.warning(f"Salesforce token expired for user {user_id} but no refresh token available")
         return None
-    
+
     refreshed = await _refresh_access_token(refresh_token)
     if not refreshed:
         return None
-    
+
     new_access = refreshed.get("access_token")
     new_instance_url = refreshed.get("instance_url", instance_url)
     if not new_access:
         return None
-    
+
     new_expiry = None
     issued_at = refreshed.get("issued_at")
     expires_in = refreshed.get("expires_in")
-    
+
     if issued_at and expires_in:
         try:
             issued_datetime = datetime.fromtimestamp(int(issued_at) / 1000, tz=timezone.utc)
@@ -183,14 +183,14 @@
             pass
     elif expires_in:
         new_expiry = _resolve_expiry(expires_in)
-    
+
     await update_salesforce_access_token(
         tokens["id"],
         new_access,
         new_instance_url,
         new_expiry,
     )
-    
+
     return (new_access, new_instance_url)
 
 
@@ -198,14 +198,14 @@
     """Refresh a Salesforce access token using refresh_token."""
     login_base_url = get_salesforce_login_base_url()
     token_url = f"{login_base_url}/services/oauth2/token"
-    
+
     data = {
         "grant_type": "refresh_token",
         "refresh_token": refresh_token,
         "client_id": get_salesforce_client_id(),
         "client_secret": get_salesforce_client_secret(),
     }
-    
+
     try:
         return await _request_token(token_url, data)
     except HTTPException:
@@ -244,4 +244,3 @@
     if value.tzinfo is None:
         value = value.replace(tzinfo=timezone.utc)
     return value <= datetime.now(timezone.utc)
-

--- app/salesforce/repositories/salesforce_tokens_repository.py
+++ app/salesforce/repositories/salesforce_tokens_repository.py
@@ -106,4 +106,3 @@
             expires_at,
             token_id,
         )
-

--- app/salesforce/router.py
+++ app/salesforce/router.py
@@ -65,9 +65,9 @@
     token_result = await get_valid_salesforce_access_token(user_id)
     if not token_result:
         return {"connected": False}
-    
+
     access_token, instance_url = token_result
-    
+
     try:
         tokens = await get_salesforce_oauth_tokens_by_user_id(user_id)
         if tokens:
@@ -79,7 +79,7 @@
             }
     except Exception:
         pass
-    
+
     return {
         "connected": True,
         "instance_url": instance_url,
@@ -132,7 +132,7 @@
     token_result = await get_valid_salesforce_access_token(user_id)
     if not token_result:
         raise HTTPException(status_code=401, detail="salesforce_not_connected")
-    
+
     access_token, instance_url = token_result
     contacts = await search_contacts_by_email(access_token, instance_url, [email])
     return contacts
@@ -150,7 +150,7 @@
     token_result = await get_valid_salesforce_access_token(user_id)
     if not token_result:
         raise HTTPException(status_code=401, detail="salesforce_not_connected")
-    
+
     access_token, instance_url = token_result
     contacts = await search_contacts_by_email(access_token, instance_url, request.emails)
     return contacts
@@ -169,10 +169,10 @@
     token_result = await get_valid_salesforce_access_token(user_id)
     if not token_result:
         raise HTTPException(status_code=401, detail="salesforce_not_connected")
-    
+
     if not name and not domain:
         raise HTTPException(status_code=400, detail="Either name or domain must be provided")
-    
+
     access_token, instance_url = token_result
     accounts = await search_accounts(access_token, instance_url, name=name, domain=domain)
     return accounts
@@ -190,7 +190,7 @@
     token_result = await get_valid_salesforce_access_token(user_id)
     if not token_result:
         raise HTTPException(status_code=401, detail="salesforce_not_connected")
-    
+
     access_token, instance_url = token_result
     opportunities = await search_opportunities(access_token, instance_url, request)
     return opportunities
@@ -208,7 +208,7 @@
     token_result = await get_valid_salesforce_access_token(user_id)
     if not token_result:
         raise HTTPException(status_code=401, detail="salesforce_not_connected")
-    
+
     access_token, instance_url = token_result
     users = await list_users(access_token, instance_url, active_only=active_only)
     return users
@@ -227,8 +227,7 @@
     token_result = await get_valid_salesforce_access_token(user_id)
     if not token_result:
         raise HTTPException(status_code=401, detail="salesforce_not_connected")
-    
+
     access_token, instance_url = token_result
     context = await get_salesforce_context_by_email(access_token, instance_url, email)
     return context
-

--- app/schemas/__init__.py
+++ app/schemas/__init__.py
@@ -1,3 +1,3 @@
 """
 Schema definitions for the Ardessa agent system.
-""" 
\ No newline at end of file
+"""

--- app/schemas/investment_firm_schema.py
+++ app/schemas/investment_firm_schema.py
@@ -1,7 +1,7 @@
 """
 Investment firm schema definition for data fusion.
 
-This schema defines the structure for investment firm data that will be used 
+This schema defines the structure for investment firm data that will be used
 by the data fusion agent to combine information from multiple sources.
 """
 
@@ -28,66 +28,59 @@
                         "videos_found": {"type": "integer"},
                         "videos_ingested": {"type": "integer"},
                         "video_summaries": {"type": "array"},
-                        "insights": {"type": "object"}
-                    }
+                        "insights": {"type": "object"},
+                    },
                 },
                 "facebook_url": {"type": "string"},
                 "instagram_url": {"type": "string"},
-                "linkedin_url": {"type": "string"}
-            }
-        }
+                "linkedin_url": {"type": "string"},
+            },
+        },
     },
-    
     "investment_strategy_mandate": {
         "primary_investment_strategy": {"type": "string", "description": "Main investment approach"},
         "primary_investment_asset_class": {"type": "string", "description": "Main asset class focus (PE, VC, Infrastructure, etc.)"},
         "sector_focus": {"type": "array", "description": "List of industry sectors the firm focuses on"},
         "geographic_focus": {"type": "array", "description": "Regions where the firm invests"},
         "stage_preference": {"type": "array", "description": "Investment stages (early, growth, buyout, etc.)"},
-        "typical_check_size": {"type": "string", "description": "Range of typical investment amounts"}
+        "typical_check_size": {"type": "string", "description": "Range of typical investment amounts"},
     },
-    
     "fund_information": {
         "current_funds_active": {"type": "array", "description": "List of currently active funds"},
         "historical_fund_performance": {"type": "string", "description": "Historical performance metrics and track record"},
         "recent_fund_launches": {"type": "array", "description": "Details of recent fund launches"},
         "recent_fund_closures": {"type": "array", "description": "Details of recent fund closures"},
-        "recent_fund_activities": {"type": "array", "description": "Other recent fund activities and developments"}
+        "recent_fund_activities": {"type": "array", "description": "Other recent fund activities and developments"},
     },
-    
     "capital_commitments": {
         "recent_LP_commitments": {"type": "array", "description": "Recent limited partner commitments"},
         "co_investment_activity": {"type": "string", "description": "Details on co-investment approach and recent activity"},
         "history_investing_similar_funds": {"type": "string", "description": "Track record in similar investments"},
-        "public_RFPs_allocation_shifts": {"type": "string", "description": "Information on public RFPs or allocation changes"}
+        "public_RFPs_allocation_shifts": {"type": "string", "description": "Information on public RFPs or allocation changes"},
     },
-    
     "assets_under_management": {
         "aum": {"type": "string", "description": "Total assets under management"},
         "breakdown_by_asset_class": {"type": "array", "description": "AUM breakdown by asset class"},
         "growth_decline_trajectory": {"type": "string", "description": "Year-over-year trend in AUM"},
-        "esg_dei_strategy": {"type": "string", "description": "ESG and DEI investment focus including sustainable investing strategies"}
+        "esg_dei_strategy": {"type": "string", "description": "ESG and DEI investment focus including sustainable investing strategies"},
     },
-    
     "organization_decision_making": {
         "key_decision_makers": {"type": "array", "description": "List of key decision makers"},
         "firm_ownership_type": {"type": "string", "description": "Ownership structure of the firm"},
         "recent_leadership_changes": {"type": "array", "description": "Details of recent leadership changes"},
-        "parent_company": {"type": "object", "description": "Parent company information"}
+        "parent_company": {"type": "object", "description": "Parent company information"},
     },
-    
     "distribution_coverage_relevance": {
         "target_investor_type": {"type": "array", "description": "Types of investors targeted"},
         "regulated_by": {"type": "array", "description": "Regulatory bodies overseeing the firm"},
-        "litigation_regulatory_risk": {"type": "array", "description": "Litigation, investigations, or regulatory risk against the firm"}
+        "litigation_regulatory_risk": {"type": "array", "description": "Litigation, investigations, or regulatory risk against the firm"},
     },
-    
     "activity_sentiment_signals": {
         "recent_hires": {"type": "array", "description": "Information about recent hires"},
         "new_offices_expansion": {"type": "array", "description": "Details of physical expansion"},
         "M&A_fund_spinouts": {"type": "array", "description": "Information on M&A activity or fund spin-outs"},
         "awards_events_press": {"type": "array", "description": "Notable recognition or media coverage"},
         "public_fund_strategy_commentary": {"type": "string", "description": "Recent public statements about fund strategy"},
-        "competitive_landscape": {"type": "array", "description": "Information about competitors"}
-    }
-} 
+        "competitive_landscape": {"type": "array", "description": "Information about competitors"},
+    },
+}

--- app/scripts/embed_profiles_pgvector.py
+++ app/scripts/embed_profiles_pgvector.py
@@ -37,7 +37,8 @@
 
 # Load environment variables from .env file FIRST (before importing config)
 from dotenv import load_dotenv
-env_path = Path(__file__).parent.parent.parent / '.env'
+
+env_path = Path(__file__).parent.parent.parent / ".env"
 load_dotenv(env_path)
 
 import psycopg
@@ -80,33 +81,33 @@
             investor_profile = {}
     else:
         investor_profile = {}
-    
+
     profile_parts = []
-    
+
     # 1. Firm Description (with fallback to top-level column)
     firm_desc = investor_profile.get("firm_description") or row.get("firm_description")
     if firm_desc:
         profile_parts.append(f"Firm Description: {firm_desc}")
-    
+
     # 2. Key Objectives (with fallback to top-level column)
     key_obj = investor_profile.get("key_objectives") or row.get("key_objectives")
     if key_obj:
         profile_parts.append(f"Key Objectives: {key_obj}")
-    
+
     # 3. Geographic Focus (from JSONB only, handle list format)
     geo_focus = investor_profile.get("geographic_focus")
     if geo_focus:
         if isinstance(geo_focus, list):
             geo_focus = ", ".join(geo_focus)
         profile_parts.append(f"Geographic Focus: {geo_focus}")
-    
+
     # 4. Sector Focus (from JSONB only, handle list format)
     sector_focus = investor_profile.get("sector_focus")
     if sector_focus:
         if isinstance(sector_focus, list):
             sector_focus = ", ".join(sector_focus)
         profile_parts.append(f"Sector Focus: {sector_focus}")
-    
+
     # 5. LP Target Types (from JSONB only, with proper formatting)
     lp_target_types = investor_profile.get("lp_target_types")
     if lp_target_types:
@@ -119,9 +120,9 @@
                 formatted_types = f"{', '.join(lp_target_types[:-1])}, and {lp_target_types[-1]}"
         else:
             formatted_types = str(lp_target_types)
-        
+
         profile_parts.append(f"Seeking LP Types: {formatted_types}")
-    
+
     return " | ".join(profile_parts)
 
 
@@ -188,13 +189,13 @@
         client = openai.AsyncOpenAI(api_key=api_key)
         response = await client.embeddings.create(
             model="text-embedding-3-large",
-            input=texts  # OpenAI API supports batch processing
+            input=texts,  # OpenAI API supports batch processing
         )
-        
+
         # Extract embeddings from response
         embeddings = [item.embedding for item in response.data]
         return embeddings
-        
+
     except Exception as e:
         print(f"‚ùå Embedding generation failed: {e}")
         raise
@@ -210,8 +211,7 @@
     version: str,
     model: str,
 ):
-    sql = (
-        """
+    sql = """
         INSERT INTO profiles_embeddings (
             user_id, embedding_purpose, format_version, model_name,
             profile_text, profile_text_hash, embedding, updated_at
@@ -225,7 +225,6 @@
             embedding = EXCLUDED.embedding,
             updated_at = NOW();
         """
-    )
     with conn.cursor() as cur:
         for row, text, h, vec in zip(rows, texts, hashes, vectors):
             cur.execute(
@@ -244,9 +243,7 @@
 
 
 def needs_embedding(conn: psycopg.Connection, user_id: str, purpose: str, version: str, h: str) -> bool:
-    sql = (
-        "SELECT 1 FROM profiles_embeddings WHERE user_id=%s AND embedding_purpose=%s AND format_version=%s AND profile_text_hash=%s"
-    )
+    sql = "SELECT 1 FROM profiles_embeddings WHERE user_id=%s AND embedding_purpose=%s AND format_version=%s AND profile_text_hash=%s"
     with conn.cursor() as cur:
         cur.execute(sql, (user_id, purpose, version, h))
         return cur.fetchone() is None
@@ -274,7 +271,7 @@
     print(f"   Port: {get_postgres_port()}")
     print(f"   Database: {get_postgres_db()}")
     print(f"   User: {get_postgres_user()}")
-    
+
     try:
         conn = get_conn()
         print(f"‚úÖ Connected successfully")
@@ -323,7 +320,7 @@
 
         if args.dry_run:
             processed += len(batch)
-            print(f"üß™ Dry-run batch {i//args.batch_size+1}: total={len(batch)} to_embed={len(todo_rows)} skipped={len(batch)-len(todo_rows)}")
+            print(f"üß™ Dry-run batch {i // args.batch_size + 1}: total={len(batch)} to_embed={len(todo_rows)} skipped={len(batch) - len(todo_rows)}")
             continue
 
         if todo_rows:
@@ -342,12 +339,12 @@
 
         processed += len(batch)
         print(
-            f"üìà Progress: {processed}/{total} | upserts={upserts} skipped={skipped} | batches={i//args.batch_size+1}/{(total+args.batch_size-1)//args.batch_size}"
+            f"üìà Progress: {processed}/{total} | upserts={upserts} skipped={skipped} | batches={i // args.batch_size + 1}/{(total + args.batch_size - 1) // args.batch_size}"
         )
 
     dur = time.time() - start
     print(f"‚úÖ Done. processed={processed} upserts={upserts} skipped={skipped} duration={dur:.2f}s")
-    
+
     # Create indexes after embeddings are inserted (makes queries faster)
     if not args.dry_run and upserts > 0:
         print(f"\nüìä Creating indexes for faster queries...")
@@ -356,7 +353,7 @@
             cur.execute("SELECT COUNT(*) as count FROM profiles_embeddings")
             row = cur.fetchone()
             row_count = row["count"] if row else 0
-            
+
             # Vector similarity search index (ivfflat for cosine similarity)
             # Only create if we have enough data (pgvector recommends min 10 rows)
             if row_count >= 10:
@@ -378,7 +375,7 @@
                     conn.rollback()
             else:
                 print(f"‚è≠Ô∏è  Skipping vector index (need ‚â•10 rows, have {row_count})")
-            
+
             # Filter index for embedding_purpose + format_version queries
             try:
                 cur.execute("""
@@ -394,5 +391,3 @@
 
 if __name__ == "__main__":
     asyncio.run(main())
-
-

--- app/security/clerk_tokens.py
+++ app/security/clerk_tokens.py
@@ -48,14 +48,16 @@
         issuer = get_clerk_issuer()
         audience = get_clerk_audience()
 
-        missing = [name for name, value in (
-            ("CLERK_JWKS_URL", jwks_url),
-            ("CLERK_ISSUER", issuer),
-        ) if not value]
-        if missing:
-            raise ClerkConfigurationError(
-                f"Missing Clerk configuration value(s): {', '.join(missing)}"
+        missing = [
+            name
+            for name, value in (
+                ("CLERK_JWKS_URL", jwks_url),
+                ("CLERK_ISSUER", issuer),
             )
+            if not value
+        ]
+        if missing:
+            raise ClerkConfigurationError(f"Missing Clerk configuration value(s): {', '.join(missing)}")
 
         return cls(jwks_url=jwks_url, issuer=issuer, audience=audience)
 
@@ -71,14 +73,16 @@
     issuer = get_lp_clerk_issuer()
     audience = get_lp_clerk_audience()
 
-    missing = [name for name, value in (
-        ("LP_CLERK_JWKS_URL", jwks_url),
-        ("LP_CLERK_ISSUER", issuer),
-    ) if not value]
+    missing = [
+        name
+        for name, value in (
+            ("LP_CLERK_JWKS_URL", jwks_url),
+            ("LP_CLERK_ISSUER", issuer),
+        )
+        if not value
+    ]
     if missing:
-        raise ClerkConfigurationError(
-            f"Missing LP Clerk configuration value(s): {', '.join(missing)}"
-        )
+        raise ClerkConfigurationError(f"Missing LP Clerk configuration value(s): {', '.join(missing)}")
 
     return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)
 
@@ -102,6 +106,7 @@
     if http_get is not None:
         fetcher = http_get
     else:
+
         def fetcher(url: str) -> Dict[str, Any]:
             logger.info(f"üîç Fetching JWKS from: {url}")
             try:

--- app/services/auth_service.py
+++ app/services/auth_service.py
@@ -104,7 +104,7 @@
         background_tasks=None,
     ) -> UserProfileResponse:
         """Create a new profile for the supplied Clerk user.
-        
+
         If a profile already exists but has NULL/empty values for required fields
         (from legacy data or Clerk sync with partial data), it will be updated
         instead of raising an error.
@@ -122,11 +122,7 @@
             )
             if existing:
                 # Check if profile is empty (created by _ensure_profile with NULL values)
-                is_empty = (
-                    not existing.get("firm_description") and
-                    not existing.get("key_differentiators") and
-                    not existing.get("key_objectives")
-                )
+                is_empty = not existing.get("firm_description") and not existing.get("key_differentiators") and not existing.get("key_objectives")
                 if is_empty:
                     # Profile exists but is empty - update it instead of failing
                     return await self.update_user_profile(user_id, profile, db, background_tasks)
@@ -181,7 +177,7 @@
                     "SELECT firm_name FROM users WHERE user_id = $1",
                     user_id,
                 )
-            
+
             # Update city and country in users table
             # Normalize empty strings to None, then use COALESCE to preserve existing values
             city_value = (profile.city or "").strip() or None
@@ -221,7 +217,7 @@
                 profile.last_name,
                 now,
             )
-            
+
             # Trigger profile embedding in background (if enabled)
             if background_tasks is not None:
                 # Fetch the created profile row for embedding
@@ -234,22 +230,12 @@
                     user_id,
                 )
                 if profile_row:
-                    from app.services.profile_embedding_service import (
-                        upsert_profile_embedding,
-                        upsert_profile_embedding_for_feedback
-                    )
+                    from app.services.profile_embedding_service import upsert_profile_embedding, upsert_profile_embedding_for_feedback
+
                     profile_row_dict = dict(profile_row)
                     # Create both investor_search and feedback_analysis embeddings
-                    background_tasks.add_task(
-                        upsert_profile_embedding,
-                        user_id=user_id,
-                        profile_row=profile_row_dict
-                    )
-                    background_tasks.add_task(
-                        upsert_profile_embedding_for_feedback,
-                        user_id=user_id,
-                        profile_row=profile_row_dict
-                    )
+                    background_tasks.add_task(upsert_profile_embedding, user_id=user_id, profile_row=profile_row_dict)
+                    background_tasks.add_task(upsert_profile_embedding_for_feedback, user_id=user_id, profile_row=profile_row_dict)
 
         return UserProfileResponse(
             user_id=user_id,
@@ -331,10 +317,7 @@
                 now,
                 user_id,
             )
-            logger.info(
-                "Profile database updated",
-                extra={"user_id": user_id, "updated_at": now.isoformat()}
-            )
+            logger.info("Profile database updated", extra={"user_id": user_id, "updated_at": now.isoformat()})
 
             firm_name_value = (profile.firm_name or "").strip() or None
             if firm_name_value:
@@ -354,7 +337,7 @@
                     "SELECT firm_name FROM users WHERE user_id = $1",
                     user_id,
                 )
-            
+
             # Update city and country in users table
             # Normalize empty strings to None, then use COALESCE to preserve existing values
             city_value = (profile.city or "").strip() or None
@@ -372,7 +355,7 @@
                 now,
                 user_id,
             )
-            
+
             # Fetch city/country from users table to get actual saved values
             user_row = await conn.fetchrow(
                 "SELECT city, country FROM users WHERE user_id = $1",
@@ -380,7 +363,7 @@
             )
             city_value = user_row.get("city") if user_row else profile.city
             country_value = user_row.get("country") if user_row else profile.country
-            
+
             # Trigger profile embedding in background (if enabled)
             if background_tasks is not None:
                 # Fetch the updated profile row for embedding
@@ -393,58 +376,29 @@
                     user_id,
                 )
                 if profile_row:
-                    from app.services.profile_embedding_service import (
-                        upsert_profile_embedding,
-                        upsert_profile_embedding_for_feedback
-                    )
+                    from app.services.profile_embedding_service import upsert_profile_embedding, upsert_profile_embedding_for_feedback
+
                     profile_row_dict = dict(profile_row)
                     # Create both investor_search and feedback_analysis embeddings
-                    background_tasks.add_task(
-                        upsert_profile_embedding,
-                        user_id=user_id,
-                        profile_row=profile_row_dict
-                    )
-                    background_tasks.add_task(
-                        upsert_profile_embedding_for_feedback,
-                        user_id=user_id,
-                        profile_row=profile_row_dict
-                    )
-                    logger.info(
-                        "Profile embedding background tasks queued (investor_search and feedback_analysis)",
-                        extra={"user_id": user_id}
-                    )
-                    
+                    background_tasks.add_task(upsert_profile_embedding, user_id=user_id, profile_row=profile_row_dict)
+                    background_tasks.add_task(upsert_profile_embedding_for_feedback, user_id=user_id, profile_row=profile_row_dict)
+                    logger.info("Profile embedding background tasks queued (investor_search and feedback_analysis)", extra={"user_id": user_id})
+
                     # Also regenerate all radar context embeddings for this user
                     try:
                         from app.services.radar_context_embedding_service import regenerate_all_user_radar_embeddings
-                        background_tasks.add_task(
-                            regenerate_all_user_radar_embeddings,
-                            user_id=user_id
-                        )
-                        logger.info(
-                            "Radar context embedding regeneration queued",
-                            extra={"user_id": user_id}
-                        )
+
+                        background_tasks.add_task(regenerate_all_user_radar_embeddings, user_id=user_id)
+                        logger.info("Radar context embedding regeneration queued", extra={"user_id": user_id})
                     except Exception as e:
-                        logger.warning(
-                            f"Failed to queue radar context embedding regeneration for user {user_id}: {e}",
-                            exc_info=True
-                        )
+                        logger.warning(f"Failed to queue radar context embedding regeneration for user {user_id}: {e}", exc_info=True)
                 else:
-                    logger.warning(
-                        "Profile row not found after update, skipping embedding task",
-                        extra={"user_id": user_id}
-                    )
+                    logger.warning("Profile row not found after update, skipping embedding task", extra={"user_id": user_id})
             else:
-                logger.debug(
-                    "Background tasks not available, skipping profile embedding",
-                    extra={"user_id": user_id}
-                )
+                logger.debug("Background tasks not available, skipping profile embedding", extra={"user_id": user_id})
 
         investor_profile = (
-            profile.investor_profile
-            if profile.investor_profile is not None
-            else self._normalize_investor_profile(existing.get("investor_profile"))
+            profile.investor_profile if profile.investor_profile is not None else self._normalize_investor_profile(existing.get("investor_profile"))
         )
 
         return UserProfileResponse(

--- app/services/email_service.py
+++ app/services/email_service.py
@@ -1,4 +1,5 @@
 from dotenv import load_dotenv
+
 load_dotenv()
 import os
 import httpx
@@ -13,6 +14,7 @@
 APP_NAME = get_app_name()
 BACKEND_URL = get_backend_url()
 
+
 async def send_password_reset_email(to_email: str, code: str):
     if not MAILGUN_API_KEY or not MAILGUN_DOMAIN:
         raise RuntimeError("Mailgun API key or domain not set in environment variables.")
@@ -56,14 +58,11 @@
         "html": html,
     }
     async with httpx.AsyncClient() as client:
-        response = await client.post(
-            MAILGUN_BASE_URL,
-            auth=("api", MAILGUN_API_KEY),
-            data=data
-        )
+        response = await client.post(MAILGUN_BASE_URL, auth=("api", MAILGUN_API_KEY), data=data)
         if response.status_code != 200:
             raise RuntimeError(f"Mailgun email failed: {response.status_code} {response.text}")
 
+
 async def send_verification_email(to_email: str, token: str):
     if not MAILGUN_API_KEY or not MAILGUN_DOMAIN:
         raise RuntimeError("Mailgun API key or domain not set in environment variables.")
@@ -107,11 +106,7 @@
         "html": html,
     }
     async with httpx.AsyncClient() as client:
-        response = await client.post(
-            MAILGUN_BASE_URL,
-            auth=("api", MAILGUN_API_KEY),
-            data=data
-        )
+        response = await client.post(MAILGUN_BASE_URL, auth=("api", MAILGUN_API_KEY), data=data)
         if response.status_code != 200:
             raise RuntimeError(f"Mailgun email failed: {response.status_code} {response.text}")
 
@@ -119,35 +114,35 @@
 def _extract_finding_preview(finding_data: Dict[str, Any], radar_type: str) -> str:
     """
     Extract finding preview text based on radar type.
-    
+
     Args:
         finding_data: The finding_data JSONB from database
         radar_type: Type of radar (e.g., 'company_new_hires', 'company_mentions')
-        
+
     Returns:
         Full preview text (not truncated)
     """
     preview_parts = []
-    
-    if radar_type in ['company_mentions', 'company_social_posts', 'company_social_posts_cxo']:
+
+    if radar_type in ["company_mentions", "company_social_posts", "company_social_posts_cxo"]:
         # Social posts and mentions - use post/mention content
-        content = finding_data.get('mention_content') or finding_data.get('post_content', '')
+        content = finding_data.get("mention_content") or finding_data.get("post_content", "")
         if content:
             preview_parts.append(str(content))
-    elif radar_type == 'company_new_hires':
+    elif radar_type == "company_new_hires":
         # New hires - format as "Title in Department"
-        title = finding_data.get('title', '')
-        department = finding_data.get('department', '')
+        title = finding_data.get("title", "")
+        department = finding_data.get("department", "")
         if title and department:
             preview_parts.append(f"{title} in {department}")
         elif title:
             preview_parts.append(title)
         elif department:
             preview_parts.append(department)
-    elif radar_type == 'company_job_openings':
+    elif radar_type == "company_job_openings":
         # Job openings - title and description
-        title = finding_data.get('title', '')
-        description = finding_data.get('description', '')
+        title = finding_data.get("title", "")
+        description = finding_data.get("description", "")
         if title:
             preview_parts.append(title)
         if description:
@@ -155,14 +150,15 @@
     else:
         # Fallback: stringify finding_data and take first 200 chars
         import json
+
         preview_parts.append(json.dumps(finding_data, indent=2))
-    
+
     preview = " ".join(preview_parts).strip()
-    
+
     # Limit to 200 characters for preview
     if len(preview) > 200:
         preview = preview[:197] + "..."
-    
+
     return preview if preview else "View finding details on your watchlist."
 
 
@@ -178,7 +174,7 @@
 ) -> None:
     """
     Send Tier 1 radar alert email to user.
-    
+
     Args:
         to_email: Recipient email address
         company_name: Name of the company being tracked
@@ -190,23 +186,23 @@
     """
     if not MAILGUN_API_KEY or not MAILGUN_DOMAIN:
         raise RuntimeError("Mailgun API key or domain not set in environment variables.")
-    
+
     frontend_url = get_frontend_url()
-    
+
     # Build watchlist URL
     if radar_id:
         watchlist_url = f"{frontend_url}/watchlist?finding={finding_id}&radar={radar_id}"
     else:
         watchlist_url = f"{frontend_url}/watchlist?finding={finding_id}"
-    
+
     # Build preferences URL
     preferences_url = f"{frontend_url}/settings/notifications"
-    
+
     # Build greeting with first name if available
     greeting = f"Hello {user_first_name}," if user_first_name else "Hello,"
-    
+
     subject = f"Tier 1 Alert: {company_name} - {category}"
-    
+
     text = f"""
 {greeting}
 
@@ -229,7 +225,7 @@
 You're receiving this because you enabled High-Priority Signals email alerts for {company_name}.
 Manage your email preferences: {preferences_url}
 """
-    
+
     html = f"""
     <!DOCTYPE html>
     <html>
@@ -289,7 +285,7 @@
       </body>
     </html>
     """
-    
+
     data = {
         "from": f"Ardessa <alerts@{MAILGUN_DOMAIN}>",
         "to": [to_email],
@@ -297,15 +293,12 @@
         "text": text,
         "html": html,
     }
-    
+
     async with httpx.AsyncClient() as client:
-        response = await client.post(
-            MAILGUN_BASE_URL,
-            auth=("api", MAILGUN_API_KEY),
-            data=data
-        )
+        response = await client.post(MAILGUN_BASE_URL, auth=("api", MAILGUN_API_KEY), data=data)
         if response.status_code != 200:
-            raise RuntimeError(f"Mailgun email failed: {response.status_code} {response.text}") 
+            raise RuntimeError(f"Mailgun email failed: {response.status_code} {response.text}")
+
 
 async def send_weekly_wrapup_email(
     to_email: str,
@@ -316,7 +309,7 @@
 ) -> None:
     """
     Send weekly wrap-up email to user.
-    
+
     Args:
         to_email: Recipient email address
         user_name: Optional user name for personalization
@@ -332,39 +325,39 @@
     """
     if not MAILGUN_API_KEY or not MAILGUN_DOMAIN:
         raise RuntimeError("Mailgun API key or domain not set in environment variables.")
-    
+
     frontend_url = get_frontend_url()
-    
+
     # Build watchlist URL (same as Tier 1 email)
     watchlist_url = f"{frontend_url}/watchlist"
-    
+
     # Build preferences URL (same as Tier 1 email)
     preferences_url = f"{frontend_url}/settings/notifications"
-    
+
     # Format date range
     date_range = f"{week_start.strftime('%B %d')} - {week_end.strftime('%B %d, %Y')}"
-    
+
     # Build greeting - use first name if available, otherwise full name
     if user_name:
         first_name = user_name.split()[0] if user_name else None
         greeting = f"Hello {first_name}," if first_name else f"Hello{', ' + user_name},"
     else:
         greeting = "Hello,"
-    
+
     # Build company sections
     company_sections_html = ""
     company_sections_text = ""
-    
+
     for company in companies_with_summaries:
         company_name = company.get("company_name", company.get("domain", "Company"))
         domain = company.get("domain", "")
         summary = company.get("summary", "")
         tier1_count = company.get("tier1_count", 0)
         tier2_count = company.get("tier2_count", 0)
-        
+
         # Build company watchlist URL
         company_watchlist_url = f"{watchlist_url}?domain={domain}" if domain else watchlist_url
-        
+
         # Stats text
         stats_parts = []
         if tier1_count > 0:
@@ -372,7 +365,7 @@
         if tier2_count > 0:
             stats_parts.append(f"{tier2_count} Tier 2 update{'s' if tier2_count != 1 else ''}")
         stats_text = " ‚Ä¢ ".join(stats_parts) if stats_parts else "Updates"
-        
+
         # HTML section
         company_sections_html += f"""
           <div style='border: 1px solid #e0e0e0; border-radius: 6px; padding: 20px; margin-bottom: 24px;'>
@@ -384,7 +377,7 @@
             </a>
           </div>
         """
-        
+
         # Text section
         company_sections_text += f"""
 {company_name}
@@ -396,16 +389,16 @@
 View on Watchlist: {company_watchlist_url}
 
 """
-    
+
     total_companies = len(companies_with_summaries)
     subject = f"Your Weekly Radar Update - {date_range}"
-    
+
     text = f"""
 {greeting}
 
 Here's your weekly update from {APP_NAME} covering {date_range}.
 
-You have updates from {total_companies} compan{'y' if total_companies == 1 else 'ies'}:
+You have updates from {total_companies} compan{"y" if total_companies == 1 else "ies"}:
 
 {company_sections_text}
 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
@@ -418,7 +411,7 @@
 Thank you,
 The {APP_NAME} Team
 """
-    
+
     html = f"""
     <!DOCTYPE html>
     <html>
@@ -441,7 +434,7 @@
           
           <p style='color: #555; line-height: 1.6; margin: 0 0 32px 0;'>
             Here's your weekly update covering <strong>{date_range}</strong>.
-            You have updates from <strong>{total_companies} compan{'y' if total_companies == 1 else 'ies'}</strong>.
+            You have updates from <strong>{total_companies} compan{"y" if total_companies == 1 else "ies"}</strong>.
           </p>
 
           <!-- Company Sections -->
@@ -472,7 +465,7 @@
       </body>
     </html>
     """
-    
+
     data = {
         "from": f"{APP_NAME} <alerts@{MAILGUN_DOMAIN}>",
         "to": [to_email],
@@ -480,12 +473,8 @@
         "text": text,
         "html": html,
     }
-    
+
     async with httpx.AsyncClient() as client:
-        response = await client.post(
-            MAILGUN_BASE_URL,
-            auth=("api", MAILGUN_API_KEY),
-            data=data
-        )
+        response = await client.post(MAILGUN_BASE_URL, auth=("api", MAILGUN_API_KEY), data=data)
         if response.status_code != 200:
             raise RuntimeError(f"Mailgun email failed: {response.status_code} {response.text}")

--- app/services/excel_export_service.py
+++ app/services/excel_export_service.py
@@ -1,10 +1,8 @@
 from typing import Dict, Any, List, Optional
 import io
 from app.utils.db_utils import ProspectingDB
-from app.tools.json_to_excel import (
-    convert_company_enrichment_json_to_excel,
-    convert_person_enrichment_json_to_excel
-)
+from app.tools.json_to_excel import convert_company_enrichment_json_to_excel, convert_person_enrichment_json_to_excel
+
 
 class ExcelExportService:
     def __init__(self, db: ProspectingDB):
@@ -13,11 +11,11 @@
     async def generate_company_excel(self, run_id: str, user_id: str) -> Optional[bytes]:
         """
         Generate Excel file for company enrichment data.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
-            
+
         Returns:
             Excel file as bytes or None if data not found
         """
@@ -45,11 +43,11 @@
     async def generate_person_excel(self, run_id: str, user_id: str) -> Optional[bytes]:
         """
         Generate Excel file for person enrichment data.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
-            
+
         Returns:
             Excel file as bytes or None if data not found
         """
@@ -67,4 +65,4 @@
 
         except Exception as e:
             print(f"‚ùå Error generating person Excel: {str(e)}")
-            return None 
\ No newline at end of file
+            return None

--- app/services/feedback_collection_service.py
+++ app/services/feedback_collection_service.py
@@ -12,26 +12,19 @@
 from qdrant_client.models import Filter, FieldCondition, MatchValue
 
 from app.services.vector_search.fund_lp_engine import FundLPMatchingEngine
-from app.utils.config import (
-    get_qdrant_url_search_rec,
-    get_qdrant_api_key_search_rec,
-    get_qdrant_collection
-)
+from app.utils.config import get_qdrant_url_search_rec, get_qdrant_api_key_search_rec, get_qdrant_collection
 
 logger = logging.getLogger(__name__)
 
 
-async def get_company_embedding_from_qdrant(
-    company_domain: str,
-    collection_name: Optional[str] = None
-) -> Optional[List[float]]:
+async def get_company_embedding_from_qdrant(company_domain: str, collection_name: Optional[str] = None) -> Optional[List[float]]:
     """
     Retrieve company embedding from Qdrant by domain.
-    
+
     Args:
         company_domain: Company domain (e.g., "example.com")
         collection_name: Qdrant collection name (from config if not provided)
-        
+
     Returns:
         Company embedding vector or None if not found
     """
@@ -39,58 +32,50 @@
         qdrant_url = get_qdrant_url_search_rec()
         qdrant_api_key = get_qdrant_api_key_search_rec()
         collection = collection_name or get_qdrant_collection()
-        
+
         client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key, timeout=60)
-        
+
         # Search for company by domain in payload
         # Try different possible field names
         domain_variations = [company_domain]
         if not company_domain.startswith("www."):
             domain_variations.append(f"www.{company_domain}")
-        
+
         for domain in domain_variations:
             try:
                 # Try domain field
                 results = client.scroll(
                     collection_name=collection,
-                    scroll_filter=Filter(
-                        must=[
-                            FieldCondition(key="domain", match=MatchValue(value=domain))
-                        ]
-                    ),
+                    scroll_filter=Filter(must=[FieldCondition(key="domain", match=MatchValue(value=domain))]),
                     limit=1,
                     with_vectors=True,
-                    with_payload=True
+                    with_payload=True,
                 )
                 points, _ = results
-                
+
                 if points:
                     return points[0].vector
-                
+
                 # Try company_domain field
                 results = client.scroll(
                     collection_name=collection,
-                    scroll_filter=Filter(
-                        must=[
-                            FieldCondition(key="company_domain", match=MatchValue(value=domain))
-                        ]
-                    ),
+                    scroll_filter=Filter(must=[FieldCondition(key="company_domain", match=MatchValue(value=domain))]),
                     limit=1,
                     with_vectors=True,
-                    with_payload=True
+                    with_payload=True,
                 )
                 points, _ = results
-                
+
                 if points:
                     return points[0].vector
-                    
+
             except Exception as e:
                 logger.debug(f"Error searching for domain {domain}: {e}")
                 continue
-        
+
         logger.warning(f"Company embedding not found for domain: {company_domain}")
         return None
-        
+
     except Exception as e:
         logger.error(f"Failed to retrieve company embedding: {e}", exc_info=True)
         return None
@@ -106,17 +91,17 @@
     company_domain: str,
     ranking_position: Optional[int],
     qdrant_filters_applied: Optional[Dict[str, Any]],
-    db
+    db,
 ) -> bool:
     """
     Collect feedback data when user selects a company from search results.
-    
+
     This runs as a background task and collects:
     - Query embedding (pure prompt, for analysis)
     - Company embedding (from Qdrant)
     - Profile embedding (optional, for collaborative filtering)
     - Selection context (ranking, filters)
-    
+
     Args:
         run_id: Search run ID
         user_id: User ID
@@ -129,34 +114,27 @@
         filters_applied: Filters that were active during search
         db: Database connection pool
         profile_embedding: Optional user profile embedding (will fetch if None and user has profile)
-        
+
     Returns:
         True if successful, False otherwise
     """
     try:
-        logger.info(
-            f"Collecting feedback for company selection",
-            extra={
-                "run_id": run_id,
-                "user_id": user_id,
-                "company_domain": company_domain
-            }
-        )
-        
+        logger.info(f"Collecting feedback for company selection", extra={"run_id": run_id, "user_id": user_id, "company_domain": company_domain})
+
         # Generate query embedding (pure prompt, for analysis)
         engine = FundLPMatchingEngine()
         query_embedding = await engine.get_query_embedding(query_text)
-        
+
         # Fetch company embedding from Qdrant
         company_embedding = await get_company_embedding_from_qdrant(company_domain)
-        
+
         # Prepare qdrant_filters_applied JSONB
         filters_jsonb = json.dumps(qdrant_filters_applied) if qdrant_filters_applied else None
-        
+
         # Convert embeddings to string format for asyncpg (pgvector format: [0.1,0.2,0.3])
         query_embedding_str = "[" + ",".join(str(x) for x in query_embedding) + "]" if query_embedding else None
         company_embedding_str = "[" + ",".join(str(x) for x in company_embedding) + "]" if company_embedding else None
-        
+
         # Insert feedback record
         async with db.pool.acquire() as conn:
             await conn.execute(
@@ -187,29 +165,17 @@
                 company_domain,
                 company_embedding_str,
                 ranking_position,
-                filters_jsonb
+                filters_jsonb,
             )
-        
-        logger.info(
-            f"Feedback collected successfully",
-            extra={
-                "run_id": run_id,
-                "user_id": user_id,
-                "company_domain": company_domain
-            }
-        )
+
+        logger.info(f"Feedback collected successfully", extra={"run_id": run_id, "user_id": user_id, "company_domain": company_domain})
         return True
-        
+
     except Exception as e:
         logger.error(
             f"Failed to collect feedback",
             exc_info=True,
-            extra={
-                "run_id": run_id,
-                "user_id": user_id,
-                "company_domain": company_domain,
-                "error": str(e)
-            }
+            extra={"run_id": run_id, "user_id": user_id, "company_domain": company_domain, "error": str(e)},
         )
         return False
 
@@ -217,30 +183,30 @@
 async def update_feedback_type(
     feedback_id: int,
     feedback_type: str,  # 'thumbs_up', 'thumbs_down', 'saved'
-    db
+    db,
 ) -> bool:
     """
     Update feedback type (add to JSONB array with mutual exclusivity handling).
-    
+
     Rules:
     - 'thumbs_up' and 'thumbs_down' are mutually exclusive (clicking one removes the other)
     - 'saved' can coexist with either thumbs_up or thumbs_down
     - 'viewed' is always present (added automatically when company is selected)
-    
+
     Args:
         feedback_id: Feedback record ID
         feedback_type: Type to add ('thumbs_up', 'thumbs_down', 'saved')
         db: Database connection pool
-        
+
     Returns:
         True if successful, False otherwise
     """
     try:
         async with db.pool.acquire() as conn:
-            if feedback_type in ['thumbs_up', 'thumbs_down']:
+            if feedback_type in ["thumbs_up", "thumbs_down"]:
                 # Handle mutual exclusivity: remove the opposite type
-                opposite_type = 'thumbs_down' if feedback_type == 'thumbs_up' else 'thumbs_up'
-                
+                opposite_type = "thumbs_down" if feedback_type == "thumbs_up" else "thumbs_up"
+
                 # Remove opposite type and add new type in one operation
                 await conn.execute(
                     """
@@ -256,9 +222,9 @@
                     """,
                     json.dumps(opposite_type),
                     json.dumps([feedback_type]),
-                    feedback_id
+                    feedback_id,
                 )
-            elif feedback_type == 'unsaved':
+            elif feedback_type == "unsaved":
                 # Remove 'saved' from feedback_types array
                 await conn.execute(
                     """
@@ -271,7 +237,7 @@
                         feedback_updated_at = NOW()
                     WHERE id = $1
                     """,
-                    feedback_id
+                    feedback_id,
                 )
             else:
                 # For 'saved' or other types, just add if not present
@@ -284,12 +250,12 @@
                       AND NOT (feedback_types @> $1::jsonb)
                     """,
                     json.dumps([feedback_type]),
-                    feedback_id
+                    feedback_id,
                 )
-        
+
         logger.info(f"Updated feedback type: {feedback_type} for feedback_id: {feedback_id}")
         return True
-        
+
     except Exception as e:
         logger.error(f"Failed to update feedback type: {e}", exc_info=True)
         return False
@@ -304,13 +270,13 @@
     saved_from_query_text: Optional[str],
     category: Optional[str],
     notes: Optional[str],
-    db
+    db,
 ) -> bool:
     """
     Save company to user's favorites.
-    
+
     Creates record in user_saved_companies table.
-    
+
     Args:
         user_id: User ID
         company_id: Company identifier
@@ -321,14 +287,14 @@
         category: User-defined category
         notes: User notes
         db: Database connection pool
-        
+
     Returns:
         True if successful, False otherwise
     """
     try:
         # Get company embedding
         company_embedding = await get_company_embedding_from_qdrant(company_domain)
-        
+
         async with db.pool.acquire() as conn:
             # Use ON CONFLICT to handle duplicate saves (update if exists)
             await conn.execute(
@@ -362,51 +328,47 @@
                 saved_from_run_id,
                 saved_from_query_text,
                 category,
-                notes
+                notes,
             )
-        
+
         logger.info(f"Company saved to favorites: {company_domain} for user: {user_id}")
         return True
-        
+
     except Exception as e:
         logger.error(f"Failed to save company to favorites: {e}", exc_info=True)
         return False
 
 
-async def archive_saved_company(
-    user_id: str,
-    db,
-    company_id: Optional[str] = None,
-    company_domain: Optional[str] = None
-) -> bool:
+async def archive_saved_company(user_id: str, db, company_id: Optional[str] = None, company_domain: Optional[str] = None) -> bool:
     """
     Archive (unsave) a company from user's favorites.
-    
+
     Sets is_archived = TRUE and archived_at = NOW() in user_saved_companies table.
-    
+
     Args:
         user_id: User ID
         company_id: Company identifier (optional, but either company_id or company_domain required)
         company_domain: Company domain (optional, but either company_id or company_domain required)
         db: Database connection pool
-        
+
     Returns:
         True if successful, False otherwise
     """
     if not company_id and not company_domain:
         logger.error("archive_saved_company: Either company_id or company_domain must be provided")
         return False
-    
+
     try:
         # Normalize domain if provided (remove protocol, www, paths, lowercase)
         from app.utils.domain_utils import clean_domain
+
         normalized_domain = None
         if company_domain:
             normalized_domain = clean_domain(company_domain).lower().strip()
             if not normalized_domain:
                 logger.warning(f"archive_saved_company: Domain normalization resulted in empty string: {company_domain}")
                 normalized_domain = company_domain.lower().strip()  # Fallback to simple lowercase
-        
+
         async with db.pool.acquire() as conn:
             # Build query to find and archive the saved company
             # Prefer matching by both company_id AND domain for accuracy (handles subsidiaries with same domain)
@@ -433,7 +395,7 @@
                     """,
                     user_id,
                     company_id,
-                    normalized_domain
+                    normalized_domain,
                 )
             elif company_id:
                 # Match by company_id only (unique identifier)
@@ -448,7 +410,7 @@
                       AND is_archived = FALSE
                     """,
                     user_id,
-                    company_id
+                    company_id,
                 )
             elif normalized_domain:
                 # Match by domain only (less specific - could match multiple subsidiaries)
@@ -471,12 +433,12 @@
                       AND is_archived = FALSE
                     """,
                     user_id,
-                    normalized_domain
+                    normalized_domain,
                 )
             else:
                 logger.error("archive_saved_company: Neither company_id nor company_domain provided")
                 return False
-            
+
             # Check if any rows were updated
             # asyncpg returns string like "UPDATE 0" or "UPDATE 1"
             # Extract the number to check if rows were affected
@@ -490,8 +452,8 @@
                         "company_domain_original": company_domain,
                         "company_domain_normalized": normalized_domain,
                         "company_id": company_id,
-                        "match_strategy": "both" if (company_id and normalized_domain) else ("company_id" if company_id else "domain")
-                    }
+                        "match_strategy": "both" if (company_id and normalized_domain) else ("company_id" if company_id else "domain"),
+                    },
                 )
                 # Check if company exists but is already archived
                 # Normalize stored domain for comparison (remove protocol, www, and paths)
@@ -511,7 +473,7 @@
                     """,
                     user_id,
                     company_id or "",
-                    normalized_domain or ""
+                    normalized_domain or "",
                 )
                 if check_row:
                     logger.info(
@@ -521,15 +483,14 @@
                             "found_company_domain": check_row["company_domain"],
                             "found_is_archived": check_row["is_archived"],
                             "requested_company_id": company_id,
-                            "requested_company_domain_normalized": normalized_domain
-                        }
+                            "requested_company_domain_normalized": normalized_domain,
+                        },
                     )
                 return False
-        
+
         logger.info(f"Company archived (unsaved): {company_domain or company_id} for user: {user_id}")
         return True
-        
+
     except Exception as e:
         logger.error(f"Failed to archive saved company: {e}", exc_info=True)
         return False
-

--- app/services/llm_tier_classification_service.py
+++ app/services/llm_tier_classification_service.py
@@ -33,21 +33,18 @@
     if not api_key:
         logger.warning("OpenAI API key not configured (need OPENAI_API_KEY)")
         return None
-    
-    return ChatOpenAI(
-        model="gpt-5-mini",
-        api_key=api_key
-    )
 
+    return ChatOpenAI(model="gpt-5-mini", api_key=api_key)
+
 
 async def classify_finding_tier(finding_text: str, company_name: str = "the company") -> Optional[Dict[str, Any]]:
     """
     Phase 1: Base tier classification (finding only).
-    
+
     Args:
         finding_text: The finding text to classify
         company_name: The name of the company being tracked (default: "the company")
-        
+
     Returns:
         Dict with tier, category, confidence if classification passes threshold,
         None otherwise
@@ -55,16 +52,16 @@
     if not finding_text or not finding_text.strip():
         logger.warning("Empty finding text, skipping classification")
         return None
-    
+
     llm = _get_openai_client()
     if not llm:
         logger.error("OpenAI client not available, skipping classification")
         return None
-    
+
     try:
         prompt = get_base_classification_prompt(finding_text, company_name)
         response = await llm.ainvoke([("user", prompt)])
-        
+
         # Parse response
         content = response.content.strip()
         # Remove markdown code blocks if present
@@ -75,37 +72,33 @@
         if content.endswith("```"):
             content = content[:-3]
         content = content.strip()
-        
+
         result = json.loads(content)
-        
+
         # Validate JSON structure
         if not isinstance(result, dict):
             logger.error(f"LLM returned non-dict result: {result}")
             return None
-        
+
         # Check confidence threshold (higher for Tier 3)
         confidence = result.get("confidence", 0.0)
         tier = result.get("tier")
-        
+
         # Tier 3 requires higher confidence threshold
         if tier == 3:
             if confidence < TIER_3_CONFIDENCE_THRESHOLD:
-                logger.debug(
-                    f"Tier 3 classification confidence {confidence:.2f} below threshold {TIER_3_CONFIDENCE_THRESHOLD}, skipping"
-                )
+                logger.debug(f"Tier 3 classification confidence {confidence:.2f} below threshold {TIER_3_CONFIDENCE_THRESHOLD}, skipping")
                 return None
         elif confidence < LLM_CONFIDENCE_THRESHOLD:
-            logger.debug(
-                f"Classification confidence {confidence:.2f} below threshold {LLM_CONFIDENCE_THRESHOLD}, skipping"
-            )
+            logger.debug(f"Classification confidence {confidence:.2f} below threshold {LLM_CONFIDENCE_THRESHOLD}, skipping")
             return None
-        
+
         if tier is None:
             logger.debug("Finding classified as null (no tier match)")
             return None
-        
+
         category = result.get("category")
-        
+
         # Normalize category names to exact matches from tier descriptions
         if category and tier:
             category_lower = category.lower().strip()
@@ -143,26 +136,20 @@
                     category = "Industry Committee Participation"
                 elif "executive" in category_lower or "ceo" in category_lower or "coo" in category_lower or "cfo" in category_lower:
                     category = "Executive Leadership Announcements"
-        
+
         # Validate category - should not be "Tier X" format
-        if category and (category.startswith('Tier ') or category in ['Tier 1', 'Tier 2', 'Tier 3']):
-            logger.warning(
-                f"LLM returned invalid category '{category}' - category should be a specific name, not a tier number. "
-                f"Setting to None."
-            )
+        if category and (category.startswith("Tier ") or category in ["Tier 1", "Tier 2", "Tier 3"]):
+            logger.warning(f"LLM returned invalid category '{category}' - category should be a specific name, not a tier number. Setting to None.")
             category = None
-        
-        logger.debug(
-            f"Base classification: Tier {tier}, category {category}, "
-            f"confidence {confidence:.2f}"
-        )
-        
+
+        logger.debug(f"Base classification: Tier {tier}, category {category}, confidence {confidence:.2f}")
+
         return {
             "tier": tier,
             "category": category,
             "confidence": confidence,
         }
-    
+
     except json.JSONDecodeError as e:
         logger.error(f"Failed to parse LLM response as JSON: {e}, response: {content[:200]}")
         return None
@@ -178,18 +165,18 @@
 ) -> Optional[Dict[str, Any]]:
     """
     User-specific relevance classification (finding + user context).
-    
+
     This is an independent classification method that runs when base classification found nothing.
     It classifies findings based on how well they match the user's investment profile and watchlist interests.
-    
+
     Returns Tier 1, 2, or 3 if finding is relevant to user's watchlist interests
     and investment objectives, and directly mentions the company. If not relevant, returns None.
-    
+
     Args:
         finding_text: The finding text
         user_context: User's radar context narrative (profile + watchlist notes)
         company_name: The name of the company being tracked in this radar
-        
+
     Returns:
         Dict with tier (1, 2, or 3), category, confidence if relevant to user,
         None otherwise
@@ -197,20 +184,20 @@
     if not finding_text or not finding_text.strip():
         logger.warning("Empty finding text, skipping user relevance check")
         return None
-    
+
     if not user_context or not user_context.strip():
         logger.debug("No user context provided, cannot run user relevance classification")
         return None
-    
+
     llm = _get_openai_client()
     if not llm:
         logger.error("OpenAI client not available, skipping user relevance check")
         return None
-    
+
     try:
         prompt = get_user_relevance_prompt(finding_text, user_context, company_name)
         response = await llm.ainvoke([("user", prompt)])
-        
+
         # Parse response
         content = response.content.strip()
         # Remove markdown code blocks if present
@@ -221,60 +208,55 @@
         if content.endswith("```"):
             content = content[:-3]
         content = content.strip()
-        
+
         result = json.loads(content)
-        
+
         # Validate JSON structure
         if not isinstance(result, dict):
             logger.error(f"LLM returned non-dict result: {result}")
             return None
-        
+
         # Check user relevance threshold (higher than base classification)
         confidence = result.get("confidence", 0.0)
         if confidence < USER_RELEVANCE_THRESHOLD:
-            logger.debug(
-                f"User relevance confidence {confidence:.2f} below threshold {USER_RELEVANCE_THRESHOLD}, "
-                f"not relevant to user"
-            )
+            logger.debug(f"User relevance confidence {confidence:.2f} below threshold {USER_RELEVANCE_THRESHOLD}, not relevant to user")
             return None
-        
+
         tier = result.get("tier")
         if tier is None:
             logger.debug("Finding not relevant to user or doesn't mention company (tier is null)")
             return None
-        
+
         # User relevance check can return Tier 1, 2, or 3 based on profile/context relevance
         if tier not in [1, 2, 3]:
-            logger.debug(
-                f"User relevance check returned Tier {tier}, but should be 1, 2, or 3. Setting to null."
-            )
+            logger.debug(f"User relevance check returned Tier {tier}, but should be 1, 2, or 3. Setting to null.")
             return None
-        
+
         category = result.get("category")
-        
+
         # Map of valid category names by tier
         VALID_TIER_1_CATEGORIES = [
             "Direct Allocation Announcements",
             "Senior Investment Staff Changes",
             "Investment Committee Meeting Schedules",
             "Distribution/Re-up Discussions",
-            "Seeking Managers Statements"
+            "Seeking Managers Statements",
         ]
         VALID_TIER_2_CATEGORIES = [
             "New Mandate Announcements",
             "Team Expansion in Asset Classes",
             "Conference Speaking on Relevant Topics",
             "New Allocation Announcements",
-            "Partners that they work with"
+            "Partners that they work with",
         ]
         VALID_TIER_3_CATEGORIES = [
             "General Thought Leadership",
             "Lower-Seniority Hires",
             "Moderate Engagement Social Posts",
             "Industry Committee Participation",
-            "Executive Leadership Announcements"
+            "Executive Leadership Announcements",
         ]
-        
+
         # Normalize category names (handle close matches)
         if category:
             category_lower = category.lower().strip()
@@ -324,13 +306,13 @@
                     # Invalid category for Tier 3
                     logger.warning(f"Invalid Tier 3 category '{category}', using default")
                     category = "General Thought Leadership"
-        
+
         # Final validation - reject generic names
-        if category and (category.startswith('Tier ') or category in ['Tier 1', 'Tier 2', 'Tier 3', 'profile_match', 'Conference Speaking', 'Strong Signals Worth Monitoring']):
-            logger.warning(
-                f"LLM returned invalid generic category '{category}' for user relevance. "
-                f"Using tier-appropriate default."
-            )
+        if category and (
+            category.startswith("Tier ")
+            or category in ["Tier 1", "Tier 2", "Tier 3", "profile_match", "Conference Speaking", "Strong Signals Worth Monitoring"]
+        ):
+            logger.warning(f"LLM returned invalid generic category '{category}' for user relevance. Using tier-appropriate default.")
             # Use a default based on tier
             if tier == 1:
                 category = "Direct Allocation Announcements"
@@ -338,22 +320,18 @@
                 category = "New Mandate Announcements"
             elif tier == 3:
                 category = "General Thought Leadership"
-        
-        logger.debug(
-            f"User relevance: Tier {tier}, category {category}, "
-            f"confidence {confidence:.2f}"
-        )
-        
+
+        logger.debug(f"User relevance: Tier {tier}, category {category}, confidence {confidence:.2f}")
+
         return {
             "tier": tier,
             "category": category,
             "confidence": confidence,
         }
-    
+
     except json.JSONDecodeError as e:
         logger.error(f"Failed to parse LLM response as JSON: {e}, response: {content[:200]}")
         return None
     except Exception as e:
         logger.error(f"Failed to check user relevance: {e}", exc_info=True)
         return None
-

--- app/services/lp_profile_service.py
+++ app/services/lp_profile_service.py
@@ -87,21 +87,21 @@
         "- If the text implies something but does not state it explicitly, infer conservatively.\n"
         "- If information is unavailable or unclear, set the value to null.\n"
         "- Always output syntactically valid JSON ‚Äî no trailing commas, no markdown.\n"
-        "- When matching against multiple-choice \"options,\" choose the closest fit from the list (case-insensitive). If nothing fits, use \"Other\" and include the original extracted text as the custom value.\n"
+        '- When matching against multiple-choice "options," choose the closest fit from the list (case-insensitive). If nothing fits, use "Other" and include the original extracted text as the custom value.\n'
         "- You may combine multiple relevant values into arrays for multi-select fields.\n"
         "- Do not include explanations, reasoning, or any text outside of the JSON object.\n"
         "- Keep inferred values realistic for institutional allocators (foundations, pensions, etc.).\n"
         "\n"
         "Inference guidelines:\n"
-        "- \"10+ year horizon\" or \"long-term capital growth\" -> riskTolerance = 1 (Balanced)\n"
-        "- Mentions of \"private equity\", \"real assets\", or \"alternatives\" -> add to assetClasses\n"
+        '- "10+ year horizon" or "long-term capital growth" -> riskTolerance = 1 (Balanced)\n'
+        '- Mentions of "private equity", "real assets", or "alternatives" -> add to assetClasses\n'
         "- ESG pool or sustainability language -> include ESG Integration in esgRequirements\n"
-        "- Mentions of \"moderate fluctuations\", \"long-term pool\" -> lockupTolerance = \"8 - 12 years\"\n"
-        "- \"Board\", \"Investment Committee\", \"CEO/CFO\" -> populate decisionMaker and decisionMakerRole\n"
-        "- \"Quarterly/annual reports\", \"meetings\" -> include Quarterly Reviews, Investment Committee Presentations\n"
-        "- Reporting via \"statements\", \"PDFs\", or \"Excel\" -> include both under reportingFormat\n"
-        "- Re-up/co-investment decisions mentioned as \"case by case\" or \"selective\" -> use corresponding Selective options\n"
-        "- For community foundations or small institutional pools, assume preferredFundSize = \"$250M - $750M (inferred)\" and gpPreference = \"Established Managers (inferred)\"\n"
+        '- Mentions of "moderate fluctuations", "long-term pool" -> lockupTolerance = "8 - 12 years"\n'
+        '- "Board", "Investment Committee", "CEO/CFO" -> populate decisionMaker and decisionMakerRole\n'
+        '- "Quarterly/annual reports", "meetings" -> include Quarterly Reviews, Investment Committee Presentations\n'
+        '- Reporting via "statements", "PDFs", or "Excel" -> include both under reportingFormat\n'
+        '- Re-up/co-investment decisions mentioned as "case by case" or "selective" -> use corresponding Selective options\n'
+        '- For community foundations or small institutional pools, assume preferredFundSize = "$250M - $750M (inferred)" and gpPreference = "Established Managers (inferred)"\n'
         "- Decision-Maker Name must be the exact personal name stated in the document. If no individual name appears, leave the field null rather than inferring or guessing.\n"
     )
 
@@ -271,9 +271,7 @@
         base_dir = storage_dir or Path(get_config_value("LP_PROFILE_UPLOAD_DIR", "output/lp_profile_uploads"))
         self.storage_dir = base_dir
         inline_env = os.getenv("LP_PROFILE_RUN_JOBS_INLINE")
-        self.run_jobs_inline = (
-            run_jobs_inline if run_jobs_inline is not None else (inline_env.lower() == "true" if inline_env else False)
-        )
+        self.run_jobs_inline = run_jobs_inline if run_jobs_inline is not None else (inline_env.lower() == "true" if inline_env else False)
 
     async def create_manual_profile(self, *, request: LPProfileManualRequest, lp_user_id: str) -> LPProfileManualResponse:
         self._enforce_required_fields(request.profile)
@@ -445,8 +443,8 @@
                     lp_user_id,
                     "queued" if self.extractor.is_configured else "failed",
                     str(stored_path),
-                        upload.filename,
-                upload.content_type,
+                    upload.filename,
+                    upload.content_type,
                     json.dumps(submission_metadata),
                     error_details,
                 )

--- app/services/profile_embedding_service.py
+++ app/services/profile_embedding_service.py
@@ -37,7 +37,7 @@
     1. Firm Description
     2. Key Objectives
     3. Firm's Investment Focus (Geographic Focus, Sector Focus, Limited Partner Target Types)
-    
+
     Uses structured format with headings and newlines for better embedding quality.
     """
     # Extract investor_profile JSONB, with fallback to top-level columns
@@ -53,26 +53,26 @@
             investor_profile = {}
     else:
         investor_profile = {}
-    
+
     sections = []
-    
+
     # 1. Firm Description (with fallback to top-level column)
     firm_desc = investor_profile.get("firm_description") or row.get("firm_description")
     if firm_desc and firm_desc.strip():
         sections.append("Firm Description")
         sections.append(firm_desc.strip())
         sections.append("")  # Empty line after section
-    
+
     # 2. Key Objectives (with fallback to top-level column)
     key_obj = investor_profile.get("key_objectives") or row.get("key_objectives")
     if key_obj and key_obj.strip():
         sections.append("Key Objectives")
         sections.append(key_obj.strip())
         sections.append("")  # Empty line after section
-    
+
     # 3. Firm's Investment Focus section
     investment_focus_fields = []
-    
+
     # Geographic Focus (from JSONB only, handle list format)
     geo_focus = investor_profile.get("geographic_focus")
     if geo_focus:
@@ -82,7 +82,7 @@
             formatted_geo = str(geo_focus)
         if formatted_geo:
             investment_focus_fields.append(f"Geographic Focus: {formatted_geo}")
-    
+
     # Sector Focus (from JSONB only, handle list format)
     sector_focus = investor_profile.get("sector_focus")
     if sector_focus:
@@ -92,7 +92,7 @@
             formatted_sector = str(sector_focus)
         if formatted_sector:
             investment_focus_fields.append(f"Sector Focus: {formatted_sector}")
-    
+
     # LP Target Types (from JSONB only, with proper formatting)
     lp_target_types = investor_profile.get("lp_target_types")
     if lp_target_types:
@@ -105,18 +105,18 @@
                 formatted_types = f"{', '.join(lp_target_types[:-1])}, and {lp_target_types[-1]}"
         else:
             formatted_types = str(lp_target_types)
-        
+
         investment_focus_fields.append(f"Limited Partner Target Types: {formatted_types}")
-    
+
     # Add Investment Focus section if it has any fields
     if investment_focus_fields:
         sections.append("Firm's Investment Focus")
         sections.extend(investment_focus_fields)
         sections.append("")  # Empty line after section
-    
+
     # Join all sections with newlines
     result = "\n".join(sections).strip()
-    
+
     # Return empty string if no content (all fields were empty)
     return result if result else ""
 
@@ -124,21 +124,21 @@
 def build_profile_embedding_text_for_feedback(row: dict) -> str:
     """
     Build profile embedding text for feedback analysis with enhanced context.
-    
+
     This creates a structured text representation of the user's profile with
     clear section headings and field labels that provide context for the
     embedding model. Used for profile-based collaborative filtering.
-    
+
     Structure:
     - Firm Description
     - Key Differentiators
     - Key Objectives
     - Firm's Fund Profile (with sub-fields)
     - Firms Investment Focus (with sub-fields)
-    
+
     Args:
         row: Profile row dict from database
-        
+
     Returns:
         Formatted text string with sections and fields
     """
@@ -155,9 +155,9 @@
             investor_profile = {}
     else:
         investor_profile = {}
-    
+
     sections = []
-    
+
     # Helper to format list values
     def format_list_value(value):
         if value is None:
@@ -167,7 +167,7 @@
                 return None
             return ", ".join(str(v) for v in value if v)
         return str(value) if value else None
-    
+
     # Helper to format number with units
     def format_currency(value, unit="USD"):
         if value is None:
@@ -175,115 +175,115 @@
         try:
             num = float(value)
             if num >= 1_000_000_000:
-                return f"${num/1_000_000_000:.1f}B {unit}"
+                return f"${num / 1_000_000_000:.1f}B {unit}"
             elif num >= 1_000_000:
-                return f"${num/1_000_000:.1f}M {unit}"
+                return f"${num / 1_000_000:.1f}M {unit}"
             elif num >= 1_000:
-                return f"${num/1_000:.1f}K {unit}"
+                return f"${num / 1_000:.1f}K {unit}"
             else:
                 return f"${num:.0f} {unit}"
         except (ValueError, TypeError):
             return str(value) if value else None
-    
+
     # 1. Firm Description
     firm_desc = investor_profile.get("firm_description") or row.get("firm_description")
     if firm_desc and firm_desc.strip():
         sections.append("Firm Description")
         sections.append(firm_desc.strip())
         sections.append("")  # Empty line after section
-    
+
     # 2. Key Differentiators
     key_diff = investor_profile.get("key_differentiators") or row.get("key_differentiators")
     if key_diff and key_diff.strip():
         sections.append("Key Differentiators")
         sections.append(key_diff.strip())
         sections.append("")  # Empty line after section
-    
+
     # 3. Key Objectives
     key_obj = investor_profile.get("key_objectives") or row.get("key_objectives")
     if key_obj and key_obj.strip():
         sections.append("Key Objectives")
         sections.append(key_obj.strip())
         sections.append("")  # Empty line after section
-    
+
     # 4. Firm's Fund Profile section
     fund_profile_fields = []
-    
+
     # Fund Type
     fund_type = investor_profile.get("fund_type")
     if fund_type and str(fund_type).strip():
         fund_profile_fields.append(f"Fund Type (e.g., Venture Capital, Private Equity, Growth Equity): {fund_type}")
-    
+
     # Investment Stage Focus
     investment_stage = investor_profile.get("investment_stage_focus") or investor_profile.get("investment_stage")
     if investment_stage:
         formatted_stage = format_list_value(investment_stage)
         if formatted_stage:
             fund_profile_fields.append(f"Investment Stage Focus (e.g., Seed, Series A, Growth, Buyout): {formatted_stage}")
-    
+
     # Fund Size (AUM)
     fund_size = investor_profile.get("fund_size") or investor_profile.get("aum")
     if fund_size:
         formatted_size = format_currency(fund_size)
         if formatted_size:
             fund_profile_fields.append(f"Fund Size - Assets Under Management (AUM in USD): {formatted_size}")
-    
+
     # Current Fundraising Status
     fundraising_status = investor_profile.get("fundraising_status") or investor_profile.get("current_fundraising_status")
     if fundraising_status and str(fundraising_status).strip():
         fund_profile_fields.append(f"Current Fundraising Status (e.g., Actively Fundraising, Not Fundraising, Fund Closed): {fundraising_status}")
-    
+
     # Primary Role
     primary_role = investor_profile.get("primary_role")
     if primary_role and str(primary_role).strip():
         fund_profile_fields.append(f"Primary Role in Fund (e.g., Managing Partner, General Partner, Investment Partner): {primary_role}")
-    
+
     # Investment Ticket Size
     ticket_size = investor_profile.get("ticket_size") or investor_profile.get("investment_ticket_size")
     if ticket_size:
         formatted_ticket = format_currency(ticket_size)
         if formatted_ticket:
             fund_profile_fields.append(f"Investment Ticket Size (typical investment amount per deal in USD): {formatted_ticket}")
-    
+
     # Add Fund Profile section if it has any fields
     if fund_profile_fields:
         sections.append("Firm's Fund Profile")
         sections.extend(fund_profile_fields)
         sections.append("")  # Empty line after section
-    
+
     # 5. Firms Investment Focus section
     investment_focus_fields = []
-    
+
     # Geographic Focus
     geo_focus = investor_profile.get("geographic_focus")
     if geo_focus:
         formatted_geo = format_list_value(geo_focus)
         if formatted_geo:
             investment_focus_fields.append(f"Geographic Focus: {formatted_geo}")
-    
+
     # Sector Focus
     sector_focus = investor_profile.get("sector_focus")
     if sector_focus:
         formatted_sector = format_list_value(sector_focus)
         if formatted_sector:
             investment_focus_fields.append(f"Sector Focus: {formatted_sector}")
-    
+
     # LP Target Types
     lp_target_types = investor_profile.get("lp_target_types")
     if lp_target_types:
         formatted_lp = format_list_value(lp_target_types)
         if formatted_lp:
             investment_focus_fields.append(f"Limited Partner Target Types: {formatted_lp}")
-    
+
     # Add Investment Focus section if it has any fields
     if investment_focus_fields:
         sections.append("Firms Investment Focus")
         sections.extend(investment_focus_fields)
         sections.append("")  # Empty line after section
-    
+
     # Join all sections with newlines
     result = "\n".join(sections).strip()
-    
+
     # Return empty string if no content (all fields were empty)
     return result if result else ""
 
@@ -308,34 +308,37 @@
 async def get_profile_embedding(user_id: str) -> Optional[List[float]]:
     """
     Retrieve pre-computed profile embedding from pgvector.
-    
+
     Args:
         user_id: The user ID to look up
-        
+
     Returns:
         List[float]: The embedding vector (3072 dimensions) if found, None otherwise
     """
     if not is_profile_embedding_enabled():
         logger.debug(f"Profile embedding disabled, skipping retrieval for user_id: {user_id}")
         return None
-    
+
     try:
         # Run blocking psycopg operations in executor for async compatibility
         import asyncio
-        
+
         def _fetch_embedding():
             """Synchronous function to run in executor."""
             conn = _get_psycopg_conn()
             try:
                 with conn.cursor() as cur:
-                    cur.execute("""
+                    cur.execute(
+                        """
                         SELECT embedding 
                         FROM profiles_embeddings 
                         WHERE user_id = %s 
                           AND embedding_purpose = %s 
                           AND format_version = %s
-                    """, (user_id, "investor_search", "v1"))
-                    
+                    """,
+                        (user_id, "investor_search", "v1"),
+                    )
+
                     row = cur.fetchone()
                     if row:
                         embedding = row["embedding"]
@@ -344,15 +347,15 @@
                             # Handle different return types from pgvector
                             if isinstance(embedding, (list, tuple)):
                                 return [float(x) for x in embedding]
-                            elif hasattr(embedding, 'tolist'):  # numpy array
+                            elif hasattr(embedding, "tolist"):  # numpy array
                                 return [float(x) for x in embedding.tolist()]
                             elif isinstance(embedding, str):
                                 # pgvector returns VECTOR type as string like "[0.1,0.2,0.3,...]"
                                 # Remove brackets and split by comma
-                                cleaned = embedding.strip('[]').strip()
+                                cleaned = embedding.strip("[]").strip()
                                 if cleaned:
                                     try:
-                                        return [float(x.strip()) for x in cleaned.split(',') if x.strip()]
+                                        return [float(x.strip()) for x in cleaned.split(",") if x.strip()]
                                     except (ValueError, AttributeError) as e:
                                         logger.warning(f"Failed to parse embedding string: {e}")
                                         return None
@@ -367,58 +370,57 @@
                     return None
             finally:
                 conn.close()
-        
+
         # Run blocking operations in thread pool executor
         loop = asyncio.get_event_loop()
         embedding = await loop.run_in_executor(None, _fetch_embedding)
-        
+
         if embedding:
             logger.debug(f"Retrieved pre-computed profile embedding for user_id: {user_id}")
         else:
             logger.debug(f"No pre-computed profile embedding found for user_id: {user_id}")
-        
+
         return embedding
-        
+
     except Exception as e:
-        logger.warning(
-            f"Failed to retrieve profile embedding for user_id: {user_id}",
-            exc_info=True,
-            extra={"user_id": user_id, "error": str(e)}
-        )
+        logger.warning(f"Failed to retrieve profile embedding for user_id: {user_id}", exc_info=True, extra={"user_id": user_id, "error": str(e)})
         return None
 
 
 async def get_profile_embedding_for_feedback(user_id: str) -> Optional[List[float]]:
     """
     Retrieve pre-computed profile embedding for feedback analysis from pgvector.
-    
+
     Args:
         user_id: The user ID to look up
-        
+
     Returns:
         List[float]: The embedding vector (3072 dimensions) if found, None otherwise
     """
     if not is_profile_embedding_enabled():
         logger.debug(f"Profile embedding disabled, skipping retrieval for feedback analysis for user_id: {user_id}")
         return None
-    
+
     try:
         # Run blocking psycopg operations in executor for async compatibility
         import asyncio
-        
+
         def _fetch_embedding():
             """Synchronous function to run in executor."""
             conn = _get_psycopg_conn()
             try:
                 with conn.cursor() as cur:
-                    cur.execute("""
+                    cur.execute(
+                        """
                         SELECT embedding 
                         FROM profiles_embeddings 
                         WHERE user_id = %s 
                           AND embedding_purpose = %s 
                           AND format_version = %s
-                    """, (user_id, "profile_feedback_analysis", "v1"))
-                    
+                    """,
+                        (user_id, "profile_feedback_analysis", "v1"),
+                    )
+
                     row = cur.fetchone()
                     if row:
                         embedding = row["embedding"]
@@ -427,15 +429,15 @@
                             # Handle different return types from pgvector
                             if isinstance(embedding, (list, tuple)):
                                 return [float(x) for x in embedding]
-                            elif hasattr(embedding, 'tolist'):  # numpy array
+                            elif hasattr(embedding, "tolist"):  # numpy array
                                 return [float(x) for x in embedding.tolist()]
                             elif isinstance(embedding, str):
                                 # pgvector returns VECTOR type as string like "[0.1,0.2,0.3,...]"
                                 # Remove brackets and split by comma
-                                cleaned = embedding.strip('[]').strip()
+                                cleaned = embedding.strip("[]").strip()
                                 if cleaned:
                                     try:
-                                        return [float(x.strip()) for x in cleaned.split(',') if x.strip()]
+                                        return [float(x.strip()) for x in cleaned.split(",") if x.strip()]
                                     except (ValueError, AttributeError) as e:
                                         logger.warning(f"Failed to parse embedding string: {e}")
                                         return None
@@ -448,26 +450,26 @@
                                     logger.warning(f"Unknown embedding type: {type(embedding)}")
                                     return None
                     return None
-                    
+
             finally:
                 conn.close()
-        
+
         # Run blocking operations in thread pool executor
         loop = asyncio.get_event_loop()
         embedding = await loop.run_in_executor(None, _fetch_embedding)
-        
+
         if embedding:
             logger.debug(f"Retrieved pre-computed profile embedding for feedback analysis for user_id: {user_id}")
         else:
             logger.debug(f"No pre-computed profile embedding for feedback analysis found for user_id: {user_id}")
-        
+
         return embedding
-        
+
     except Exception as e:
         logger.warning(
             f"Failed to retrieve profile embedding for feedback analysis for user_id: {user_id}",
             exc_info=True,
-            extra={"user_id": user_id, "error": str(e)}
+            extra={"user_id": user_id, "error": str(e)},
         )
         return None
 
@@ -475,76 +477,76 @@
 async def upsert_profile_embedding(user_id: str, profile_row: Dict[str, Any]) -> bool:
     """
     Generate and store profile embedding in pgvector.
-    
+
     This is designed to be called as a background task after profile
     creation or update. It will:
     1. Build profile text from the row data
     2. Check if embedding is needed (hash-based idempotency)
     3. Generate embedding if needed
     4. Upsert to profiles_embeddings table
-    
+
     Args:
         user_id: The user ID
         profile_row: Profile data dict (should match user_profiles table structure)
-        
+
     Returns:
         bool: True if successful, False otherwise
     """
     logger.info(f"Profile embedding task started for user_id: {user_id}")
-    
+
     if not is_profile_embedding_enabled():
         logger.debug(f"Profile embedding disabled, skipping for user_id: {user_id}")
         return False
-    
+
     try:
         # Build profile text
         profile_text = build_profile_text(profile_row)
-        
+
         # Skip if profile text is empty
         if not profile_text or profile_text.strip() == "":
             logger.debug(f"Empty profile text, skipping embedding for user_id: {user_id}")
             return False
-        
+
         # Compute hash for change detection
         profile_hash = sha256_text(profile_text)
-        
+
         # Check if embedding already exists with same hash
         # Run psycopg (blocking) operations in executor for async compatibility
         import asyncio
-        
+
         def _check_and_upsert_embedding():
             """Synchronous function to run in executor."""
             conn = _get_psycopg_conn()
             try:
                 with conn.cursor() as cur:
                     # Check if embedding is needed
-                    cur.execute("""
+                    cur.execute(
+                        """
                         SELECT 1 FROM profiles_embeddings 
                         WHERE user_id=%s 
                           AND embedding_purpose=%s 
                           AND format_version=%s 
                           AND profile_text_hash=%s
-                    """, (user_id, "investor_search", "v1", profile_hash))
-                    
+                    """,
+                        (user_id, "investor_search", "v1", profile_hash),
+                    )
+
                     if cur.fetchone():
                         logger.info(f"Profile embedding unchanged for user_id: {user_id}, skipping regeneration")
                         return True
-                    
+
                     # Get OpenAI API key
                     api_key = get_openai_api_key()
                     if not api_key:
                         logger.warning(f"OpenAI API key not available, skipping embedding for user_id: {user_id}")
                         return False
-                    
+
                     # Generate embedding (synchronous OpenAI call for executor)
                     logger.info(f"Generating embedding for user_id: {user_id}")
                     client = openai.OpenAI(api_key=api_key)
-                    response = client.embeddings.create(
-                        model="text-embedding-3-large",
-                        input=profile_text
-                    )
+                    response = client.embeddings.create(model="text-embedding-3-large", input=profile_text)
                     embedding = response.data[0].embedding
-                    
+
                     # Ensure schema exists with composite primary key (supports multiple purposes per user)
                     cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
                     cur.execute("""
@@ -560,9 +562,10 @@
                             PRIMARY KEY (user_id, embedding_purpose)
                         );
                     """)
-                    
+
                     # Upsert embedding (composite key handles multiple purposes per user)
-                    cur.execute("""
+                    cur.execute(
+                        """
                         INSERT INTO profiles_embeddings (
                             user_id, embedding_purpose, format_version, model_name,
                             profile_text, profile_text_hash, embedding, updated_at
@@ -575,112 +578,110 @@
                             embedding = EXCLUDED.embedding,
                             updated_at = NOW()
                         WHERE profiles_embeddings.profile_text_hash != EXCLUDED.profile_text_hash;
-                    """, (
-                        user_id,
-                        "investor_search",
-                        "v1",
-                        "text-embedding-3-large",
-                        profile_text,
-                        profile_hash,
-                        embedding,
-                    ))
-                    
+                    """,
+                        (
+                            user_id,
+                            "investor_search",
+                            "v1",
+                            "text-embedding-3-large",
+                            profile_text,
+                            profile_hash,
+                            embedding,
+                        ),
+                    )
+
                     conn.commit()
                     logger.info(f"Successfully stored profile embedding for user_id: {user_id}")
                     return True
-                    
+
             finally:
                 conn.close()
-        
+
         # Run blocking operations in thread pool executor
         loop = asyncio.get_event_loop()
         return await loop.run_in_executor(None, _check_and_upsert_embedding)
-            
+
     except Exception as e:
-        logger.error(
-            f"Failed to generate/store profile embedding for user_id: {user_id}",
-            exc_info=True,
-            extra={"user_id": user_id, "error": str(e)}
-        )
+        logger.error(f"Failed to generate/store profile embedding for user_id: {user_id}", exc_info=True, extra={"user_id": user_id, "error": str(e)})
         return False
 
 
 async def upsert_profile_embedding_for_feedback(user_id: str, profile_row: Dict[str, Any]) -> bool:
     """
     Generate and store profile embedding for feedback analysis in pgvector.
-    
+
     This creates a separate embedding with enhanced context specifically designed
     for profile-based collaborative filtering and feedback analysis.
-    
+
     This is designed to be called as a background task after profile
     creation or update. It will:
     1. Build profile text with enhanced context using build_profile_embedding_text_for_feedback()
     2. Check if embedding is needed (hash-based idempotency)
     3. Generate embedding if needed
     4. Upsert to profiles_embeddings table with purpose='profile_feedback_analysis'
-    
+
     Args:
         user_id: The user ID
         profile_row: Profile data dict (should match user_profiles table structure)
-        
+
     Returns:
         bool: True if successful, False otherwise
     """
     logger.info(f"Profile embedding for feedback analysis task started for user_id: {user_id}")
-    
+
     if not is_profile_embedding_enabled():
         logger.debug(f"Profile embedding disabled, skipping for user_id: {user_id}")
         return False
-    
+
     try:
         # Build profile text with enhanced context
         profile_text = build_profile_embedding_text_for_feedback(profile_row)
-        
+
         # Skip if profile text is empty
         if not profile_text or profile_text.strip() == "":
             logger.debug(f"Empty profile text, skipping embedding for user_id: {user_id}")
             return False
-        
+
         # Compute hash for change detection
         profile_hash = sha256_text(profile_text)
-        
+
         # Check if embedding already exists with same hash
         # Run psycopg (blocking) operations in executor for async compatibility
         import asyncio
-        
+
         def _check_and_upsert_embedding():
             """Synchronous function to run in executor."""
             conn = _get_psycopg_conn()
             try:
                 with conn.cursor() as cur:
                     # Check if embedding is needed
-                    cur.execute("""
+                    cur.execute(
+                        """
                         SELECT 1 FROM profiles_embeddings 
                         WHERE user_id=%s 
                           AND embedding_purpose=%s 
                           AND format_version=%s 
                           AND profile_text_hash=%s
-                    """, (user_id, "profile_feedback_analysis", "v1", profile_hash))
-                    
+                    """,
+                        (user_id, "profile_feedback_analysis", "v1", profile_hash),
+                    )
+
                     if cur.fetchone():
                         logger.info(f"Profile embedding for feedback analysis unchanged for user_id: {user_id}, skipping regeneration")
                         return True
-                    
+
                     # Get OpenAI API key
                     api_key = get_openai_api_key()
                     if not api_key:
                         logger.warning(f"OpenAI API key not available, skipping embedding for user_id: {user_id}")
                         return False
-                    
+
                     # Generate embedding (synchronous OpenAI call for executor)
                     logger.info(f"Generating profile embedding for feedback analysis for user_id: {user_id}")
                     client = openai.OpenAI(api_key=api_key)
-                    response = client.embeddings.create(
-                        model="text-embedding-3-large",
-                        input=profile_text
-                    )
+                    response = client.embeddings.create(model="text-embedding-3-large", input=profile_text)
                     embedding = response.data[0].embedding
-                    
+
                     # Ensure schema exists with composite primary key
                     cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
                     cur.execute("""
@@ -696,9 +697,10 @@
                             PRIMARY KEY (user_id, embedding_purpose)
                         );
                     """)
-                    
+
                     # Upsert embedding with new purpose (composite key handles multiple purposes per user)
-                    cur.execute("""
+                    cur.execute(
+                        """
                         INSERT INTO profiles_embeddings (
                             user_id, embedding_purpose, format_version, model_name,
                             profile_text, profile_text_hash, embedding, updated_at
@@ -711,32 +713,34 @@
                             embedding = EXCLUDED.embedding,
                             updated_at = NOW()
                         WHERE profiles_embeddings.profile_text_hash != EXCLUDED.profile_text_hash;
-                    """, (
-                        user_id,
-                        "profile_feedback_analysis",
-                        "v1",
-                        "text-embedding-3-large",
-                        profile_text,
-                        profile_hash,
-                        embedding,
-                    ))
-                    
+                    """,
+                        (
+                            user_id,
+                            "profile_feedback_analysis",
+                            "v1",
+                            "text-embedding-3-large",
+                            profile_text,
+                            profile_hash,
+                            embedding,
+                        ),
+                    )
+
                     conn.commit()
                     logger.info(f"Successfully stored profile embedding for feedback analysis for user_id: {user_id}")
                     return True
-                    
+
             finally:
                 conn.close()
-        
+
         # Run blocking operations in thread pool executor
         loop = asyncio.get_event_loop()
         return await loop.run_in_executor(None, _check_and_upsert_embedding)
-        
+
     except Exception as e:
         logger.error(
             f"Failed to generate/store profile embedding for feedback analysis for user_id: {user_id}",
             exc_info=True,
-            extra={"user_id": user_id, "error": str(e)}
+            extra={"user_id": user_id, "error": str(e)},
         )
         return False
 
@@ -744,4 +748,3 @@
 # Note: FastAPI BackgroundTasks supports async functions, so we use
 # the async version directly. If you need a sync version for other contexts,
 # you can add one here.
-

--- app/services/radar_context_builder.py
+++ app/services/radar_context_builder.py
@@ -10,30 +10,27 @@
 import json
 
 
-def build_radar_context_narrative(
-    user_profile: Optional[Dict[str, Any]] = None,
-    watchlist_notes: Optional[str] = None
-) -> str:
+def build_radar_context_narrative(user_profile: Optional[Dict[str, Any]] = None, watchlist_notes: Optional[str] = None) -> str:
     """
     Build a narrative sentence describing what posts are relevant
     based purely on user profile and watchlist notes.
-    
+
     Args:
-        user_profile: User profile dict with fund_type, sector_focus, 
+        user_profile: User profile dict with fund_type, sector_focus,
                      geographic_focus, key_objectives
         watchlist_notes: Free text notes about what user is watching for
-        
+
     Returns:
         Narrative sentence describing relevant posts
     """
     parts = []
-    
+
     # Extract profile fields
     fund_type = None
     sector_focus = None
     geographic_focus = None
     key_objectives = None
-    
+
     if user_profile:
         # Handle investor_profile JSONB or top-level fields
         investor_profile = user_profile.get("investor_profile")
@@ -44,12 +41,12 @@
                 investor_profile = {}
         elif investor_profile is None:
             investor_profile = {}
-        
+
         fund_type = investor_profile.get("fund_type") or user_profile.get("fund_type")
         sector_focus = investor_profile.get("sector_focus") or user_profile.get("sector_focus")
         geographic_focus = investor_profile.get("geographic_focus") or user_profile.get("geographic_focus")
         key_objectives = investor_profile.get("key_objectives") or user_profile.get("key_objectives")
-    
+
     # Format list values
     def format_list(value):
         if value is None:
@@ -64,85 +61,87 @@
             else:
                 return f"{', '.join(str(v) for v in value[:-1])}, and {value[-1]}"
         return str(value)
-    
+
     # Build the narrative sentence
     narrative_parts = []
-    
+
     # Start with fund type
     if fund_type:
         narrative_parts.append(f"{fund_type} fund")
     else:
         narrative_parts.append("investment fund")
-    
+
     # Add sector focus
     if sector_focus:
         sectors = format_list(sector_focus)
         if sectors:
             narrative_parts.append(f"focused on {sectors}")
-    
+
     # Add geographic focus
     if geographic_focus:
         geos = format_list(geographic_focus)
         if geos:
             narrative_parts.append(f"in {geos}")
-    
+
     # Add key objectives
     if key_objectives:
         narrative_parts.append(f"with objectives around {key_objectives}")
-    
+
     # Add watchlist notes
     if watchlist_notes and watchlist_notes.strip():
         narrative_parts.append(f"watching for {watchlist_notes.strip()}")
-    
+
     # Combine into sentence
     if len(narrative_parts) == 1:
         narrative = f"A {narrative_parts[0]}"
     else:
         narrative = f"A {' '.join(narrative_parts[:1])} {', '.join(narrative_parts[1:])}"
-    
+
     # Build specific post topics they're interested in
     post_topics = []
-    
+
     # Add sector-specific topics
     if sector_focus:
         sectors = format_list(sector_focus)
         if sectors:
             post_topics.append(f"companies in {sectors} sectors")
-    
+
     # Add geographic-specific topics
     if geographic_focus:
         geos = format_list(geographic_focus)
         if geos:
             post_topics.append(f"companies or organizations in {geos}")
-    
+
     # Add watchlist-specific topics
     if watchlist_notes and watchlist_notes.strip():
         post_topics.append(f"companies or organizations discussing {watchlist_notes.strip()}")
-    
+
     # Add objectives-specific topics
     if key_objectives:
         post_topics.append(f"companies or organizations implementing or discussing {key_objectives}")
-    
+
     # Add fund type-specific topics
     if fund_type:
         fund_lower = str(fund_type).lower()
-        if 'venture' in fund_lower or 'vc' in fund_lower:
+        if "venture" in fund_lower or "vc" in fund_lower:
             post_topics.append("venture capital funding rounds, early-stage company growth, startup announcements")
-        elif 'private equity' in fund_lower or 'pe' in fund_lower:
+        elif "private equity" in fund_lower or "pe" in fund_lower:
             post_topics.append("private equity transactions, buyout deals, portfolio company news")
-        elif 'growth' in fund_lower:
+        elif "growth" in fund_lower:
             post_topics.append("growth equity investments, scaling companies, expansion announcements")
-    
+
     # Add general fundraising signals
-    post_topics.append("fundraising activity, investment rounds, strategic partnerships, financial milestones, expansion plans, investor announcements")
-    
+    post_topics.append(
+        "fundraising activity, investment rounds, strategic partnerships, financial milestones, expansion plans, investor announcements"
+    )
+
     # Combine into final narrative
     if post_topics:
         topics_text = ", ".join(post_topics)
         narrative += f" would be interested in posts about {topics_text}."
     else:
         narrative += " would be interested in posts about companies, organizations, or investment opportunities relevant to their investment focus."
-    
+
     return narrative
 
 
@@ -152,12 +151,12 @@
     radar_type: Optional[str] = None,
     focus_areas: Optional[list] = None,
     geography: Optional[list] = None,
-    user_interests: Optional[str] = None
+    user_interests: Optional[str] = None,
 ) -> str:
     """
     Build comprehensive radar context text for embedding.
     Includes narrative + additional context fields.
-    
+
     Args:
         user_profile: User profile dict
         watchlist_notes: Free text notes
@@ -165,31 +164,30 @@
         focus_areas: List of focus areas (optional)
         geography: List of geographic areas (optional)
         user_interests: User interests string (optional)
-        
+
     Returns:
         Complete radar context text
     """
     parts = []
-    
+
     # Main narrative (profile + watchlist notes)
     narrative = build_radar_context_narrative(user_profile, watchlist_notes)
     parts.append(narrative)
-    
+
     # Add radar type if provided
     if radar_type:
         parts.append(f"Radar Type: {radar_type}")
-    
+
     # Add focus areas if provided
     if focus_areas:
         parts.append(f"Focus Areas: {', '.join(focus_areas)}")
-    
+
     # Add geography if provided
     if geography:
         parts.append(f"Geographic Focus: {', '.join(geography)}")
-    
+
     # Add user interests if provided
     if user_interests:
         parts.append(f"User Interests: {user_interests}")
-    
+
     return "\n".join(parts)
-

--- app/services/radar_context_embedding_service.py
+++ app/services/radar_context_embedding_service.py
@@ -30,11 +30,11 @@
 async def get_radar_context_narrative(user_id: str, radar_id: str) -> Optional[str]:
     """
     Retrieve radar context narrative text from database.
-    
+
     Args:
         user_id: The user ID
         radar_id: The radar ID
-        
+
     Returns:
         Narrative text if found, None otherwise
     """
@@ -42,7 +42,7 @@
     if not db:
         logger.warning(f"Database not available, cannot fetch narrative for user {user_id}, radar {radar_id}")
         return None
-    
+
     async with db.pool.acquire() as conn:
         row = await conn.fetchrow(
             """
@@ -56,10 +56,10 @@
             radar_id,
             FORMAT_VERSION,
         )
-        
+
         if row:
             return row.get("context_text")
-    
+
     return None
 
 
@@ -71,18 +71,18 @@
 ) -> bool:
     """
     Generate and store radar context narrative text in database.
-    
+
     This builds a narrative description from user profile and watchlist notes,
     then stores it for later use in LLM prompts for relevance scoring.
-    
+
     NOTE: Embedding generation removed. Only narrative text is stored.
-    
+
     Args:
         user_id: The user ID
         radar_id: The radar ID
         user_profile: User profile dict (optional, will fetch if not provided)
         watchlist_notes: Watchlist notes for this radar (optional)
-        
+
     Returns:
         True if successful, False otherwise
     """
@@ -92,7 +92,7 @@
         if not db:
             logger.warning(f"Database not available, cannot fetch profile for user {user_id}")
             return False
-        
+
         async with db.pool.acquire() as conn:
             profile_row = await conn.fetchrow(
                 """
@@ -102,20 +102,20 @@
                 """,
                 user_id,
             )
-            
+
             if profile_row:
                 user_profile = dict(profile_row)
             else:
                 logger.warning(f"User profile not found for user {user_id}")
                 user_profile = {}
-    
+
     # Fetch watchlist notes if not provided
     if watchlist_notes is None:
         db = await get_global_db()
         if not db:
             logger.warning(f"Database not available, cannot fetch watchlist notes for radar {radar_id}")
             return False
-        
+
         async with db.pool.acquire() as conn:
             radar_user_row = await conn.fetchrow(
                 """
@@ -123,31 +123,32 @@
                 FROM tamradar_user_radars
                 WHERE user_id = $1 AND radar_id = $2
                 """,
-                user_id, radar_id
+                user_id,
+                radar_id,
             )
-            
+
             if radar_user_row:
                 watchlist_notes = radar_user_row.get("user_interests")
-    
+
     # Build narrative
     try:
         context_text = build_radar_context_narrative(user_profile, watchlist_notes)
     except Exception as e:
         logger.error(f"Failed to build radar context narrative for user {user_id}, radar {radar_id}: {e}")
         return False
-    
+
     if not context_text or not context_text.strip():
         logger.warning(f"Empty context text for user {user_id}, radar {radar_id}, skipping storage")
         return False
-    
+
     # Store in database (text only, no embedding)
     context_hash = sha256_text(context_text)
-    
+
     db = await get_global_db()
     if not db:
         logger.warning(f"Database not available, cannot store narrative for user {user_id}, radar {radar_id}")
         return False
-    
+
     async with db.pool.acquire() as conn:
         try:
             # Check if record exists and hash hasn't changed (optimization to avoid unnecessary updates)
@@ -161,13 +162,14 @@
                 radar_id,
                 FORMAT_VERSION,
             )
-            
+
             # Skip update if hash hasn't changed
             if existing_row and existing_row["context_text_hash"] == context_hash:
                 logger.debug(f"Radar context narrative unchanged for user {user_id}, radar {radar_id}, skipping update")
                 return True
-            
-            await conn.execute("""
+
+            await conn.execute(
+                """
                 INSERT INTO radar_context_embeddings (
                     user_id, radar_id, context_text, context_text_hash,
                     format_version, updated_at
@@ -183,7 +185,7 @@
                 context_hash,
                 FORMAT_VERSION,
             )
-            
+
             logger.info(f"Stored radar context narrative for user {user_id}, radar {radar_id}")
             return True
         except Exception as e:
@@ -209,12 +211,12 @@
     """
     Regenerate all radar context narratives for a user.
     Called when user profile is updated.
-    
+
     NOTE: Embedding generation removed. Only narrative text is regenerated.
-    
+
     Args:
         user_id: The user ID
-        
+
     Returns:
         Number of narratives regenerated
     """
@@ -222,7 +224,7 @@
     if not db:
         logger.warning(f"Database not available, cannot regenerate narratives for user {user_id}")
         return 0
-    
+
     # Fetch user profile
     async with db.pool.acquire() as conn:
         profile_row = await conn.fetchrow(
@@ -233,13 +235,13 @@
             """,
             user_id,
         )
-        
+
         if not profile_row:
             logger.warning(f"User profile not found for user {user_id}")
             return 0
-        
+
         user_profile = dict(profile_row)
-        
+
         # Get all user's radars
         radar_rows = await conn.fetch(
             """
@@ -249,23 +251,22 @@
             """,
             user_id,
         )
-    
+
     # Regenerate narratives for each radar
     count = 0
     for radar_row in radar_rows:
         radar_id = radar_row["radar_id"]
         watchlist_notes = radar_row.get("user_interests")
-        
+
         success = await upsert_radar_context_narrative(
             user_id=user_id,
             radar_id=radar_id,
             user_profile=user_profile,
             watchlist_notes=watchlist_notes,
         )
-        
+
         if success:
             count += 1
-    
+
     logger.info(f"Regenerated {count} radar context narratives for user {user_id}")
     return count
-

--- app/services/relevance_scoring_service.py
+++ app/services/relevance_scoring_service.py
@@ -37,84 +37,84 @@
 def extract_finding_text(finding_data: Dict[str, Any], radar_type: str, company_name: str = None) -> str:
     """
     Extract text content from finding_data for LLM classification.
-    
+
     Args:
         finding_data: The finding_data JSONB from database
         radar_type: Type of radar (e.g., 'company_new_hires', 'company_mentions')
         company_name: Name of the tracked company (needed for new_hires to provide context)
-        
+
     Returns:
         Combined text string for LLM input
     """
     text_parts = []
-    
+
     # Handle different radar types - extract main content + useful author context
-    if radar_type in ['company_mentions', 'company_social_posts', 'company_social_posts_cxo']:
+    if radar_type in ["company_mentions", "company_social_posts", "company_social_posts_cxo"]:
         # Add source metadata to indicate post origin (helps LLM weight internal sources higher)
-        if radar_type == 'company_social_posts':
+        if radar_type == "company_social_posts":
             text_parts.append("[Source: Direct company post]")
-        elif radar_type == 'company_social_posts_cxo':
+        elif radar_type == "company_social_posts_cxo":
             text_parts.append("[Source: Direct Post from executive in the company]")
-        elif radar_type == 'company_mentions':
+        elif radar_type == "company_mentions":
             text_parts.append("[Source: Third-party mention]")
-        
+
         # Social posts and mentions - post/mention content + author context for better classification
-        if 'mention_content' in finding_data:
-            text_parts.append(str(finding_data['mention_content']))
-        if 'post_content' in finding_data:
-            text_parts.append(str(finding_data['post_content']))
-        
+        if "mention_content" in finding_data:
+            text_parts.append(str(finding_data["mention_content"]))
+        if "post_content" in finding_data:
+            text_parts.append(str(finding_data["post_content"]))
+
         # Add author context fields that help with classification
         # These provide important context about who posted and their role/company
-        author = finding_data.get('author')
+        author = finding_data.get("author")
         if author and isinstance(author, dict):
             author_context = []
             # profile_type helps understand if it's from a person or company
-            if author.get('profile_type'):
+            if author.get("profile_type"):
                 author_context.append(f"Author type: {author['profile_type']}")
             # title helps identify if it's from investment staff (critical for tier classification)
-            if author.get('title'):
+            if author.get("title"):
                 author_context.append(f"Author title: {author['title']}")
             # company_name helps identify the source company
-            if author.get('company_name'):
+            if author.get("company_name"):
                 author_context.append(f"Author company: {author['company_name']}")
             # country helps with geographic relevance
-            if author.get('country'):
+            if author.get("country"):
                 author_context.append(f"Author country: {author['country']}")
-            
+
             if author_context:
                 # Format as context section for better LLM understanding
                 text_parts.append(f"[Context: {', '.join(author_context)}]")
-    
-    elif radar_type == 'company_new_hires':
+
+    elif radar_type == "company_new_hires":
         # New hires - Include company context so LLM knows this is FROM the tracked company
         # Format: "{company_name} hired [title]" to make it clear this is about the tracked company
         hire_parts = []
         if company_name:
             hire_parts.append(f"{company_name} hired")
-        if finding_data.get('title'):
-            hire_parts.append(str(finding_data['title']))
-        if finding_data.get('department'):
+        if finding_data.get("title"):
+            hire_parts.append(str(finding_data["title"]))
+        if finding_data.get("department"):
             hire_parts.append(f"in {finding_data['department']}")
-        
+
         if hire_parts:
             text_parts.append(" ".join(hire_parts))
         else:
             # Fallback if no company_name provided
-            if finding_data.get('title'):
-                text_parts.append(str(finding_data['title']))
-            if finding_data.get('department'):
-                text_parts.append(str(finding_data['department']))
-    
-    elif radar_type == 'company_job_openings':
+            if finding_data.get("title"):
+                text_parts.append(str(finding_data["title"]))
+            if finding_data.get("department"):
+                text_parts.append(str(finding_data["department"]))
+
+    elif radar_type == "company_job_openings":
         # Job openings - ONLY the job description and title (main content)
         # Exclude: location, company_name, URLs as they're metadata
-        if finding_data.get('title'):
-            text_parts.append(str(finding_data['title']))
-        if finding_data.get('description'):
-            text_parts.append(str(finding_data['description']))
+        if finding_data.get("title"):
+            text_parts.append(str(finding_data["title"]))
+        if finding_data.get("description"):
+            text_parts.append(str(finding_data["description"]))
         # Note: We exclude location, company_name as they're metadata
-    
+
     return " ".join(text_parts).strip()
 
 
@@ -130,7 +130,7 @@
     Implements two-phase approach:
     1. Base classification (finding only) - filters irrelevant findings
     2. User-specific relevance check (finding + user context) - determines if relevant to user
-    
+
     Args:
         finding_id: The finding ID
         radar_id: The radar ID
@@ -141,19 +141,19 @@
     if not user_ids:
         logger.warning(f"No users to score for finding {finding_id}")
         return
-    
+
     # Acquire semaphore to limit concurrent tasks (prevents system overload)
     semaphore = get_scoring_semaphore()
     async with semaphore:
         try:
             logger.info(f"Starting LLM relevance scoring for finding {finding_id}, {len(user_ids)} users")
-            
+
             # Get company name from radar first (needed for finding text extraction for some radar types)
             db = await get_global_db()
             if not db:
                 logger.error("Database not available for storing relevance scores")
                 return
-            
+
             async with db.pool.acquire() as conn:
                 radar_row = await conn.fetchrow(
                     """
@@ -162,34 +162,31 @@
                     FROM tamradar_radars
                     WHERE radar_id = $1
                     """,
-                    radar_id
+                    radar_id,
                 )
                 company_name = "the company"
                 if radar_row:
-                    company_name = radar_row.get('company_name') or radar_row.get('domain') or "the company"
-            
+                    company_name = radar_row.get("company_name") or radar_row.get("domain") or "the company"
+
             # Step 1: Extract finding text (with company context for new_hires)
             finding_text = extract_finding_text(finding_data, radar_type, company_name)
             if not finding_text:
                 logger.warning(f"Empty finding text, skipping relevance scoring for {finding_id}")
                 return
-            
+
             logger.debug(f"Extracted finding text for {finding_id}: {len(finding_text)} characters")
-            
+
             # Step 2: Phase 1 - Base classification (finding only)
             base_classification = await classify_finding_tier(finding_text, company_name)
-            
+
             if base_classification:
                 logger.debug(
                     f"Base classification passed for finding {finding_id}: Tier {base_classification['tier']}, "
                     f"confidence {base_classification['confidence']:.2f}"
                 )
             else:
-                logger.debug(
-                    f"Finding {finding_id} did not pass base classification threshold, "
-                    f"will use user relevance check as fallback"
-                )
-            
+                logger.debug(f"Finding {finding_id} did not pass base classification threshold, will use user relevance check as fallback")
+
             # Step 3: Phase 2 - User-specific relevance checks (parallel)
             # Company name already retrieved above, reuse it
             # Create tasks for parallel user processing
@@ -205,10 +202,10 @@
                     company_name=company_name,
                 )
                 user_tasks.append(task)
-            
+
             # Execute all user checks in parallel
             await asyncio.gather(*user_tasks, return_exceptions=True)
-            
+
             # Log summary: how many users got scored
             db_summary = await get_global_db()
             if db_summary:
@@ -218,20 +215,16 @@
                         SELECT COUNT(*) FROM tamradar_user_findings
                         WHERE finding_id = $1 AND relevance_score IS NOT NULL
                         """,
-                        finding_id
+                        finding_id,
                     )
                     logger.info(
-                        f"Completed LLM relevance scoring for finding {finding_id}: "
-                        f"{scored_count}/{len(user_ids)} user(s) assigned tier ratings"
+                        f"Completed LLM relevance scoring for finding {finding_id}: {scored_count}/{len(user_ids)} user(s) assigned tier ratings"
                     )
             else:
                 logger.info(f"Completed LLM relevance scoring for finding {finding_id}")
-        
+
         except Exception as e:
-            logger.error(
-                f"Error in LLM relevance scoring for finding {finding_id}: {e}",
-                exc_info=True
-            )
+            logger.error(f"Error in LLM relevance scoring for finding {finding_id}: {e}", exc_info=True)
 
 
 async def _score_finding_for_user(
@@ -244,10 +237,10 @@
 ) -> None:
     """
     Score finding for a single user.
-    
+
     If base_classification found a tier, use it directly.
     If base_classification is None, run user relevance check as fallback.
-    
+
     Args:
         finding_id: The finding ID
         radar_id: The radar ID
@@ -268,23 +261,21 @@
         else:
             # Base classification found nothing - run user relevance check as fallback
             user_context = await get_radar_context_narrative(user_id, radar_id)
-            
+
             if not user_context:
                 logger.debug(
                     f"No radar context narrative for user {user_id}, radar {radar_id}, "
                     f"and base classification found nothing - skipping tier assignment"
                 )
                 return
-            
-            logger.debug(
-                f"Base classification found nothing, running user relevance check for user {user_id}, finding {finding_id}"
-            )
+
+            logger.debug(f"Base classification found nothing, running user relevance check for user {user_id}, finding {finding_id}")
             user_classification = await classify_finding_relevance_for_user(
                 finding_text=finding_text,
                 user_context=user_context,
                 company_name=company_name,
             )
-            
+
             if user_classification:
                 # User-specific classification found (Tier 1, 2, or 3 based on profile/context relevance)
                 final_classification = user_classification
@@ -296,25 +287,22 @@
                 )
             else:
                 # User relevance check also found nothing - finding is not relevant to this user
-                logger.debug(
-                    f"User {user_id}: Both base and user relevance checks found nothing, "
-                    f"skipping tier assignment for finding {finding_id}"
-                )
+                logger.debug(f"User {user_id}: Both base and user relevance checks found nothing, skipping tier assignment for finding {finding_id}")
                 return
-        
+
         # Update database with classification
         db = await get_global_db()
         if not db:
             logger.error("Database not available for storing relevance scores")
             return
-        
+
         async with db.pool.acquire() as conn:
             # Get category from classification (should always be provided by LLM)
-            category = final_classification.get('category')
-            
+            category = final_classification.get("category")
+
             # Normalize category names (handle close matches from LLM)
-            if category and final_classification.get('tier'):
-                tier = final_classification['tier']
+            if category and final_classification.get("tier"):
+                tier = final_classification["tier"]
                 category_lower = category.lower().strip()
                 # Map common variations to exact category names
                 if tier == 1:
@@ -350,61 +338,64 @@
                         category = "Industry Committee Participation"
                     elif "executive" in category_lower or "ceo" in category_lower or "coo" in category_lower or "cfo" in category_lower:
                         category = "Executive Leadership Announcements"
-            
+
             # Final validation - reject generic names
-            if category and (category.startswith('Tier ') or category in ['Tier 1', 'Tier 2', 'Tier 3', 'profile_match', 'Conference Speaking', 'Strong Signals Worth Monitoring']):
+            if category and (
+                category.startswith("Tier ")
+                or category in ["Tier 1", "Tier 2", "Tier 3", "profile_match", "Conference Speaking", "Strong Signals Worth Monitoring"]
+            ):
                 logger.warning(
                     f"Invalid category format '{category}' for finding {finding_id}, user {user_id}. "
                     f"Category should be a specific name, not a tier number or generic name."
                 )
                 # Set to null if it's a generic name (invalid)
                 category = None
-            
+
             # Only set defaults if category is truly missing (not just empty string from LLM)
             # LLM should always return a specific category name, not generic fallbacks
-            if not category or (isinstance(category, str) and category.strip() == ''):
+            if not category or (isinstance(category, str) and category.strip() == ""):
                 # No fallback - LLM should provide specific category names
                 category = None
                 logger.warning(
                     f"Missing category for Tier {final_classification['tier']} finding {finding_id}, user {user_id}. "
                     f"LLM should return a specific category name."
                 )
-            
+
             # Ensure tier is an integer (database expects INTEGER, not string)
-            tier_value = final_classification['tier']
+            tier_value = final_classification["tier"]
             if isinstance(tier_value, str):
                 # Handle string formats like "tier_1", "1", etc.
-                if tier_value.startswith('tier_'):
-                    tier_value = int(tier_value.replace('tier_', ''))
+                if tier_value.startswith("tier_"):
+                    tier_value = int(tier_value.replace("tier_", ""))
                 else:
                     tier_value = int(tier_value)
             elif not isinstance(tier_value, int):
                 logger.error(f"Invalid tier type: {type(tier_value)}, value: {tier_value}")
                 return
-            
+
             # Validate tier is 1, 2, or 3
             if tier_value not in [1, 2, 3]:
                 logger.error(f"Invalid tier value: {tier_value}, must be 1, 2, or 3")
                 return
-            
+
             await conn.execute(
                 """
                 UPDATE tamradar_user_findings
                 SET relevance_score = $1, priority_tier = $2, category = $3
                 WHERE user_id = $4 AND finding_id = $5
                 """,
-                final_classification['confidence'],
+                final_classification["confidence"],
                 tier_value,  # Use validated integer
                 category,
                 user_id,
                 finding_id,
             )
-            
+
             logger.debug(
                 f"Updated relevance for user {user_id}, finding {finding_id}: "
                 f"tier={tier_value}, score={final_classification['confidence']:.3f}, category={category or 'N/A'}"
             )
-            
+
             # If Tier 1, check if user has email alerts enabled
             if tier_value == 1:
                 # Check if user has tier1_email_alerts enabled for this radar
@@ -415,9 +406,9 @@
                     WHERE user_id = $1 AND radar_id = $2
                     """,
                     user_id,
-                    radar_id
+                    radar_id,
                 )
-                
+
                 if preference_row and preference_row.get("tier1_email_alerts"):
                     # Bypass verification for user relevance Tier 1 findings (already validated against user profile)
                     if classification_source == "user_relevance":
@@ -429,15 +420,16 @@
                                 "radar_id": radar_id,
                                 "company_name": company_name,
                                 "category": category,
-                            }
+                            },
                         )
-                        
+
                         # Generate simple headline for user relevance Tier 1
                         headline = f"Tier 1 Alert: {company_name} - {category or 'Investment Signal'}"
-                        
+
                         # Trigger email sending task directly (bypass verification)
                         try:
                             from app.tasks.tamradar_tasks import send_tier1_email_alert_task
+
                             send_tier1_email_alert_task.delay(
                                 user_id=user_id,
                                 finding_id=finding_id,
@@ -446,14 +438,9 @@
                                 company_name=company_name,
                                 category=category or "Unknown",
                             )
-                            logger.debug(
-                                f"Tier 1 email alert task triggered (bypassed verification) for user {user_id}, finding {finding_id}"
-                            )
+                            logger.debug(f"Tier 1 email alert task triggered (bypassed verification) for user {user_id}, finding {finding_id}")
                         except Exception as e:
-                            logger.warning(
-                                f"Failed to trigger Tier 1 email alert task: {e}",
-                                exc_info=True
-                            )
+                            logger.warning(f"Failed to trigger Tier 1 email alert task: {e}", exc_info=True)
                     else:
                         # Base classification Tier 1 - run verification
                         logger.info(
@@ -464,46 +451,37 @@
                                 "radar_id": radar_id,
                                 "company_name": company_name,
                                 "category": category,
-                            }
+                            },
                         )
-                        
+
                         # Trigger verification task (fire-and-forget)
                         try:
                             from app.tasks.tamradar_tasks import verify_tier1_and_generate_headline_task
+
                             verify_tier1_and_generate_headline_task.delay(
                                 finding_id=finding_id,
                                 radar_id=radar_id,
                                 user_id=user_id,
                                 company_name=company_name,
                                 category=category or "Unknown",
-                                initial_confidence=final_classification['confidence'],
+                                initial_confidence=final_classification["confidence"],
                                 finding_text=finding_text,
                             )
-                            logger.debug(
-                                f"Tier 1 verification task triggered for user {user_id}, finding {finding_id}"
-                            )
+                            logger.debug(f"Tier 1 verification task triggered for user {user_id}, finding {finding_id}")
                         except Exception as e:
                             # Don't fail relevance scoring if email verification fails
-                            logger.warning(
-                                f"Failed to trigger Tier 1 verification task: {e}",
-                                exc_info=True
-                            )
+                            logger.warning(f"Failed to trigger Tier 1 verification task: {e}", exc_info=True)
                 else:
-                    logger.debug(
-                        f"Tier 1 finding detected but user {user_id} has email alerts disabled for radar {radar_id}"
-                    )
-            
+                    logger.debug(f"Tier 1 finding detected but user {user_id} has email alerts disabled for radar {radar_id}")
+
             # Invalidate dashboard cache for any tier match (Tier 1, 2, or 3)
             try:
                 from app.utils.dashboard_cache import mark_tier1_2_finding_added
+
                 mark_tier1_2_finding_added(user_id)
             except Exception as e:
                 logger.warning(f"Failed to invalidate dashboard cache for user {user_id}: {e}")
-    
+
     except Exception as e:
-        logger.error(
-            f"Error scoring relevance for user {user_id}, finding {finding_id}: {e}",
-            exc_info=True
-        )
+        logger.error(f"Error scoring relevance for user {user_id}, finding {finding_id}: {e}", exc_info=True)
         # Continue with next user (exception already logged)
-

--- app/services/save_share_delegation_service.py
+++ app/services/save_share_delegation_service.py
@@ -139,9 +139,10 @@
         safe_link = html.escape(resume_link, quote=True)
         safe_message = html.escape(message) if message else None
         message_block = (
-            f"<p style='margin-top: 24px;'><strong>Personal message from {html.escape(display_name)}:</strong><br>"
-            f"{safe_message}</p>"
-        ) if safe_message else ""
+            (f"<p style='margin-top: 24px;'><strong>Personal message from {html.escape(display_name)}:</strong><br>{safe_message}</p>")
+            if safe_message
+            else ""
+        )
         return f"""
             <html>
               <body style='font-family: Arial, sans-serif; color: #1a1a1a; background: #f5f7fb; padding: 24px;'>

--- app/services/tamradar_agent_helpers.py
+++ app/services/tamradar_agent_helpers.py
@@ -8,13 +8,13 @@
 
     # In CompanyEnrichAgent:
     from app.services.tamradar_service import tamradar_service
-    
+
     domain = company_data.get('website_url', '').replace('https://', '').replace('http://', '').split('/')[0]
     findings = await tamradar_service.get_findings_by_domain(
         domain,
         radar_types=['job_openings', 'new_hires', 'promotions']
     )
-    
+
     # In PersonEnrichAgent:
     linkedin_url = person_data.get('linkedin_url')
     if linkedin_url:
@@ -29,114 +29,84 @@
 
 
 async def enrich_company_with_tamradar_findings(
-    company_data: Dict[str, Any],
-    radar_types: Optional[List[str]] = None,
-    limit: int = 10,
-    include_industry: bool = True
+    company_data: Dict[str, Any], radar_types: Optional[List[str]] = None, limit: int = 10, include_industry: bool = True
 ) -> Dict[str, Any]:
     """
     Helper to enrich company data with TAMradar findings.
-    
+
     Includes both company-specific radars and industry radars that mention the company.
-    
+
     Args:
         company_data: Company dict with 'website_url' or 'domain' key
         radar_types: Optional list of radar types to include
         limit: Maximum findings to return per type
         include_industry: Whether to include industry findings (default: True)
-        
+
     Returns:
         Dict with 'tamradar_findings' key containing list of findings
     """
     domain = None
-    
+
     # Try to extract domain from various possible keys
-    if 'website_url' in company_data:
-        url = company_data['website_url']
-        domain = url.replace('https://', '').replace('http://', '').replace('www.', '').split('/')[0]
-    elif 'domain' in company_data:
-        domain = company_data['domain']
-    elif 'website' in company_data:
-        url = company_data['website']
-        domain = url.replace('https://', '').replace('http://', '').replace('www.', '').split('/')[0]
-    
+    if "website_url" in company_data:
+        url = company_data["website_url"]
+        domain = url.replace("https://", "").replace("http://", "").replace("www.", "").split("/")[0]
+    elif "domain" in company_data:
+        domain = company_data["domain"]
+    elif "website" in company_data:
+        url = company_data["website"]
+        domain = url.replace("https://", "").replace("http://", "").replace("www.", "").split("/")[0]
+
     if not domain:
         return {"tamradar_findings": []}
-    
+
     # Get company-specific findings
-    findings = await tamradar_service.get_findings_by_domain(
-        domain,
-        radar_types=radar_types,
-        limit=limit
-    )
-    
+    findings = await tamradar_service.get_findings_by_domain(domain, radar_types=radar_types, limit=limit)
+
     # Also get industry findings that mention this domain
     if include_industry:
-        industry_findings = await tamradar_service.get_industry_findings_by_domain(
-            domain,
-            radar_types=radar_types,
-            limit=limit
-        )
+        industry_findings = await tamradar_service.get_industry_findings_by_domain(domain, radar_types=radar_types, limit=limit)
         findings.extend(industry_findings)
-    
-    return {
-        "tamradar_findings": findings,
-        "tamradar_findings_count": len(findings)
-    }
 
+    return {"tamradar_findings": findings, "tamradar_findings_count": len(findings)}
+
 
 async def enrich_person_with_tamradar_findings(
-    person_data: Dict[str, Any],
-    radar_types: Optional[List[str]] = None,
-    limit: int = 10
+    person_data: Dict[str, Any], radar_types: Optional[List[str]] = None, limit: int = 10
 ) -> Dict[str, Any]:
     """
     Helper to enrich person data with TAMradar findings.
-    
+
     Args:
         person_data: Person dict with 'linkedin_url' key
         radar_types: Optional list of radar types to include
         limit: Maximum findings to return
-        
+
     Returns:
         Dict with 'tamradar_findings' key containing list of findings
     """
     findings = []
-    
+
     # Get LinkedIn URL
-    linkedin_url = person_data.get('linkedin_url') or person_data.get('linkedin')
+    linkedin_url = person_data.get("linkedin_url") or person_data.get("linkedin")
     if linkedin_url:
-        findings = await tamradar_service.get_findings_by_linkedin_url(
-            linkedin_url,
-            radar_types=radar_types,
-            limit=limit
-        )
-    
-    return {
-        "tamradar_findings": findings,
-        "tamradar_findings_count": len(findings)
-    }
+        findings = await tamradar_service.get_findings_by_linkedin_url(linkedin_url, radar_types=radar_types, limit=limit)
+
+    return {"tamradar_findings": findings, "tamradar_findings_count": len(findings)}
 
 
-async def get_tamradar_summary_for_company(
-    domain: str,
-    days: int = 30
-) -> Dict[str, Any]:
+async def get_tamradar_summary_for_company(domain: str, days: int = 30) -> Dict[str, Any]:
     """
     Get a quick summary of TAMradar findings for a company.
-    
+
     Useful for agents to quickly check if TAMradar has recent data
     before doing expensive enrichment operations.
-    
+
     Args:
         domain: Company domain
         days: Number of days to look back
-        
+
     Returns:
         Summary with counts by radar_type
     """
-    return await tamradar_service.get_recent_findings_summary(
-        domain=domain,
-        days=days
-    )
-
+    return await tamradar_service.get_recent_findings_summary(domain=domain, days=days)

--- app/services/tamradar_balance_monitor.py
+++ app/services/tamradar_balance_monitor.py
@@ -92,7 +92,7 @@
 
         base_url = f"https://{region}.mailgun.net/v3/{domain}/messages"
         subject = f"‚ö†Ô∏è TAMradar Balance Alert: ${balance:.2f} remaining"
-        
+
         text_body = f"""
 TAMradar account balance has dropped below the threshold.
 
@@ -139,10 +139,7 @@
                     data=data,
                 )
                 response.raise_for_status()
-                logger.info(
-                    f"Balance alert sent to {len(self.admin_emails)} admin(s): "
-                    f"${balance:.2f} remaining (threshold: ${threshold:.2f})"
-                )
+                logger.info(f"Balance alert sent to {len(self.admin_emails)} admin(s): ${balance:.2f} remaining (threshold: ${threshold:.2f})")
         except httpx.HTTPError as e:
             logger.error(f"Failed to send balance alert email: {e}")
 
@@ -166,14 +163,9 @@
                     # Record alert
                     await self._record_alert(balance, threshold)
                 else:
-                    logger.debug(
-                        f"Balance ${balance:.2f} below threshold ${threshold:.2f}, "
-                        f"but alert already sent recently"
-                    )
+                    logger.debug(f"Balance ${balance:.2f} below threshold ${threshold:.2f}, but alert already sent recently")
             else:
-                logger.debug(
-                    f"Balance ${balance:.2f} is above threshold ${threshold:.2f}"
-                )
+                logger.debug(f"Balance ${balance:.2f} is above threshold ${threshold:.2f}")
         except TAMradarServiceError as e:
             logger.error(f"Error checking TAMradar balance: {e}")
         except Exception as e:
@@ -182,4 +174,3 @@
 
 # Singleton instance
 tamradar_balance_monitor = TAMradarBalanceMonitor()
-

--- app/services/tamradar_service.py
+++ app/services/tamradar_service.py
@@ -45,6 +45,7 @@
 
 class TAMradarServiceError(Exception):
     """Base exception for TAMradar service errors."""
+
     def __init__(self, message: str, status_code: int = 500):
         self.message = message
         self.status_code = status_code
@@ -58,12 +59,13 @@
         self.api_key = get_tamradar_api_key()
         self.api_url = get_tamradar_api_url()
         self.webhook_url = get_tamradar_webhook_url()
-        
+
         if not self.api_key:
             logger.warning("TAMradar API key not configured")
-        
+
         # Import here to avoid circular dependencies
         from app.utils.config import get_perplexity_api_key
+
         self.perplexity_api_key = get_perplexity_api_key()
 
     def _get_headers(self) -> Dict[str, str]:
@@ -73,21 +75,19 @@
             "Content-Type": "application/json",
         }
 
-    async def validate_domain_before_creation(
-        self, domain: str, user_id: Optional[str] = None
-    ) -> Dict[str, Any]:
+    async def validate_domain_before_creation(self, domain: str, user_id: Optional[str] = None) -> Dict[str, Any]:
         """
         Validate domain before creating a radar.
-        
+
         Checks:
         1. Failed radar cache (if domain failed before)
         2. Domain normalization and subdomain detection
         3. Domain reachability (DNS lookup)
-        
+
         Args:
             domain: Domain string to validate
             user_id: Optional user ID for user-specific checks
-            
+
         Returns:
             Dict with validation results:
             - is_valid: bool
@@ -98,7 +98,7 @@
         warnings = []
         errors = []
         previous_failures = []
-        
+
         # Normalize domain
         normalized = normalize_domain(domain)
         if not normalized:
@@ -109,12 +109,12 @@
                 "errors": errors,
                 "previous_failures": previous_failures,
             }
-        
+
         # Check domain reachability
         is_reachable, reachability_error = check_domain_reachability(normalized)
         if not is_reachable:
             warnings.append(f"Domain may not be reachable: {reachability_error}")
-        
+
         # Check failed radar cache
         db = await get_global_db()
         if db:
@@ -135,7 +135,7 @@
                     LIMIT 20
                     """
                 )
-                
+
                 # Filter by normalized domain in Python
                 failed_radars = []
                 normalized_root = extract_root_domain(normalized) if normalized else ""
@@ -147,26 +147,25 @@
                         # Compare both normalized and root domains to catch all variations
                         if row_normalized == normalized or row_domain == normalized or row_root == normalized_root:
                             failed_radars.append(row)
-                
+
                 if failed_radars:
                     for row in failed_radars:
-                        previous_failures.append({
-                            "domain": row["domain"],
-                            "failure_reason": row["failure_reason"],
-                            "failed_at": row["failed_at"].isoformat() if row.get("failed_at") else None,
-                        })
-                    
+                        previous_failures.append(
+                            {
+                                "domain": row["domain"],
+                                "failure_reason": row["failure_reason"],
+                                "failed_at": row["failed_at"].isoformat() if row.get("failed_at") else None,
+                            }
+                        )
+
                     # Get most recent failure reason
                     most_recent = previous_failures[0] if previous_failures else None
                     if most_recent:
-                        warnings.append(
-                            f"This domain previously failed: {most_recent.get('failure_reason', 'Unknown reason')}. "
-                            "Continue anyway?"
-                        )
-        
+                        warnings.append(f"This domain previously failed: {most_recent.get('failure_reason', 'Unknown reason')}. Continue anyway?")
+
         # Determine if valid (warnings don't make it invalid, only errors do)
         is_valid = len(errors) == 0
-        
+
         return {
             "is_valid": is_valid,
             "warnings": warnings,
@@ -177,29 +176,29 @@
     def _map_tamradar_status_to_our_status(self, tamradar_status: Optional[str]) -> str:
         """
         Map TAMradar API status to our internal status values.
-        
+
         TAMradar Status Mapping:
         - "active" -> "active" (radar is working)
         - "pending" -> "pending" (radar is being set up)
         - "inactive" -> "failed" (radar failed - TAMradar uses "inactive" for failed radars)
         - "tracking_failed" -> "failed" (explicit failure status)
         - "failed" -> "failed" (direct failure status)
-        
+
         Important Notes:
         - When creating a radar, TAMradar ALWAYS returns "active" status initially
         - If TAMradar later determines it can't track, it sends a "radar_failure" webhook (24-96 hours later)
         - After failure, TAMradar returns "inactive" status when you GET the radar
         - We map "inactive" to "failed" to maintain consistency with our webhook handling
-        
+
         Args:
             tamradar_status: Status from TAMradar API (can be None)
-            
+
         Returns:
             Our internal status: 'pending', 'active', or 'failed' (defaults to 'pending' if unknown)
         """
         if not tamradar_status:
             return "pending"
-        
+
         status_mapping = {
             "active": "active",
             "pending": "pending",
@@ -207,43 +206,43 @@
             "inactive": "failed",  # TAMradar uses "inactive" for failed radars
             "tracking_failed": "failed",
         }
-        
+
         return status_mapping.get(tamradar_status.lower(), "pending")
 
     async def check_radar_status(self, radar_id: str) -> Optional[str]:
         """
         Check radar status with TAMradar API and update database if changed.
-        
+
         Args:
             radar_id: TAMradar radar ID
-            
+
         Returns:
             Current status ('pending', 'active', 'failed') or None if error
         """
         try:
             # Query TAMradar API for radar status
             response = await self._make_request("GET", f"radars/{radar_id}")
-            
+
             # TAMradar API should return status in response
             # Adjust field name based on actual API response structure
             status_from_api = response.get("status") or response.get("radar_status")
-            
+
             if not status_from_api:
                 logger.warning(f"No status in TAMradar API response for radar {radar_id}")
                 return None
-            
+
             # Map TAMradar status to our status values
             our_status = self._map_tamradar_status_to_our_status(status_from_api)
-            
+
             # Update database if status changed
             await self.update_radar_status(
                 radar_id=radar_id,
                 status=our_status,
                 failure_reason=response.get("failure_reason") if our_status == "failed" else None,
             )
-            
+
             return our_status
-            
+
         except Exception as e:
             logger.error(f"Failed to check radar status for {radar_id}: {e}")
             return None
@@ -256,7 +255,7 @@
     ) -> None:
         """
         Update radar status in database.
-        
+
         Args:
             radar_id: TAMradar radar ID
             status: New status ('active' or 'failed', or 'pending' for edge cases)
@@ -266,9 +265,9 @@
         if not db:
             logger.warning("Database not available, cannot update radar status")
             return
-        
+
         now = datetime.now(timezone.utc).replace(tzinfo=None)
-        
+
         async with db.pool.acquire() as conn:
             if status == "failed":
                 await conn.execute(
@@ -309,25 +308,25 @@
                     now,
                     radar_id,
                 )
-            
+
             logger.info(f"Updated radar {radar_id} status to {status}")
 
     async def get_failed_radars_for_domain(self, domain: str) -> List[Dict[str, Any]]:
         """
         Get failed radars for a given domain (for pre-validation).
-        
+
         Args:
             domain: Domain to check
-            
+
         Returns:
             List of failed radar records
         """
         db = await get_global_db()
         if not db:
             return []
-        
+
         normalized = normalize_domain(domain)
-        
+
         async with db.pool.acquire() as conn:
             rows = await conn.fetch(
                 """
@@ -342,16 +341,14 @@
                 domain,
                 normalized,
             )
-            
+
             return [dict(row) for row in rows]
 
-    async def _make_request(
-        self, method: str, endpoint: str, data: Optional[Dict[str, Any]] = None
-    ) -> Dict[str, Any]:
+    async def _make_request(self, method: str, endpoint: str, data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
         """Make HTTP request to TAMradar API."""
         url = f"{self.api_url}/{endpoint.lstrip('/')}"
         logger.debug(f"TAMradar API request: {method} {url}")
-        
+
         try:
             async with httpx.AsyncClient(timeout=30.0) as client:
                 response = await client.request(
@@ -367,7 +364,7 @@
             user_friendly_message = ""
             radar_id_from_error = None  # Extract radar_id from 409 responses if available
             try:
-                if hasattr(e.response, 'text') and e.response.text:
+                if hasattr(e.response, "text") and e.response.text:
                     error_detail = e.response.text[:1000]  # Get more of the response
                     # Try to parse JSON error response
                     try:
@@ -375,7 +372,7 @@
                         if isinstance(error_json, dict):
                             error_detail = error_json.get("message", error_detail)
                             user_friendly_message = error_detail  # Use TAMradar's message as user-friendly
-                            
+
                             # Check for radar_id in 409 responses (for existing radar conflicts)
                             if e.response.status_code == 409:
                                 # Check various locations where radar_id might be
@@ -389,17 +386,19 @@
                                         # Check first item in array
                                         if isinstance(data[0], dict) and "radar_id" in data[0]:
                                             radar_id_from_error = data[0]["radar_id"]
-                                
+
                                 if radar_id_from_error:
                                     logger.info(f"Extracted radar_id from 409 response: {radar_id_from_error}")
                                 else:
                                     logger.debug(f"409 response structure: {json.dumps(error_json, indent=2, default=str)}")
-                            
+
                             if error_json.get("data"):
                                 # Extract field-specific error messages
                                 data = error_json.get("data", [])
                                 if isinstance(data, list) and len(data) > 0:
-                                    field_errors = [f"{item.get('field', 'unknown')}: {item.get('reason', '')}" for item in data if isinstance(item, dict)]
+                                    field_errors = [
+                                        f"{item.get('field', 'unknown')}: {item.get('reason', '')}" for item in data if isinstance(item, dict)
+                                    ]
                                     if field_errors:
                                         error_detail = f"{error_detail}. {'; '.join(field_errors)}"
                                         user_friendly_message = f"{user_friendly_message}. {'; '.join(field_errors)}"
@@ -407,68 +406,54 @@
                         pass  # Use raw text if JSON parsing fails
             except Exception:
                 error_detail = str(e)
-            
+
             logger.error(
-                f"TAMradar API error: {e.response.status_code}",
-                extra={"url": url, "status_code": e.response.status_code, "response": error_detail}
+                f"TAMradar API error: {e.response.status_code}", extra={"url": url, "status_code": e.response.status_code, "response": error_detail}
             )
             # Log full URL and headers for debugging
             logger.error(f"Request URL: {url}, Method: {method}, Headers: {self._get_headers()}")
-            
+
             # Create user-friendly error messages based on status code
             status_code = e.response.status_code
             if status_code == 409:
                 # Store radar_id in exception if found (for later use in creation handlers)
-                error = TAMradarServiceError(
-                    user_friendly_message or f"Radar already exists: {error_detail}",
-                    status_code=409
-                )
+                error = TAMradarServiceError(user_friendly_message or f"Radar already exists: {error_detail}", status_code=409)
                 if radar_id_from_error:
                     error.radar_id = radar_id_from_error  # Attach radar_id to exception
                 raise error from e
             elif status_code == 400:
                 # Bad request - likely invalid input (domain not found, invalid format, etc.)
                 raise TAMradarServiceError(
-                    user_friendly_message or f"Invalid request: {error_detail}. Please check the company domain or contact information and try again.",
-                    status_code=400
+                    user_friendly_message
+                    or f"Invalid request: {error_detail}. Please check the company domain or contact information and try again.",
+                    status_code=400,
                 ) from e
             elif status_code == 404:
                 # Not found - company/contact doesn't exist
                 raise TAMradarServiceError(
-                    user_friendly_message or f"Company or contact not found: {error_detail}. Please verify the domain or contact information is correct.",
-                    status_code=404
+                    user_friendly_message
+                    or f"Company or contact not found: {error_detail}. Please verify the domain or contact information is correct.",
+                    status_code=404,
                 ) from e
             elif status_code == 401 or status_code == 403:
                 # Authentication/authorization error
-                raise TAMradarServiceError(
-                    "Authentication failed. Please contact support.",
-                    status_code=status_code
-                ) from e
+                raise TAMradarServiceError("Authentication failed. Please contact support.", status_code=status_code) from e
             elif status_code >= 500:
                 # Server error on service side
-                raise TAMradarServiceError(
-                    "Service is temporarily unavailable. Please try again in a few moments.",
-                    status_code=503
-                ) from e
+                raise TAMradarServiceError("Service is temporarily unavailable. Please try again in a few moments.", status_code=503) from e
             else:
                 # Other errors
-                raise TAMradarServiceError(
-                    user_friendly_message or f"Service error: {error_detail}",
-                    status_code=status_code
-                ) from e
+                raise TAMradarServiceError(user_friendly_message or f"Service error: {error_detail}", status_code=status_code) from e
         except httpx.RequestError as e:
             logger.error(f"TAMradar API request failed: {e}", extra={"url": url})
             raise TAMradarServiceError(
-                "Unable to connect to the service. Please check your internet connection and try again.",
-                status_code=503
+                "Unable to connect to the service. Please check your internet connection and try again.", status_code=503
             ) from e
 
-    def _generate_radar_unique_key(
-        self, radar_category: str, radar_type: str, config: Dict[str, Any]
-    ) -> str:
+    def _generate_radar_unique_key(self, radar_category: str, radar_type: str, config: Dict[str, Any]) -> str:
         """Generate unique key for radar to enable sharing.
-        
-        Uses extract_root_domain() to ensure domains like 'www.schroders.com' 
+
+        Uses extract_root_domain() to ensure domains like 'www.schroders.com'
         and 'schroders.com' generate the same unique key.
         """
         if radar_category == "company":
@@ -480,12 +465,7 @@
             domain = config.get("domain", "")
             # Extract root domain to remove 'www.' and other subdomains
             root_domain = extract_root_domain(domain) if domain else ""
-            contact_id = (
-                config.get("contact_email")
-                or config.get("contact_linkedin_url")
-                or config.get("contact_name")
-                or ""
-            )
+            contact_id = config.get("contact_email") or config.get("contact_linkedin_url") or config.get("contact_name") or ""
             return f"contact_{radar_type}_{root_domain}_{contact_id}"
         elif radar_category == "industry":
             keyword = config.get("industry_keyword", "")
@@ -494,14 +474,10 @@
             raise ValueError(f"Unknown radar category: {radar_category}")
 
     async def _find_existing_radar(
-        self, 
-        unique_key: str, 
-        domain: Optional[str] = None,
-        radar_category: Optional[str] = None,
-        radar_type: Optional[str] = None
+        self, unique_key: str, domain: Optional[str] = None, radar_category: Optional[str] = None, radar_type: Optional[str] = None
     ) -> Optional[Dict[str, Any]]:
         """Check if radar with this unique key already exists.
-        
+
         Also searches by domain root as fallback to handle legacy radars
         with 'www.' in their unique_key.
         """
@@ -523,7 +499,7 @@
             )
             if row:
                 return dict(row)
-            
+
             # Fallback: If unique_key search failed and we have domain info,
             # search by domain root to find legacy radars with 'www.' in unique_key
             if domain and radar_category and radar_type:
@@ -553,11 +529,13 @@
                             candidate_normalized = normalize_domain(candidate_domain)
                             candidate_root = extract_root_domain(candidate_normalized) if candidate_normalized else ""
                             if candidate_root == root_domain:
-                                logger.info(f"Found existing radar by domain root fallback: {candidate['radar_id']} (unique_key mismatch, domain match: {candidate_domain} -> {candidate_root})")
+                                logger.info(
+                                    f"Found existing radar by domain root fallback: {candidate['radar_id']} (unique_key mismatch, domain match: {candidate_domain} -> {candidate_root})"
+                                )
                                 return dict(candidate)
-            
+
             return None
-            
+
             result = dict(row)
             # Parse radar_config if it's a JSON string
             # PostgreSQL JSONB returns as dict, but handle string case too
@@ -573,7 +551,7 @@
                 logger.warning(f"radar_config is not dict or string (type: {type(radar_config)}), setting to empty dict")
                 result["radar_config"] = {}
             # If it's already a dict (from PostgreSQL JSONB), keep it as is
-            
+
             return result
 
     async def _store_radar(
@@ -591,7 +569,7 @@
     ) -> None:
         """
         Store radar in database.
-        
+
         Args:
             conn: Optional database connection. If provided, uses it; otherwise acquires a new one.
         """
@@ -670,7 +648,7 @@
         """
         Derive a readable company name from domain name.
         Used as a fallback until Perplexity provides the official name.
-        
+
         Examples:
         - "thirdlinecapital.com" ‚Üí "Thirdlinecapital"
         - "guinness-gi.com" ‚Üí "Guinness Gi"
@@ -679,40 +657,40 @@
         """
         if not domain:
             return ""
-        
+
         # Remove protocol if present
         clean = domain
-        if clean.startswith('http://'):
+        if clean.startswith("http://"):
             clean = clean[7:]
-        elif clean.startswith('https://'):
+        elif clean.startswith("https://"):
             clean = clean[8:]
-        
+
         # Remove www. prefix
-        if clean.startswith('www.'):
+        if clean.startswith("www."):
             clean = clean[4:]
-        
+
         # Extract domain part (before first slash)
-        if '/' in clean:
-            clean = clean.split('/')[0]
-        if '?' in clean:
-            clean = clean.split('?')[0]
-        
+        if "/" in clean:
+            clean = clean.split("/")[0]
+        if "?" in clean:
+            clean = clean.split("?")[0]
+
         # Remove TLD (e.g., .com, .co.uk, .io)
         # Common TLDs
-        tlds = ['.com', '.co.uk', '.io', '.net', '.org', '.co', '.uk', '.us', '.ca', '.au', '.de', '.fr', '.jp', '.cn']
+        tlds = [".com", ".co.uk", ".io", ".net", ".org", ".co", ".uk", ".us", ".ca", ".au", ".de", ".fr", ".jp", ".cn"]
         for tld in sorted(tlds, key=len, reverse=True):  # Sort by length to match longer TLDs first
             if clean.endswith(tld):
-                clean = clean[:-len(tld)]
+                clean = clean[: -len(tld)]
                 break
-        
+
         # Split on dots, dashes, underscores
-        parts = clean.replace('.', ' ').replace('-', ' ').replace('_', ' ').split()
-        
+        parts = clean.replace(".", " ").replace("-", " ").replace("_", " ").split()
+
         # Capitalize each word and join
         if parts:
             capitalized_parts = [part.capitalize() for part in parts if part]
-            return ' '.join(capitalized_parts)
-        
+            return " ".join(capitalized_parts)
+
         # Fallback: return domain as-is if we can't parse it
         return clean.capitalize() if clean else domain
 
@@ -720,25 +698,25 @@
         """
         Get company name from domain using Perplexity API with enhanced web search.
         Runs in parallel with radar creation to avoid blocking.
-        
+
         Uses Perplexity's web search capabilities to find the official company name
         from the domain, matching the pattern used in web research agent and search module.
         """
         if not self.perplexity_api_key:
             logger.debug("Perplexity API key not configured, skipping company name lookup")
             return None
-        
+
         # API monitoring: Track Perplexity API call
         api_call_start_time = time.time()
-        
+
         try:
             chat = ChatPerplexity(
                 temperature=0,
                 model="sonar-pro",
                 api_key=self.perplexity_api_key,
-                timeout=10.0  # Add 10 second timeout to prevent long waits
+                timeout=10.0,  # Add 10 second timeout to prevent long waits
             )
-            
+
             # Enhanced system prompt for better accuracy
             system_prompt = """You are an expert at identifying company names from website domains. 
             Your task is to find the official, legal company name associated with a given domain.
@@ -760,43 +738,38 @@
             - Domain: "apple.com" ‚Üí "Apple Inc."
             - Domain: "guinnessgi.com" ‚Üí "Guinness Asset Management Ltd"
             """
-            
+
             user_prompt = f"What is the official company name for the website domain {domain}? Search the web to find the most accurate and official company name associated with this domain. Return only the company name, nothing else."
-            
-            logger.info(f"Perplexity API call initiated for company name lookup", extra={
-                "domain": domain,
-                "model": "sonar-pro",
-                "operation": "get_company_name_from_domain"
-            })
-            
-            with trace_operation("tamradar_get_company_name", {
-                "domain": domain,
-                "model": "sonar-pro",
-                "search_type": "company_name_discovery"
-            }):
+
+            logger.info(
+                f"Perplexity API call initiated for company name lookup",
+                extra={"domain": domain, "model": "sonar-pro", "operation": "get_company_name_from_domain"},
+            )
+
+            with trace_operation("tamradar_get_company_name", {"domain": domain, "model": "sonar-pro", "search_type": "company_name_discovery"}):
                 response = await chat.ainvoke(
-                    [
-                        ("system", system_prompt),
-                        ("user", user_prompt)
-                    ],
+                    [("system", system_prompt), ("user", user_prompt)],
                     extra_body={
                         "web_search_options": {"search_context_size": "low"}  # Use "low" for faster response
-                    }
+                    },
                 )
-            
+
             api_response_time = time.time() - api_call_start_time
-            
+
             # API monitoring: Track successful Perplexity API call
-            logger.info("Perplexity API call successful", extra={
-                "domain": domain,
-                "model": "sonar-pro",
-                "response_time": api_response_time,
-                "api_call_type": "get_company_name_from_domain",
-                "content_length": len(response.content) if response.content else 0
-            })
-            
+            logger.info(
+                "Perplexity API call successful",
+                extra={
+                    "domain": domain,
+                    "model": "sonar-pro",
+                    "response_time": api_response_time,
+                    "api_call_type": "get_company_name_from_domain",
+                    "content_length": len(response.content) if response.content else 0,
+                },
+            )
+
             company_name = response.content.strip() if response.content else None
-            
+
             # Clean up response - remove quotes, markdown, "null" strings, etc.
             if company_name:
                 # Remove markdown code blocks if present
@@ -804,52 +777,49 @@
                     lines = company_name.split("\n")
                     company_name = "\n".join(lines[1:-1]) if len(lines) > 2 else company_name
                     company_name = company_name.strip("`").strip()
-                
+
                 # Remove quotes
-                company_name = company_name.strip('"\'')
-                
+                company_name = company_name.strip("\"'")
+
                 # Remove common prefixes/suffixes
                 company_name = company_name.replace("Company name:", "").replace("Company:", "").strip()
-                
+
                 # Check for null/unknown values
-                if company_name.lower() in ['null', 'none', 'unknown', 'n/a', 'not found', '']:
+                if company_name.lower() in ["null", "none", "unknown", "n/a", "not found", ""]:
                     logger.debug(f"Perplexity returned null/unknown for domain {domain}")
                     return None
-            
+
             if company_name:
-                logger.info(f"Retrieved company name '{company_name}' for domain {domain}", extra={
-                    "domain": domain,
-                    "company_name": company_name,
-                    "response_time": api_response_time
-                })
+                logger.info(
+                    f"Retrieved company name '{company_name}' for domain {domain}",
+                    extra={"domain": domain, "company_name": company_name, "response_time": api_response_time},
+                )
                 return company_name
             else:
                 logger.debug(f"No company name found for domain {domain}")
                 return None
-            
+
         except asyncio.TimeoutError:
             api_response_time = time.time() - api_call_start_time
-            logger.warning(f"Perplexity API call timed out for domain {domain}", extra={
-                "domain": domain,
-                "response_time": api_response_time,
-                "error_type": "timeout"
-            })
+            logger.warning(
+                f"Perplexity API call timed out for domain {domain}",
+                extra={"domain": domain, "response_time": api_response_time, "error_type": "timeout"},
+            )
             return None
         except Exception as e:
             api_response_time = time.time() - api_call_start_time
-            logger.warning(f"Failed to get company name from Perplexity for domain {domain}", extra={
-                "domain": domain,
-                "error": str(e),
-                "error_type": type(e).__name__,
-                "response_time": api_response_time
-            }, exc_info=True)
+            logger.warning(
+                f"Failed to get company name from Perplexity for domain {domain}",
+                extra={"domain": domain, "error": str(e), "error_type": type(e).__name__, "response_time": api_response_time},
+                exc_info=True,
+            )
             return None
 
     async def _link_user_to_radar(
-        self, 
-        user_id: str, 
-        radar_id: str, 
-        is_owner: bool = False, 
+        self,
+        user_id: str,
+        radar_id: str,
+        is_owner: bool = False,
         watchlist_category: Optional[str] = None,
         user_interests: Optional[str] = None,
         weekly_wrapup_email: bool = False,
@@ -858,7 +828,7 @@
     ) -> None:
         """
         Link user to radar in junction table.
-        
+
         Args:
             conn: Optional database connection. If provided, uses it; otherwise acquires a new one.
         """
@@ -912,58 +882,50 @@
                     weekly_wrapup_email,
                     tier1_email_alerts,
                 )
-        
+
         # Trigger radar context embedding generation (background task)
         try:
             import asyncio
             from app.services.radar_context_embedding_service import upsert_radar_context_embedding
-            
+
             # Trigger as background task (non-blocking)
-            asyncio.create_task(upsert_radar_context_embedding(
-                user_id=user_id,
-                radar_id=radar_id,
-                watchlist_notes=None,  # Will be fetched from DB (or use user_interests as fallback)
-            ))
+            asyncio.create_task(
+                upsert_radar_context_embedding(
+                    user_id=user_id,
+                    radar_id=radar_id,
+                    watchlist_notes=None,  # Will be fetched from DB (or use user_interests as fallback)
+                )
+            )
             logger.debug(f"Triggered radar context embedding generation for user {user_id}, radar {radar_id}")
         except Exception as e:
             # Log but don't fail the link operation if embedding fails
-            logger.warning(
-                f"Failed to trigger radar context embedding for user {user_id}, radar {radar_id}: {e}",
-                exc_info=True
-                )
+            logger.warning(f"Failed to trigger radar context embedding for user {user_id}, radar {radar_id}: {e}", exc_info=True)
 
-    async def _link_existing_findings_to_user(
-        self,
-        user_id: str,
-        radar_id: str,
-        conn=None,
-        days: int = 5,
-        limit: int = 100
-    ) -> List[Dict[str, Any]]:
+    async def _link_existing_findings_to_user(self, user_id: str, radar_id: str, conn=None, days: int = 5, limit: int = 100) -> List[Dict[str, Any]]:
         """
         Link existing findings from the last N days to a newly subscribed user.
         Returns formatted findings data for immediate display.
-        
+
         Args:
             user_id: User ID to link findings to
             radar_id: Radar ID to get findings from
             conn: Optional database connection (for transactions)
             days: Number of days to look back (default: 5)
             limit: Maximum number of findings to link (default: 100)
-            
+
         Returns:
             List of formatted finding dictionaries (same format as /findings endpoint)
         """
         from datetime import timedelta
-        
+
         db = await get_global_db()
         if not db:
             logger.warning(f"Database not available, cannot link existing findings for user {user_id}")
             return []
-        
+
         # Calculate date threshold
         date_threshold = datetime.now(timezone.utc) - timedelta(days=days)
-        
+
         # Use provided connection or acquire new one
         if conn:
             connection = conn
@@ -975,12 +937,9 @@
             # Strategy: Get findings that have been classified as Tier 1/2 by ANY user first,
             # then get unclassified findings to fill remaining slots
             finding_rows = []
-            
-            logger.debug(
-                f"Linking existing findings for user {user_id}, radar {radar_id}, "
-                f"date_threshold: {date_threshold}, limit: {limit}"
-            )
-            
+
+            logger.debug(f"Linking existing findings for user {user_id}, radar {radar_id}, date_threshold: {date_threshold}, limit: {limit}")
+
             # First, get Tier 1 findings (highest priority) - classified by ANY user
             tier1_rows = await connection.fetch(
                 """
@@ -1005,11 +964,11 @@
                 radar_id,
                 date_threshold.replace(tzinfo=None),
                 user_id,
-                limit
+                limit,
             )
             logger.debug(f"Tier 1 query returned {len(tier1_rows)} findings")
             finding_rows.extend(tier1_rows)
-            
+
             # Then get Tier 2 findings (if we haven't hit the limit)
             remaining_slots = limit - len(finding_rows)
             if remaining_slots > 0:
@@ -1041,7 +1000,7 @@
                         date_threshold.replace(tzinfo=None),
                         user_id,
                         tier1_ids,
-                        remaining_slots
+                        remaining_slots,
                     )
                 else:
                     tier2_rows = await connection.fetch(
@@ -1067,11 +1026,11 @@
                         radar_id,
                         date_threshold.replace(tzinfo=None),
                         user_id,
-                    remaining_slots
-                )
+                        remaining_slots,
+                    )
                 logger.debug(f"Tier 2 query returned {len(tier2_rows)} findings")
                 finding_rows.extend(tier2_rows)
-            
+
             # Finally, get other findings (unclassified) to fill remaining slots
             remaining_slots = limit - len(finding_rows)
             if remaining_slots > 0:
@@ -1101,7 +1060,7 @@
                         date_threshold.replace(tzinfo=None),
                         user_id,
                         already_selected_ids,
-                        remaining_slots
+                        remaining_slots,
                     )
                 else:
                     other_rows = await connection.fetch(
@@ -1125,32 +1084,32 @@
                         radar_id,
                         date_threshold.replace(tzinfo=None),
                         user_id,
-                    remaining_slots
-                )
+                        remaining_slots,
+                    )
                 logger.debug(f"Unclassified query returned {len(other_rows)} findings")
                 finding_rows.extend(other_rows)
-            
+
             logger.debug(
                 f"Total findings found: {len(finding_rows)} "
                 f"(Tier 1: {sum(1 for r in finding_rows if r.get('priority_tier') == 1)}, "
                 f"Tier 2: {sum(1 for r in finding_rows if r.get('priority_tier') == 2)}, "
                 f"Unclassified: {sum(1 for r in finding_rows if r.get('priority_tier') is None)})"
             )
-            
+
             if not finding_rows:
                 logger.debug(f"No existing findings found for radar {radar_id} in last {days} days")
                 return []
-            
+
             # Link findings to user (batch insert)
             linked_findings = []
             for row in finding_rows:
                 finding_id = row["finding_id"]
-                
+
                 # Get tier/score/category from query result (from other users' classifications)
                 priority_tier = row.get("priority_tier")
                 relevance_score = row.get("relevance_score")
                 category = row.get("category")
-                
+
                 # Insert into tamradar_user_findings (ON CONFLICT handles duplicates)
                 # Preserve tier information from other users' classifications
                 await connection.execute(
@@ -1168,7 +1127,7 @@
                     priority_tier,
                     category,
                 )
-                
+
                 # Parse finding_data if it's a JSON string
                 finding_data = row["finding_data"]
                 if isinstance(finding_data, str):
@@ -1176,7 +1135,7 @@
                         finding_data = json.loads(finding_data)
                     except (JSONDecodeError, TypeError, ValueError):
                         finding_data = {}
-                
+
                 # Parse radar_config if it's a JSON string
                 radar_config = row.get("radar_config")
                 if isinstance(radar_config, str):
@@ -1186,9 +1145,9 @@
                         radar_config = {}
                 elif radar_config is None:
                     radar_config = {}
-                
+
                 # Tier/score/category already retrieved above before INSERT
-                
+
                 # Format finding (same format as /findings endpoint)
                 formatted_finding = {
                     "finding_id": finding_id,
@@ -1205,21 +1164,19 @@
                     "category": category,
                     "radar_config": radar_config,
                 }
-                
+
                 linked_findings.append(formatted_finding)
-            
-            logger.info(
-                f"Linked {len(linked_findings)} existing findings to user {user_id} for radar {radar_id}"
-            )
-            
+
+            logger.info(f"Linked {len(linked_findings)} existing findings to user {user_id} for radar {radar_id}")
+
             # Queue relevance scoring tasks for each finding (non-blocking)
             try:
                 from app.tasks.tamradar_tasks import score_finding_relevance_task
-                
+
                 for row in finding_rows:
                     finding_id = row["finding_id"]
                     radar_type = row["radar_type"]
-                    
+
                     # Parse finding_data for the task
                     finding_data = row["finding_data"]
                     if isinstance(finding_data, str):
@@ -1227,88 +1184,71 @@
                             finding_data = json.loads(finding_data)
                         except (JSONDecodeError, TypeError, ValueError):
                             finding_data = {}
-                    
+
                     # Queue relevance scoring task
                     score_finding_relevance_task.delay(
                         finding_id=finding_id,
                         radar_id=radar_id,
                         finding_data=finding_data,
                         radar_type=radar_type,
-                        user_ids=[user_id]  # Only score for this new user
+                        user_ids=[user_id],  # Only score for this new user
                     )
-                
-                logger.debug(
-                    f"Queued {len(finding_rows)} relevance scoring tasks for user {user_id}, radar {radar_id}"
-                )
+
+                logger.debug(f"Queued {len(finding_rows)} relevance scoring tasks for user {user_id}, radar {radar_id}")
             except Exception as e:
                 # Log but don't fail if relevance scoring queue fails
-                logger.warning(
-                    f"Failed to queue relevance scoring tasks for existing findings: {e}",
-                    exc_info=True
-                )
-            
+                logger.warning(f"Failed to queue relevance scoring tasks for existing findings: {e}", exc_info=True)
+
             return linked_findings
-            
+
         finally:
             # Only release connection if we acquired it
             if not conn:
                 await db.pool.release(connection)
 
-    async def create_company_radar(
-        self, user_id: str, request: CompanyRadarRequest
-    ) -> Dict[str, Any]:
+    async def create_company_radar(self, user_id: str, request: CompanyRadarRequest) -> Dict[str, Any]:
         """
         Create company radars - by default creates 4 radar types:
         - company_social_posts_cxo
         - company_social_posts
         - company_new_hires
         - company_mentions
-        
+
         If radar_type is specified in request, only creates that one type.
         Returns list of all created/linked radars.
         """
         # Default radar types to create for a company (if radar_type not specified)
         # According to TAMradar API docs: https://tamradar.readme.io/reference/createcompanyradar
-        # Valid values: company_new_hires, company_job_openings, company_promotions, 
-        # company_reviews, company_mentions, company_social_posts, company_social_posts_cxo, 
+        # Valid values: company_new_hires, company_job_openings, company_promotions,
+        # company_reviews, company_mentions, company_social_posts, company_social_posts_cxo,
         # company_social_engagements
-        default_radar_types = [
-            "company_social_posts_cxo",
-            "company_social_posts",
-            "company_new_hires",
-            "company_mentions"
-        ]
-        
+        default_radar_types = ["company_social_posts_cxo", "company_social_posts", "company_new_hires", "company_mentions"]
+
         # If specific radar_type requested, only create that one
         if request.radar_type:
             default_radar_types = [request.radar_type]
-        
+
         # Check user's current radar count and limit (counts unique companies, not individual radars)
         count_info = await self.get_user_radar_count_and_limit(user_id)
         current_count = count_info["current_count"]
         user_limit = count_info["limit"]
-        
+
         # Normalize domain for comparison
         normalized_domain = normalize_domain(request.domain)
         # Extract root domain to remove 'www.' and other subdomains for comparison
         root_domain = extract_root_domain(normalized_domain) if normalized_domain else ""
-        
+
         # Validate domain before creating radar
-        validation_result = await self.validate_domain_before_creation(
-            request.domain,
-            user_id=user_id
-        )
+        validation_result = await self.validate_domain_before_creation(request.domain, user_id=user_id)
         # Store validation result for API response (warnings/errors)
         # Don't block creation on warnings, only on errors
-        
+
         # Check if user already has this company tracked
         existing_radars = await self.get_user_radars(user_id, group_by_company=True)
         domain_already_tracked = any(
-            extract_root_domain(normalize_domain(r.get("domain", ""))) == root_domain 
-            for r in existing_radars 
-            if r.get("radar_category") == "company"
+            extract_root_domain(normalize_domain(r.get("domain", ""))) == root_domain for r in existing_radars if r.get("radar_category") == "company"
         )
-        
+
         # If company is already tracked, allow adding more radar types (no limit check)
         # If company is new, check if adding 1 company would exceed limit
         if not domain_already_tracked and current_count >= user_limit:
@@ -1316,14 +1256,14 @@
                 f"Cannot add another company. "
                 f"You currently have {current_count} companies tracked and your limit is {user_limit}. "
                 f"Please remove some companies or contact an admin to increase your limit.",
-                status_code=400
+                status_code=400,
             )
-        
+
         # Check if we need to run Perplexity
         # Only run if no existing radars (in database, not just user-linked) have a Perplexity-derived company name
         derived_name = self._derive_company_name_from_domain(normalized_domain)
         needs_perplexity = True
-        
+
         # Check database directly for any existing radars with this domain
         db = await get_global_db()
         if db:
@@ -1339,41 +1279,42 @@
                           AND status IN ('active', 'pending')
                         LIMIT 1
                         """,
-                        normalized_domain
+                        normalized_domain,
                     )
-                    
+
                     if row:
                         radar_config = row.get("radar_config")
                         # Parse if needed
                         if isinstance(radar_config, str):
                             try:
                                 import json
+
                                 radar_config = json.loads(radar_config)
                             except (json.JSONDecodeError, TypeError, ValueError):
                                 radar_config = {}
                         elif not isinstance(radar_config, dict):
                             radar_config = {}
-                        
+
                         existing_company_name = radar_config.get("company_name") if isinstance(radar_config, dict) else None
                         if existing_company_name and existing_company_name != derived_name:
                             # Already has Perplexity name, don't need to run it again
                             needs_perplexity = False
-                            logger.info(f"Domain {normalized_domain} already has Perplexity name '{existing_company_name}' in database, skipping Perplexity task creation")
+                            logger.info(
+                                f"Domain {normalized_domain} already has Perplexity name '{existing_company_name}' in database, skipping Perplexity task creation"
+                            )
             except Exception as e:
                 logger.warning(f"Failed to check for existing Perplexity name in database: {e}, will create Perplexity task")
                 # If check fails, create the task anyway to be safe
-        
+
         # Start Perplexity call in parallel only if needed (non-blocking)
         company_name_task = None
         if needs_perplexity:
-            company_name_task = asyncio.create_task(
-                self._get_company_name_from_domain(normalized_domain)
-            )
-        
+            company_name_task = asyncio.create_task(self._get_company_name_from_domain(normalized_domain))
+
         # Create all radar types
         created_radars = []
         webhook_url = request.webhook_url or self.webhook_url
-        
+
         for radar_type in default_radar_types:
             try:
                 radar = await self._create_single_company_radar(
@@ -1387,30 +1328,28 @@
                     user_interests=request.user_interests,
                     weekly_wrapup_email=request.weekly_wrapup_email,
                     tier1_email_alerts=request.tier1_email_alerts,
-                    company_name_task=company_name_task if radar_type == default_radar_types[0] else None  # Only use for first radar
+                    company_name_task=company_name_task if radar_type == default_radar_types[0] else None,  # Only use for first radar
                 )
                 # Handle both dict (with recent_findings) and TAMradarRadarResponse
                 if isinstance(radar, dict):
                     # Radar is existing, includes recent_findings
                     created_radars.append(radar["radar"])
                     # Store recent_findings for later attachment to response
-                    if not hasattr(self, '_recent_findings_by_radar'):
+                    if not hasattr(self, "_recent_findings_by_radar"):
                         self._recent_findings_by_radar = {}
                     recent_findings_list = radar.get("recent_findings", [])
-                    logger.debug(
-                        f"Storing {len(recent_findings_list)} recent findings for radar {radar['radar'].radar_id}"
-                    )
+                    logger.debug(f"Storing {len(recent_findings_list)} recent findings for radar {radar['radar'].radar_id}")
                     self._recent_findings_by_radar[radar["radar"].radar_id] = recent_findings_list
                 else:
                     created_radars.append(radar)
             except Exception as e:
                 logger.error(
                     f"Failed to create radar type {radar_type} for {request.domain}: {e}",
-                    exc_info=True  # Include full traceback
+                    exc_info=True,  # Include full traceback
                 )
                 # Continue with other radars even if one fails
                 continue
-        
+
         # Wait for company name lookup to complete and update all radars
         if company_name_task:
             try:
@@ -1424,31 +1363,25 @@
                             await self._update_radar_config(radar.radar_id, radar.radar_config)
             except Exception as e:
                 logger.warning(f"Company name lookup failed: {e}")
-        
+
         if not created_radars:
             raise TAMradarServiceError(
-                "Failed to create any radars for the company. Please try again or contact support if the issue persists.",
-                status_code=500
+                "Failed to create any radars for the company. Please try again or contact support if the issue persists.", status_code=500
             )
-        
+
         # Attach recent_findings to response if any were found
-        recent_findings_by_radar = getattr(self, '_recent_findings_by_radar', {})
-        logger.info(
-            f"Returning {len(created_radars)} radars with recent_findings_by_radar: {list(recent_findings_by_radar.keys())}"
-        )
+        recent_findings_by_radar = getattr(self, "_recent_findings_by_radar", {})
+        logger.info(f"Returning {len(created_radars)} radars with recent_findings_by_radar: {list(recent_findings_by_radar.keys())}")
         for radar_id, findings in recent_findings_by_radar.items():
             logger.debug(f"Radar {radar_id} has {len(findings)} recent findings")
-        
-        result = {
-            "radars": created_radars,
-            "recent_findings_by_radar": recent_findings_by_radar
-        }
+
+        result = {"radars": created_radars, "recent_findings_by_radar": recent_findings_by_radar}
         # Clean up temporary attribute
-        if hasattr(self, '_recent_findings_by_radar'):
-            delattr(self, '_recent_findings_by_radar')
-        
+        if hasattr(self, "_recent_findings_by_radar"):
+            delattr(self, "_recent_findings_by_radar")
+
         return result
-    
+
     async def _create_single_company_radar(
         self,
         user_id: str,
@@ -1461,7 +1394,7 @@
         user_interests: Optional[str] = None,
         weekly_wrapup_email: bool = False,
         tier1_email_alerts: bool = False,
-        company_name_task: Optional[asyncio.Task] = None
+        company_name_task: Optional[asyncio.Task] = None,
     ) -> TAMradarRadarResponse:
         """Create a single company radar of a specific type."""
         # Normalize and extract root domain before storing (removes 'www.' and other subdomains)
@@ -1477,28 +1410,24 @@
         if existing:
             logger.info(f"Found existing radar {existing['radar_id']} for {radar_type}, linking user {user_id}")
             await self._link_user_to_radar(
-                user_id, existing["radar_id"], is_owner=False, 
+                user_id,
+                existing["radar_id"],
+                is_owner=False,
                 watchlist_category=watchlist_category,
                 user_interests=user_interests,
                 weekly_wrapup_email=weekly_wrapup_email,
-                tier1_email_alerts=tier1_email_alerts
+                tier1_email_alerts=tier1_email_alerts,
             )
-            
+
             # Link existing findings from last 5 days to the new user
             # If this fails, we still want to return the radar, so catch exceptions
             recent_findings = []
             try:
-                recent_findings = await self._link_existing_findings_to_user(
-                    user_id=user_id,
-                    radar_id=existing["radar_id"]
-                )
+                recent_findings = await self._link_existing_findings_to_user(user_id=user_id, radar_id=existing["radar_id"])
             except Exception as e:
                 # Log but don't fail radar creation if linking findings fails
-                logger.warning(
-                    f"Failed to link existing findings for user {user_id}, radar {existing['radar_id']}: {e}",
-                    exc_info=True
-                )
-            
+                logger.warning(f"Failed to link existing findings for user {user_id}, radar {existing['radar_id']}: {e}", exc_info=True)
+
             # Parse next_charge_at if it's a datetime object
             next_charge_at_str = None
             if existing.get("next_charge_at"):
@@ -1507,7 +1436,7 @@
                     next_charge_at_str = next_charge_at.isoformat()
                 elif isinstance(next_charge_at, str):
                     next_charge_at_str = next_charge_at
-            
+
             # Parse created_at if it's a datetime object
             created_at_str = ""
             if existing.get("created_at"):
@@ -1516,7 +1445,7 @@
                     created_at_str = created_at.isoformat()
                 elif isinstance(created_at, str):
                     created_at_str = created_at
-            
+
             # Parse custom_fields and filters if they're JSON strings
             custom_fields = existing.get("custom_fields")
             if isinstance(custom_fields, str):
@@ -1524,14 +1453,14 @@
                     custom_fields = json.loads(custom_fields)
                 except (JSONDecodeError, TypeError, ValueError):
                     custom_fields = None
-            
+
             filters = existing.get("filters")
             if isinstance(filters, str):
                 try:
                     filters = json.loads(filters)
                 except (JSONDecodeError, TypeError, ValueError):
                     filters = None
-            
+
             # Sync company_name to custom_fields.watchlist_name for frontend compatibility
             existing_radar_config = existing.get("radar_config", {})
             # Ensure radar_config is a dict (PostgreSQL JSONB returns as dict, but handle string case too)
@@ -1544,7 +1473,7 @@
             elif not isinstance(existing_radar_config, dict):
                 logger.warning(f"existing_radar_config is not dict or string (type: {type(existing_radar_config)}), setting to empty dict")
                 existing_radar_config = {}
-            
+
             existing_company_name = existing_radar_config.get("company_name")
             logger.info(
                 f"Existing radar {existing['radar_id']} for domain {normalized_domain}: "
@@ -1552,7 +1481,7 @@
                 f"company_name='{existing_company_name}', "
                 f"radar_config keys={list(existing_radar_config.keys()) if isinstance(existing_radar_config, dict) else 'N/A'}"
             )
-            
+
             # Parse existing custom_fields
             existing_custom_fields = existing.get("custom_fields")
             if isinstance(existing_custom_fields, str):
@@ -1562,18 +1491,18 @@
                     existing_custom_fields = {}
             elif existing_custom_fields is None:
                 existing_custom_fields = {}
-            
+
             # Calculate what the derived name would be
             derived_name = self._derive_company_name_from_domain(normalized_domain)
             needs_update = False
             should_run_perplexity = False
-            
+
             logger.info(
                 f"Decision logic for existing radar {existing['radar_id']} for domain {normalized_domain}: "
                 f"existing_company_name='{existing_company_name}', derived_name='{derived_name}', "
                 f"equal={existing_company_name == derived_name if existing_company_name else 'N/A'}"
             )
-            
+
             if not existing_company_name:
                 # No company_name exists - set derived name and run Perplexity
                 if derived_name:
@@ -1588,7 +1517,9 @@
                 # Company name equals derived name - means it hasn't been updated by Perplexity yet
                 # Run Perplexity to get the real company name
                 should_run_perplexity = True
-                logger.info(f"Existing radar has derived name '{existing_company_name}' (matches derived '{derived_name}'), triggering Perplexity lookup for domain {normalized_domain}")
+                logger.info(
+                    f"Existing radar has derived name '{existing_company_name}' (matches derived '{derived_name}'), triggering Perplexity lookup for domain {normalized_domain}"
+                )
             elif existing_company_name and existing_company_name != derived_name:
                 # Radar already has Perplexity-derived company_name (different from derived name)
                 # Sync to watchlist_name but don't run Perplexity
@@ -1596,14 +1527,21 @@
                 if existing_custom_fields.get("watchlist_name") != existing_company_name:
                     existing_custom_fields["watchlist_name"] = existing_company_name
                     needs_update = True
-                    logger.info(f"Synced company_name '{existing_company_name}' to watchlist_name for existing radar {existing['radar_id']} (skipping Perplexity - already has real name)")
+                    logger.info(
+                        f"Synced company_name '{existing_company_name}' to watchlist_name for existing radar {existing['radar_id']} (skipping Perplexity - already has real name)"
+                    )
                 else:
-                    logger.info(f"Existing radar {existing['radar_id']} already has Perplexity name '{existing_company_name}' synced to watchlist_name, skipping Perplexity")
+                    logger.info(
+                        f"Existing radar {existing['radar_id']} already has Perplexity name '{existing_company_name}' synced to watchlist_name, skipping Perplexity"
+                    )
             else:
-                logger.warning(f"Unexpected branch reached for radar {existing['radar_id']}: existing_company_name='{existing_company_name}', derived_name='{derived_name}'")
-            
+                logger.warning(
+                    f"Unexpected branch reached for radar {existing['radar_id']}: existing_company_name='{existing_company_name}', derived_name='{derived_name}'"
+                )
+
             # Trigger Perplexity lookup ONLY if needed
             if should_run_perplexity:
+
                 async def lookup_and_update_company_name():
                     try:
                         perplexity_name = await self._get_company_name_from_domain(normalized_domain)
@@ -1616,7 +1554,7 @@
                             logger.debug(f"Perplexity did not return company name for {normalized_domain}, keeping current name '{current_name}'")
                     except Exception as e:
                         logger.warning(f"Failed to lookup and update company name for existing radar: {e}")
-                
+
                 # If company_name_task is provided, use it; otherwise start new lookup
                 if company_name_task:
                     # Wait for task with timeout, then update if available
@@ -1632,7 +1570,7 @@
                             asyncio.create_task(lookup_and_update_company_name())
                         except Exception as e:
                             logger.warning(f"Failed to get company name from task: {e}")
-                    
+
                     asyncio.create_task(wait_and_update())
                 else:
                     # Start new Perplexity lookup in background
@@ -1640,11 +1578,13 @@
             else:
                 # Perplexity should NOT run - don't use the task if provided
                 if company_name_task:
-                    logger.info(f"Skipping Perplexity lookup for existing radar {existing['radar_id']} - already has real company name '{existing_company_name}'")
+                    logger.info(
+                        f"Skipping Perplexity lookup for existing radar {existing['radar_id']} - already has real company name '{existing_company_name}'"
+                    )
                     # Don't cancel the task as it might be used by other radars being created in parallel
                     # Just ignore it - it will complete in the background but won't update anything
                     # since should_run_perplexity is False
-            
+
             # Update database if needed
             if needs_update:
                 db = await get_global_db()
@@ -1662,7 +1602,7 @@
                             json.dumps(existing_custom_fields),
                             existing["radar_id"],
                         )
-            
+
             # Return response - recent_findings will be attached in the router
             return {
                 "radar": TAMradarRadarResponse(
@@ -1679,7 +1619,7 @@
                     updated_at=None,
                     deactivated_at=None,
                 ),
-                "recent_findings": recent_findings
+                "recent_findings": recent_findings,
             }
 
         # Create new radar via API
@@ -1689,13 +1629,13 @@
                 f"No webhook URL configured for {radar_type} - TAMradar will not be able to send webhooks. "
                 "Set TAMRADAR_WEBHOOK_URL to your API Gateway endpoint."
             )
-        
+
         # Use normalized domain for TAMradar API (TAMradar normalizes domains internally)
         payload = {
             "domain": normalized_domain,
             "radar_type": radar_type,
         }
-        
+
         # Only include webhook_url if provided (TAMradar API may require it)
         if webhook_url:
             payload["webhook_url"] = webhook_url
@@ -1713,20 +1653,20 @@
                 # Try to find the existing radar by querying TAMradar
                 try:
                     synced_radar = await self._sync_existing_radar_from_tamradar(
-                        domain=normalized_domain,
-                        radar_type=radar_type,
-                        radar_category="company"
+                        domain=normalized_domain, radar_type=radar_type, radar_category="company"
                     )
                     if synced_radar:
                         # Link user to the synced radar
                         await self._link_user_to_radar(
-                            user_id, synced_radar["radar_id"], is_owner=False, 
+                            user_id,
+                            synced_radar["radar_id"],
+                            is_owner=False,
                             watchlist_category=watchlist_category,
                             user_interests=user_interests,
                             weekly_wrapup_email=weekly_wrapup_email,
-                            tier1_email_alerts=tier1_email_alerts
+                            tier1_email_alerts=tier1_email_alerts,
                         )
-                        
+
                         # Check if company_name is missing and set derived name, then trigger Perplexity lookup
                         synced_radar_config = synced_radar.get("radar_config", {})
                         if isinstance(synced_radar_config, dict):
@@ -1744,9 +1684,9 @@
                                             synced_custom_fields = {}
                                     elif synced_custom_fields is None:
                                         synced_custom_fields = {}
-                                    
+
                                     synced_custom_fields["watchlist_name"] = derived_name
-                                    
+
                                     # Update both radar_config and custom_fields in database
                                     db = await get_global_db()
                                     if db:
@@ -1764,7 +1704,7 @@
                                                 synced_radar["radar_id"],
                                             )
                                     logger.info(f"Set derived company name '{derived_name}' for synced radar {synced_radar['radar_id']}")
-                                
+
                                 # Trigger Perplexity lookup in background to get official name
                                 async def lookup_and_update_company_name():
                                     try:
@@ -1772,14 +1712,18 @@
                                         if perplexity_name and perplexity_name != derived_name:
                                             # Update with Perplexity name if different from derived name
                                             await self._update_all_radars_company_name_for_domain(normalized_domain, perplexity_name)
-                                            logger.info(f"Updated company name from '{derived_name}' to '{perplexity_name}' for domain {normalized_domain}")
+                                            logger.info(
+                                                f"Updated company name from '{derived_name}' to '{perplexity_name}' for domain {normalized_domain}"
+                                            )
                                         elif not perplexity_name:
-                                            logger.debug(f"Perplexity did not return company name for {normalized_domain}, keeping derived name '{derived_name}'")
+                                            logger.debug(
+                                                f"Perplexity did not return company name for {normalized_domain}, keeping derived name '{derived_name}'"
+                                            )
                                     except Exception as e:
                                         logger.warning(f"Failed to lookup and update company name for synced radar: {e}")
-                                
+
                                 asyncio.create_task(lookup_and_update_company_name())
-                        
+
                         return TAMradarRadarResponse(
                             radar_id=synced_radar["radar_id"],
                             radar_category="company",
@@ -1796,28 +1740,21 @@
                         )
                 except Exception as sync_error:
                     logger.warning(f"Failed to sync existing radar {radar_type}: {sync_error}")
-                
+
                 # If sync failed, provide helpful error message
                 raise TAMradarServiceError(
-                    f"A radar for domain '{domain}' with type '{radar_type}' already exists. "
-                    f"Please try a different domain/type combination.",
-                    status_code=409
+                    f"A radar for domain '{domain}' with type '{radar_type}' already exists. Please try a different domain/type combination.",
+                    status_code=409,
                 ) from e
             raise
-        
+
         if response_data.get("status") != "success":
-            raise TAMradarServiceError(
-                f"Failed to create radar: {response_data.get('message', 'Unknown error')}",
-                status_code=500
-            )
+            raise TAMradarServiceError(f"Failed to create radar: {response_data.get('message', 'Unknown error')}", status_code=500)
 
         radar_data = response_data.get("data", {})
         radar_id = radar_data.get("radar_id")
         if not radar_id:
-            raise TAMradarServiceError(
-                "Invalid response from the service. Please try again or contact support.",
-                status_code=500
-            )
+            raise TAMradarServiceError("Invalid response from the service. Please try again or contact support.", status_code=500)
 
         # Set derived name immediately if company_name is not set
         if not radar_config.get("company_name"):
@@ -1829,9 +1766,10 @@
                     response_custom_fields = {}
                 response_custom_fields["watchlist_name"] = derived_name
                 logger.info(f"Set derived company name '{derived_name}' for new radar {domain}")
-        
+
         # Trigger Perplexity lookup in background to get official name (if task provided)
         if company_name_task:
+
             async def update_with_perplexity_name():
                 try:
                     perplexity_name = await company_name_task
@@ -1849,13 +1787,13 @@
                     logger.debug("Company name lookup was cancelled")
                 except Exception as e:
                     logger.warning(f"Company name lookup failed or timed out: {e}")
-            
+
             # Run in background (non-blocking)
             asyncio.create_task(update_with_perplexity_name())
 
         # Extract fields from TAMradar response (matches GET /radars structure)
         # TAMradar API uses "radar_status" not "status"
-        # 
+        #
         # IMPORTANT: TAMradar Status Behavior
         # - When creating a radar, TAMradar ALWAYS returns "active" status initially
         # - If TAMradar later determines it can't track, it sends a "radar_failure" webhook (24-96 hours later)
@@ -1881,11 +1819,8 @@
         # If either fails, nothing is stored locally
         db = await get_global_db()
         if not db:
-            raise TAMradarServiceError(
-                "Database not available - cannot store radar locally. Please try again or contact support.",
-                status_code=500
-            )
-        
+            raise TAMradarServiceError("Database not available - cannot store radar locally. Please try again or contact support.", status_code=500)
+
         async with db.pool.acquire() as conn:
             async with conn.transaction():
                 # Store radar in database (within transaction)
@@ -1901,10 +1836,12 @@
                     next_charge_at=radar_data.get("next_charge_at"),
                     conn=conn,  # Pass connection for transaction
                 )
-                
+
                 # Link user as owner (within same transaction)
                 await self._link_user_to_radar(
-                    user_id, radar_id, is_owner=True, 
+                    user_id,
+                    radar_id,
+                    is_owner=True,
                     watchlist_category=watchlist_category,
                     user_interests=user_interests,
                     weekly_wrapup_email=weekly_wrapup_email,
@@ -1926,14 +1863,14 @@
             updated_at=updated_at,
             deactivated_at=deactivated_at,
         )
-    
+
     async def _update_radar_config(self, radar_id: str, radar_config: Dict[str, Any]) -> None:
         """Update radar_config in database."""
         db = await get_global_db()
         if not db:
             logger.warning("Database not available, skipping radar config update")
             return
-        
+
         async with db.pool.acquire() as conn:
             await conn.execute(
                 """
@@ -1955,7 +1892,7 @@
         if not db:
             logger.warning("Database not available, skipping company name update for all radars")
             return
-        
+
         try:
             async with db.pool.acquire() as conn:
                 # Find all radars with this domain
@@ -1966,19 +1903,19 @@
                     WHERE radar_config->>'domain' = $1
                       AND radar_category = 'company'
                     """,
-                    domain
+                    domain,
                 )
-                
+
                 if not rows:
                     logger.debug(f"No radars found for domain {domain} to update company_name")
                     return
-                
+
                 updated_count = 0
                 for row in rows:
                     radar_id = row["radar_id"]
                     radar_config = row["radar_config"]
                     custom_fields = row.get("custom_fields")
-                    
+
                     # Parse radar_config if it's a string
                     if isinstance(radar_config, str):
                         try:
@@ -1988,7 +1925,7 @@
                             continue
                     elif not isinstance(radar_config, dict):
                         radar_config = {}
-                    
+
                     # Parse custom_fields if it's a string
                     if isinstance(custom_fields, str):
                         try:
@@ -1997,17 +1934,17 @@
                             custom_fields = {}
                     elif custom_fields is None:
                         custom_fields = {}
-                    
+
                     # Update radar_config.company_name
                     if radar_config.get("company_name") != company_name:
                         radar_config["company_name"] = company_name
                         updated_count += 1
-                    
+
                     # Update custom_fields.watchlist_name (for frontend compatibility)
                     if custom_fields.get("watchlist_name") != company_name:
                         custom_fields["watchlist_name"] = company_name
                         updated_count += 1
-                    
+
                     # Update both fields in database
                     await conn.execute(
                         """
@@ -2021,22 +1958,22 @@
                         json.dumps(custom_fields),
                         radar_id,
                     )
-                
+
                 if updated_count > 0:
-                    logger.info(f"Updated company_name '{company_name}' and custom_fields.watchlist_name for {len(rows)} radar(s) with domain {domain}")
-                
+                    logger.info(
+                        f"Updated company_name '{company_name}' and custom_fields.watchlist_name for {len(rows)} radar(s) with domain {domain}"
+                    )
+
         except Exception as e:
             logger.error(f"Failed to update all radars for domain {domain} with company name '{company_name}': {e}", exc_info=True)
 
-    async def create_contact_radar(
-        self, user_id: str, request: ContactRadarRequest
-    ) -> TAMradarRadarResponse:
+    async def create_contact_radar(self, user_id: str, request: ContactRadarRequest) -> TAMradarRadarResponse:
         """Create a contact radar, or link user to existing one if it exists."""
         # Check user's current radar count and limit
         count_info = await self.get_user_radar_count_and_limit(user_id)
         current_count = count_info["current_count"]
         user_limit = count_info["limit"]
-        
+
         # Normalize and extract root domain before storing (removes 'www.' and other subdomains)
         normalized_domain = normalize_domain(request.domain)
         root_domain = extract_root_domain(normalized_domain) if normalized_domain else ""
@@ -2055,11 +1992,13 @@
         if existing:
             logger.info(f"Found existing radar {existing['radar_id']}, linking user {user_id}")
             await self._link_user_to_radar(
-                user_id, existing["radar_id"], is_owner=False, 
+                user_id,
+                existing["radar_id"],
+                is_owner=False,
                 watchlist_category=request.watchlist_category,
                 user_interests=request.user_interests,
                 weekly_wrapup_email=request.weekly_wrapup_email,
-                tier1_email_alerts=request.tier1_email_alerts
+                tier1_email_alerts=request.tier1_email_alerts,
             )
             # Parse fields from existing radar (same logic as company radar)
             next_charge_at_str = None
@@ -2069,7 +2008,7 @@
                     next_charge_at_str = next_charge_at.isoformat()
                 elif isinstance(next_charge_at, str):
                     next_charge_at_str = next_charge_at
-            
+
             created_at_str = ""
             if existing.get("created_at"):
                 created_at = existing["created_at"]
@@ -2077,21 +2016,21 @@
                     created_at_str = created_at.isoformat()
                 elif isinstance(created_at, str):
                     created_at_str = created_at
-            
+
             custom_fields = existing.get("custom_fields")
             if isinstance(custom_fields, str):
                 try:
                     custom_fields = json.loads(custom_fields)
                 except (JSONDecodeError, TypeError, ValueError):
                     custom_fields = None
-            
+
             filters = existing.get("filters")
             if isinstance(filters, str):
                 try:
                     filters = json.loads(filters)
                 except (JSONDecodeError, TypeError, ValueError):
                     filters = None
-            
+
             return TAMradarRadarResponse(
                 radar_id=existing["radar_id"],
                 radar_category="contact",
@@ -2106,27 +2045,27 @@
                 updated_at=None,
                 deactivated_at=None,
             )
-        
+
         # If creating new radar, check limit (existing radars already checked above)
         if current_count >= user_limit:
             raise TAMradarServiceError(
                 f"Cannot add another contact radar. "
                 f"You currently have {current_count} radars tracked and your limit is {user_limit}. "
                 f"Please remove some radars or contact an admin to increase your limit.",
-                status_code=400
+                status_code=400,
             )
 
         webhook_url = request.webhook_url or self.webhook_url
-        
+
         if not webhook_url:
             logger.warning("No webhook URL configured for contact radar")
-        
+
         # Use normalized domain for TAMradar API
         payload = {
             "domain": normalized_domain,
             "radar_type": request.radar_type,
         }
-        
+
         if webhook_url:
             payload["webhook_url"] = webhook_url
         if request.contact_email:
@@ -2146,17 +2085,17 @@
                 logger.info(f"Contact radar already exists in TAMradar, attempting to sync...")
                 try:
                     synced_radar = await self._sync_existing_radar_from_tamradar(
-                        domain=normalized_domain,
-                        radar_type=request.radar_type,
-                        radar_category="contact"
+                        domain=normalized_domain, radar_type=request.radar_type, radar_category="contact"
                     )
                     if synced_radar:
                         await self._link_user_to_radar(
-                            user_id, synced_radar["radar_id"], is_owner=False, 
+                            user_id,
+                            synced_radar["radar_id"],
+                            is_owner=False,
                             watchlist_category=request.watchlist_category,
                             user_interests=request.user_interests,
                             weekly_wrapup_email=request.weekly_wrapup_email,
-                            tier1_email_alerts=request.tier1_email_alerts
+                            tier1_email_alerts=request.tier1_email_alerts,
                         )
                         return TAMradarRadarResponse(
                             radar_id=synced_radar["radar_id"],
@@ -2174,29 +2113,20 @@
                         )
                 except Exception as sync_error:
                     logger.warning(f"Failed to sync existing contact radar: {sync_error}")
-                raise TAMradarServiceError(
-                    f"A contact radar already exists. Please try again.",
-                    status_code=409
-                ) from e
+                raise TAMradarServiceError(f"A contact radar already exists. Please try again.", status_code=409) from e
             raise
-        
+
         if response_data.get("status") != "success":
-            raise TAMradarServiceError(
-                f"Failed to create radar: {response_data.get('message', 'Unknown error')}",
-                status_code=500
-            )
+            raise TAMradarServiceError(f"Failed to create radar: {response_data.get('message', 'Unknown error')}", status_code=500)
 
         radar_data = response_data.get("data", {})
         radar_id = radar_data.get("radar_id")
         if not radar_id:
-            raise TAMradarServiceError(
-                "Invalid response from the service. Please try again or contact support.",
-                status_code=500
-            )
+            raise TAMradarServiceError("Invalid response from the service. Please try again or contact support.", status_code=500)
 
         # Extract fields from TAMradar response (matches GET /radars structure)
         # TAMradar API uses "radar_status" not "status"
-        # 
+        #
         # IMPORTANT: TAMradar Status Behavior
         # - When creating a radar, TAMradar ALWAYS returns "active" status initially
         # - If TAMradar later determines it can't track, it sends a "radar_failure" webhook (24-96 hours later)
@@ -2220,11 +2150,8 @@
         # If either fails, nothing is stored locally
         db = await get_global_db()
         if not db:
-            raise TAMradarServiceError(
-                "Database not available - cannot store radar locally. Please try again or contact support.",
-                status_code=500
-            )
-        
+            raise TAMradarServiceError("Database not available - cannot store radar locally. Please try again or contact support.", status_code=500)
+
         async with db.pool.acquire() as conn:
             async with conn.transaction():
                 # Store radar in database (within transaction)
@@ -2240,10 +2167,12 @@
                     next_charge_at=radar_data.get("next_charge_at"),
                     conn=conn,  # Pass connection for transaction
                 )
-                
+
                 # Link user as owner (within same transaction)
                 await self._link_user_to_radar(
-                    user_id, radar_id, is_owner=True, 
+                    user_id,
+                    radar_id,
+                    is_owner=True,
                     watchlist_category=request.watchlist_category,
                     user_interests=request.user_interests,
                     weekly_wrapup_email=request.weekly_wrapup_email,
@@ -2254,6 +2183,7 @@
         # Check balance after creating radar
         try:
             from app.services.tamradar_balance_monitor import tamradar_balance_monitor
+
             await tamradar_balance_monitor.check_balance_and_alert()
         except Exception as e:
             logger.warning(f"Failed to check balance after radar creation: {e}")
@@ -2273,15 +2203,13 @@
             deactivated_at=deactivated_at,
         )
 
-    async def create_industry_radar(
-        self, user_id: str, request: IndustryRadarRequest
-    ) -> TAMradarRadarResponse:
+    async def create_industry_radar(self, user_id: str, request: IndustryRadarRequest) -> TAMradarRadarResponse:
         """Create an industry radar, or link user to existing one if it exists."""
         # Check user's current radar count and limit
         count_info = await self.get_user_radar_count_and_limit(user_id)
         current_count = count_info["current_count"]
         user_limit = count_info["limit"]
-        
+
         radar_config = {
             "industry_keyword": request.industry_keyword,
             "unique_key": None,
@@ -2294,11 +2222,13 @@
         if existing:
             logger.info(f"Found existing radar {existing['radar_id']}, linking user {user_id}")
             await self._link_user_to_radar(
-                user_id, existing["radar_id"], is_owner=False, 
+                user_id,
+                existing["radar_id"],
+                is_owner=False,
                 watchlist_category=request.watchlist_category,
                 user_interests=request.user_interests,
                 weekly_wrapup_email=request.weekly_wrapup_email,
-                tier1_email_alerts=request.tier1_email_alerts
+                tier1_email_alerts=request.tier1_email_alerts,
             )
             # Parse fields from existing radar (same logic as company radar)
             next_charge_at_str = None
@@ -2308,7 +2238,7 @@
                     next_charge_at_str = next_charge_at.isoformat()
                 elif isinstance(next_charge_at, str):
                     next_charge_at_str = next_charge_at
-            
+
             created_at_str = ""
             if existing.get("created_at"):
                 created_at = existing["created_at"]
@@ -2316,21 +2246,21 @@
                     created_at_str = created_at.isoformat()
                 elif isinstance(created_at, str):
                     created_at_str = created_at
-            
+
             custom_fields = existing.get("custom_fields")
             if isinstance(custom_fields, str):
                 try:
                     custom_fields = json.loads(custom_fields)
                 except (JSONDecodeError, TypeError, ValueError):
                     custom_fields = None
-            
+
             filters = existing.get("filters")
             if isinstance(filters, str):
                 try:
                     filters = json.loads(filters)
                 except (JSONDecodeError, TypeError, ValueError):
                     filters = None
-            
+
             # Return response with recent_findings attached
             return {
                 "radar": TAMradarRadarResponse(
@@ -2347,28 +2277,28 @@
                     updated_at=None,
                     deactivated_at=None,
                 ),
-                "recent_findings": recent_findings
+                "recent_findings": recent_findings,
             }
-        
+
         # If creating new radar, check limit (existing radars already checked above)
         if current_count >= user_limit:
             raise TAMradarServiceError(
                 f"Cannot add another industry radar. "
                 f"You currently have {current_count} radars tracked and your limit is {user_limit}. "
                 f"Please remove some radars or contact an admin to increase your limit.",
-                status_code=400
+                status_code=400,
             )
 
         webhook_url = request.webhook_url or self.webhook_url
-        
+
         if not webhook_url:
             logger.warning("No webhook URL configured for industry radar")
-        
+
         payload = {
             "industry_keyword": request.industry_keyword,
             "radar_type": request.radar_type,
         }
-        
+
         if webhook_url:
             payload["webhook_url"] = webhook_url
         if request.custom_fields:
@@ -2385,15 +2315,17 @@
                         domain=None,  # Industry radars don't have domain
                         radar_type=request.radar_type,
                         radar_category="industry",
-                        industry_keyword=request.industry_keyword
+                        industry_keyword=request.industry_keyword,
                     )
                     if synced_radar and synced_radar.get("radar_config", {}).get("industry_keyword") == request.industry_keyword:
                         await self._link_user_to_radar(
-                            user_id, synced_radar["radar_id"], is_owner=False, 
+                            user_id,
+                            synced_radar["radar_id"],
+                            is_owner=False,
                             watchlist_category=request.watchlist_category,
                             user_interests=request.user_interests,
                             weekly_wrapup_email=request.weekly_wrapup_email,
-                            tier1_email_alerts=request.tier1_email_alerts
+                            tier1_email_alerts=request.tier1_email_alerts,
                         )
                         return TAMradarRadarResponse(
                             radar_id=synced_radar["radar_id"],
@@ -2412,28 +2344,21 @@
                 except Exception as sync_error:
                     logger.warning(f"Failed to sync existing industry radar: {sync_error}")
                 raise TAMradarServiceError(
-                    f"An industry radar for '{request.industry_keyword}' already exists. Please try again.",
-                    status_code=409
+                    f"An industry radar for '{request.industry_keyword}' already exists. Please try again.", status_code=409
                 ) from e
             raise
-        
+
         if response_data.get("status") != "success":
-            raise TAMradarServiceError(
-                f"Failed to create radar: {response_data.get('message', 'Unknown error')}",
-                status_code=500
-            )
+            raise TAMradarServiceError(f"Failed to create radar: {response_data.get('message', 'Unknown error')}", status_code=500)
 
         radar_data = response_data.get("data", {})
         radar_id = radar_data.get("radar_id")
         if not radar_id:
-            raise TAMradarServiceError(
-                "Invalid response from the service. Please try again or contact support.",
-                status_code=500
-            )
+            raise TAMradarServiceError("Invalid response from the service. Please try again or contact support.", status_code=500)
 
         # Extract fields from TAMradar response (matches GET /radars structure)
         # TAMradar API uses "radar_status" not "status"
-        # 
+        #
         # IMPORTANT: TAMradar Status Behavior
         # - When creating a radar, TAMradar ALWAYS returns "active" status initially
         # - If TAMradar later determines it can't track, it sends a "radar_failure" webhook (24-96 hours later)
@@ -2457,11 +2382,8 @@
         # If either fails, nothing is stored locally
         db = await get_global_db()
         if not db:
-            raise TAMradarServiceError(
-                "Database not available - cannot store radar locally. Please try again or contact support.",
-                status_code=500
-            )
-        
+            raise TAMradarServiceError("Database not available - cannot store radar locally. Please try again or contact support.", status_code=500)
+
         async with db.pool.acquire() as conn:
             async with conn.transaction():
                 # Store radar in database (within transaction)
@@ -2477,10 +2399,12 @@
                     next_charge_at=radar_data.get("next_charge_at"),
                     conn=conn,  # Pass connection for transaction
                 )
-                
+
                 # Link user as owner (within same transaction)
                 await self._link_user_to_radar(
-                    user_id, radar_id, is_owner=True, 
+                    user_id,
+                    radar_id,
+                    is_owner=True,
                     watchlist_category=request.watchlist_category,
                     user_interests=request.user_interests,
                     weekly_wrapup_email=request.weekly_wrapup_email,
@@ -2491,6 +2415,7 @@
         # Check balance after creating radar
         try:
             from app.services.tamradar_balance_monitor import tamradar_balance_monitor
+
             await tamradar_balance_monitor.check_balance_and_alert()
         except Exception as e:
             logger.warning(f"Failed to check balance after radar creation: {e}")
@@ -2518,7 +2443,7 @@
                 return None
 
             radar_data = response_data.get("data", {})
-            
+
             # Update database
             db = await get_global_db()
             if db:
@@ -2532,12 +2457,12 @@
                     except Exception as e:
                         logger.warning(f"Failed to parse next_charge_at '{next_charge_at_str}': {e}")
                         next_charge_at_parsed = None
-                
+
                 async with db.pool.acquire() as conn:
                     # Map TAMradar status to our status (inactive -> failed)
                     tamradar_status = radar_data.get("status") or radar_data.get("radar_status", "active")
                     our_status = self._map_tamradar_status_to_our_status(tamradar_status)
-                    
+
                     await conn.execute(
                         """
                         UPDATE tamradar_radars
@@ -2556,7 +2481,7 @@
     async def deactivate_radar(self, radar_id: str) -> bool:
         """
         Deactivate radar via TAMradar API.
-        
+
         This enters a grace period until the end of the billing cycle.
         During grace period:
         - Radar status in TAMradar is 'inactive' but still receives webhooks
@@ -2567,16 +2492,16 @@
         """
         try:
             response_data = await self._make_request("DELETE", f"/radars/{radar_id}")
-            
+
             if response_data.get("status") != "success":
                 logger.error(f"TAMradar deactivation failed: {response_data}")
                 return False
-            
+
             # Parse TAMradar response
             data = response_data.get("data", {})
             radar_status = data.get("radar_status", "inactive")
             deactivated_at_str = data.get("deactivated_at")
-            
+
             # Parse deactivated_at timestamp if provided
             deactivated_at_parsed = None
             if deactivated_at_str:
@@ -2586,7 +2511,7 @@
                 except Exception as e:
                     logger.warning(f"Failed to parse deactivated_at '{deactivated_at_str}': {e}")
                     deactivated_at_parsed = datetime.now(timezone.utc).replace(tzinfo=None)
-            
+
             # Update database - mark as inactive (grace period)
             # Radar will continue receiving webhooks during grace period
             db = await get_global_db()
@@ -2604,7 +2529,7 @@
                         deactivated_at_parsed,
                         radar_id,
                     )
-            
+
             logger.info(
                 f"Radar {radar_id} deactivated in TAMradar. "
                 f"Status: {radar_status}, Grace period active until end of billing cycle. "
@@ -2625,30 +2550,30 @@
         try:
             # Fetch all radars from TAMradar
             response_data = await self._make_request("GET", "/radars")
-            
+
             if response_data.get("status") != "success":
                 raise TAMradarServiceError(f"Failed to fetch radars: {response_data.get('message')}")
-            
+
             radars = response_data.get("data", [])
             if not isinstance(radars, list):
                 radars = []
-            
+
             synced_count = 0
             updated_count = 0
             error_count = 0
-            
+
             for radar_data in radars:
                 try:
                     radar_id = radar_data.get("radar_id")
                     if not radar_id:
                         continue
-                    
+
                     radar_category = radar_data.get("radar_category", "")
                     radar_type = radar_data.get("radar_type", "")
                     # Map TAMradar status to our status (inactive -> failed)
                     tamradar_status = radar_data.get("status") or radar_data.get("radar_status", "active")
                     status = self._map_tamradar_status_to_our_status(tamradar_status)
-                    
+
                     # Build radar_config from radar_data
                     # Normalize domains before storing
                     radar_config = {}
@@ -2671,20 +2596,17 @@
                         radar_config["contact_name"] = radar_data.get("radar_config", {}).get("contact_name")
                     elif radar_category == "industry":
                         radar_config["industry_keyword"] = radar_data.get("radar_config", {}).get("industry_keyword", "")
-                    
+
                     # Generate unique key
                     unique_key = self._generate_radar_unique_key(radar_category, radar_type, radar_config)
                     radar_config["unique_key"] = unique_key
-                    
+
                     # Check if radar exists locally
                     db = await get_global_db()
                     if db:
                         async with db.pool.acquire() as conn:
-                            existing = await conn.fetchrow(
-                                "SELECT radar_id FROM tamradar_radars WHERE radar_id = $1",
-                                radar_id
-                            )
-                            
+                            existing = await conn.fetchrow("SELECT radar_id FROM tamradar_radars WHERE radar_id = $1", radar_id)
+
                             if existing:
                                 # Update existing radar
                                 await self._store_radar(
@@ -2716,14 +2638,8 @@
                 except Exception as e:
                     logger.error(f"Failed to sync radar {radar_data.get('radar_id', 'unknown')}: {e}")
                     error_count += 1
-            
-            return {
-                "status": "success",
-                "synced": synced_count,
-                "updated": updated_count,
-                "errors": error_count,
-                "total_in_tamradar": len(radars)
-            }
+
+            return {"status": "success", "synced": synced_count, "updated": updated_count, "errors": error_count, "total_in_tamradar": len(radars)}
         except TAMradarServiceError as e:
             logger.error(f"Failed to sync radars from TAMradar: {e}")
             raise
@@ -2742,26 +2658,25 @@
             # Fetch all radars and find matching one
             logger.info(f"Fetching all radars from TAMradar to find matching {radar_category}/{radar_type} radar...")
             response_data = await self._make_request("GET", "/radars")
-            
+
             logger.info(f"TAMradar GET /radars response: status={response_data.get('status')}, has_data={bool(response_data.get('data'))}")
             logger.info(f"Full TAMradar response structure: {json.dumps(response_data, indent=2, default=str)}")
-            
+
             if response_data.get("status") != "success":
                 logger.warning(f"TAMradar GET /radars returned non-success status: {response_data.get('status')}")
                 return None
-            
+
             radars = response_data.get("data", [])
             if not isinstance(radars, list):
                 logger.warning(f"TAMradar GET /radars returned non-list data: {type(radars)}")
                 return None
-            
+
             logger.info(f"Found {len(radars)} radars in TAMradar, searching for match...")
-            
+
             # Find matching radar
             for idx, radar_data in enumerate(radars):
-                logger.info(f"Checking radar {idx+1}/{len(radars)}: radar_id={radar_data.get('radar_id')}, "
-                           f"type={radar_data.get('radar_type')}")
-                
+                logger.info(f"Checking radar {idx + 1}/{len(radars)}: radar_id={radar_data.get('radar_id')}, type={radar_data.get('radar_type')}")
+
                 # TAMradar API doesn't return radar_category, infer it from radar_type
                 # radar_type format: "company_mentions", "contact_mentions", "industry_news", etc.
                 radar_type_from_api = radar_data.get("radar_type", "")
@@ -2772,51 +2687,55 @@
                     inferred_category = "contact"
                 elif radar_type_from_api.startswith("industry_"):
                     inferred_category = "industry"
-                
+
                 logger.info(f"  Inferred category: {inferred_category} (from type: {radar_type_from_api})")
-                
+
                 # Check if category matches
                 if inferred_category != radar_category:
                     logger.debug(f"  Category mismatch: {inferred_category} != {radar_category}")
                     continue
-                
+
                 # Check if type matches
                 if radar_type_from_api != radar_type:
                     logger.debug(f"  Type mismatch: {radar_type_from_api} != {radar_type}")
                     continue
-                
+
                 # TAMradar returns domain/keyword at top level, not in radar_config
                 is_match = False
-                
+
                 if radar_category == "company" and domain:
                     radar_domain = radar_data.get("domain")
                     # Normalize both domains and extract root domain for comparison
                     normalized_radar_domain = normalize_domain(radar_domain) if radar_domain else ""
                     normalized_input_domain = normalize_domain(domain) if domain else ""
-                    
+
                     # Extract root domain to remove 'www.' and other subdomains for comparison
                     radar_domain_root = extract_root_domain(normalized_radar_domain) if normalized_radar_domain else ""
                     input_domain_root = extract_root_domain(normalized_input_domain) if normalized_input_domain else ""
-                    
+
                     is_match = radar_domain_root == input_domain_root
-                    logger.info(f"  Domain comparison: '{radar_domain}' (normalized: '{normalized_radar_domain}', root: '{radar_domain_root}') == '{domain}' (normalized: '{normalized_input_domain}', root: '{input_domain_root}')? {is_match}")
+                    logger.info(
+                        f"  Domain comparison: '{radar_domain}' (normalized: '{normalized_radar_domain}', root: '{radar_domain_root}') == '{domain}' (normalized: '{normalized_input_domain}', root: '{input_domain_root}')? {is_match}"
+                    )
                 elif radar_category == "contact" and domain:
                     radar_domain = radar_data.get("domain")
                     # Normalize both domains and extract root domain for comparison
                     normalized_radar_domain = normalize_domain(radar_domain) if radar_domain else ""
                     normalized_input_domain = normalize_domain(domain) if domain else ""
-                    
+
                     # Extract root domain to remove 'www.' and other subdomains for comparison
                     radar_domain_root = extract_root_domain(normalized_radar_domain) if normalized_radar_domain else ""
                     input_domain_root = extract_root_domain(normalized_input_domain) if normalized_input_domain else ""
-                    
+
                     is_match = radar_domain_root == input_domain_root
-                    logger.info(f"  Domain comparison: '{radar_domain}' (normalized: '{normalized_radar_domain}', root: '{radar_domain_root}') == '{domain}' (normalized: '{normalized_input_domain}', root: '{input_domain_root}')? {is_match}")
+                    logger.info(
+                        f"  Domain comparison: '{radar_domain}' (normalized: '{normalized_radar_domain}', root: '{radar_domain_root}') == '{domain}' (normalized: '{normalized_input_domain}', root: '{input_domain_root}')? {is_match}"
+                    )
                 elif radar_category == "industry" and industry_keyword:
                     radar_keyword = radar_data.get("industry_keyword")
                     is_match = radar_keyword == industry_keyword
                     logger.info(f"  Keyword comparison: '{radar_keyword}' == '{industry_keyword}'? {is_match}")
-                
+
                 if is_match:
                     # Found matching radar, store it locally
                     radar_id = radar_data.get("radar_id")
@@ -2824,7 +2743,7 @@
                     if not radar_id:
                         logger.warning(f"Matched radar but no radar_id found in response")
                         continue
-                    
+
                     # Build radar_config with unique_key from top-level fields
                     # TAMradar API returns domain/keyword at top level, not in radar_config
                     # Normalize and extract root domain before storing
@@ -2848,17 +2767,14 @@
                             "contact_linkedin_url": radar_data.get("contact_linkedin_url"),
                             "contact_email": radar_data.get("contact_email"),
                             "contact_name": radar_data.get("contact_name"),
-                            "unique_key": None
+                            "unique_key": None,
                         }
                     else:  # industry
-                        config = {
-                            "industry_keyword": industry_keyword or radar_data.get("industry_keyword", ""),
-                            "unique_key": None
-                        }
-                    
+                        config = {"industry_keyword": industry_keyword or radar_data.get("industry_keyword", ""), "unique_key": None}
+
                     unique_key = self._generate_radar_unique_key(radar_category, radar_type, config)
                     config["unique_key"] = unique_key
-                    
+
                     # TAMradar API uses "radar_status" not "status"
                     # Map TAMradar status to our status (inactive -> failed)
                     # Note: When creating a radar, TAMradar always returns "active" initially
@@ -2866,7 +2782,7 @@
                     # Default to "active" since TAMradar always returns "active" on creation
                     tamradar_status = radar_data.get("radar_status") or radar_data.get("status", "active")
                     radar_status = self._map_tamradar_status_to_our_status(tamradar_status)
-                    
+
                     await self._store_radar(
                         radar_id=radar_id,
                         radar_category=radar_category,
@@ -2878,7 +2794,7 @@
                         filters=radar_data.get("filters"),
                         next_charge_at=radar_data.get("next_charge_at"),
                     )
-                    
+
                     return {
                         "radar_id": radar_id,
                         "radar_category": radar_category,
@@ -2893,7 +2809,7 @@
                         "updated_at": radar_data.get("updated_at"),
                         "deactivated_at": radar_data.get("deactivated_at"),
                     }
-            
+
             logger.warning(f"No matching radar found in TAMradar for {radar_category}/{radar_type} (domain={domain}, keyword={industry_keyword})")
             return None
         except Exception as e:
@@ -2903,15 +2819,17 @@
     async def get_account_summary(self) -> Optional[TAMradarAccountSummary]:
         """
         Get account summary including balance.
-        
+
         Reference: https://tamradar.readme.io/reference/getaccountsummary
         Endpoint: GET https://api.tamradar.com/v1/account
         """
         try:
             logger.info("Making request to TAMradar /account endpoint")
             response_data = await self._make_request("GET", "/account")
-            logger.info(f"TAMradar account response status: {response_data.get('status')}, full response: {json.dumps(response_data, indent=2, default=str)}")
-            
+            logger.info(
+                f"TAMradar account response status: {response_data.get('status')}, full response: {json.dumps(response_data, indent=2, default=str)}"
+            )
+
             if response_data.get("status") != "success":
                 logger.warning(f"TAMradar account summary request failed: {response_data}")
                 return None
@@ -2930,7 +2848,7 @@
             if not data:
                 logger.warning(f"TAMradar account summary response has no data. Full response: {response_data}")
                 return None
-                
+
             # Extract balance - check both data level and top level
             balance = data.get("balance_remaining_usd") or response_data.get("balance_remaining_usd", 0.0)
             if isinstance(balance, str):
@@ -2939,21 +2857,21 @@
                 except (ValueError, TypeError):
                     logger.warning(f"Could not parse balance as float: {balance}")
                     balance = 0.0
-            
+
             # Extract account section
             account_data = data.get("account", {})
             if not account_data:
                 logger.warning(f"No account section in response. Data keys: {list(data.keys())}")
                 account_data = {}
-            
+
             # Extract usage section
             usage_data = data.get("usage", {})
             if not usage_data:
                 logger.warning(f"No usage section in response. Data keys: {list(data.keys())}")
                 usage_data = {}
-            
+
             logger.info(f"Parsing account data: balance_remaining_usd={balance}, account={account_data}, usage={usage_data}")
-            
+
             summary = TAMradarAccountSummary(
                 balance_remaining_usd=float(balance),
                 account=TAMradarAccountStatus(
@@ -2968,7 +2886,9 @@
                     radars_failed=int(usage_data.get("radars_failed", 0)),
                 ),
             )
-            logger.info(f"Successfully parsed account summary: balance=${summary.balance_remaining_usd}, active_radars={summary.account.active_radars}, total_radars={summary.account.total_radars}")
+            logger.info(
+                f"Successfully parsed account summary: balance=${summary.balance_remaining_usd}, active_radars={summary.account.active_radars}, total_radars={summary.account.total_radars}"
+            )
             return summary
         except TAMradarServiceError as e:
             logger.error(f"TAMradarServiceError getting account summary: {e}")
@@ -2980,11 +2900,11 @@
     async def get_user_radars(self, user_id: str, category: Optional[str] = None, group_by_company: bool = True) -> List[Dict[str, Any]]:
         """
         Get all radars for a user, optionally filtered by category.
-        
+
         Returns radars with status 'active', 'pending', or 'failed'.
         Excludes failed radars older than 7 days (they're kept in DB for cache but not shown to user).
         Both 'active' and 'pending' radars count towards user's radar limit.
-        
+
         If group_by_company is True (default), groups radars by domain/company and returns
         one entry per company with all radar types included.
         """
@@ -2994,6 +2914,7 @@
 
         # Calculate cutoff date for failed radars (7 days ago)
         from datetime import timedelta
+
         cutoff_date = datetime.now(timezone.utc) - timedelta(days=7)
         cutoff_date_naive = cutoff_date.replace(tzinfo=None)
 
@@ -3044,14 +2965,15 @@
                     user_id,
                     cutoff_date_naive,
                 )
-            
+
             radars = [dict(row) for row in rows]
-            
+
             # Group by company/domain if requested
             if group_by_company:
                 import json as json_module
+
                 grouped = {}
-                
+
                 for radar in radars:
                     # Parse radar_config if it's a string
                     radar_config = radar.get("radar_config", {})
@@ -3060,12 +2982,12 @@
                             radar_config = json_module.loads(radar_config)
                         except (JSONDecodeError, TypeError, ValueError):
                             radar_config = {}
-                    
+
                     # Normalize and extract root domain in radar_config if it exists (for backward compatibility with old data)
                     if "domain" in radar_config and radar_config["domain"]:
                         normalized = normalize_domain(radar_config["domain"])
                         radar_config["domain"] = extract_root_domain(normalized) if normalized else ""
-                    
+
                     # Get domain for grouping
                     domain = radar_config.get("domain")
                     if not domain:
@@ -3079,16 +3001,16 @@
                     else:
                         # Domain is already normalized above, use it directly
                         pass
-                    
+
                     company_name = radar_config.get("company_name", "")
-                    
+
                     # Use domain as the grouping key
                     if domain not in grouped:
                         # Store radar_config for backward compatibility (domain is already normalized)
                         grouped_radar_config = radar_config.copy() if isinstance(radar_config, dict) else {}
                         grouped_radar_config["domain"] = domain
                         grouped_radar_config["company_name"] = company_name
-                        
+
                         grouped[domain] = {
                             "domain": domain,
                             "company_name": company_name,
@@ -3113,7 +3035,7 @@
                             "custom_fields": radar.get("custom_fields"),
                             "filters": radar.get("filters"),
                         }
-                    
+
                     # Store this radar's preferences in the map (for per-radar lookups)
                     radar_id = radar.get("radar_id")
                     grouped[domain]["_radar_preferences"][radar_id] = {
@@ -3121,23 +3043,22 @@
                         "weekly_wrapup_email": radar.get("weekly_wrapup_email", False),
                         "tier1_email_alerts": radar.get("tier1_email_alerts", False),
                         "watchlist_category": radar.get("watchlist_category"),
-                        }
-                    
+                    }
+
                     # Add this radar type to the list
                     grouped[domain]["radar_types"].append(radar.get("radar_type"))
                     grouped[domain]["radar_ids"].append(radar.get("radar_id"))
-                    
+
                     # Update is_owner if any radar is owned
                     if radar.get("is_owner"):
                         grouped[domain]["is_owner"] = True
-                    
+
                     # Update subscribed_at to earliest one
                     if radar.get("subscribed_at") and (
-                        not grouped[domain]["subscribed_at"] or 
-                        radar.get("subscribed_at") < grouped[domain]["subscribed_at"]
+                        not grouped[domain]["subscribed_at"] or radar.get("subscribed_at") < grouped[domain]["subscribed_at"]
                     ):
                         grouped[domain]["subscribed_at"] = radar.get("subscribed_at")
-                    
+
                     # Update top-level preferences from this radar
                     # Since we update all radar_ids for a company to have the same preferences,
                     # all radars should have the same values. We update from each radar to ensure
@@ -3148,12 +3069,12 @@
                         tier1_value = False
                     else:
                         tier1_value = bool(tier1_value)
-                    
+
                     grouped[domain]["user_interests"] = radar.get("user_interests")
                     grouped[domain]["weekly_wrapup_email"] = radar.get("weekly_wrapup_email", False)
                     grouped[domain]["tier1_email_alerts"] = tier1_value
                     grouped[domain]["watchlist_category"] = radar.get("watchlist_category")
-                    
+
                     # Debug logging for tier1_email_alerts
                     if domain == "visionedge.one" or (radar.get("tier1_email_alerts") and radar.get("tier1_email_alerts") != tier1_value):
                         logger.debug(
@@ -3165,9 +3086,9 @@
                                 "raw_type": type(radar.get("tier1_email_alerts")).__name__,
                                 "final_value": tier1_value,
                                 "final_type": type(tier1_value).__name__,
-                            }
+                            },
                         )
-                
+
                 # Before returning, verify and fix any inconsistent preferences
                 # If all radars for a domain have the same preference value, ensure the grouped value matches
                 for domain_key, group_data in grouped.items():
@@ -3178,12 +3099,13 @@
                         if tier1_values:
                             # Get the most common value (should all be the same after our updates)
                             from collections import Counter
+
                             value_counts = Counter(tier1_values)
                             most_common_value = value_counts.most_common(1)[0][0]
-                            
+
                             # Update top-level to match (in case there was an inconsistency)
                             group_data["tier1_email_alerts"] = bool(most_common_value)
-                            
+
                             # Log if there was an inconsistency
                             if len(value_counts) > 1:
                                 logger.warning(
@@ -3192,41 +3114,42 @@
                                         "domain": domain_key,
                                         "value_counts": dict(value_counts),
                                         "using_value": most_common_value,
-                                    }
+                                    },
                                 )
-                
+
                 # Convert grouped dict to list, sorted by subscribed_at
                 result = list(grouped.values())
                 # Sort by subscribed_at (most recent first)
                 result.sort(key=lambda x: x.get("subscribed_at") or datetime.min.replace(tzinfo=None), reverse=True)
                 return result
-            
+
             # When not grouping, normalize domains in radar_config for each radar
             for radar in radars:
                 radar_config = radar.get("radar_config", {})
                 if isinstance(radar_config, str):
                     try:
                         import json as json_module
+
                         radar_config = json_module.loads(radar_config)
                     except (JSONDecodeError, TypeError, ValueError):
                         radar_config = {}
-                
+
                 # Normalize and extract root domain in radar_config if it exists
                 if "domain" in radar_config and radar_config["domain"]:
                     normalized = normalize_domain(radar_config["domain"])
                     radar_config["domain"] = extract_root_domain(normalized) if normalized else ""
                     radar["radar_config"] = radar_config
-            
+
             return radars
-    
+
     async def get_user_radar_count_and_limit(self, user_id: str) -> Dict[str, Any]:
         """
         Get user's current radar count and their radar limit.
-        
+
         Counts unique companies/domains, not individual radars.
         For company radars, groups by domain. For contact/industry radars, counts each as one.
         Counts both 'active' and 'pending' radars towards the limit (failed radars don't count).
-        
+
         Returns:
             {
                 "current_count": int,  # Number of unique companies/radars user has
@@ -3237,7 +3160,7 @@
         db = await get_global_db()
         if not db:
             return {"current_count": 0, "limit": 5, "remaining": 5}
-        
+
         async with db.pool.acquire() as conn:
             # Get user's radar limit
             user_row = await conn.fetchrow(
@@ -3245,7 +3168,7 @@
                 user_id,
             )
             limit = user_row["tamradar_radar_limit"] if user_row and user_row["tamradar_radar_limit"] is not None else 5
-            
+
             # Count unique companies/domains for company radars, and individual radars for contact/industry
             # For company radars, count distinct domains
             # For contact/industry radars, count each radar separately
@@ -3267,16 +3190,14 @@
             )
             current_count = count_row["count"] if count_row else 0
             remaining = max(0, limit - current_count)
-            
+
             return {
                 "current_count": current_count,
                 "limit": limit,
                 "remaining": remaining,
             }
-    
-    async def update_radar_category(
-        self, user_id: str, radar_id: str, category: Optional[str]
-    ) -> None:
+
+    async def update_radar_category(self, user_id: str, radar_id: str, category: Optional[str]) -> None:
         """Update the watchlist category for a user's radar."""
         db = await get_global_db()
         if not db:
@@ -3294,21 +3215,21 @@
                 user_id,
                 radar_id,
             )
-    
+
     async def update_radar_user_preferences(
-        self, 
-        user_id: str, 
-        radar_ids: List[str], 
+        self,
+        user_id: str,
+        radar_ids: List[str],
         user_interests: Optional[str] = None,
         weekly_wrapup_email: Optional[bool] = None,
-        tier1_email_alerts: Optional[bool] = None
+        tier1_email_alerts: Optional[bool] = None,
     ) -> None:
         """
         Update user interests, weekly wrap-up email preference, and/or tier1 email alerts for one or more radars.
-        
+
         For company radars, this should be called with all radar_ids for the same company/domain
         to ensure all radar types have consistent preferences (matching CREATE behavior).
-        
+
         Uses the same approach as _link_user_to_radar: updates each radar individually in a loop.
         """
         db = await get_global_db()
@@ -3324,25 +3245,25 @@
         updates = []
         params = []
         param_count = 0
-        
+
         if user_interests is not None:
             param_count += 1
             updates.append(f"user_interests = ${param_count}")
             params.append(user_interests)
-        
+
         if weekly_wrapup_email is not None:
             param_count += 1
             updates.append(f"weekly_wrapup_email = ${param_count}")
             params.append(weekly_wrapup_email)
-        
+
         if tier1_email_alerts is not None:
             param_count += 1
             updates.append(f"tier1_email_alerts = ${param_count}")
             params.append(tier1_email_alerts)
-        
+
         if not updates:
             return  # Nothing to update
-        
+
         # Update each radar individually (same approach as _link_user_to_radar)
         async with db.pool.acquire() as conn:
             for radar_id in radar_ids:
@@ -3350,47 +3271,50 @@
                 query_params = params.copy()
                 query_params.append(user_id)
                 query_params.append(radar_id)
-                
+
                 # Build query with correct parameter placeholders
                 user_id_param = param_count + 1
                 radar_id_param = param_count + 2
-                
+
                 query = f"""
                     UPDATE tamradar_user_radars
-                    SET {', '.join(updates)}
+                    SET {", ".join(updates)}
                     WHERE user_id = ${user_id_param} 
                       AND radar_id = ${radar_id_param}
                 """
-                
+
                 await conn.execute(query, *query_params)
-            
+
             logger.info(
                 f"Updated preferences for {len(radar_ids)} radar(s)",
                 extra={
                     "user_id": user_id,
                     "radar_ids": radar_ids,
                     "radar_count": len(radar_ids),
-                    "updated_fields": [f for f in ["user_interests", "weekly_wrapup_email", "tier1_email_alerts"] 
-                                      if (f == "user_interests" and user_interests is not None) or
-                                         (f == "weekly_wrapup_email" and weekly_wrapup_email is not None) or
-                                         (f == "tier1_email_alerts" and tier1_email_alerts is not None)]
-                }
+                    "updated_fields": [
+                        f
+                        for f in ["user_interests", "weekly_wrapup_email", "tier1_email_alerts"]
+                        if (f == "user_interests" and user_interests is not None)
+                        or (f == "weekly_wrapup_email" and weekly_wrapup_email is not None)
+                        or (f == "tier1_email_alerts" and tier1_email_alerts is not None)
+                    ],
+                },
             )
 
     async def check_deletion_limit(self, user_id: str) -> Tuple[bool, int, int]:
         """
         Check if user has exceeded monthly deletion limit.
-        
+
         Regular users: 5 deletions per month (default)
         Admin users: 50 deletions per month
-        
+
         Returns:
             (can_delete, current_count, limit) tuple
         """
         db = await get_global_db()
         if not db:
             return (True, 0, 5)  # Allow if DB unavailable (fail open)
-        
+
         async with db.pool.acquire() as conn:
             # Get user's deletion limit (defaults to 5 for regular users, 50 for admins)
             user_row = await conn.fetchrow(
@@ -3405,7 +3329,7 @@
                 """,
                 user_id,
             )
-            
+
             if not user_row:
                 # User not found, default to 5
                 limit = 5
@@ -3414,7 +3338,7 @@
                 # If limit is NULL, set based on admin status
                 if limit is None:
                     limit = 50 if user_row.get("is_admin") else 5
-            
+
             # Count deletions in current month (last 30 days)
             count_row = await conn.fetchrow(
                 """
@@ -3426,17 +3350,17 @@
                 user_id,
             )
             current_count = count_row["count"] if count_row else 0
-            
+
             can_delete = current_count < limit
             return (can_delete, current_count, limit)
-    
+
     async def record_radar_deletion(self, user_id: str, radar_id: str) -> None:
         """Record a radar deletion for monthly limit tracking."""
         db = await get_global_db()
         if not db:
             logger.warning("Database not available, skipping deletion record")
             return
-        
+
         async with db.pool.acquire() as conn:
             await conn.execute(
                 """
@@ -3451,21 +3375,21 @@
     async def deactivate_user_radar(self, user_id: str, radar_id: str, record_deletion: bool = True) -> bool:
         """
         Remove user's subscription to radar.
-        
+
         IMPORTANT: Nothing happens locally unless TAMradar API call succeeds.
-        
+
         If this is the ONLY user subscribed:
         - First deactivates radar in TAMradar (enters grace period)
         - Only if TAMradar succeeds, then removes user's subscription locally
         - Radar status becomes 'inactive' but continues receiving webhooks during grace period
         - Radar disappears from user's watchlist
         - User's radar limit slot is freed
-        
+
         If OTHER users are subscribed:
         - Only removes this user's subscription (no TAMradar call needed)
         - Radar remains active in TAMradar
         - Other users continue to see the radar
-        
+
         During grace period:
         - Webhooks continue to be received and stored
         - Agents can still access all findings (active and grace period radars)
@@ -3490,35 +3414,27 @@
             # cross-environment deletion issues
             if subscriber_count <= 1:
                 environment = get_environment()
-                
+
                 if environment == "production":
-                    logger.info(
-                        f"Last user removing from radar {radar_id} in production. "
-                        f"Deactivating in TAMradar first (entering grace period)."
-                    )
+                    logger.info(f"Last user removing from radar {radar_id} in production. Deactivating in TAMradar first (entering grace period).")
                     # Try to deactivate in TAMradar first
                     deactivation_success = await self.deactivate_radar(radar_id)
                     if not deactivation_success:
-                        logger.error(
-                            f"Failed to deactivate radar {radar_id} in TAMradar API. "
-                            f"Not removing user subscription locally."
-                        )
+                        logger.error(f"Failed to deactivate radar {radar_id} in TAMradar API. Not removing user subscription locally.")
                         return False
-                    
+
                     # TAMradar deactivation succeeded, now remove user's subscription locally
                     await conn.execute(
                         "DELETE FROM tamradar_user_radars WHERE user_id = $1 AND radar_id = $2",
                         user_id,
                         radar_id,
                     )
-                    
+
                     # Record the deletion for monthly limit tracking (only if requested)
                     if record_deletion:
                         await self.record_radar_deletion(user_id, radar_id)
-                    
-                    logger.info(
-                        f"Successfully deactivated radar {radar_id} in TAMradar and removed user {user_id} subscription."
-                    )
+
+                    logger.info(f"Successfully deactivated radar {radar_id} in TAMradar and removed user {user_id} subscription.")
                 else:
                     # Non-production: only remove local subscription, don't call TAMradar
                     logger.info(
@@ -3530,11 +3446,11 @@
                         user_id,
                         radar_id,
                     )
-                    
+
                     # Record the deletion for monthly limit tracking (only if requested)
                     if record_deletion:
                         await self.record_radar_deletion(user_id, radar_id)
-                    
+
                     logger.warning(
                         f"Removed user {user_id} subscription from radar {radar_id} in {environment}. "
                         f"Radar remains active in TAMradar. Only production can deactivate radars globally."
@@ -3547,31 +3463,26 @@
                     user_id,
                     radar_id,
                 )
-                
+
                 # Record the deletion for monthly limit tracking (only if requested)
                 if record_deletion:
                     await self.record_radar_deletion(user_id, radar_id)
-                
-                logger.info(
-                    f"Removed user {user_id} subscription from radar {radar_id}. "
-                    f"Remaining subscribers: {subscriber_count - 1}"
-                )
 
+                logger.info(f"Removed user {user_id} subscription from radar {radar_id}. Remaining subscribers: {subscriber_count - 1}")
+
         return True
 
-    async def get_findings_by_domain(
-        self, domain: str, radar_types: Optional[List[str]] = None, limit: int = 50
-    ) -> List[Dict[str, Any]]:
+    async def get_findings_by_domain(self, domain: str, radar_types: Optional[List[str]] = None, limit: int = 50) -> List[Dict[str, Any]]:
         """
         Get TAMradar findings for a company by domain.
-        
+
         Useful for agents to enrich company data with recent TAMradar findings.
-        
+
         Args:
             domain: Company domain (e.g., "example.com")
             radar_types: Optional list of radar types to filter (e.g., ["company_job_openings", "company_new_hires"])
             limit: Maximum number of findings to return
-            
+
         Returns:
             List of findings with radar metadata
         """
@@ -3609,7 +3520,8 @@
                 ORDER BY f.discovered_at DESC
                 LIMIT ${param_count}
                 """,
-                *params, limit
+                *params,
+                limit,
             )
 
             return [
@@ -3624,19 +3536,17 @@
                 for r in rows
             ]
 
-    async def get_findings_by_linkedin_url(
-        self, linkedin_url: str, radar_types: Optional[List[str]] = None, limit: int = 50
-    ) -> List[Dict[str, Any]]:
+    async def get_findings_by_linkedin_url(self, linkedin_url: str, radar_types: Optional[List[str]] = None, limit: int = 50) -> List[Dict[str, Any]]:
         """
         Get TAMradar findings for a person by LinkedIn URL.
-        
+
         Useful for agents to enrich person data with recent TAMradar findings.
-        
+
         Args:
             linkedin_url: LinkedIn profile URL (e.g., "https://linkedin.com/in/john-doe")
             radar_types: Optional list of radar types to filter (e.g., ["contact_job_changes"])
             limit: Maximum number of findings to return
-            
+
         Returns:
             List of findings with radar metadata
         """
@@ -3648,10 +3558,7 @@
         normalized_url = linkedin_url.lower().strip()
 
         async with db.pool.acquire() as conn:
-            where_clauses = [
-                "r.radar_config->>'contact_linkedin_url' = $1",
-                "r.radar_category = 'contact'"
-            ]
+            where_clauses = ["r.radar_config->>'contact_linkedin_url' = $1", "r.radar_category = 'contact'"]
             params = [normalized_url]
             param_count = 1
 
@@ -3677,7 +3584,8 @@
                 ORDER BY f.discovered_at DESC
                 LIMIT ${param_count}
                 """,
-                *params, limit
+                *params,
+                limit,
             )
 
             return [
@@ -3692,22 +3600,20 @@
                 for r in rows
             ]
 
-    async def get_industry_findings_by_domain(
-        self, domain: str, radar_types: Optional[List[str]] = None, limit: int = 50
-    ) -> List[Dict[str, Any]]:
+    async def get_industry_findings_by_domain(self, domain: str, radar_types: Optional[List[str]] = None, limit: int = 50) -> List[Dict[str, Any]]:
         """
         Get TAMradar industry findings that mention a company domain.
-        
+
         Industry radars track mentions and funding rounds for industry keywords.
         This searches the finding_data JSONB for references to the company domain.
-        
+
         Useful for agents to find industry news/mentions about a company.
-        
+
         Args:
             domain: Company domain to search for in industry findings
             radar_types: Optional list of radar types to filter (e.g., ["industry_mentions", "industry_funding_rounds"])
             limit: Maximum number of findings to return
-            
+
         Returns:
             List of industry findings that mention the domain
         """
@@ -3722,7 +3628,7 @@
             where_clauses = [
                 "r.radar_category = 'industry'",
                 # Search for domain in finding_data JSONB (case-insensitive)
-                "(f.finding_data::text ILIKE $1 OR f.finding_data::text ILIKE $2)"
+                "(f.finding_data::text ILIKE $1 OR f.finding_data::text ILIKE $2)",
             ]
             params = [f"%{normalized_domain}%", f"%{domain}%"]  # Search for both normalized and original
             param_count = 2
@@ -3749,7 +3655,8 @@
                 ORDER BY f.discovered_at DESC
                 LIMIT ${param_count}
                 """,
-                *params, limit
+                *params,
+                limit,
             )
 
             return [
@@ -3765,30 +3672,29 @@
             ]
 
     async def get_recent_findings_summary(
-        self, domain: Optional[str] = None, linkedin_url: Optional[str] = None,
-        days: int = 30, include_industry: bool = True
+        self, domain: Optional[str] = None, linkedin_url: Optional[str] = None, days: int = 30, include_industry: bool = True
     ) -> Dict[str, Any]:
         """
         Get a summary of recent TAMradar findings for a company or person.
-        
+
         Returns counts by radar_type and most recent finding per type.
         Useful for agents to quickly assess what TAMradar data is available.
-        
+
         Args:
             domain: Company domain (optional)
             linkedin_url: Person LinkedIn URL (optional)
             days: Number of days to look back (default: 30)
             include_industry: Whether to include industry findings that mention the domain (default: True)
-            
+
         Returns:
             Summary dict with counts and recent findings
         """
         findings = []
-        
+
         if domain:
             # Get company-specific findings
             findings = await self.get_findings_by_domain(domain)
-            
+
             # Also get industry findings that mention this domain
             if include_industry:
                 industry_findings = await self.get_industry_findings_by_domain(domain)
@@ -3800,10 +3706,10 @@
 
         # Filter by date
         from datetime import datetime, timedelta, timezone
+
         cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
         recent_findings = [
-            f for f in findings
-            if f.get("discovered_at") and datetime.fromisoformat(f["discovered_at"].replace("Z", "+00:00")) >= cutoff_date
+            f for f in findings if f.get("discovered_at") and datetime.fromisoformat(f["discovered_at"].replace("Z", "+00:00")) >= cutoff_date
         ]
 
         # Group by radar_type
@@ -3817,12 +3723,11 @@
                     "most_recent_date": None,
                 }
             by_type[radar_type]["count"] += 1
-            
+
             finding_date = finding.get("discovered_at")
             if finding_date:
                 finding_dt = datetime.fromisoformat(finding_date.replace("Z", "+00:00"))
-                if (by_type[radar_type]["most_recent_date"] is None or
-                    finding_dt > by_type[radar_type]["most_recent_date"]):
+                if by_type[radar_type]["most_recent_date"] is None or finding_dt > by_type[radar_type]["most_recent_date"]:
                     by_type[radar_type]["most_recent"] = finding
                     by_type[radar_type]["most_recent_date"] = finding_dt
 
@@ -3835,4 +3740,3 @@
 
 # Singleton instance
 tamradar_service = TAMradarService()
-

--- app/services/tamradar_webhook_service.py
+++ app/services/tamradar_webhook_service.py
@@ -33,7 +33,7 @@
         if not internal_secret or not internal_secret.strip():
             logger.warning("INTERNAL_WEBHOOK_SECRET not configured or empty")
             return False
-        
+
         # Check query parameter first
         secret = request.query_params.get("secret")
         if secret and secret.strip() and secret == internal_secret:
@@ -50,7 +50,7 @@
                 "has_query_secret": "secret" in request.query_params,
                 "has_header_secret": "X-Internal-Webhook-Secret" in request.headers,
                 "internal_secret_configured": bool(internal_secret),
-            }
+            },
         )
         return False
 
@@ -79,7 +79,7 @@
     async def verify_radar_exists(self, radar_id: str) -> bool:
         """
         Verify radar_id exists in database.
-        
+
         Accepts both 'active' and 'inactive' radars because:
         - Inactive radars are in grace period and still receive webhooks
         - We want to continue processing and storing webhooks during grace period
@@ -112,9 +112,7 @@
             )
             return row is not None
 
-    async def store_webhook_event(
-        self, payload: Dict[str, Any], radar_id: str
-    ) -> int:
+    async def store_webhook_event(self, payload: Dict[str, Any], radar_id: str) -> int:
         """Store raw webhook payload in database."""
         db = await get_global_db()
         if not db:
@@ -145,15 +143,13 @@
             )
             return row["id"] if row else 0
 
-    async def process_radar_finding(
-        self, webhook_event_id: int, payload: Dict[str, Any]
-    ) -> None:
+    async def process_radar_finding(self, webhook_event_id: int, payload: Dict[str, Any]) -> None:
         """
         Process a radar_finding event.
-        
+
         Extracts data according to TAMradar webhook payload structure:
         https://tamradar.readme.io/reference/webhook-payload
-        
+
         Payload structure:
         - event_id, event_type, record_id, discovered_at (top level)
         - data: { radar_id, radar_type, domain, next_charge_at, custom_fields }
@@ -165,7 +161,7 @@
         content = payload.get("content", {})
         record_id = payload.get("record_id")
         discovered_at = payload.get("discovered_at")
-        
+
         # Note: domain, next_charge_at, and custom_fields are available in data
         # but we don't need them here as they're already stored in tamradar_radars table
 
@@ -262,16 +258,13 @@
                     webhook_event_id,
                 )
 
-        logger.info(
-            f"Processed radar_finding: {finding_id} for radar {radar_id}, "
-            f"linked to {len(user_rows)} users"
-        )
-        
+        logger.info(f"Processed radar_finding: {finding_id} for radar {radar_id}, linked to {len(user_rows)} users")
+
         # Trigger relevance scoring as a separate Celery task (non-blocking)
         # This ensures it completes even if the webhook processing task finishes first
         try:
             from app.tasks.tamradar_tasks import score_finding_relevance_task
-            
+
             # Parse finding_data from JSON string if needed
             finding_data = content
             if isinstance(finding_data, str):
@@ -279,7 +272,7 @@
                     finding_data = json.loads(finding_data)
                 except (json.JSONDecodeError, TypeError):
                     finding_data = content
-            
+
             # Queue relevance scoring as a separate Celery task
             score_finding_relevance_task.delay(
                 finding_id=finding_id,
@@ -291,20 +284,15 @@
             logger.debug(f"Queued relevance scoring Celery task for finding {finding_id}")
         except Exception as e:
             # Log but don't fail the webhook processing if relevance scoring fails
-            logger.warning(
-                f"Failed to queue relevance scoring task for finding {finding_id}: {e}",
-                exc_info=True
-        )
+            logger.warning(f"Failed to queue relevance scoring task for finding {finding_id}: {e}", exc_info=True)
 
-    async def process_radar_failure(
-        self, webhook_event_id: int, payload: Dict[str, Any]
-    ) -> None:
+    async def process_radar_failure(self, webhook_event_id: int, payload: Dict[str, Any]) -> None:
         """
         Process a radar_failure event.
-        
+
         According to TAMradar documentation:
         https://tamradar.readme.io/reference/webhook-payload
-        
+
         Failure payload structure:
         - event_id, event_type, record_id (null), discovered_at (top level)
         - data: { radar_id, radar_type, domain, next_charge_at (null), custom_fields }
@@ -317,6 +305,7 @@
         # Validate failure content structure
         try:
             from app.models.tamradar_models import RadarFailureContent
+
             failure_content = RadarFailureContent(**content)
             failure_reason = failure_content.failure_reason
             message = failure_content.message
@@ -347,11 +336,9 @@
                 failure_reason,
                 radar_id,
             )
-            
-            logger.info(
-                f"Updated radar {radar_id} status to 'failed': {failure_reason}"
-            )
 
+            logger.info(f"Updated radar {radar_id} status to 'failed': {failure_reason}")
+
             # Mark webhook event as processed
             await conn.execute(
                 """
@@ -362,16 +349,12 @@
                 webhook_event_id,
             )
 
-        logger.info(
-            f"Processed radar_failure for radar {radar_id}: {failure_reason} - {message}"
-        )
+        logger.info(f"Processed radar_failure for radar {radar_id}: {failure_reason} - {message}")
 
-    async def process_webhook_event(
-        self, payload: Dict[str, Any], webhook_event_id: Optional[int] = None
-    ) -> None:
+    async def process_webhook_event(self, payload: Dict[str, Any], webhook_event_id: Optional[int] = None) -> None:
         """
         Main processing logic for webhook events.
-        
+
         Args:
             payload: Webhook payload from TAMradar
             webhook_event_id: Optional pre-stored webhook event ID (to avoid double storage)
@@ -413,4 +396,3 @@
 
 # Singleton instance
 tamradar_webhook_service = TAMradarWebhookService()
-

--- app/services/tier1_verification_service.py
+++ app/services/tier1_verification_service.py
@@ -19,21 +19,16 @@
 logger = logging.getLogger(__name__)
 
 
-async def verify_tier1_finding(
-    finding_text: str,
-    company_name: str,
-    category: str,
-    initial_confidence: float
-) -> Optional[Dict[str, Any]]:
+async def verify_tier1_finding(finding_text: str, company_name: str, category: str, initial_confidence: float) -> Optional[Dict[str, Any]]:
     """
     Verify that a Tier 1 finding definitively meets criteria and generate headline.
-    
+
     Args:
         finding_text: The finding text to verify
         company_name: The name of the company being tracked
         category: The category from initial classification (e.g., "Direct Allocation Announcements")
         initial_confidence: The confidence score from initial classification (0.0-1.0)
-        
+
     Returns:
         Dict with verified, confidence, headline, reasoning if verification passes,
         None if verification fails or API call fails
@@ -41,43 +36,32 @@
     if not finding_text or not finding_text.strip():
         logger.warning("Empty finding text, skipping Tier 1 verification")
         return None
-    
+
     if not company_name:
         logger.warning("Missing company name, skipping Tier 1 verification")
         return None
-    
+
     if not category:
         logger.warning("Missing category, skipping Tier 1 verification")
         return None
-    
+
     api_key = get_openai_api_key()
     if not api_key:
         logger.warning("OpenAI API key not configured (need OPENAI_API_KEY)")
         return None
-    
+
     try:
         prompt = get_tier1_verification_prompt(
-            finding_text=finding_text,
-            company_name=company_name,
-            category=category,
-            initial_confidence=initial_confidence
+            finding_text=finding_text, company_name=company_name, category=category, initial_confidence=initial_confidence
         )
-        
-        logger.debug(
-            f"Verifying Tier 1 finding for {company_name}, category: {category}, "
-            f"initial confidence: {initial_confidence:.2f}"
-        )
-        
+
+        logger.debug(f"Verifying Tier 1 finding for {company_name}, category: {category}, initial confidence: {initial_confidence:.2f}")
+
         # Create ChatOpenAI client inline (same pattern as company_enrich_agent)
-        chat = ChatOpenAI(
-            model="gpt-4o-mini",
-            temperature=0.1,
-            max_tokens=300,
-            api_key=api_key
-        )
-        
+        chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=300, api_key=api_key)
+
         response = await chat.ainvoke([("user", prompt)])
-        
+
         # Parse response
         content = response.content.strip()
         # Remove markdown code blocks if present
@@ -88,70 +72,61 @@
         if content.endswith("```"):
             content = content[:-3]
         content = content.strip()
-        
+
         result = json.loads(content)
-        
+
         # Validate JSON structure
         if not isinstance(result, dict):
             logger.error(f"OpenAI returned non-dict result: {result}")
             return None
-        
+
         # Validate required fields
         verified = result.get("verified", False)
         confidence = result.get("confidence", 0.0)
         headline = result.get("headline")
         reasoning = result.get("reasoning", "")
-        
+
         # Validate types
         if not isinstance(verified, bool):
             logger.error(f"Invalid verified type: {type(verified)}, value: {verified}")
             return None
-        
+
         if not isinstance(confidence, (int, float)) or confidence < 0.0 or confidence > 1.0:
             logger.error(f"Invalid confidence value: {confidence}")
             return None
-        
+
         # Check confidence threshold
         if verified and confidence < TIER1_VERIFICATION_CONFIDENCE_THRESHOLD:
-            logger.debug(
-                f"Tier 1 verification confidence {confidence:.2f} below threshold "
-                f"{TIER1_VERIFICATION_CONFIDENCE_THRESHOLD}, rejecting"
-            )
+            logger.debug(f"Tier 1 verification confidence {confidence:.2f} below threshold {TIER1_VERIFICATION_CONFIDENCE_THRESHOLD}, rejecting")
             return {
                 "verified": False,
                 "confidence": confidence,
                 "headline": None,
-                "reasoning": f"Confidence {confidence:.2f} below threshold {TIER1_VERIFICATION_CONFIDENCE_THRESHOLD}"
+                "reasoning": f"Confidence {confidence:.2f} below threshold {TIER1_VERIFICATION_CONFIDENCE_THRESHOLD}",
             }
-        
+
         # Validate headline if verified
         if verified:
             if not headline or not isinstance(headline, str) or not headline.strip():
                 logger.warning("Verified but missing headline, rejecting")
-                return {
-                    "verified": False,
-                    "confidence": confidence,
-                    "headline": None,
-                    "reasoning": "Missing headline"
-                }
-        
+                return {"verified": False, "confidence": confidence, "headline": None, "reasoning": "Missing headline"}
+
         logger.info(
             f"Tier 1 verification complete for {company_name}: "
             f"verified={verified}, confidence={confidence:.2f}, "
             f"headline={'present' if headline else 'none'}"
         )
-        
+
         return {
             "verified": verified,
             "confidence": float(confidence),
             "headline": headline.strip() if headline else None,
-            "reasoning": reasoning[:100] if reasoning else ""  # Limit to 100 chars
+            "reasoning": reasoning[:100] if reasoning else "",  # Limit to 100 chars
         }
-        
+
     except json.JSONDecodeError as e:
         logger.error(f"Failed to parse OpenAI JSON response: {e}, content: {content[:200]}")
         return None
     except Exception as e:
         logger.error(f"Tier 1 verification failed: {e}", exc_info=True)
         return None
-

--- app/services/user_sync_service.py
+++ app/services/user_sync_service.py
@@ -83,11 +83,7 @@
                     existing_firm_name = row.get("firm_name")
                     existing_is_admin = bool(row.get("is_admin"))
                     resolved_is_admin = existing_is_admin or is_admin_claim
-                    resolved_firm_name = (
-                        existing_firm_name
-                        if existing_firm_name and existing_firm_name.strip()
-                        else self.DEFAULT_FIRM_NAME
-                    )
+                    resolved_firm_name = existing_firm_name if existing_firm_name and existing_firm_name.strip() else self.DEFAULT_FIRM_NAME
                     current_limit = row["daily_agent_run_limit"] or self.DEFAULT_RUN_LIMIT
                     new_limit_value = daily_limit_override or current_limit
                     existing_is_active = bool(row["is_active"]) if row["is_active"] is not None else True
@@ -226,7 +222,7 @@
             "user_id": user_id,
             "clerk_user_id": clerk_user_id,
             "email": normalized_email,
-            "is_admin": resolved_is_admin if 'resolved_is_admin' in locals() else is_admin_claim,
+            "is_admin": resolved_is_admin if "resolved_is_admin" in locals() else is_admin_claim,
             "is_active": approved_is_active,
             "daily_agent_run_limit": new_limit_value,
             "account_status": registration_status,
@@ -279,13 +275,8 @@
 
         # Only create/update profile if Clerk metadata contains profile data
         # Don't create empty profiles - let them be created when user submits registration form
-        has_profile_data = (
-            firm_description or
-            key_differentiators or
-            key_objectives or
-            investor_profile
-        )
-        
+        has_profile_data = firm_description or key_differentiators or key_objectives or investor_profile
+
         if not has_profile_data:
             # No profile data in Clerk metadata - skip creating profile
             # Profile will be created when user submits registration form
@@ -333,38 +324,35 @@
             """,
             user_id,
         )
-        
+
         # Trigger embeddings as background tasks (non-blocking)
         # Only create tasks if profile embedding is enabled
         if profile_row:
             try:
                 from app.utils.config import is_profile_embedding_enabled
-                
+
                 if is_profile_embedding_enabled():
-                    from app.services.profile_embedding_service import (
-                        upsert_profile_embedding,
-                        upsert_profile_embedding_for_feedback
-                    )
+                    from app.services.profile_embedding_service import upsert_profile_embedding, upsert_profile_embedding_for_feedback
+
                     # Create tasks to run in background (doesn't block the sync process)
                     import asyncio
+
                     profile_row_dict = dict(profile_row)
                     # Create both investor_search and feedback_analysis embeddings
                     asyncio.create_task(upsert_profile_embedding(user_id, profile_row_dict))
                     asyncio.create_task(upsert_profile_embedding_for_feedback(user_id, profile_row_dict))
-                    
+
                     # Also regenerate all radar context embeddings for this user
                     try:
                         from app.services.radar_context_embedding_service import regenerate_all_user_radar_embeddings
+
                         asyncio.create_task(regenerate_all_user_radar_embeddings(user_id))
                         logger.debug(f"Triggered radar context embedding regeneration for user_id: {user_id}")
                     except Exception as e:
                         logger.warning(f"Failed to trigger radar context embedding regeneration for user_id: {user_id}: {e}")
             except Exception as e:
                 # Log but don't fail the sync if embedding fails
-                logger.warning(
-                    f"Failed to trigger profile embedding for user_id: {user_id}",
-                    extra={"user_id": user_id, "error": str(e)}
-                )
+                logger.warning(f"Failed to trigger profile embedding for user_id: {user_id}", extra={"user_id": user_id, "error": str(e)})
 
         # Only create history entry if there's actual profile data to track
         if any([firm_description, key_differentiators, key_objectives]):

--- app/services/vector_search/fund_lp_engine.py
+++ app/services/vector_search/fund_lp_engine.py
@@ -17,6 +17,7 @@
 
 # Load environment variables from .env file
 from dotenv import load_dotenv
+
 load_dotenv()
 from app.utils.config import get_openai_api_key, get_qdrant_url_search_rec, get_qdrant_collection, get_qdrant_api_key_search_rec, get_environment
 
@@ -33,8 +34,14 @@
 
 class FundLPMatchingEngine:
     """Complete engine for fund-to-LP matching with LLM filter extraction."""
-    
-    def __init__(self, qdrant_url: Optional[str] = None, collection_name: Optional[str] = None, openai_api_key: Optional[str] = None, qdrant_api_key: Optional[str] = None):
+
+    def __init__(
+        self,
+        qdrant_url: Optional[str] = None,
+        collection_name: Optional[str] = None,
+        openai_api_key: Optional[str] = None,
+        qdrant_api_key: Optional[str] = None,
+    ):
         # Resolve configuration via centralized config getters
         resolved_qdrant_url = qdrant_url or get_qdrant_url_search_rec()
         resolved_collection = collection_name or get_qdrant_collection()
@@ -51,40 +58,30 @@
         # Initialize Qdrant client with API key if available
         client_kwargs = {
             "url": resolved_qdrant_url,
-            "timeout": 60  # Increase timeout to handle large filtered semantic queries
+            "timeout": 60,  # Increase timeout to handle large filtered semantic queries
         }
         if resolved_qdrant_api_key:
             client_kwargs["api_key"] = resolved_qdrant_api_key
-        
+
         print(f"üîó Connecting to Qdrant at: {resolved_qdrant_url}")
         print(f"üì¶ Using collection: {resolved_collection}")
         if resolved_qdrant_api_key:
             print(f"üîë Using API key authentication")
-        
+
         self.client = QdrantClient(**client_kwargs)
         self.collection_name = resolved_collection
-        
+
         # Initialize LLM using LangChain pattern
         self.api_key = resolved_api_key
         if not self.api_key:
             raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
-        
+
         # Initialize ChatOpenAI client
-        self.llm = ChatOpenAI(
-            model="gpt-4.1",
-            max_tokens=1000,
-            api_key=self.api_key
-        )
-        
+        self.llm = ChatOpenAI(model="gpt-4.1", max_tokens=1000, api_key=self.api_key)
+
         # Performance tracking
-        self.stats = {
-            "total_calls": 0,
-            "total_time": 0,
-            "total_cost": 0,
-            "extraction_calls": 0,
-            "mapping_calls": 0
-        }
-        
+        self.stats = {"total_calls": 0, "total_time": 0, "total_cost": 0, "extraction_calls": 0, "mapping_calls": 0}
+
         # Investor type ID to name mapping (for Qdrant filters)
         # Updated to match actual types in test dataset
         self.investor_type_mapping = {
@@ -124,9 +121,9 @@
             33: "PE/Buyout",
             34: "Lender/Debt Provider",
             35: "Corporate Development",
-            36: "Limited Partner"
+            36: "Limited Partner",
         }
-        
+
         # Financial acronyms and term expansions
         self.financial_acronyms = {
             # Investment Types
@@ -139,7 +136,6 @@
             "infra": ["infrastructure", "public infrastructure", "critical infrastructure"],
             "hedge": ["hedge fund", "alternative investment", "absolute return"],
             "fo": ["family office", "single family office", "multi family office"],
-            
             # LP Types
             "lp": ["limited partner", "limited partners"],
             "gp": ["general partner", "general partners"],
@@ -149,7 +145,6 @@
             "insurance": ["insurance company", "insurer", "insurance fund"],
             "bank": ["investment bank", "commercial bank", "universal bank"],
             "corp": ["corporate", "corporation", "corporate development"],
-            
             # Geographic
             "us": ["united states", "america", "usa"],
             "uk": ["united kingdom", "britain", "england", "gb"],
@@ -157,7 +152,6 @@
             "asia": ["asian", "apac", "asia pacific"],
             "emea": ["europe middle east africa"],
             "latam": ["latin america", "south america"],
-            
             # Sectors
             "tech": ["technology", "software", "digital"],
             "fintech": ["financial technology", "payments", "banking tech"],
@@ -166,7 +160,6 @@
             "ai": ["artificial intelligence", "machine learning", "ml"],
             "saas": ["software as a service", "cloud software"],
             "ecommerce": ["e-commerce", "online retail", "digital commerce"],
-            
             # Financial Terms
             "aum": ["assets under management", "fund size"],
             "irr": ["internal rate of return"],
@@ -175,7 +168,6 @@
             "nav": ["net asset value"],
             "liquidity": ["liquid", "cash"],
             "leverage": ["leverage ratio", "debt to equity"],
-            
             # Deal Types
             "ipo": ["initial public offering", "public offering"],
             "m&a": ["mergers and acquisitions", "acquisitions"],
@@ -183,7 +175,6 @@
             "rollup": ["roll-up", "consolidation", "platform strategy"],
             "add-on": ["add-on acquisition", "bolt-on"],
             "exit": ["exit", "liquidity event", "realization"],
-            
             # Fund Stages
             "seed": ["seed stage", "pre-seed"],
             "series a": ["series a round", "a round"],
@@ -191,59 +182,56 @@
             "growth": ["growth stage", "expansion stage"],
             "late": ["late stage", "pre-ipo"],
             "buyout": ["control buyout", "majority buyout"],
-            
             # Fund Sizes
             "mega": ["mega fund", "large fund"],
             "mid": ["mid-market", "middle market"],
             "small": ["small cap", "lower middle market"],
             "micro": ["micro cap", "small cap"],
         }
-    
+
     async def call_chatgpt_api(self, prompt: str, temperature: float = 0.1, max_tokens: int = 500) -> str:
         """Call OpenAI API with gpt-4.1 using LangChain pattern."""
-        
+
         try:
             # API monitoring: Track OpenAI API call
             api_call_start_time = time.time()
-            
+
             # Use LangChain ChatOpenAI
-            response = await self.llm.ainvoke([
-                ("user", prompt)
-            ])
-            
+            response = await self.llm.ainvoke([("user", prompt)])
+
             api_response_time = time.time() - api_call_start_time
-            
+
             # Track performance
             self.stats["total_calls"] += 1
             self.stats["total_time"] += api_response_time
-            
+
             # Estimate cost (gpt-4.1 pricing: $2.00/1M input, $8.00/1M output)
             input_tokens = self.estimate_tokens(prompt)
             output_tokens = self.estimate_tokens(response.content)
             cost = (input_tokens / 1_000_000 * 2.00) + (output_tokens / 1_000_000 * 8.00)
             self.stats["total_cost"] += cost
-            
+
             return response.content.strip()
-            
+
         except Exception as e:
             print(f"‚ùå OpenAI API call failed: {e}")
             return ""
-    
+
     def estimate_tokens(self, text: str) -> int:
         """Rough token estimation (4 chars per token average)."""
         return len(text) // 4
-    
+
     def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
         """Calculate cost for gpt-5."""
         input_cost = (input_tokens * 1.25) / 1000000
         output_cost = (output_tokens * 10.00) / 1000000
         return input_cost + output_cost
-    
+
     async def extract_filters_with_llm(self, user_prompt: str) -> Dict[str, Any]:
         """Extract all filters from user prompt using LLM."""
-        
+
         start_time = time.time()
-        
+
         prompt = f"""Extract filter criteria from this search query: "{user_prompt}"
 
 Only extract information explicitly mentioned.
@@ -302,48 +290,43 @@
     "amount2": 2000000000
   }}
 }}"""
-        
+
         response = await self.call_chatgpt_api(prompt, temperature=0.1, max_tokens=500)
         extraction_time = time.time() - start_time
-        
+
         # Update stats
         self.stats["extraction_calls"] += 1
         self.stats["total_calls"] += 1
         self.stats["total_time"] += extraction_time
-        
+
         try:
             extracted_data = json.loads(response)
-            
+
             # Estimate token costs
             input_tokens = self.estimate_tokens(prompt)
             output_tokens = self.estimate_tokens(response)
             cost = self.calculate_cost(input_tokens, output_tokens)
             self.stats["total_cost"] += cost
-            
+
             print(f"üîç Filter extraction completed in {extraction_time:.2f}s")
             print(f"üí∞ Estimated cost: ${cost:.4f} ({input_tokens} input + {output_tokens} output tokens)")
-            
+
             # Ensure expected keys exist
             if not isinstance(extracted_data.get("investor_focus"), list):
                 extracted_data["investor_focus"] = []
             return extracted_data
-            
+
         except json.JSONDecodeError:
             print(f"‚ùå JSON parsing failed after {extraction_time:.2f}s")
             return self.extract_filters_fallback(user_prompt)
-    
+
     def extract_filters_fallback(self, user_prompt: str) -> Dict[str, Any]:
         """Fallback extraction using simple keyword matching."""
-        
+
         print("üîÑ Using fallback extraction...")
-        
-        extracted_data = {
-            "geographic": {"cities": [], "countries": [], "regions": []},
-            "investor_types": [],
-            "investor_focus": [],
-            "aum_filters": {}
-        }
-        
+
+        extracted_data = {"geographic": {"cities": [], "countries": [], "regions": []}, "investor_types": [], "investor_focus": [], "aum_filters": {}}
+
         # Simple geographic extraction
         geographic_keywords = {
             "european": {"regions": ["Europe"]},
@@ -353,15 +336,15 @@
             "asian": {"regions": ["Asia"]},
             "asia": {"regions": ["Asia"]},
             "nordic": {"regions": ["Europe"]},
-            "uk": {"countries": ["United Kingdom"]}
+            "uk": {"countries": ["United Kingdom"]},
         }
-        
+
         prompt_lower = user_prompt.lower()
         for keyword, geo_data in geographic_keywords.items():
             if keyword in prompt_lower:
                 for geo_type, values in geo_data.items():
                     extracted_data["geographic"][geo_type].extend(values)
-        
+
         # Simple investor type extraction
         investor_keywords = {
             "pension": "pension fund",
@@ -369,13 +352,13 @@
             "sovereign wealth": "sovereign wealth fund",
             "venture capital": "venture capital",
             "private equity": "private equity",
-            "endowment": "endowment"
+            "endowment": "endowment",
         }
-        
+
         for keyword, investor_type in investor_keywords.items():
             if keyword in prompt_lower:
                 extracted_data["investor_types"].append(investor_type)
-        
+
         # Simple investor focus extraction (verb-based, concise phrases)
         verbs = [
             "targeting",
@@ -408,15 +391,15 @@
                     seen.add(f)
                     unique_focus.append(f)
             extracted_data["investor_focus"].extend(unique_focus[:3])
-        
+
         return extracted_data
-    
+
     def map_country_name(self, country: str) -> str:
         """Map country variations to standard names."""
-        
+
         country_mapping = {
             "US": "United States",
-            "USA": "United States", 
+            "USA": "United States",
             "America": "United States",
             "United States of America": "United States",
             "UK": "United Kingdom",
@@ -424,21 +407,21 @@
             "Great Britain": "United Kingdom",
             "England": "United Kingdom",
             "Scotland": "United Kingdom",
-            "Wales": "United Kingdom"
+            "Wales": "United Kingdom",
         }
-        
+
         return country_mapping.get(country, country)
-    
+
     def process_geographic_filters(self, extracted_geo: Dict[str, List[str]]) -> Dict[str, List[str]]:
         """Process geographic filters with proper mapping."""
-        
+
         filters = {}
         valid_region_list = ["Africa", "Middle East", "Americas", "Europe", "Oceania", "Asia"]
-        
+
         # Process cities
         if extracted_geo.get("cities"):
             filters["hq_city"] = extracted_geo["cities"]
-        
+
         # Process countries with proper mapping (exclude regions)
         if extracted_geo.get("countries"):
             mapped_countries = []
@@ -451,7 +434,7 @@
                     mapped_countries.append(mapped_country)
             if mapped_countries:
                 filters["hq_country_region"] = mapped_countries
-        
+
         # Process regions with validation
         if extracted_geo.get("regions"):
             valid_regions = []
@@ -460,47 +443,43 @@
                     valid_regions.append(region)
             if valid_regions:
                 filters["hq_global_region"] = valid_regions
-        
+
         return filters
-    
+
     def check_collection_info(self) -> None:
         """Check collection info and sample data for debugging."""
-        
+
         try:
             # Get collection info
             collection_info = self.client.get_collection(self.collection_name)
             print(f"üîç Collection '{self.collection_name}' info:")
             print(f"   Points count: {collection_info.points_count}")
             print(f"   Vector size: {collection_info.config.params.vectors.size}")
-            
+
             # Get a few sample points to see the structure
-            sample_points = self.client.scroll(
-                collection_name=self.collection_name,
-                limit=3,
-                with_payload=True
-            )[0]
-            
+            sample_points = self.client.scroll(collection_name=self.collection_name, limit=3, with_payload=True)[0]
+
             print(f"üîç Sample payload structure:")
             for i, point in enumerate(sample_points):
-                print(f"   Point {i+1}: {list(point.payload.keys())}")
-                if 'filters' in point.payload:
+                print(f"   Point {i + 1}: {list(point.payload.keys())}")
+                if "filters" in point.payload:
                     print(f"      Filters: {point.payload['filters']}")
-                if 'metadata' in point.payload:
+                if "metadata" in point.payload:
                     print(f"      Metadata: {list(point.payload['metadata'].keys())}")
-                    
+
         except Exception as e:
             print(f"‚ùå Collection check failed: {e}")
-    
+
     async def match_investor_types(self, extracted_types: List[str]) -> List[int]:
         """Match extracted types to database categories (only if non-null)."""
-        
+
         if not extracted_types:
             return []
-        
+
         print(f"\nüîç DEBUG - match_investor_types() called with: {extracted_types}")
-        
+
         start_time = time.time()
-        
+
         mapping_prompt = f"""Map these investor type terms to the closest and most relevant database categories:
 
 Extracted terms: {extracted_types}
@@ -569,61 +548,56 @@
 - If no relevant field is found or if it's uncertain, return []
 
 JSON:"""
-        
+
         # Use gpt-4.1-mini for mapping (cheaper for simple tasks)
-        mapping_llm = ChatOpenAI(
-            model="gpt-4.1-mini",
-            temperature=0.1,
-            max_tokens=200,
-            api_key=self.api_key
-        )
-        
+        mapping_llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.1, max_tokens=200, api_key=self.api_key)
+
         response = await mapping_llm.ainvoke([("user", mapping_prompt)])
         mapping_time = time.time() - start_time
-        
+
         print(f"\nüîç DEBUG - MAPPING LLM RAW RESPONSE:")
         print(f"   Raw response content: {response.content}")
-        
+
         # Update stats
         self.stats["mapping_calls"] += 1
         self.stats["total_calls"] += 1
         self.stats["total_time"] += mapping_time
-        
+
         try:
             category_ids = json.loads(response.content)
             print(f"   Parsed category_ids: {category_ids}")
-            
+
             # Ensure all IDs are integers
             category_ids = [int(id) for id in category_ids if str(id).isdigit()]
-            
+
             # Estimate token costs (gpt-4.1-mini pricing: $0.40/1M input, $1.60/1M output)
             input_tokens = self.estimate_tokens(mapping_prompt)
             output_tokens = self.estimate_tokens(response.content)
             cost = (input_tokens / 1_000_000 * 0.40) + (output_tokens / 1_000_000 * 1.60)
             self.stats["total_cost"] += cost
-            
+
             print(f"üîç Investor type mapping completed in {mapping_time:.2f}s")
             print(f"üí∞ Estimated cost: ${cost:.4f} ({input_tokens} input + {output_tokens} output tokens)")
             print(f"üîç Mapped to IDs: {category_ids}")
-            
+
             return category_ids
-            
+
         except json.JSONDecodeError as e:
             print(f"‚ùå Investor type mapping failed after {mapping_time:.2f}s")
             print(f"üîç Debug - Raw response: {response.content[:200]}...")
             print(f"üîç Debug - JSON Error: {e}")
             return []
-    
+
     def process_aum_filters(self, extracted_aum: Dict[str, Any]) -> Optional[Dict[str, float]]:
         """Process AUM filters for direct numeric filtering."""
-        
+
         if not extracted_aum or not extracted_aum.get("operator"):
             return None
-        
+
         aum_filters = {}
         operator = extracted_aum["operator"]
         amount = extracted_aum.get("amount", 0)
-        
+
         if operator == "greater_than":
             aum_filters["aum_min"] = amount
         elif operator == "less_than":
@@ -631,15 +605,15 @@
         elif operator == "between":
             aum_filters["aum_min"] = amount
             aum_filters["aum_max"] = extracted_aum.get("amount2", amount)
-        
+
         return aum_filters
-    
+
     def build_qdrant_filter(self, filters: Dict[str, Any]) -> Optional[Filter]:
         """Build Qdrant filter from extracted filters.
-        
+
         Always includes a filter to only return firms with non-null website values.
         """
-        
+
         filter_conditions = []
 
         # Helper to add a grouped OR (should) for a given field and list of values, as a nested filter in must
@@ -664,7 +638,7 @@
 
         # Collect all investor type conditions (flags + mapped types) for OR logic
         investor_type_conditions = []
-        
+
         # Category flags (pension, endowment, foundation, insurance, charity)
         if "categories" in filters:
             categories = filters["categories"]
@@ -676,18 +650,16 @@
                     "endowment": "filters.is_endowment",
                     "foundation": "filters.is_foundation",
                     "insurance": "filters.is_insurance",
-                    "charity": "filters.is_charity"
+                    "charity": "filters.is_charity",
                 }
-                
+
                 for category in categories:
                     flag_field = category_to_flag.get(category.lower())
                     if flag_field:
                         print(f"   Mapping category '{category}' -> {flag_field} = True")
                         # Create condition for flag = true
-                        investor_type_conditions.append(
-                            FieldCondition(key=flag_field, match=MatchValue(value=True))
-                        )
-        
+                        investor_type_conditions.append(FieldCondition(key=flag_field, match=MatchValue(value=True)))
+
         # Primary investor types (convert IDs to names, OR across multiple values)
         if "primary_investor_types" in filters:
             investor_type_ids = filters["primary_investor_types"]
@@ -698,43 +670,35 @@
                 # Create OR condition for mapped types
                 mapped_type_conditions = []
                 for type_name in type_names:
-                    mapped_type_conditions.append(
-                        FieldCondition(key="filters.primary_investor_types", match=MatchValue(value=type_name))
-                    )
+                    mapped_type_conditions.append(FieldCondition(key="filters.primary_investor_types", match=MatchValue(value=type_name)))
                 if mapped_type_conditions:
                     if len(mapped_type_conditions) == 1:
                         investor_type_conditions.append(mapped_type_conditions[0])
                     else:
                         # Multiple mapped types: wrap in MinShould
-                        investor_type_conditions.append(
-                            Filter(min_should=MinShould(conditions=mapped_type_conditions, min_count=1))
-                        )
-        
+                        investor_type_conditions.append(Filter(min_should=MinShould(conditions=mapped_type_conditions, min_count=1)))
+
         # Add all investor type conditions as a single OR group
         if investor_type_conditions:
             if len(investor_type_conditions) == 1:
                 # Single condition: add directly
-                print(f"   Adding single investor type condition: {investor_type_conditions[0].key if hasattr(investor_type_conditions[0], 'key') else 'MinShould'}")
+                print(
+                    f"   Adding single investor type condition: {investor_type_conditions[0].key if hasattr(investor_type_conditions[0], 'key') else 'MinShould'}"
+                )
                 filter_conditions.append(investor_type_conditions[0])
             else:
                 # Multiple conditions (flags + mapped types): wrap in OR logic
                 print(f"   Creating OR group for investor types: {len(investor_type_conditions)} conditions (flags + mapped types)")
-                filter_conditions.append(
-                    Filter(min_should=MinShould(conditions=investor_type_conditions, min_count=1))
-                )
-        
+                filter_conditions.append(Filter(min_should=MinShould(conditions=investor_type_conditions, min_count=1)))
+
         # AUM filters (direct numeric filtering)
         if "aum" in filters:
             aum_filters = filters["aum"]
             if "aum_min" in aum_filters:
-                filter_conditions.append(
-                    FieldCondition(key="metadata.aum", range=Range(gte=aum_filters["aum_min"]))
-                )
+                filter_conditions.append(FieldCondition(key="metadata.aum", range=Range(gte=aum_filters["aum_min"])))
             if "aum_max" in aum_filters:
-                filter_conditions.append(
-                    FieldCondition(key="metadata.aum", range=Range(lte=aum_filters["aum_max"]))
-                )
-        
+                filter_conditions.append(FieldCondition(key="metadata.aum", range=Range(lte=aum_filters["aum_max"])))
+
         # Website filter: only include firms with non-null, non-empty website values
         # This filter is always applied to ensure we only return LPs with website information
         # Use must_not to exclude both null and empty/missing fields
@@ -742,82 +706,82 @@
         # IsEmptyCondition: excludes points where field is empty or missing
         must_not_conditions = [
             IsNullCondition(is_null=PayloadField(key="metadata.website")),
-            IsEmptyCondition(is_empty=PayloadField(key="metadata.website"))
+            IsEmptyCondition(is_empty=PayloadField(key="metadata.website")),
         ]
-        
+
         # Always return a filter (at minimum, the website filter)
         return Filter(must=filter_conditions, must_not=must_not_conditions)
-    
+
     def build_enhanced_query(self, user_prompt: str, fund_profile: Dict[str, Any]) -> str:
         """Build enhanced query combining user prompt with fund profile context."""
-        
+
         profile_context_parts = []
         investor_profile = fund_profile.get("investor_profile", {})
-        
+
         profile_context_parts.append(investor_profile.get("firm_description", ""))
-        
+
         if any(word in user_prompt.lower() for word in ["invest", "fund", "raise", "capital"]):
             profile_context_parts.append(investor_profile.get("key_objectives", ""))
-        
+
         profile_context_parts.append(investor_profile.get("key_differentiators", ""))
-        
+
         if investor_profile.get("sector_focus"):
             profile_context_parts.append(f"Sector focus: {investor_profile['sector_focus']}")
-        
+
         if investor_profile.get("geographic_focus"):
             profile_context_parts.append(f"Geographic focus: {investor_profile['geographic_focus']}")
-        
+
         if investor_profile.get("fund_type"):
             profile_context_parts.append(f"Fund type: {investor_profile['fund_type']}")
-        
+
         enhanced_query = f"""
         Primary Query: {user_prompt}
-        Fund Profile: {' | '.join(profile_context_parts)}
+        Fund Profile: {" | ".join(profile_context_parts)}
         """
-        
+
         return enhanced_query.strip()
-    
+
     def build_separate_queries(self, user_prompt: str, fund_profile: Dict[str, Any]) -> Tuple[str, str]:
         """Build separate prompt and profile queries for weighted combination."""
-        
+
         # Clean prompt query
         prompt_query = user_prompt.strip()
-        
+
         # Build structured profile context with optimal ordering
         profile_query = self.build_structured_profile_query(fund_profile)
-        
+
         return prompt_query, profile_query
-    
+
     def build_structured_profile_query(self, fund_profile: Dict[str, Any]) -> str:
         """Build structured profile query with optimal ordering for vector search.
-        
+
         Structure matches build_profile_text() exactly:
         1. Firm Description
         2. Key Objectives
         3. Firm's Investment Focus (Geographic Focus, Sector Focus, Limited Partner Target Types)
-        
+
         Uses structured format with headings and newlines for better embedding quality.
         """
         investor_profile = fund_profile.get("investor_profile", {})
         sections = []
-        
+
         # 1. Firm Description (most important - core identity)
         firm_desc = investor_profile.get("firm_description")
         if firm_desc and firm_desc.strip():
             sections.append("Firm Description")
             sections.append(firm_desc.strip())
             sections.append("")  # Empty line after section
-        
+
         # 2. Key Objectives (what the fund is trying to achieve)
         key_obj = investor_profile.get("key_objectives")
         if key_obj and key_obj.strip():
             sections.append("Key Objectives")
             sections.append(key_obj.strip())
             sections.append("")  # Empty line after section
-        
+
         # 3. Firm's Investment Focus section
         investment_focus_fields = []
-        
+
         # Geographic Focus (where the fund operates)
         geo_focus = investor_profile.get("geographic_focus")
         if geo_focus:
@@ -827,7 +791,7 @@
                 formatted_geo = str(geo_focus)
             if formatted_geo:
                 investment_focus_fields.append(f"Geographic Focus: {formatted_geo}")
-        
+
         # Sector Focus (industry specialization)
         sector_focus = investor_profile.get("sector_focus")
         if sector_focus:
@@ -837,7 +801,7 @@
                 formatted_sector = str(sector_focus)
             if formatted_sector:
                 investment_focus_fields.append(f"Sector Focus: {formatted_sector}")
-        
+
         # LP Target Types (specific LP preferences - format list properly)
         lp_target_types = investor_profile.get("lp_target_types")
         if lp_target_types:
@@ -850,34 +814,34 @@
                     formatted_types = f"{', '.join(lp_target_types[:-1])}, and {lp_target_types[-1]}"
             else:
                 formatted_types = str(lp_target_types)
-            
+
             investment_focus_fields.append(f"Limited Partner Target Types: {formatted_types}")
-        
+
         # Add Investment Focus section if it has any fields
         if investment_focus_fields:
             sections.append("Firm's Investment Focus")
             sections.extend(investment_focus_fields)
             sections.append("")  # Empty line after section
-        
+
         # Join all sections with newlines
         result = "\n".join(sections).strip()
-        
+
         # Return empty string if no content (all fields were empty)
         return result if result else ""
-    
+
     def enhance_query(self, user_prompt: str) -> str:
         """Enhance query with financial term expansion and normalization."""
-        
+
         # Normalize to lowercase for detection; keep original for output
         processed = user_prompt.lower()
-        
+
         # Tokenize into whole-word tokens for safe acronym matching
         tokens = set(re.findall(r"\b\w+\b", processed))
-        
+
         # Helper to test phrase (multi-word) presence with word boundaries
         def contains_phrase(text: str, phrase: str) -> bool:
             return re.search(rf"\b{re.escape(phrase)}\b", text) is not None
-        
+
         expanded_terms = []
         for term, synonyms in self.financial_acronyms.items():
             is_multi_word = " " in term
@@ -887,64 +851,53 @@
             else:
                 # single token: require exact token match (prevents 'tech' triggering on 'biotech', 'fo' on 'focus')
                 present = term in tokens
-            
+
             if present:
                 for synonym in synonyms:
                     # Avoid duplicates and only add synonyms not already present as whole words
                     if not contains_phrase(processed, synonym):
                         expanded_terms.append(synonym)
-        
+
         # Combine original query with a limited number of expansions
         if expanded_terms:
             expanded_query = f"{user_prompt} {' '.join(expanded_terms[:2])}"
         else:
             expanded_query = user_prompt
-        
+
         return expanded_query
-    
-    def calculate_enhanced_score(
-        self, 
-        vector_score: float, 
-        lp_result: Dict[str, Any], 
-        user_prompt: str, 
-        fund_profile: Dict[str, Any]
-    ) -> float:
+
+    def calculate_enhanced_score(self, vector_score: float, lp_result: Dict[str, Any], user_prompt: str, fund_profile: Dict[str, Any]) -> float:
         """Calculate enhanced score with 100% semantic search weighting."""
-        
+
         # 100% semantic search - no additional factors
         return vector_score
-    
-    
-    
+
     async def get_query_embedding(self, query_text: str) -> List[float]:
         """Get embedding for query text using OpenAI API."""
-        
+
         try:
             # Use OpenAI embeddings API (new format for openai>=1.0.0)
             client = openai.AsyncOpenAI(api_key=self.api_key)
-            response = await client.embeddings.create(
-                model="text-embedding-3-large",
-                input=query_text
-            )
-            
+            response = await client.embeddings.create(model="text-embedding-3-large", input=query_text)
+
             return response.data[0].embedding
-            
+
         except Exception as e:
             print(f"‚ùå Embedding generation failed: {e}")
             # Fallback to random vector if embedding fails
             return [random.random() for _ in range(3072)]
-    
+
     async def get_weighted_query_embedding(
-        self, 
-        user_prompt: str, 
+        self,
+        user_prompt: str,
         fund_profile: Dict[str, Any],
         use_cached_profile_embedding: bool = False,
         user_id: Optional[str] = None,
-        profile_row: Optional[Dict[str, Any]] = None
+        profile_row: Optional[Dict[str, Any]] = None,
     ) -> List[float]:
         """
         Get weighted combination of prompt and profile embeddings (70/30).
-        
+
         Args:
             user_prompt: User's search query
             fund_profile: Fund profile dictionary
@@ -952,38 +905,39 @@
             user_id: User ID for retrieving cached embedding (required if use_cached_profile_embedding=True)
             profile_row: Full profile row from database (for storing on-demand generated embeddings)
         """
-        
+
         # Enhance query first (expand financial terms)
         enhanced_prompt = self.enhance_query(user_prompt)
-        
+
         # Build separate queries
         prompt_query, profile_query = self.build_separate_queries(enhanced_prompt, fund_profile)
-        
+
         print(f"üîç Enhanced prompt query: {prompt_query}")
         print(f"üîç Structured profile query: {profile_query}")
-        
+
         # Get prompt embedding (always fresh)
         prompt_embedding = await self.get_query_embedding(prompt_query)
-        
+
         # Get profile embedding: try cached first, fallback to generation
         profile_embedding = None
         embedding_was_generated = False  # Track if we generated on-demand
-        
+
         if use_cached_profile_embedding and user_id:
             # Try to retrieve pre-computed embedding from pgvector
             try:
                 from app.services.profile_embedding_service import get_profile_embedding
+
                 profile_embedding = await get_profile_embedding(user_id)
-                
+
                 if profile_embedding:
                     # Ensure profile_embedding is a list of floats
                     if not isinstance(profile_embedding, list):
                         # Convert if needed (handle pgvector return types)
-                        if hasattr(profile_embedding, 'tolist'):
+                        if hasattr(profile_embedding, "tolist"):
                             profile_embedding = [float(x) for x in profile_embedding.tolist()]
                         else:
                             profile_embedding = [float(x) for x in profile_embedding]
-                    
+
                     # Ensure correct length (3072 dimensions)
                     if len(profile_embedding) != 3072:
                         print(f"‚ö†Ô∏è Profile embedding has wrong length: {len(profile_embedding)}, expected 3072")
@@ -1018,11 +972,11 @@
             else:
                 # If no profile context, use zero vector (pure prompt search)
                 profile_embedding = [0.0] * 3072
-        
+
         # Store on-demand generated embedding to pgvector (background task)
         if embedding_was_generated:
             print(f"üîç DEBUG: embedding_was_generated=True, user_id={user_id}, profile_row={'present' if profile_row else 'MISSING'}")
-            
+
             if not user_id:
                 print(f"‚ö†Ô∏è Cannot store embedding: user_id is missing")
             elif not profile_row:
@@ -1033,8 +987,9 @@
                     from app.services.profile_embedding_service import (
                         upsert_profile_embedding,
                         upsert_profile_embedding_for_feedback,
-                        get_profile_embedding_for_feedback
+                        get_profile_embedding_for_feedback,
                     )
+
                     # Store investor_search embedding as background task (non-blocking)
                     asyncio.create_task(upsert_profile_embedding(user_id, profile_row))
                     print(f"üíæ Storing on-demand generated embedding to pgvector for user_id: {user_id} (background task)")
@@ -1049,23 +1004,23 @@
                     print(f"‚ö†Ô∏è Failed to store on-demand embedding (non-critical): {e}")
         else:
             print(f"üîç DEBUG: embedding_was_generated=False, skipping storage")
-        
+
         # Weighted combination: 70% prompt, 30% profile (applies to both cached and generated paths)
         weighted_embedding = []
         for i in range(len(prompt_embedding)):
             weighted_value = (0.7 * prompt_embedding[i]) + (0.3 * profile_embedding[i])
             weighted_embedding.append(weighted_value)
-        
+
         print(f"üîç Using weighted embedding: 70% prompt + 30% profile")
         return weighted_embedding
-    
+
     def _detect_flag_categories(self, raw_types: list[str]) -> tuple[list[str], list[str]]:
         """
         Detect flag-based categories (pension, endowment, foundation, insurance, charity) from investor types.
-        
+
         Args:
             raw_types: List of investor types extracted from prompt
-            
+
         Returns:
             Tuple of (categories_or, cleaned_types):
             - categories_or: List of flag category names to filter by
@@ -1073,19 +1028,19 @@
         """
         categories_or: list[str] = []
         cleaned_types: list[str] = []
-        
+
         def matches_any(patterns: list[str], text: str) -> bool:
             for pat in patterns:
                 if re.search(pat, text):
                     return True
             return False
-        
+
         pension_patterns = [r"\bpension(s)?\b", r"\bretirement\s+fund(s)?\b", r"\bsuperannuation\b"]
         endowment_patterns = [r"\bendow(ment|ed)\b"]
         foundation_patterns = [r"\bfoundation(s)?\b", r"\bgrant[-\s]?making\b", r"\bcharitable\s+trust\b"]
         insurance_patterns = [r"\binsur(ance|er|ers)\b", r"\breinsur(ance|er)\b"]
         charity_patterns = [r"\bcharit(y|ies|able)\b"]
-        
+
         for t in raw_types:
             tnorm = (t or "").strip().lower()
             if not tnorm:
@@ -1112,24 +1067,26 @@
                 continue
             # keep non-flag types for mapping
             cleaned_types.append(t)
-        
+
         return categories_or, cleaned_types
-    
-    def build_field_specific_anchors(self, categories_or: list[str], use_merged_charity_foundation: bool = False, raw_types: list[str] = None) -> list[str]:
+
+    def build_field_specific_anchors(
+        self, categories_or: list[str], use_merged_charity_foundation: bool = False, raw_types: list[str] = None
+    ) -> list[str]:
         """
         Build field-specific semantic anchors for flag-based investor categories.
-        
+
         Args:
             categories_or: List of category names (pension, endowment, foundation, insurance, charity)
             use_merged_charity_foundation: If True, use merged charity+foundation anchor instead of individual ones
             raw_types: List of full investor types extracted from prompt (e.g., ["Public Sector Pension Fund", "University Endowment"])
-            
+
         Returns:
             List of anchor strings to add to the semantic prompt
         """
         anchor_lines = []
         raw_types = raw_types or []
-        
+
         def extract_subtype(category: str, raw_types: list[str]) -> str:
             """Extract subtype from raw_types if it matches the category."""
             category_lower = category.lower()
@@ -1149,7 +1106,7 @@
                         subtype = " ".join(parts[:category_index])
                         return subtype.title()
             return None
-        
+
         # Handle merged charity/foundation first
         if use_merged_charity_foundation:
             # Check for subtype in raw_types (e.g., "private foundation", "charitable foundation")
@@ -1164,98 +1121,106 @@
                         if parts[0] not in ["charitable", "charity", "foundation"]:
                             subtype = parts[0].title()
                             break
-            
+
             subtype_prefix = f"{subtype} " if subtype else ""
-            anchor_lines.extend([
-                f"REQUIRE: Companies that ARE {subtype_prefix}charities, charitable organizations, or foundations and whose primary identity is charitable organization, registered charity, or grant-making foundation",
-                f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}charities or foundations but are not charitable organizations or foundations themselves"
-            ])
-        
+            anchor_lines.extend(
+                [
+                    f"REQUIRE: Companies that ARE {subtype_prefix}charities, charitable organizations, or foundations and whose primary identity is charitable organization, registered charity, or grant-making foundation",
+                    f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}charities or foundations but are not charitable organizations or foundations themselves",
+                ]
+            )
+
         # Process other categories (excluding charity and foundation if merged)
         processed_categories = set()
         if use_merged_charity_foundation:
             processed_categories.add("charity")
             processed_categories.add("foundation")
-        
+
         for category in categories_or:
             if category.lower() in processed_categories:
                 continue
-            
+
             category_lower = category.lower()
-            
+
             if category_lower == "pension":
                 subtype = extract_subtype("pension", raw_types)
                 subtype_prefix = f"{subtype} " if subtype else ""
-                
-                anchor_lines.extend([
-                    f"REQUIRE: Companies that ARE {subtype_prefix}pension funds and whose primary identity is managing pension assets or retirement benefits",
-                    f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}pension funds but are not {subtype_prefix}pension funds themselves"
-                ])
+
+                anchor_lines.extend(
+                    [
+                        f"REQUIRE: Companies that ARE {subtype_prefix}pension funds and whose primary identity is managing pension assets or retirement benefits",
+                        f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}pension funds but are not {subtype_prefix}pension funds themselves",
+                    ]
+                )
                 processed_categories.add("pension")
-            
+
             elif category_lower == "endowment":
                 subtype = extract_subtype("endowment", raw_types)
                 subtype_prefix = f"{subtype} " if subtype else ""
-                
-                anchor_lines.extend([
-                    f"REQUIRE: Companies that ARE {subtype_prefix}endowments and whose primary identity is managing endowment assets",
-                    f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}endowments but are not {subtype_prefix}endowments themselves"
-                ])
+
+                anchor_lines.extend(
+                    [
+                        f"REQUIRE: Companies that ARE {subtype_prefix}endowments and whose primary identity is managing endowment assets",
+                        f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}endowments but are not {subtype_prefix}endowments themselves",
+                    ]
+                )
                 processed_categories.add("endowment")
-            
+
             elif category_lower == "insurance":
                 subtype = extract_subtype("insurance", raw_types)
                 subtype_prefix = f"{subtype} " if subtype else ""
-                
-                anchor_lines.extend([
-                    f"REQUIRE: Companies that ARE {subtype_prefix}insurance companies or insurers and whose primary business is insurance underwriting or reinsurance",
-                    f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}insurance companies but are not {subtype_prefix}insurance companies themselves"
-                ])
+
+                anchor_lines.extend(
+                    [
+                        f"REQUIRE: Companies that ARE {subtype_prefix}insurance companies or insurers and whose primary business is insurance underwriting or reinsurance",
+                        f"EXCLUDE: Companies that interact with, work with, provide services for, or seek investment from {subtype_prefix}insurance companies but are not {subtype_prefix}insurance companies themselves",
+                    ]
+                )
                 processed_categories.add("insurance")
-        
+
         return anchor_lines
-    
+
     async def search_lps_for_fund(
-        self, 
-        fund_profile: Dict[str, Any], 
-        user_prompt: str, 
+        self,
+        fund_profile: Dict[str, Any],
+        user_prompt: str,
         limit: int = 10,
         use_cached_profile_embedding: bool = False,
         user_id: Optional[str] = None,
-        profile_row: Optional[Dict[str, Any]] = None
+        profile_row: Optional[Dict[str, Any]] = None,
     ) -> List[Dict[str, Any]]:
         """Complete search process: extract filters, build query, search LPs."""
-        
+
         print(f"\nüîç Searching for LPs for: {fund_profile['firm_name']}")
         print(f"üìù Query: {user_prompt}")
-        
+
         # Extract filters using LLM
         extracted_data = await self.extract_filters_with_llm(user_prompt)
-        
+
         # DEBUG: Show raw extraction LLM output
         print(f"\nüîç DEBUG - EXTRACTION LLM OUTPUT:")
         print(f"   Raw extracted_data: {extracted_data}")
         print(f"   investor_types (raw): {extracted_data.get('investor_types', [])}")
         print(f"   investor_focus: {extracted_data.get('investor_focus', [])}")
-        
+
         # Process each filter type
         filters = {}
-        
+
         # Geographic filters
         geo_filters = self.process_geographic_filters(extracted_data.get("geographic", {}))
         filters.update(geo_filters)
-        
+
         # Derive category flags from investor_types (robust word-boundary matching)
         raw_types = extracted_data.get("investor_types", []) or []
         categories_or, cleaned_types = self._detect_flag_categories(raw_types)
-        
+
         print(f"\nüîç DEBUG - FLAG DETECTION RESULTS:")
         print(f"   categories_or (flag types filtered out): {categories_or}")
         print(f"   cleaned_types (going to mapping LLM): {cleaned_types}")
-        
+
         if categories_or:
             filters["categories"] = categories_or
-        
+
         # Primary investor types (only if non-null) after removing flagged ones
         if cleaned_types:
             print(f"\nüîç DEBUG - MAPPING LLM INPUT:")
@@ -1268,14 +1233,14 @@
         else:
             print(f"\nüîç DEBUG - MAPPING LLM:")
             print(f"   SKIPPED: cleaned_types is empty (all types were flags)")
-        
+
         # AUM filters (direct numeric filtering)
         aum_filters = self.process_aum_filters(extracted_data.get("aum_filters", {}))
         if aum_filters:
             filters["aum"] = aum_filters
-        
+
         print(f"üéØ Extracted filters: {filters}")
-        
+
         # Handle charity/foundation merging: if either is detected, add both to filter and use merged anchor
         use_merged_charity_foundation = False
         if "charity" in categories_or or "foundation" in categories_or:
@@ -1289,12 +1254,12 @@
                 print(f"   üîó Added 'foundation' to filter (charity detected)")
             # Update filters with both
             filters["categories"] = categories_or
-        
+
         # --- Build anchored semantic prompt ---
         # MUST MATCH investor type should reflect raw investor_types (pre-removal),
         # preserving user intent wording (e.g., "pension fund", "venture capital").
         must_types = [t.strip() for t in (extracted_data.get("investor_types") or []) if t and t.strip()]
-        
+
         # Preferred investor focus: use either LLM-provided investor_focus or cleaned_types
         preferred_focus_parts: list[str] = []
         focus_list = extracted_data.get("investor_focus") or []
@@ -1309,96 +1274,90 @@
                 preferred_focus_parts[-1] = preferred_focus_parts[-1] + f" in {geo_part}"
             else:
                 preferred_focus_parts.append(f"in {geo_part}")
-        
+
         anchor_lines: list[str] = []
-        
+
         # Reordered structure: Query ‚Üí MUST MATCH ‚Üí REQUIRE ‚Üí EXCLUDE ‚Üí PREFERRED
         # 1. Primary Query FIRST (user intent prioritized)
         anchor_lines.append(f"Primary Query: {user_prompt}")
-        
+
         # 2. MUST MATCH second (direct constraint)
         if must_types:
             anchor_lines.append(f"MUST MATCH: investor type = {', '.join(must_types)}")
-        
+
         # 3. Build field-specific semantic anchors (REQUIRE and EXCLUDE lines)
         field_anchors = self.build_field_specific_anchors(categories_or, use_merged_charity_foundation, raw_types)
         if field_anchors:
             anchor_lines.extend(field_anchors)
-        
+
         # 4. PREFERRED last (soft signal)
         if preferred_focus_parts:
             anchor_lines.append(f"PREFERRED: investor focus ‚âà {preferred_focus_parts[0]}")
-        
+
         anchored_prompt = "\n".join(anchor_lines)
-        
+
         # Check collection info for debugging
         self.check_collection_info()
-        
+
         # Build enhanced query (for display purposes)
         enhanced_query = self.build_enhanced_query(anchored_prompt, fund_profile)
-        
+
         # Build structured profile query for display
         profile_query = self.build_structured_profile_query(fund_profile)
-        
+
         # Enhance anchored prompt (for actual embedding - expands financial terms)
         enhanced_anchored_prompt = self.enhance_query(anchored_prompt)
-        
+
         # Store semantic block components for display (what actually gets embedded)
         self.last_semantic_block = {
             "anchored_prompt": anchored_prompt,
             "enhanced_anchored_prompt": enhanced_anchored_prompt,  # What actually gets embedded
             "profile_query": profile_query,
-            "weighting": "70% enhanced_anchored_prompt + 30% profile_query"
+            "weighting": "70% enhanced_anchored_prompt + 30% profile_query",
         }
-        
+
         # Create weighted query vector (70% prompt, 30% profile)
         query_vector = await self.get_weighted_query_embedding(
-            anchored_prompt, 
-            fund_profile,
-            use_cached_profile_embedding=use_cached_profile_embedding,
-            user_id=user_id,
-            profile_row=profile_row
+            anchored_prompt, fund_profile, use_cached_profile_embedding=use_cached_profile_embedding, user_id=user_id, profile_row=profile_row
         )
-        
+
         # Build Qdrant filter
         qdrant_filter = self.build_qdrant_filter(filters)
-        
+
         print(f"üîç Qdrant filter: {qdrant_filter}")
         print(f"üîç Filter is None: {qdrant_filter is None}")
         if qdrant_filter:
             print(f"üîç Filter must conditions: {qdrant_filter.must}")
             print(f"üîç Filter type: {type(qdrant_filter)}")
         print(f"üîç Query vector length: {len(query_vector)}")
-        
+
         # Perform hybrid search: metadata filtering + semantic search in single call
         print(f"üîç Performing hybrid search with filters and semantic ranking...")
-        
+
         # Perform hybrid search
         results = self.client.search(
             collection_name=self.collection_name,
             query_vector=query_vector,  # Use actual query embedding
             query_filter=qdrant_filter,  # Apply metadata filters first
             limit=limit,  # Get exact number requested
-            with_payload=True
+            with_payload=True,
         )
-        
+
         print(f"üîç Found {len(results)} results from hybrid search")
-        
+
         # DEBUG: Check the is_pension values in the actual results
         if results:
             print(f"\nüîç DEBUG - FIRST RESULT CHECK:")
             first_result = results[0]
             print(f"   Result payload filters: {first_result.payload.get('filters', {})}")
             print(f"   is_pension value: {first_result.payload.get('filters', {}).get('is_pension', 'NOT FOUND')}")
-        
+
         # Store filter counts for return
-        filter_counts = {
-            "hybrid_search_results": len(results)
-        }
-        
+        filter_counts = {"hybrid_search_results": len(results)}
+
         # Convert results to LP format
         lp_results = []
-        
+
         for result in results:
             lp_data = {
                 "lp_name": result.payload.get("metadata", {}).get("company_name", "Unknown LP"),
@@ -1409,30 +1368,30 @@
                 "hq_city": result.payload.get("filters", {}).get("hq_city", "Unknown"),
                 "aum": result.payload.get("metadata", {}).get("aum", "Unknown"),
                 "score": result.score,  # Semantic similarity score
-                "raw_payload": result.payload
+                "raw_payload": result.payload,
             }
-            
+
             lp_results.append(lp_data)
-        
+
         # Results are already sorted by semantic similarity from Qdrant
         # Add filter counts to each result for easy access
         for lp_result in lp_results:
             lp_result["filter_counts"] = filter_counts
-        
+
         print(f"‚úÖ Returned {len(lp_results)} results")
-        
+
         # Return results with filters dict for feedback collection
         return {
             "results": lp_results,
             "filters": filters,  # The filters dict used to build Qdrant filter
-            "filter_counts": filter_counts
+            "filter_counts": filter_counts,
         }
-    
+
     def evaluate_lp_match(self, fund_profile: Dict[str, Any], lp_result: Dict[str, Any], user_prompt: str) -> Dict[str, int]:
         """Evaluate how well an LP matches a fund's needs."""
-        
+
         scores = {}
-        
+
         # Geographic alignment
         fund_geo = fund_profile.get("investor_profile", {}).get("geographic_focus", "")
         lp_geo = lp_result.get("geographic_focus", "")
@@ -1442,7 +1401,7 @@
             scores["geographic_alignment"] = 3
         else:
             scores["geographic_alignment"] = 1
-        
+
         # Sector alignment
         fund_sector = fund_profile.get("investor_profile", {}).get("sector_focus", "")
         lp_sector = lp_result.get("sector_preferences", "")
@@ -1452,7 +1411,7 @@
             scores["sector_alignment"] = 3
         else:
             scores["sector_alignment"] = 1
-        
+
         # LP type alignment
         fund_lp_types = fund_profile.get("investor_profile", {}).get("lp_target_types", [])
         lp_type = lp_result.get("lp_type", "")
@@ -1460,7 +1419,7 @@
             scores["lp_type_alignment"] = 5
         else:
             scores["lp_type_alignment"] = 2
-        
+
         # Query relevance
         query_words = user_prompt.lower().split()
         lp_name = lp_result.get("lp_name", "").lower()
@@ -1468,19 +1427,19 @@
             scores["query_relevance"] = 5
         else:
             scores["query_relevance"] = 3
-        
+
         # Overall score
         scores["overall_score"] = sum(scores.values()) // len(scores)
-        
+
         return scores
-    
+
     def _build_query_from_filters(self, filters: Dict[str, Any]) -> str:
         """Convert structured filters to natural language query for embedding generation.
-        
+
         This helps the embedding capture the search intent even when using structured filters.
         """
         query_parts = []
-        
+
         # Geographic filters
         if filters.get("hq_city"):
             cities = filters["hq_city"]
@@ -1491,7 +1450,7 @@
                     query_parts.append(f"headquartered in {', '.join(cities[:-1])} or {cities[-1]}")
             elif isinstance(cities, str):
                 query_parts.append(f"headquartered in {cities}")
-        
+
         if filters.get("hq_country_region"):
             countries = filters["hq_country_region"]
             if isinstance(countries, list) and countries:
@@ -1501,7 +1460,7 @@
                     query_parts.append(f"based in {', '.join(countries[:-1])} or {countries[-1]}")
             elif isinstance(countries, str):
                 query_parts.append(f"based in {countries}")
-        
+
         if filters.get("hq_global_region"):
             regions = filters["hq_global_region"]
             if isinstance(regions, list) and regions:
@@ -1511,13 +1470,13 @@
                     query_parts.append(f"located in {', '.join(regions[:-1])} or {regions[-1]}")
             elif isinstance(regions, str):
                 query_parts.append(f"located in {regions}")
-        
+
         # AUM filters
         aum_filters = filters.get("aum", {})
         if aum_filters:
             aum_min = aum_filters.get("aum_min")
             aum_max = aum_filters.get("aum_max")
-            
+
             if aum_min and aum_max:
                 # Convert to millions for readability
                 min_m = aum_min / 1_000_000
@@ -1529,25 +1488,25 @@
             elif aum_max:
                 max_m = aum_max / 1_000_000
                 query_parts.append(f"with AUM less than ${max_m:.0f}M")
-        
+
         # Build final query
         if query_parts:
             query_text = "investors " + " ".join(query_parts)
         else:
             query_text = "investors"
-        
+
         return query_text
-    
+
     async def search_with_structured_filters(
         self,
         filters: Dict[str, Any],
         user_id: str,
         limit: int = 50,
         use_cached_profile_embedding: bool = True,
-        profile_row: Optional[Dict[str, Any]] = None
+        profile_row: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, Any]:
         """Search LPs using structured filters (from form inputs) instead of LLM extraction.
-        
+
         Args:
             filters: Direct filter dictionary with keys:
                 - hq_city: List[str] or str
@@ -1558,19 +1517,20 @@
             limit: Maximum number of results
             use_cached_profile_embedding: Whether to use cached profile embedding
             profile_row: Optional pre-fetched profile row
-            
+
         Returns:
             Dict with results, filters, and filter_counts (same format as search_lps_for_fund)
         """
         print(f"\nüîç Structured filter search for user: {user_id}")
         print(f"üìã Filters: {filters}")
-        
+
         # Get user profile
         fund_profile = {"firm_name": "Structured Search", "investor_profile": {}}
-        
+
         if user_id:
             try:
                 from app.utils.global_db import get_global_db
+
                 db = await get_global_db()
                 if db:
                     async with db.pool.acquire() as conn:
@@ -1582,7 +1542,7 @@
                             LEFT JOIN users u ON u.user_id = up.user_id
                             WHERE up.user_id = $1
                             """,
-                            user_id
+                            user_id,
                         )
                         if profile:
                             investor_profile_raw = profile.get("investor_profile")
@@ -1593,60 +1553,48 @@
                             elif isinstance(investor_profile_raw, str):
                                 try:
                                     import json
+
                                     investor_profile = json.loads(investor_profile_raw)
                                 except (json.JSONDecodeError, TypeError):
                                     investor_profile = {}
                             else:
                                 investor_profile = {}
-                            
-                            fund_profile = {
-                                "firm_name": profile.get("firm_name") or "Structured Search",
-                                "investor_profile": investor_profile
-                            }
+
+                            fund_profile = {"firm_name": profile.get("firm_name") or "Structured Search", "investor_profile": investor_profile}
                             profile_row = dict(profile)
             except Exception as e:
                 print(f"‚ö†Ô∏è Failed to retrieve user profile: {e}")
-        
+
         # Build query text from filters for embedding generation
         query_text = self._build_query_from_filters(filters)
         print(f"üìù Generated query text: {query_text}")
-        
+
         # Enhance query (expand financial terms)
         enhanced_query = self.enhance_query(query_text)
-        
+
         # Get weighted embedding (70% query, 30% profile)
         query_vector = await self.get_weighted_query_embedding(
-            enhanced_query,
-            fund_profile,
-            use_cached_profile_embedding=use_cached_profile_embedding,
-            user_id=user_id,
-            profile_row=profile_row
+            enhanced_query, fund_profile, use_cached_profile_embedding=use_cached_profile_embedding, user_id=user_id, profile_row=profile_row
         )
-        
+
         # Build Qdrant filter from structured filters
         qdrant_filter = self.build_qdrant_filter(filters)
-        
+
         print(f"üîç Qdrant filter: {qdrant_filter}")
         print(f"üîç Query vector length: {len(query_vector)}")
-        
+
         # Perform hybrid search
         print(f"üîç Performing hybrid search with structured filters and semantic ranking...")
-        
+
         results = self.client.search(
-            collection_name=self.collection_name,
-            query_vector=query_vector,
-            query_filter=qdrant_filter,
-            limit=limit,
-            with_payload=True
+            collection_name=self.collection_name, query_vector=query_vector, query_filter=qdrant_filter, limit=limit, with_payload=True
         )
-        
+
         print(f"üîç Found {len(results)} results from hybrid search")
-        
+
         # Store filter counts
-        filter_counts = {
-            "hybrid_search_results": len(results)
-        }
-        
+        filter_counts = {"hybrid_search_results": len(results)}
+
         # Convert results to LP format
         lp_results = []
         for result in results:
@@ -1659,38 +1607,34 @@
                 "hq_city": result.payload.get("filters", {}).get("hq_city", "Unknown"),
                 "aum": result.payload.get("metadata", {}).get("aum", "Unknown"),
                 "score": result.score,
-                "raw_payload": result.payload
+                "raw_payload": result.payload,
             }
             lp_data["filter_counts"] = filter_counts
             lp_results.append(lp_data)
-        
+
         print(f"‚úÖ Returned {len(lp_results)} results")
-        
-        return {
-            "results": lp_results,
-            "filters": filters,
-            "filter_counts": filter_counts
-        }
-    
+
+        return {"results": lp_results, "filters": filters, "filter_counts": filter_counts}
+
     def present_results(self, fund_profile: Dict[str, Any], user_prompt: str, lp_results: List[Dict[str, Any]]):
         """Present search results in an easy-to-evaluate format."""
-        
-        print(f"\n{'='*80}")
+
+        print(f"\n{'=' * 80}")
         print(f"üè¢ FUND: {fund_profile['firm_name']}")
         print(f"üìù QUERY: {user_prompt}")
         print(f"üåç Geographic Focus: {fund_profile['investor_profile']['geographic_focus']}")
         print(f"üè≠ Sector Focus: {fund_profile['investor_profile']['sector_focus']}")
         print(f"üéØ Target LP Types: {', '.join(fund_profile['investor_profile']['lp_target_types'])}")
         print(f"üí∞ Fund Size: {fund_profile['investor_profile']['fund_size']}")
-        print(f"{'='*80}")
-        
+        print(f"{'=' * 80}")
+
         if not lp_results:
             print("‚ùå No LPs found matching the criteria")
             return
-        
+
         print(f"\nüìä FOUND {len(lp_results)} LPs:")
-        print(f"{'='*80}")
-        
+        print(f"{'=' * 80}")
+
         for i, lp in enumerate(lp_results, 1):
             print(f"\n{i}. {lp['lp_name']} ({lp['lp_type']})")
             print(f"   üåç Geographic Focus: {lp['geographic_focus']}")
@@ -1698,20 +1642,20 @@
             print(f"   üìç Headquarters: {lp['headquarters']}")
             print(f"   üí∞ AUM: {lp['aum']}")
             print(f"   üìà Similarity Score: {lp['score']:.3f}")
-            
+
             # Show flag columns if true
-            filters = lp.get('raw_payload', {}).get('filters', {})
-            flag_columns = ['is_foundation', 'is_pension', 'is_endowment', 'is_insurance', 'is_charity']
+            filters = lp.get("raw_payload", {}).get("filters", {})
+            flag_columns = ["is_foundation", "is_pension", "is_endowment", "is_insurance", "is_charity"]
             active_flags = []
             for flag in flag_columns:
                 flag_value = filters.get(flag)
-                if flag_value is True or (isinstance(flag_value, str) and flag_value.lower() in ['true', '1', 'yes']):
+                if flag_value is True or (isinstance(flag_value, str) and flag_value.lower() in ["true", "1", "yes"]):
                     active_flags.append(flag)
-            
+
             if active_flags:
-                flags_display = ', '.join(active_flags)
+                flags_display = ", ".join(active_flags)
                 print(f"   üè∑Ô∏è  Flags: {flags_display}")
-            
+
             # Evaluation scores
             scores = self.evaluate_lp_match(fund_profile, lp, user_prompt)
             print(f"   üéØ Geographic Match: {scores['geographic_alignment']}/5")
@@ -1719,18 +1663,18 @@
             print(f"   üè¢ LP Type Match: {scores['lp_type_alignment']}/5")
             print(f"   üìù Query Relevance: {scores['query_relevance']}/5")
             print(f"   ‚≠ê Overall Score: {scores['overall_score']}/5")
-            print(f"   {'-'*60}")
-    
+            print(f"   {'-' * 60}")
+
     def get_performance_stats(self) -> Dict[str, Any]:
         """Get performance statistics."""
-        
+
         avg_time = self.stats["total_time"] / max(1, self.stats["total_calls"])
-        
+
         return {
             "total_calls": self.stats["total_calls"],
             "extraction_calls": self.stats["extraction_calls"],
             "mapping_calls": self.stats["mapping_calls"],
             "total_time": self.stats["total_time"],
             "average_time": avg_time,
-            "total_cost": self.stats["total_cost"]
+            "total_cost": self.stats["total_cost"],
         }

--- app/services/weekly_wrapup_service.py
+++ app/services/weekly_wrapup_service.py
@@ -25,7 +25,7 @@
     async def get_users_with_wrapup_enabled(self) -> List[str]:
         """
         Get all user IDs who have at least one radar with weekly_wrapup_email enabled.
-        
+
         Returns:
             List of user_id strings
         """
@@ -46,17 +46,15 @@
             )
             return [row["user_id"] for row in rows]
 
-    async def get_weekly_findings_for_user(
-        self, user_id: str, week_start: datetime, week_end: datetime
-    ) -> List[Dict[str, Any]]:
+    async def get_weekly_findings_for_user(self, user_id: str, week_start: datetime, week_end: datetime) -> List[Dict[str, Any]]:
         """
         Get Tier 1 and Tier 2 findings for a user from the past week.
-        
+
         Args:
             user_id: User ID
             week_start: Start of week (Friday 00:00 GMT)
             week_end: End of week (Thursday 23:59:59 GMT)
-            
+
         Returns:
             List of finding dictionaries with radar and company info
         """
@@ -106,6 +104,7 @@
                 finding_data = row["finding_data"]
                 if isinstance(finding_data, str):
                     import json
+
                     try:
                         finding_data = json.loads(finding_data)
                     except (json.JSONDecodeError, TypeError):
@@ -114,37 +113,38 @@
                 radar_config = row["radar_config"]
                 if isinstance(radar_config, str):
                     import json
+
                     try:
                         radar_config = json.loads(radar_config)
                     except (json.JSONDecodeError, TypeError):
                         radar_config = {}
 
-                findings.append({
-                    "finding_id": row["finding_id"],
-                    "radar_id": row["radar_id"],
-                    "priority_tier": row["priority_tier"],
-                    "relevance_score": row["relevance_score"],
-                    "category": row["category"],
-                    "finding_data": finding_data,
-                    "discovered_at": row["discovered_at"],
-                    "radar_type": row["radar_type"],
-                    "radar_category": row["radar_category"],
-                    "domain": radar_config.get("domain", ""),
-                    "company_name": radar_config.get("company_name", ""),
-                    "user_interests": row.get("user_interests"),
-                })
+                findings.append(
+                    {
+                        "finding_id": row["finding_id"],
+                        "radar_id": row["radar_id"],
+                        "priority_tier": row["priority_tier"],
+                        "relevance_score": row["relevance_score"],
+                        "category": row["category"],
+                        "finding_data": finding_data,
+                        "discovered_at": row["discovered_at"],
+                        "radar_type": row["radar_type"],
+                        "radar_category": row["radar_category"],
+                        "domain": radar_config.get("domain", ""),
+                        "company_name": radar_config.get("company_name", ""),
+                        "user_interests": row.get("user_interests"),
+                    }
+                )
 
             return findings
 
-    def group_findings_by_company(
-        self, findings: List[Dict[str, Any]]
-    ) -> Dict[str, List[Dict[str, Any]]]:
+    def group_findings_by_company(self, findings: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
         """
         Group findings by normalized domain/company.
-        
+
         Args:
             findings: List of finding dictionaries
-            
+
         Returns:
             Dictionary mapping normalized domain to list of findings
         """
@@ -157,22 +157,20 @@
 
         return dict(grouped)
 
-    def calculate_company_score(
-        self, company_findings: List[Dict[str, Any]]
-    ) -> float:
+    def calculate_company_score(self, company_findings: List[Dict[str, Any]]) -> float:
         """
         Calculate relevance score for a company based on its findings.
-        
+
         Scoring formula:
         - Tier 1 count * 3.0
         - Tier 2 count * 1.0
         - Average Tier 1 relevance * 2.0
         - Average Tier 2 relevance * 1.0
         - Recency bonus (newer findings get slight boost)
-        
+
         Args:
             company_findings: List of findings for this company
-            
+
         Returns:
             Company relevance score
         """
@@ -213,25 +211,17 @@
                 recency_bonus += max(0, (7 - days_ago) / 7 * 0.1)
 
         # Calculate final score
-        score = (
-            (tier1_count * 3.0) +
-            (tier2_count * 1.0) +
-            (avg_tier1_score * 2.0) +
-            (avg_tier2_score * 1.0) +
-            recency_bonus
-        )
+        score = (tier1_count * 3.0) + (tier2_count * 1.0) + (avg_tier1_score * 2.0) + (avg_tier2_score * 1.0) + recency_bonus
 
         return score
 
-    def rank_companies_by_relevance(
-        self, findings_by_company: Dict[str, List[Dict[str, Any]]]
-    ) -> List[Tuple[str, List[Dict[str, Any]], float]]:
+    def rank_companies_by_relevance(self, findings_by_company: Dict[str, List[Dict[str, Any]]]) -> List[Tuple[str, List[Dict[str, Any]], float]]:
         """
         Rank companies by relevance score.
-        
+
         Args:
             findings_by_company: Dictionary mapping domain to findings
-            
+
         Returns:
             List of tuples: (domain, findings, score) sorted by score descending
         """
@@ -244,22 +234,20 @@
         company_scores.sort(key=lambda x: x[2], reverse=True)
         return company_scores
 
-    def select_top_findings_per_company(
-        self, findings: List[Dict[str, Any]], limit: int = 5
-    ) -> List[Dict[str, Any]]:
+    def select_top_findings_per_company(self, findings: List[Dict[str, Any]], limit: int = 5) -> List[Dict[str, Any]]:
         """
         Select top N findings for a company, prioritizing Tier 1, then by relevance score.
         Removes duplicates based on finding_id AND content similarity to ensure diverse findings.
-        
+
         Args:
             findings: List of findings for this company
             limit: Maximum number of findings to return (default 5)
-            
+
         Returns:
             Top N unique findings sorted by tier (1 first) then relevance score
         """
         import json
-        
+
         # Remove duplicates by finding_id first (keep first occurrence)
         seen_ids = set()
         unique_by_id = []
@@ -271,7 +259,7 @@
             elif not finding_id:
                 # If no finding_id, include it (shouldn't happen, but handle gracefully)
                 unique_by_id.append(finding)
-        
+
         # Also remove duplicates by content similarity
         seen_content = set()
         unique_findings = []
@@ -283,7 +271,7 @@
                     finding_data = json.loads(finding_data)
                 except:
                     finding_data = {}
-            
+
             radar_type = finding.get("radar_type", "")
             if radar_type == "company_mentions":
                 content = finding_data.get("mention_content", finding_data.get("content", ""))
@@ -300,18 +288,18 @@
             else:
                 # Generic: use a hash of the data
                 content = json.dumps(finding_data, sort_keys=True)
-            
+
             # Normalize content for comparison (first 500 chars, lowercase, strip)
             content_normalized = content.lower().strip()[:500] if content else ""
-            
+
             # Include radar_id and radar_type in signature to avoid false matches
             radar_id = finding.get("radar_id", "")
             signature = f"{radar_id}|{radar_type}|{content_normalized}"
-            
+
             if signature not in seen_content:
                 seen_content.add(signature)
                 unique_findings.append(finding)
-        
+
         # Sort: Tier 1 first, then Tier 2, then by relevance score descending
         sorted_findings = sorted(
             unique_findings,
@@ -329,11 +317,11 @@
     ) -> List[Tuple[str, List[Dict[str, Any]], float]]:
         """
         Select top N companies by relevance score.
-        
+
         Args:
             ranked_companies: List of (domain, findings, score) tuples
             max_companies: Maximum number of companies to include (default 10)
-            
+
         Returns:
             Top N companies
         """
@@ -343,10 +331,10 @@
         """
         Calculate week boundaries for weekly wrap-up.
         Week runs from Friday 00:00 GMT to Thursday 23:59:59 GMT.
-        
+
         Args:
             reference_date: Date to calculate from (defaults to now in GMT)
-            
+
         Returns:
             Tuple of (week_start, week_end) as naive UTC datetimes
         """
@@ -378,14 +366,14 @@
     ) -> Dict[str, str]:
         """
         Generate weekly summaries for multiple companies in one LLM call.
-        
+
         Args:
             companies_with_findings: List of company dicts, each with:
                 - company_name: str
                 - domain: str
                 - findings: List[Dict] - top findings for this company
                 - user_interests: Optional[str] - user interests for this company
-        
+
         Returns:
             Dictionary mapping company_name to summary text
         """
@@ -401,23 +389,25 @@
         # Prepare companies data for prompt
         companies_data = []
         company_names = []
-        
+
         for company in companies_with_findings:
             company_name = company.get("company_name", company.get("domain", "Company"))
             findings = company.get("findings", [])
             user_interests = company.get("user_interests")
-            
+
             if not findings:
                 continue
-            
+
             # Format findings for this company
             findings_summary = format_findings_for_summary(findings)
-            
-            companies_data.append({
-                "company_name": company_name,
-                "user_interests": user_interests,
-                "findings_summary": findings_summary,
-            })
+
+            companies_data.append(
+                {
+                    "company_name": company_name,
+                    "user_interests": user_interests,
+                    "findings_summary": findings_summary,
+                }
+            )
             company_names.append(company_name)
 
         if not companies_data:
@@ -445,27 +435,27 @@
             # Expected format: **Company Name:** summary text (paragraph)
             summaries = {}
             paragraphs = response_text.split("\n\n")
-            
+
             for paragraph in paragraphs:
                 paragraph = paragraph.strip()
                 if not paragraph:
                     continue
-                
+
                 # Look for company name in bold format: **Company Name:**
                 if paragraph.startswith("**") and ":**" in paragraph:
                     # Extract company name
                     end_idx = paragraph.find(":**")
                     if end_idx > 2:
                         company_name = paragraph[2:end_idx].strip()
-                        summary_text = paragraph[end_idx + 3:].strip()
-                        
+                        summary_text = paragraph[end_idx + 3 :].strip()
+
                         # Match company name (case-insensitive, handle variations)
                         matched_name = None
                         for name in company_names:
                             if name.lower() == company_name.lower() or company_name.lower() in name.lower():
                                 matched_name = name
                                 break
-                        
+
                         if matched_name:
                             summaries[matched_name] = summary_text
                         else:
@@ -481,10 +471,7 @@
 
             # If we couldn't parse properly, try splitting by company count
             if len(summaries) != len(company_names):
-                logger.warning(
-                    f"Could not parse all summaries. Expected {len(company_names)}, got {len(summaries)}. "
-                    f"Trying alternative parsing."
-                )
+                logger.warning(f"Could not parse all summaries. Expected {len(company_names)}, got {len(summaries)}. Trying alternative parsing.")
                 # Fallback: split by paragraphs and assign in order
                 clean_paragraphs = [p.strip() for p in paragraphs if p.strip()]
                 if len(clean_paragraphs) == len(company_names):
@@ -508,4 +495,3 @@
 
 # Singleton instance
 weekly_wrapup_service = WeeklyWrapupService()
-

--- app/services/youtube_media_service.py
+++ app/services/youtube_media_service.py
@@ -89,6 +89,7 @@
 
     def _extract_video_id(self, url: str) -> Optional[str]:
         """Extract a YouTube video ID from a URL."""
+
         def _clean(candidate: Optional[str]) -> Optional[str]:
             if not candidate:
                 return None
@@ -164,10 +165,7 @@
         try:
             # Pass search_time_filter via extra_body (not model_kwargs)
             # This is the correct way to pass Perplexity-specific parameters
-            response = await chat.ainvoke(
-                prompt,
-                extra_body={"search_time_filter": "past_year"}
-            )
+            response = await chat.ainvoke(prompt, extra_body={"search_time_filter": "past_year"})
             content = response.content if hasattr(response, "content") else str(response)
             self.logger.info(
                 f"Perplexity YouTube discovery response len={len(content)} preview={content[:1000]!r} "
@@ -275,9 +273,7 @@
             vid = item.get("video_id")
             if vid and vid not in seen:
                 if len(vid) != 11:
-                    self.logger.info(
-                        f"Skipping non-standard video id from Perplexity video_id={vid} url={item.get('url')}"
-                    )
+                    self.logger.info(f"Skipping non-standard video id from Perplexity video_id={vid} url={item.get('url')}")
                     continue
                 seen.add(vid)
                 deduped.append(item)
@@ -316,9 +312,7 @@
             content = item.get("contentDetails", {})
             stats = item.get("statistics", {})
             published_at = snippet.get("publishedAt")
-            parsed_published = (
-                datetime.fromisoformat(published_at.replace("Z", "+00:00")) if published_at else None
-            )
+            parsed_published = datetime.fromisoformat(published_at.replace("Z", "+00:00")) if published_at else None
             if parsed_published and parsed_published.tzinfo:
                 parsed_published = parsed_published.replace(tzinfo=None)
             results.append(
@@ -379,9 +373,7 @@
         )
         if perplexity_hits:
             id_to_person = {hit["video_id"]: hit.get("person_name") for hit in perplexity_hits if hit.get("video_id")}
-            hydrated = await self._hydrate_video_ids(
-                [hit["video_id"] for hit in perplexity_hits if hit.get("video_id")]
-            )
+            hydrated = await self._hydrate_video_ids([hit["video_id"] for hit in perplexity_hits if hit.get("video_id")])
             for item in hydrated:
                 vid = item.get("video_id")
                 if vid in id_to_person:
@@ -481,11 +473,7 @@
                         "view_count": video.get("view_count"),
                         "like_count": video.get("like_count"),
                         # tags may be TEXT or JSONB depending on deployment; serialize list/dict to JSON string for safety
-                        "tags": (
-                            json.dumps(video.get("tags"))
-                            if isinstance(video.get("tags"), (list, dict))
-                            else video.get("tags")
-                        ),
+                        "tags": (json.dumps(video.get("tags")) if isinstance(video.get("tags"), (list, dict)) else video.get("tags")),
                         "language": video.get("language"),
                         "has_manual_captions": video.get("has_manual_captions"),
                         "url": video.get("url"),
@@ -731,9 +719,7 @@
                             "end_seconds": row.get("end_seconds"),
                         }
                     )
-                summary_data = await self._summarize_video_hierarchical(
-                    company_name, video_meta, chunks_for_summary
-                )
+                summary_data = await self._summarize_video_hierarchical(company_name, video_meta, chunks_for_summary)
                 if summary_data:
                     summaries.append(
                         {
@@ -837,9 +823,7 @@
                                 "end_seconds": row.get("end_seconds"),
                             }
                         )
-                    summary_data = await self._summarize_video_hierarchical(
-                        company_name, video_meta, chunks_for_summary
-                    )
+                    summary_data = await self._summarize_video_hierarchical(company_name, video_meta, chunks_for_summary)
                     if summary_data:
                         summaries.append(
                             {
@@ -920,9 +904,7 @@
     ) -> List[Dict[str, Any]]:
         """Search YouTube for recent videos."""
         try:
-            service = await asyncio.to_thread(
-                build, "youtube", "v3", developerKey=self.youtube_api_key
-            )
+            service = await asyncio.to_thread(build, "youtube", "v3", developerKey=self.youtube_api_key)
         except Exception as e:
             self.logger.error("Failed to build YouTube client", extra={"error": str(e)})
             return []
@@ -991,11 +973,7 @@
             "fund strategy",
             "deployment",
         ]
-        video_ids = [
-            item["id"]["videoId"]
-            for item in items
-            if item.get("id", {}).get("kind") == "youtube#video"
-        ]
+        video_ids = [item["id"]["videoId"] for item in items if item.get("id", {}).get("kind") == "youtube#video"]
         details = await self._fetch_video_details(service, video_ids)
 
         results = []
@@ -1004,11 +982,7 @@
             snippet = item.get("snippet", {})
             detail = details.get(vid, {})
             published_at = snippet.get("publishedAt")
-            parsed_published = (
-                datetime.fromisoformat(published_at.replace("Z", "+00:00"))
-                if published_at
-                else None
-            )
+            parsed_published = datetime.fromisoformat(published_at.replace("Z", "+00:00")) if published_at else None
             person_name = None
             if exec_names and len(exec_names) == 1:
                 person_name = exec_names[0]
@@ -1111,9 +1085,7 @@
             self.logger.info(f"Transcript fetch skipped (no Socialkit key) video_id={video_id}")
             return [], "missing"
 
-        self.logger.info(
-            f"Transcript fetch start (socialkit only) video_id={video_id} lang={lang}"
-        )
+        self.logger.info(f"Transcript fetch start (socialkit only) video_id={video_id} lang={lang}")
         try:
             segments = await self._fetch_transcript_via_socialkit(video_id)
             if segments:
@@ -1122,9 +1094,7 @@
             self.logger.info(f"Transcript empty after fetch (socialkit) video_id={video_id} lang={lang}")
             return [], "missing"
         except Exception as socialkit_error:
-            self.logger.warning(
-                f"Socialkit transcript fetch failed video_id={video_id} error={socialkit_error}"
-            )
+            self.logger.warning(f"Socialkit transcript fetch failed video_id={video_id} error={socialkit_error}")
             return [], "error"
 
     async def _fetch_transcript_via_socialkit(self, video_id: str) -> List[Dict[str, Any]]:
@@ -1142,9 +1112,7 @@
                 if resp.status != 200:
                     text = await resp.text()
                     if resp.status == 400:
-                        self.logger.info(
-                            f"Socialkit transcript not available (400) video_id={video_id} body={text[:200]}"
-                        )
+                        self.logger.info(f"Socialkit transcript not available (400) video_id={video_id} body={text[:200]}")
                         return []
                     raise RuntimeError(f"Socialkit status={resp.status} body={text[:200]}")
                 try:
@@ -1211,6 +1179,7 @@
             return []
 
         embedded: List[Dict[str, Any]] = []
+
         async def _embed_one(chunk: Dict[str, Any]) -> Optional[Dict[str, Any]]:
             text = chunk.get("text") or ""
             async with self.embed_semaphore:
@@ -1219,7 +1188,9 @@
                         lambda: openai.embeddings.create(
                             input=text,
                             model=self.embedding_model,
-                        ).data[0].embedding
+                        )
+                        .data[0]
+                        .embedding
                     )
                 except Exception as e:
                     self.logger.warning("Embedding failed", extra={"error": str(e)})
@@ -1246,9 +1217,7 @@
                     embedded.append(res)
         return embedded
 
-    async def _summarize_video_hierarchical(
-        self, company_name: str, video: Dict[str, Any], chunks: List[Dict[str, Any]]
-    ) -> Optional[Dict[str, Any]]:
+    async def _summarize_video_hierarchical(self, company_name: str, video: Dict[str, Any], chunks: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
         """
         Hierarchical summarization for long videos.
         Map: summarize batches of chunks (~1k words) into short bullets.
@@ -1301,7 +1270,7 @@
             start_seconds = batch[0].get("start_seconds")
             end_seconds = batch[-1].get("end_seconds")
             user_prompt_map = (
-                f"Video: {video.get('title','')}\n"
+                f"Video: {video.get('title', '')}\n"
                 f"Channel: {video.get('channel_title') or ''}\n"
                 f"Published: {video.get('published_at')}\n"
                 f"Company: {company_name}\n\n"
@@ -1310,9 +1279,7 @@
             )
             async with self.summary_semaphore:
                 try:
-                    response = await llm.ainvoke(
-                        [SystemMessage(content=system_prompt_map), HumanMessage(content=user_prompt_map)]
-                    )
+                    response = await llm.ainvoke([SystemMessage(content=system_prompt_map), HumanMessage(content=user_prompt_map)])
                     map_text = response.content
                 except Exception as e:
                     self.logger.warning("Map summary failed", extra={"error": str(e)})
@@ -1345,16 +1312,14 @@
         )
         reduce_input = "\n\n".join(map_summaries)[:6000]
         reduce_user_prompt = (
-            f"Video: {video.get('title','')}\n"
+            f"Video: {video.get('title', '')}\n"
             f"Channel: {video.get('channel_title') or ''}\n"
             f"Published: {video.get('published_at')}\n"
             f"Company: {company_name}\n\n"
             f"Window summaries:\n{reduce_input}"
         )
         try:
-            response = await llm.ainvoke(
-                [SystemMessage(content=system_prompt_reduce), HumanMessage(content=reduce_user_prompt)]
-            )
+            response = await llm.ainvoke([SystemMessage(content=system_prompt_reduce), HumanMessage(content=reduce_user_prompt)])
             reduce_summary = response.content
         except Exception as e:
             self.logger.warning("Reduce summary failed", extra={"error": str(e)})
@@ -1387,7 +1352,13 @@
         }
 
     async def _extract_insights_from_embeddings(
-        self, company_id: str, run_id: str, user_id: str, person_name: Optional[str] = None, limit_per_theme: int = 15, video_ids: Optional[List[str]] = None
+        self,
+        company_id: str,
+        run_id: str,
+        user_id: str,
+        person_name: Optional[str] = None,
+        limit_per_theme: int = 15,
+        video_ids: Optional[List[str]] = None,
     ) -> Optional[Dict[str, Any]]:
         """
         Query stored YouTube embeddings with canonical research themes and summarize hits.
@@ -1397,10 +1368,22 @@
             return None
 
         themes = [
-            ("senior_personnel", "[Senior Personnel]", "senior personnel featured or discussed (CIO, CEO, Managing Partner, Head of Investments, Head of IR, Portfolio Manager); leadership updates"),
-            ("market_updates", "[Market Updates]", "private markets market updates: macro, sector performance, private equity/credit/infrastructure trends"),
+            (
+                "senior_personnel",
+                "[Senior Personnel]",
+                "senior personnel featured or discussed (CIO, CEO, Managing Partner, Head of Investments, Head of IR, Portfolio Manager); leadership updates",
+            ),
+            (
+                "market_updates",
+                "[Market Updates]",
+                "private markets market updates: macro, sector performance, private equity/credit/infrastructure trends",
+            ),
             ("company_strategy", "[Company Strategy]", "investment philosophy, fund strategy, sector focus, risk management in private markets"),
-            ("investment_related", "[Investment-Related]", "portfolio companies, deals, exits, deployment updates, investment rationale in private markets"),
+            (
+                "investment_related",
+                "[Investment-Related]",
+                "portfolio companies, deals, exits, deployment updates, investment rationale in private markets",
+            ),
             ("market_outlook", "[Market Outlook]", "forward-looking 6-18 month private markets outlook: rates, sector views, geographic risks"),
             ("firm_overview", "[Firm Overview]", "firm overview: history, team, AUM, and approach in private markets"),
         ]
@@ -1420,7 +1403,7 @@
                 match = pattern.match(line)
                 if match:
                     dash = match.group("dash") or "- "
-                    remainder = line[match.end():].lstrip()
+                    remainder = line[match.end() :].lstrip()
                     cleaned_lines.append(f"{dash}{remainder}" if remainder else dash.strip())
                 else:
                     cleaned_lines.append(line)
@@ -1428,9 +1411,7 @@
 
         async def _process_theme(topic: str, bucket_label: str, theme_text: str):
             try:
-                query_emb = await asyncio.to_thread(
-                    lambda: openai.embeddings.create(input=theme_text, model=self.embedding_model).data[0].embedding
-                )
+                query_emb = await asyncio.to_thread(lambda: openai.embeddings.create(input=theme_text, model=self.embedding_model).data[0].embedding)
             except Exception as e:
                 self.logger.warning("Failed to embed insight query", extra={"topic": topic, "error": str(e)})
                 return None
@@ -1468,8 +1449,7 @@
                 start_seconds = hit.get("start_seconds")
                 ts_url = self._build_timestamped_url(f"https://www.youtube.com/watch?v={video_id}", start_seconds)
                 context_lines.append(
-                    f"[{idx+1}] video_id={video_id} time={self._format_timestamp(start_seconds)} "
-                    f"link={ts_url}\nText: {hit.get('text','')}"
+                    f"[{idx + 1}] video_id={video_id} time={self._format_timestamp(start_seconds)} link={ts_url}\nText: {hit.get('text', '')}"
                 )
             context = "\n\n".join(context_lines)[:6000]
 
@@ -1486,9 +1466,7 @@
             )
 
             try:
-                response = await llm.ainvoke(
-                    [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]
-                )
+                response = await llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)])
                 bullet_text = response.content
             except Exception as e:
                 self.logger.warning("Insight summarization failed", extra={"topic": topic, "error": str(e)})

--- app/tasks/__init__.py
+++ app/tasks/__init__.py
@@ -3,4 +3,3 @@
 
 This package contains all Celery task definitions.
 """
-

--- app/tasks/meeting_prep_tasks.py
+++ app/tasks/meeting_prep_tasks.py
@@ -19,28 +19,29 @@
 @celery_app.task(bind=True, name="process_daily_meeting_prep_workflow", max_retries=3)
 def process_daily_meeting_prep_workflow(self) -> None:
     try:
+
         async def run_processing():
             GlobalDB.reset_for_testing()
             await GlobalDB.initialize()
-            
+
             users = await get_all_users_with_integrations()
-            
+
             if not users:
                 return
-            
+
             tasks_dispatched = 0
             for user_info in users:
-                user_id = user_info['user_id']
-                integration_type = user_info['integration_type']
-                
+                user_id = user_info["user_id"]
+                integration_type = user_info["integration_type"]
+
                 try:
                     collect_user_integration_data.delay(user_id, integration_type)
                     tasks_dispatched += 1
                 except Exception:
                     continue
-        
+
         asyncio.run(run_processing())
-        
+
     except Exception as e:
         raise
 
@@ -53,19 +54,20 @@
 )
 def collect_user_integration_data(self, user_id: str, integration_type: str) -> None:
     try:
+
         async def run_collection():
             GlobalDB.reset_for_testing()
             await GlobalDB.initialize()
-            
+
             events = await collect_calendar_events(user_id, integration_type)
-            
+
             if events:
                 identify_important_events_task.delay(user_id, events, integration_type)
-        
+
         asyncio.run(run_collection())
-        
+
     except Exception as e:
-        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
+        raise self.retry(exc=e, countdown=60 * (2**self.request.retries))
 
 
 @celery_app.task(
@@ -73,7 +75,7 @@
     name="identify_important_events_task",
     max_retries=3,
     default_retry_delay=60,
-    rate_limit='20/m',
+    rate_limit="20/m",
 )
 def identify_important_events_task(
     self,
@@ -82,40 +84,41 @@
     integration_type: str,
 ) -> None:
     try:
+
         async def run_identification():
             GlobalDB.reset_for_testing()
             await GlobalDB.initialize()
-            
+
             important_events = await identify_important_events(events)
-            
+
             emails = await collect_emails(user_id, integration_type)
-            
+
             for event in important_events:
                 try:
                     related_emails = await find_related_emails_for_event(event, emails)
-                    
+
                     meeting_data = {
-                        'meeting_id': event.get('event_id', event.get('id', '')),
-                        'title': event.get('title', ''),
-                        'time': event.get('time', ''),
-                        'participants': event.get('participants', []),
-                        'importance_score': event.get('importance_score', 0),
-                        'importance_reason': event.get('importance_reason', ''),
-                        'duration_minutes': event.get('duration_minutes', 0),
-                        'related_emails': related_emails,
-                        'user_id': user_id,
-                        'integration_type': integration_type,
+                        "meeting_id": event.get("event_id", event.get("id", "")),
+                        "title": event.get("title", ""),
+                        "time": event.get("time", ""),
+                        "participants": event.get("participants", []),
+                        "importance_score": event.get("importance_score", 0),
+                        "importance_reason": event.get("importance_reason", ""),
+                        "duration_minutes": event.get("duration_minutes", 0),
+                        "related_emails": related_emails,
+                        "user_id": user_id,
+                        "integration_type": integration_type,
                     }
-                    
+
                     research_meeting_participants.delay(user_id, meeting_data, integration_type)
-                    
+
                 except Exception:
                     continue
-        
+
         asyncio.run(run_identification())
-        
+
     except Exception as e:
-        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
+        raise self.retry(exc=e, countdown=60 * (2**self.request.retries))
 
 
 @celery_app.task(
@@ -123,7 +126,7 @@
     name="categorize_user_meetings",
     max_retries=3,
     default_retry_delay=60,
-    rate_limit='20/m',
+    rate_limit="20/m",
 )
 def categorize_user_meetings(
     self,
@@ -140,7 +143,7 @@
     name="research_meeting_participants",
     max_retries=3,
     default_retry_delay=60,
-    rate_limit='5/m',
+    rate_limit="5/m",
 )
 def research_meeting_participants(
     self,
@@ -148,21 +151,22 @@
     meeting_data: Dict[str, Any],
     integration_type: str,
 ) -> None:
-    meeting_id = meeting_data.get('meeting_id', 'unknown')
-    
+    meeting_id = meeting_data.get("meeting_id", "unknown")
+
     try:
+
         async def run_research():
             GlobalDB.reset_for_testing()
             await GlobalDB.initialize()
-            
+
             intelligence_data = await collect_intelligence_data(meeting_data, user_id)
-            
+
             generate_meeting_report.delay(user_id, intelligence_data, meeting_data, integration_type)
-        
+
         asyncio.run(run_research())
-        
+
     except Exception as e:
-        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
+        raise self.retry(exc=e, countdown=60 * (2**self.request.retries))
 
 
 @celery_app.task(
@@ -170,7 +174,7 @@
     name="generate_meeting_report",
     max_retries=3,
     default_retry_delay=60,
-    rate_limit='20/m',
+    rate_limit="20/m",
 )
 def generate_meeting_report(
     self,
@@ -179,38 +183,35 @@
     meeting_data: Dict[str, Any],
     integration_type: str,
 ) -> None:
-    meeting_id = meeting_data.get('meeting_id', 'unknown')
-    
+    meeting_id = meeting_data.get("meeting_id", "unknown")
+
     try:
+
         async def run_generation():
             GlobalDB.reset_for_testing()
             await GlobalDB.initialize()
-            
-            report_content = await prepare_meeting_report(
-                user_id=user_id,
-                meeting_id=meeting_id,
-                integration_type=integration_type
-            )
-            
+
+            report_content = await prepare_meeting_report(user_id=user_id, meeting_id=meeting_id, integration_type=integration_type)
+
             if not report_content:
                 return
-            
+
             report_data = {
-                'meeting_id': meeting_id,
-                'meeting_title': meeting_data.get('title'),
-                'meeting_time': meeting_data.get('time'),
-                'participants': meeting_data.get('participants', []),
-                'related_emails': meeting_data.get('related_emails', []),
-                'research_data': {},
-                'report_content': report_content,
+                "meeting_id": meeting_id,
+                "meeting_title": meeting_data.get("title"),
+                "meeting_time": meeting_data.get("time"),
+                "participants": meeting_data.get("participants", []),
+                "related_emails": meeting_data.get("related_emails", []),
+                "research_data": {},
+                "report_content": report_content,
             }
-            
+
             save_meeting_report.delay(user_id, report_data, integration_type)
-        
+
         asyncio.run(run_generation())
-        
+
     except Exception as e:
-        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
+        raise self.retry(exc=e, countdown=60 * (2**self.request.retries))
 
 
 @celery_app.task(
@@ -225,37 +226,38 @@
     report_data: Dict[str, Any],
     integration_type: str,
 ) -> None:
-    meeting_id = report_data.get('meeting_id', 'unknown')
-    
+    meeting_id = report_data.get("meeting_id", "unknown")
+
     try:
+
         async def run_save():
             GlobalDB.reset_for_testing()
             await GlobalDB.initialize()
-            
-            meeting_time_str = report_data.get('meeting_time')
+
+            meeting_time_str = report_data.get("meeting_time")
             if isinstance(meeting_time_str, str):
                 try:
-                    meeting_time = datetime.fromisoformat(meeting_time_str.replace('Z', '+00:00'))
+                    meeting_time = datetime.fromisoformat(meeting_time_str.replace("Z", "+00:00"))
                 except Exception:
                     meeting_time = datetime.now(timezone.utc)
             elif isinstance(meeting_time_str, datetime):
                 meeting_time = meeting_time_str
             else:
                 meeting_time = datetime.now(timezone.utc)
-            
+
             await save_meeting_report_to_db(
                 user_id=user_id,
-                meeting_id=report_data.get('meeting_id'),
-                meeting_title=report_data.get('meeting_title'),
+                meeting_id=report_data.get("meeting_id"),
+                meeting_title=report_data.get("meeting_title"),
                 meeting_time=meeting_time,
                 integration_type=integration_type,
-                participants=report_data.get('participants'),
-                related_emails=report_data.get('related_emails'),
-                research_data=report_data.get('research_data'),
-                report_content=report_data.get('report_content', ''),
+                participants=report_data.get("participants"),
+                related_emails=report_data.get("related_emails"),
+                research_data=report_data.get("research_data"),
+                report_content=report_data.get("report_content", ""),
             )
-        
+
         asyncio.run(run_save())
-        
+
     except Exception as e:
-        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
+        raise self.retry(exc=e, countdown=60 * (2**self.request.retries))

--- app/tasks/tamradar_tasks.py
+++ app/tasks/tamradar_tasks.py
@@ -14,18 +14,14 @@
 
 
 @celery_app.task(bind=True, max_retries=3, default_retry_delay=60)
-def process_webhook_event_task(
-    self,
-    payload: Dict[str, Any],
-    webhook_event_id: Optional[int] = None
-) -> None:
+def process_webhook_event_task(self, payload: Dict[str, Any], webhook_event_id: Optional[int] = None) -> None:
     """
     Process webhook event with automatic retry on failure.
-    
+
     Args:
         payload: Webhook payload from TAMradar
         webhook_event_id: Optional pre-stored webhook event ID (to avoid double storage)
-    
+
     This task wraps the async process_webhook_event method and handles retries
     with exponential backoff.
     """
@@ -37,9 +33,9 @@
                 "event_id": payload.get("event_id"),
                 "event_type": payload.get("event_type"),
                 "retry_count": self.request.retries,
-            }
+            },
         )
-        
+
         # Celery tasks are synchronous, but our service methods are async
         # Run the async function in a new event loop
         # Note: Celery workers run in separate processes (ForkPoolWorker),
@@ -49,31 +45,28 @@
             # Workers run in separate processes, so the parent's DB instance
             # is not valid in the worker's event loop
             from app.utils.global_db import GlobalDB
-            
+
             # Reset any existing instance (from parent process or previous task)
             GlobalDB.reset_for_testing()
-            
+
             # Initialize fresh database connection in this worker's event loop
             db = await GlobalDB.initialize()
             logger.debug(f"Database initialized in worker event loop: {id(db)}")
-            
+
             # Now run the async function
-            await tamradar_webhook_service.process_webhook_event(
-                payload,
-                webhook_event_id
-            )
-        
+            await tamradar_webhook_service.process_webhook_event(payload, webhook_event_id)
+
         # Run in new event loop
         asyncio.run(run_processing())
-        
+
         logger.info(
             f"Successfully processed webhook event",
             extra={
                 "webhook_event_id": webhook_event_id,
                 "event_id": payload.get("event_id"),
-            }
+            },
         )
-        
+
     except Exception as exc:
         logger.error(
             f"Error processing webhook event task: {exc}",
@@ -83,12 +76,12 @@
                 "retry_count": self.request.retries,
                 "max_retries": self.max_retries,
             },
-            exc_info=True
+            exc_info=True,
         )
-        
+
         # Retry with exponential backoff
         if self.request.retries < self.max_retries:
-            retry_delay = 2 ** self.request.retries * self.default_retry_delay
+            retry_delay = 2**self.request.retries * self.default_retry_delay
             logger.info(
                 f"Retrying webhook event task",
                 extra={
@@ -96,7 +89,7 @@
                     "event_id": payload.get("event_id"),
                     "retry_count": self.request.retries + 1,
                     "retry_delay": retry_delay,
-                }
+                },
             )
             raise self.retry(exc=exc, countdown=retry_delay)
         else:
@@ -106,7 +99,7 @@
                     "webhook_event_id": webhook_event_id,
                     "event_id": payload.get("event_id"),
                     "max_retries": self.max_retries,
-                }
+                },
             )
             # Re-raise to mark task as failed
             raise
@@ -123,11 +116,11 @@
 ) -> None:
     """
     Score finding relevance for all users using LLM-based tier classification.
-    
+
     This is a separate Celery task to ensure it completes even if the webhook
     processing task finishes first. The event loop issue is avoided by making
     this a proper Celery task instead of using asyncio.create_task().
-    
+
     Args:
         finding_id: The finding ID
         radar_id: The radar ID
@@ -143,26 +136,26 @@
                 "radar_id": radar_id,
                 "user_count": len(user_ids),
                 "retry_count": self.request.retries,
-            }
+            },
         )
-        
+
         # Import here to avoid circular imports
         from app.services.relevance_scoring_service import score_finding_relevance_async
-        
+
         # Celery workers run in separate processes, ensure DB is initialized
         async def run_scoring():
             # Always reset and reinitialize in Celery workers
             # Workers run in separate processes, so the parent's DB instance
             # is not valid in the worker's event loop
             from app.utils.global_db import GlobalDB
-            
+
             # Reset any existing instance (from parent process or previous task)
             GlobalDB.reset_for_testing()
-            
+
             # Initialize fresh database connection in this worker's event loop
             db = await GlobalDB.initialize()
             logger.debug(f"Database initialized for scoring in worker event loop: {id(db)}")
-            
+
             # Run the async relevance scoring function
             await score_finding_relevance_async(
                 finding_id=finding_id,
@@ -171,18 +164,18 @@
                 radar_type=radar_type,
                 user_ids=user_ids,
             )
-        
+
         # Run in new event loop
         asyncio.run(run_scoring())
-        
+
         logger.info(
             f"Successfully completed relevance scoring task",
             extra={
                 "finding_id": finding_id,
                 "radar_id": radar_id,
-            }
+            },
         )
-        
+
     except Exception as exc:
         logger.error(
             f"Error in relevance scoring task: {exc}",
@@ -192,12 +185,12 @@
                 "retry_count": self.request.retries,
                 "max_retries": self.max_retries,
             },
-            exc_info=True
+            exc_info=True,
         )
-        
+
         # Retry with exponential backoff (fewer retries than webhook processing)
         if self.request.retries < self.max_retries:
-            retry_delay = 2 ** self.request.retries * self.default_retry_delay
+            retry_delay = 2**self.request.retries * self.default_retry_delay
             logger.info(
                 f"Retrying relevance scoring task",
                 extra={
@@ -205,7 +198,7 @@
                     "radar_id": radar_id,
                     "retry_count": self.request.retries + 1,
                     "retry_delay": retry_delay,
-                }
+                },
             )
             raise self.retry(exc=exc, countdown=retry_delay)
         else:
@@ -215,7 +208,7 @@
                     "finding_id": finding_id,
                     "radar_id": radar_id,
                     "max_retries": self.max_retries,
-                }
+                },
             )
             # Re-raise to mark task as failed
             raise
@@ -234,10 +227,10 @@
 ) -> None:
     """
     Verify Tier 1 finding and generate headline for email alert.
-    
+
     This task performs a second verification using OpenAI GPT-4o-mini to ensure
     the finding definitively meets Tier 1 criteria before sending email alerts.
-    
+
     Args:
         finding_id: The finding ID
         radar_id: The radar ID
@@ -257,33 +250,30 @@
                 "company_name": company_name,
                 "category": category,
                 "retry_count": self.request.retries,
-            }
+            },
         )
-        
+
         # Import here to avoid circular imports
         from app.services.tier1_verification_service import verify_tier1_finding
         from app.config.relevance_scoring_config import TIER1_VERIFICATION_CONFIDENCE_THRESHOLD
-        
+
         # Celery workers run in separate processes, ensure DB is initialized
         async def run_verification():
             # Always reset and reinitialize in Celery workers
             from app.utils.global_db import GlobalDB
-            
+
             # Reset any existing instance
             GlobalDB.reset_for_testing()
-            
+
             # Initialize fresh database connection
             db = await GlobalDB.initialize()
             logger.debug(f"Database initialized for verification in worker event loop: {id(db)}")
-            
+
             # Run verification
             verification_result = await verify_tier1_finding(
-                finding_text=finding_text,
-                company_name=company_name,
-                category=category,
-                initial_confidence=initial_confidence
+                finding_text=finding_text, company_name=company_name, category=category, initial_confidence=initial_confidence
             )
-            
+
             if not verification_result:
                 logger.warning(
                     f"Tier 1 verification returned None (API failure or parsing error)",
@@ -291,15 +281,15 @@
                         "finding_id": finding_id,
                         "user_id": user_id,
                         "radar_id": radar_id,
-                    }
+                    },
                 )
                 return
-            
+
             verified = verification_result.get("verified", False)
             confidence = verification_result.get("confidence", 0.0)
             headline = verification_result.get("headline")
             reasoning = verification_result.get("reasoning", "")
-            
+
             logger.info(
                 f"Tier 1 verification complete",
                 extra={
@@ -309,9 +299,9 @@
                     "verified": verified,
                     "confidence": confidence,
                     "reasoning": reasoning,
-                }
+                },
             )
-            
+
             # If verified and confidence meets threshold, trigger email sending
             if verified and confidence >= TIER1_VERIFICATION_CONFIDENCE_THRESHOLD:
                 if headline:
@@ -322,7 +312,7 @@
                             "user_id": user_id,
                             "radar_id": radar_id,
                             "headline": headline,
-                        }
+                        },
                     )
                     # Trigger email sending task
                     send_tier1_email_alert_task.delay(
@@ -340,7 +330,7 @@
                             "finding_id": finding_id,
                             "user_id": user_id,
                             "radar_id": radar_id,
-                        }
+                        },
                     )
             else:
                 logger.debug(
@@ -352,21 +342,21 @@
                         "verified": verified,
                         "confidence": confidence,
                         "threshold": TIER1_VERIFICATION_CONFIDENCE_THRESHOLD,
-                    }
+                    },
                 )
-        
+
         # Run in new event loop
         asyncio.run(run_verification())
-        
+
         logger.info(
             f"Successfully completed Tier 1 verification task",
             extra={
                 "finding_id": finding_id,
                 "user_id": user_id,
                 "radar_id": radar_id,
-            }
+            },
         )
-        
+
     except Exception as exc:
         logger.error(
             f"Error in Tier 1 verification task: {exc}",
@@ -377,12 +367,12 @@
                 "retry_count": self.request.retries,
                 "max_retries": self.max_retries,
             },
-            exc_info=True
+            exc_info=True,
         )
-        
+
         # Retry with exponential backoff
         if self.request.retries < self.max_retries:
-            retry_delay = 2 ** self.request.retries * self.default_retry_delay
+            retry_delay = 2**self.request.retries * self.default_retry_delay
             logger.info(
                 f"Retrying Tier 1 verification task",
                 extra={
@@ -391,7 +381,7 @@
                     "radar_id": radar_id,
                     "retry_count": self.request.retries + 1,
                     "retry_delay": retry_delay,
-                }
+                },
             )
             raise self.retry(exc=exc, countdown=retry_delay)
         else:
@@ -402,7 +392,7 @@
                     "user_id": user_id,
                     "radar_id": radar_id,
                     "max_retries": self.max_retries,
-                }
+                },
             )
             # Don't re-raise - verification failure shouldn't block finding classification
             # Just log the error and continue
@@ -421,7 +411,7 @@
 ) -> None:
     """
     Send Tier 1 email alert to user.
-    
+
     Args:
         user_id: The user ID to send alert to
         finding_id: The finding ID
@@ -439,41 +429,38 @@
                 "radar_id": radar_id,
                 "company_name": company_name,
                 "retry_count": self.request.retries,
-            }
+            },
         )
-        
+
         # Import here to avoid circular imports
         from app.services.email_service import send_tier1_radar_alert_email, _extract_finding_preview
-        
+
         # Celery workers run in separate processes, ensure DB is initialized
         async def run_email_sending():
             # Always reset and reinitialize in Celery workers
             from app.utils.global_db import GlobalDB
-            
+
             # Reset any existing instance
             GlobalDB.reset_for_testing()
-            
+
             # Initialize fresh database connection
             db = await GlobalDB.initialize()
             logger.debug(f"Database initialized for email sending in worker event loop: {id(db)}")
-            
+
             # Query database for user email, finding details, and verify preference
             async with db.pool.acquire() as conn:
                 # Get user email and first name
-                user_row = await conn.fetchrow(
-                    "SELECT email, first_name FROM users WHERE user_id = $1",
-                    user_id
-                )
+                user_row = await conn.fetchrow("SELECT email, first_name FROM users WHERE user_id = $1", user_id)
                 if not user_row:
                     logger.error(f"User {user_id} not found, cannot send email")
                     return
-                
+
                 user_email = user_row["email"]
                 user_first_name = user_row.get("first_name")
                 if not user_email:
                     logger.error(f"User {user_id} has no email address, cannot send email")
                     return
-                
+
                 # Verify preference is still enabled
                 preference_row = await conn.fetchrow(
                     """
@@ -482,15 +469,13 @@
                     WHERE user_id = $1 AND radar_id = $2
                     """,
                     user_id,
-                    radar_id
+                    radar_id,
                 )
-                
+
                 if not preference_row or not preference_row.get("tier1_email_alerts"):
-                    logger.info(
-                        f"User {user_id} has disabled Tier 1 alerts for radar {radar_id}, skipping email"
-                    )
+                    logger.info(f"User {user_id} has disabled Tier 1 alerts for radar {radar_id}, skipping email")
                     return
-                
+
                 # Get finding details
                 finding_row = await conn.fetchrow(
                     """
@@ -500,27 +485,28 @@
                     WHERE uf.user_id = $1 AND uf.finding_id = $2
                     """,
                     user_id,
-                    finding_id
+                    finding_id,
                 )
-                
+
                 if not finding_row:
                     logger.error(f"Finding {finding_id} not found for user {user_id}, cannot send email")
                     return
-                
+
                 finding_data = finding_row["finding_data"]
                 radar_type = finding_row["radar_type"]
-                
+
                 # Parse finding_data if it's a JSON string
                 import json
+
                 if isinstance(finding_data, str):
                     try:
                         finding_data = json.loads(finding_data)
                     except (json.JSONDecodeError, TypeError):
                         logger.warning(f"Failed to parse finding_data JSON for finding {finding_id}, using as-is")
-                
+
                 # Extract finding preview
                 finding_preview = _extract_finding_preview(finding_data, radar_type)
-                
+
                 # Send email
                 await send_tier1_radar_alert_email(
                     to_email=user_email,
@@ -532,7 +518,7 @@
                     radar_id=radar_id,
                     user_first_name=user_first_name,
                 )
-                
+
                 # Update notified flag
                 await conn.execute(
                     """
@@ -543,7 +529,7 @@
                     user_id,
                     finding_id,
                 )
-                
+
                 logger.info(
                     f"Successfully sent Tier 1 email alert",
                     extra={
@@ -551,12 +537,12 @@
                         "user_id": user_id,
                         "radar_id": radar_id,
                         "user_email": user_email,
-                    }
+                    },
                 )
-        
+
         # Run in new event loop
         asyncio.run(run_email_sending())
-        
+
     except Exception as exc:
         logger.error(
             f"Error in Tier 1 email alert task: {exc}",
@@ -567,12 +553,12 @@
                 "retry_count": self.request.retries,
                 "max_retries": self.max_retries,
             },
-            exc_info=True
+            exc_info=True,
         )
-        
+
         # Retry with exponential backoff (more retries for email sending)
         if self.request.retries < self.max_retries:
-            retry_delay = 2 ** self.request.retries * self.default_retry_delay
+            retry_delay = 2**self.request.retries * self.default_retry_delay
             logger.info(
                 f"Retrying Tier 1 email alert task",
                 extra={
@@ -581,7 +567,7 @@
                     "radar_id": radar_id,
                     "retry_count": self.request.retries + 1,
                     "retry_delay": retry_delay,
-                }
+                },
             )
             raise self.retry(exc=exc, countdown=retry_delay)
         else:
@@ -592,9 +578,7 @@
                     "user_id": user_id,
                     "radar_id": radar_id,
                     "max_retries": self.max_retries,
-                }
+                },
             )
             # Re-raise to mark task as failed
             raise
-
-

--- app/tasks/weekly_wrapup_tasks.py
+++ app/tasks/weekly_wrapup_tasks.py
@@ -21,7 +21,7 @@
 def send_weekly_wrapup_emails_task(self) -> None:
     """
     Main task to send weekly wrap-up emails to all users who have opted in.
-    
+
     This task:
     1. Gets all users with weekly_wrapup_email enabled
     2. For each user, collects Tier 1 and Tier 2 findings from the past week
@@ -31,24 +31,21 @@
     6. Selects top findings per company (max 5)
     7. Generates LLM summaries for each company
     8. Sends email to user
-    
+
     Scheduled to run every Friday at 00:00 GMT via Celery Beat.
     Week boundaries: Friday 00:00 GMT to Thursday 23:59:59 GMT.
     """
     logger.info("Starting weekly wrap-up email task")
-    
+
     try:
         # Calculate week boundaries (Friday 00:00 to Thursday 23:59:59 GMT)
         week_start, week_end = weekly_wrapup_service.get_week_boundaries()
-        
-        logger.info(
-            f"Week boundaries: {week_start} to {week_end}",
-            extra={"week_start": week_start.isoformat(), "week_end": week_end.isoformat()}
-        )
-        
+
+        logger.info(f"Week boundaries: {week_start} to {week_end}", extra={"week_start": week_start.isoformat(), "week_end": week_end.isoformat()})
+
         # Get all users with wrap-up enabled and process them
         import asyncio
-        
+
         # Celery tasks are synchronous, but our service methods are async
         # Run the async function in a new event loop
         async def run_processing():
@@ -56,51 +53,47 @@
             # Workers run in separate processes, so the parent's DB instance
             # is not valid in the worker's event loop
             from app.utils.global_db import GlobalDB
-            
+
             # Reset any existing instance (from parent process or previous task)
             GlobalDB.reset_for_testing()
-            
+
             # Initialize fresh database connection in this worker's event loop
             db = await GlobalDB.initialize()
             logger.debug(f"Database initialized in worker event loop: {id(db)}")
-            
+
             # Get all users with wrap-up enabled
             user_ids = await weekly_wrapup_service.get_users_with_wrapup_enabled()
-            
+
             logger.info(f"Found {len(user_ids)} users with weekly wrap-up enabled")
-            
+
             if not user_ids:
                 logger.info("No users with weekly wrap-up enabled, skipping")
                 return
-            
+
             # Process each user
             emails_sent = 0
             errors = 0
-            
+
             for user_id in user_ids:
                 try:
                     await _process_user_wrapup(user_id, week_start, week_end)
                     emails_sent += 1
                 except Exception as e:
                     errors += 1
-                    logger.error(
-                        f"Failed to process weekly wrap-up for user {user_id}: {e}",
-                        exc_info=True,
-                        extra={"user_id": user_id}
-                    )
-            
+                    logger.error(f"Failed to process weekly wrap-up for user {user_id}: {e}", exc_info=True, extra={"user_id": user_id})
+
             logger.info(
                 f"Weekly wrap-up processing completed: {emails_sent} emails sent, {errors} errors",
-                extra={"emails_sent": emails_sent, "errors": errors}
+                extra={"emails_sent": emails_sent, "errors": errors},
             )
-            
+
             return emails_sent, errors
-        
+
         # Run in new event loop
         asyncio.run(run_processing())
-        
+
         logger.info("Weekly wrap-up task completed successfully")
-        
+
     except Exception as e:
         logger.error(f"Weekly wrap-up task failed: {e}", exc_info=True)
         raise
@@ -113,7 +106,7 @@
 ) -> None:
     """
     Process weekly wrap-up for a single user.
-    
+
     Args:
         user_id: User ID
         week_start: Start of week
@@ -124,135 +117,120 @@
     if not db:
         logger.warning(f"Database not available for user {user_id}")
         return
-    
+
     async with db.pool.acquire() as conn:
-        user_row = await conn.fetchrow(
-            "SELECT email, first_name, last_name FROM users WHERE user_id = $1",
-            user_id
-        )
-        
+        user_row = await conn.fetchrow("SELECT email, first_name, last_name FROM users WHERE user_id = $1", user_id)
+
         if not user_row or not user_row.get("email"):
             logger.warning(f"User {user_id} has no email address, skipping")
             return
-        
+
         user_email = user_row["email"]
         first_name = user_row.get("first_name")
         last_name = user_row.get("last_name")
         user_name = None
         if first_name or last_name:
             user_name = " ".join(filter(None, [first_name, last_name]))
-    
+
     # Get findings for this user
-    findings = await weekly_wrapup_service.get_weekly_findings_for_user(
-        user_id, week_start, week_end
-    )
-    
+    findings = await weekly_wrapup_service.get_weekly_findings_for_user(user_id, week_start, week_end)
+
     if not findings:
         logger.info(f"No findings for user {user_id} in this week, skipping email")
         return
-    
+
     # Group findings by company
     findings_by_company = weekly_wrapup_service.group_findings_by_company(findings)
-    
+
     if not findings_by_company:
         logger.info(f"No companies with findings for user {user_id}, skipping email")
         return
-    
+
     # Only rank companies if there are more than 5 companies (optimization)
     company_count = len(findings_by_company)
     if company_count > 5:
         # Rank companies by relevance
-        ranked_companies = weekly_wrapup_service.rank_companies_by_relevance(
-            findings_by_company
-        )
-        
+        ranked_companies = weekly_wrapup_service.rank_companies_by_relevance(findings_by_company)
+
         # Select top companies (max 5 for single LLM call)
-        top_companies = weekly_wrapup_service.select_top_companies(
-            ranked_companies, max_companies=5
-        )
+        top_companies = weekly_wrapup_service.select_top_companies(ranked_companies, max_companies=5)
     else:
         # If 5 or fewer companies, use all of them (no ranking needed)
         top_companies = [
             (domain, company_findings, 0.0)  # Score not needed when <= 5 companies
             for domain, company_findings in findings_by_company.items()
         ]
-    
+
     if not top_companies:
         logger.info(f"No companies selected for user {user_id}, skipping email")
         return
-    
+
     # Prepare companies data for batch LLM call
     companies_with_findings = []
-    
+
     for domain, company_findings, score in top_companies:
         # Select top 5 findings for this company
-        top_findings = weekly_wrapup_service.select_top_findings_per_company(
-            company_findings, limit=5
-        )
-        
+        top_findings = weekly_wrapup_service.select_top_findings_per_company(company_findings, limit=5)
+
         if not top_findings:
             continue
-        
+
         # Get company name and user interests
         company_name = top_findings[0].get("company_name", domain)
         user_interests = top_findings[0].get("user_interests")
-        
-        companies_with_findings.append({
-            "company_name": company_name,
-            "domain": domain,
-            "findings": top_findings,
-            "user_interests": user_interests,
-        })
-    
+
+        companies_with_findings.append(
+            {
+                "company_name": company_name,
+                "domain": domain,
+                "findings": top_findings,
+                "user_interests": user_interests,
+            }
+        )
+
     if not companies_with_findings:
         logger.info(f"No companies with findings for user {user_id}, skipping email")
         return
-    
+
     # Generate all summaries in one LLM call
-    summaries_dict = await weekly_wrapup_service.generate_companies_summaries_with_llm(
-        companies_with_findings
-    )
-    
+    summaries_dict = await weekly_wrapup_service.generate_companies_summaries_with_llm(companies_with_findings)
+
     if not summaries_dict:
-        logger.info(
-            f"No summaries generated for user {user_id}, skipping email"
-        )
+        logger.info(f"No summaries generated for user {user_id}, skipping email")
         return
-    
+
     # Build companies_with_summaries list
     companies_with_summaries = []
     for company_data in companies_with_findings:
         company_name = company_data["company_name"]
         domain = company_data["domain"]
         top_findings = company_data["findings"]
-        
+
         # Get summary from LLM response
         summary = summaries_dict.get(company_name)
         if not summary:
-            logger.warning(
-                f"Summary not found for {company_name} in LLM response, skipping company"
-            )
+            logger.warning(f"Summary not found for {company_name} in LLM response, skipping company")
             continue
-        
+
         # Count tiers
         tier1_count = sum(1 for f in top_findings if f.get("priority_tier") == 1)
         tier2_count = sum(1 for f in top_findings if f.get("priority_tier") == 2)
-        
-        companies_with_summaries.append({
-            "company_name": company_name,
-            "domain": domain,
-            "summary": summary,
-            "tier1_count": tier1_count,
-            "tier2_count": tier2_count,
-            "findings": top_findings,
-        })
-    
-    if not companies_with_summaries:
-        logger.info(
-            f"No companies with summaries generated for user {user_id}, skipping email"
+
+        companies_with_summaries.append(
+            {
+                "company_name": company_name,
+                "domain": domain,
+                "summary": summary,
+                "tier1_count": tier1_count,
+                "tier2_count": tier2_count,
+                "findings": top_findings,
+            }
         )
+
+    if not companies_with_summaries:
+        logger.info(f"No companies with summaries generated for user {user_id}, skipping email")
         return
-    
+
     # Send email
     await send_weekly_wrapup_email(
         to_email=user_email,
@@ -260,10 +238,6 @@
         week_start=week_start,
         week_end=week_end,
         companies_with_summaries=companies_with_summaries,
-    )
-    
-    logger.info(
-        f"Sent weekly wrap-up email to user {user_id} ({user_email}) "
-        f"with {len(companies_with_summaries)} companies"
     )
 
+    logger.info(f"Sent weekly wrap-up email to user {user_id} ({user_email}) with {len(companies_with_summaries)} companies")

--- app/tools/json_to_excel.py
+++ app/tools/json_to_excel.py
@@ -8,42 +8,45 @@
 import openpyxl
 from io import BytesIO
 
+
 def flatten_dict(prefix: str, d: dict) -> dict:
     flat = {}
     for k, v in d.items():
         key = f"{prefix}.{k}" if prefix else k
-        if isinstance(v, dict) and 'value' in v and len(v) == 2 and 'citation' in v:
+        if isinstance(v, dict) and "value" in v and len(v) == 2 and "citation" in v:
             # Typical value/citation pair, just take value
-            flat[key] = v['value']
+            flat[key] = v["value"]
         elif isinstance(v, dict):
             # Nested dict, flatten recursively
             for subk, subv in flatten_dict(key, v).items():
                 flat[subk] = subv
         elif isinstance(v, list):
             # Join lists as string
-            flat[key] = '\n'.join([str(i) for i in v])
+            flat[key] = "\n".join([str(i) for i in v])
         else:
             flat[key] = v
     return flat
 
+
 def flatten_person_for_multiindex(person: dict) -> Dict[Tuple[str, str], Any]:
     import json as _json
+
     result = {}
-    fused = person.get('fused_person_profile', {})
+    fused = person.get("fused_person_profile", {})
     allowed_sections = [
-        'contact_identity',
-        'professional_background',
-        'decision_making_influence',
-        'strategic_notes_personalization',
+        "contact_identity",
+        "professional_background",
+        "decision_making_influence",
+        "strategic_notes_personalization",
     ]
     for section in allowed_sections:
         section_data = fused.get(section, {})
         # Use the order as in the JSON for fields
-        if section != 'strategic_notes_personalization':
+        if section != "strategic_notes_personalization":
             for field, value in section_data.items():
-                if isinstance(value, dict) and 'value' in value:
-                    v = value['value']
-                    c = value.get('citation', "")
+                if isinstance(value, dict) and "value" in value:
+                    v = value["value"]
+                    c = value.get("citation", "")
                 else:
                     v = value
                     c = ""
@@ -58,9 +61,9 @@
                 result[(section, field)] = v
         else:
             for field, value in section_data.items():
-                if isinstance(value, dict) and 'value' in value:
-                    v = value['value']
-                    c = value.get('citation', "")
+                if isinstance(value, dict) and "value" in value:
+                    v = value["value"]
+                    c = value.get("citation", "")
                 else:
                     v = value
                     c = ""
@@ -73,10 +76,11 @@
                         c = _json.dumps(c, ensure_ascii=False)
                     v = f"{v}\nCitation: {c}"
                 result[(section, field)] = v
-                if field == 'company_linkedin_mentions_raw':
+                if field == "company_linkedin_mentions_raw":
                     break  # Stop after this field
     return result
 
+
 def flatten_person_enrichment_for_excel(json_data: List[Dict[str, Any]]) -> Tuple[List[List[Any]], List[str], List[str], List[str]]:
     # Collect all possible (category, field) pairs
     all_keys = set()
@@ -85,59 +89,63 @@
         flat = flatten_person_for_multiindex(person)
         all_keys.update(flat.keys())
         rows.append(flat)
-    
+
     # Remove metadata fields
-    meta_fields = set([
-        ("fusion_metadata", "completeness_percentage"),
-        ("fusion_metadata", "sources_used"),
-        ("fusion_metadata", "fusion_timestamp"),
-        ("person", "completeness_percentage"),
-        ("person", "sources_used"),
-        ("person", "fusion_timestamp"),
-    ])
+    meta_fields = set(
+        [
+            ("fusion_metadata", "completeness_percentage"),
+            ("fusion_metadata", "sources_used"),
+            ("fusion_metadata", "fusion_timestamp"),
+            ("person", "completeness_percentage"),
+            ("person", "sources_used"),
+            ("person", "fusion_timestamp"),
+        ]
+    )
     all_keys = [k for k in all_keys if k not in meta_fields]
-    
+
     # Sort keys: person fields first, then alphabetical by section/field
-    all_keys = sorted(all_keys, key=lambda x: (0 if x[0]=="person" else 1, x[0], x[1]))
-    
+    all_keys = sorted(all_keys, key=lambda x: (0 if x[0] == "person" else 1, x[0], x[1]))
+
     # Build data rows
     data = []
     for row in rows:
         data.append([row.get(k, None) for k in all_keys])
-    
+
     # Prepare headers for Excel
     categories = [k[0] for k in all_keys]
     fields = [k[1] for k in all_keys]
     colnames = [f"{k[0]}__{k[1]}" for k in all_keys]  # for DataFrame
-    
+
     return data, categories, fields, colnames
 
+
 def convert_person_enrichment_json_to_excel(json_data: List[Dict[str, Any]], output: BytesIO) -> None:
     """
     Convert person enrichment JSON data to Excel format and write to BytesIO.
-    
+
     Args:
         json_data: List of person enrichment data dictionaries
         output: BytesIO object to write Excel data to
     """
     data_rows, categories, fields, colnames = flatten_person_enrichment_for_excel(json_data)
-    
+
     # Create workbook and select active sheet
     wb = openpyxl.Workbook()
     ws = wb.active
     ws.title = "Person Enrichment"
-    
+
     # Write headers
     ws.append(categories)
     ws.append(fields)
-    
+
     # Write data rows
     for row in data_rows:
         ws.append(row)
-    
+
     # Save to BytesIO
     wb.save(output)
 
+
 def flatten_company_enrichment(json_data: Dict[str, Any]) -> pd.DataFrame:
     # Flatten all sections
     flat = {}
@@ -146,22 +154,24 @@
         flat.update(flat_section)
     return pd.DataFrame([flat])
 
+
 def flatten_company_enrichment_for_excel(json_data: Dict[str, Any]) -> Tuple[List[List[Any]], List[str], List[str]]:
     import json as _json
+
     data = []
     categories = []
     fields = []
-    
+
     # Use the order as in the JSON
     for section, section_data in json_data.items():
-        if section == 'enrichment_metadata':
+        if section == "enrichment_metadata":
             continue
         for field, value in section_data.items():
             categories.append(section)
             fields.append(field)
-            if isinstance(value, dict) and 'value' in value:
-                v = value['value']
-                c = value.get('citation', "")
+            if isinstance(value, dict) and "value" in value:
+                v = value["value"]
+                c = value.get("citation", "")
             else:
                 v = value
                 c = ""
@@ -174,60 +184,63 @@
                     c = _json.dumps(c, ensure_ascii=False)
                 v = f"{v}\nCitation: {c}"
             data.append(v)
-    
+
     return [data], categories, fields
 
+
 def convert_company_enrichment_json_to_excel(json_data: Dict[str, Any], output: BytesIO) -> None:
     """
     Convert company enrichment JSON data to Excel format and write to BytesIO.
-    
+
     Args:
         json_data: Company enrichment data dictionary
         output: BytesIO object to write Excel data to
     """
     data_rows, categories, fields = flatten_company_enrichment_for_excel(json_data)
-    
+
     # Create workbook and select active sheet
     wb = openpyxl.Workbook()
     ws = wb.active
     ws.title = "Company Enrichment"
-    
+
     # Write headers
     ws.append(categories)
     ws.append(fields)
-    
+
     # Write data rows
     for row in data_rows:
         ws.append(row)
-    
+
     # Save to BytesIO
     wb.save(output)
 
+
 def batch_convert_enrichments():
     import os
     import openpyxl
-    prospecting_dir = 'output/prospecting'
-    export_dir = 'output/excel_exports'
+
+    prospecting_dir = "output/prospecting"
+    export_dir = "output/excel_exports"
     os.makedirs(export_dir, exist_ok=True)
-    person_files = glob.glob(os.path.join(prospecting_dir, 'person_enrichment_*.json'))
-    company_files = glob.glob(os.path.join(prospecting_dir, 'company_enrichment_*.json'))
+    person_files = glob.glob(os.path.join(prospecting_dir, "person_enrichment_*.json"))
+    company_files = glob.glob(os.path.join(prospecting_dir, "company_enrichment_*.json"))
     print(f"Found {len(person_files)} person enrichment files and {len(company_files)} company enrichment files.")
     # Person enrichment with order preservation
     for f in person_files:
-        out = os.path.join(export_dir, os.path.splitext(os.path.basename(f))[0] + '_ordered.xlsx')
+        out = os.path.join(export_dir, os.path.splitext(os.path.basename(f))[0] + "_ordered.xlsx")
         try:
-            with open(f, 'r', encoding='utf-8') as fin:
+            with open(f, "r", encoding="utf-8") as fin:
                 data = json.load(fin)
             # Collect all possible (category, field) pairs in order of first appearance
             all_keys = []
             seen = set()
             for person in data:
-                fused = person.get('fused_person_profile', {})
+                fused = person.get("fused_person_profile", {})
                 for section in [
-                    'contact_identity',
-                    'professional_background',
-                    'decision_making_influence',
-                    'strategic_notes_personalization',
+                    "contact_identity",
+                    "professional_background",
+                    "decision_making_influence",
+                    "strategic_notes_personalization",
                 ]:
                     section_data = fused.get(section, {})
                     for field in section_data:
@@ -235,7 +248,7 @@
                         if k not in seen:
                             all_keys.append(k)
                             seen.add(k)
-                        if section == 'strategic_notes_personalization' and field == 'company_linkedin_mentions_raw':
+                        if section == "strategic_notes_personalization" and field == "company_linkedin_mentions_raw":
                             break
             # Build rows in order
             rows = []
@@ -257,22 +270,22 @@
             print(f"Failed to convert {f}: {e}")
     # Company enrichment with order preservation
     for f in company_files:
-        out = os.path.join(export_dir, os.path.splitext(os.path.basename(f))[0] + '_ordered.xlsx')
+        out = os.path.join(export_dir, os.path.splitext(os.path.basename(f))[0] + "_ordered.xlsx")
         try:
-            with open(f, 'r', encoding='utf-8') as fin:
+            with open(f, "r", encoding="utf-8") as fin:
                 data = json.load(fin)
             data_row = []
             categories = []
             fields = []
             for section, section_data in data.items():
-                if section == 'enrichment_metadata':
+                if section == "enrichment_metadata":
                     continue
                 for field, value in section_data.items():
                     categories.append(section)
                     fields.append(field)
-                    if isinstance(value, dict) and 'value' in value:
-                        v = value['value']
-                        c = value.get('citation', "")
+                    if isinstance(value, dict) and "value" in value:
+                        v = value["value"]
+                        c = value.get("citation", "")
                     else:
                         v = value
                         c = ""
@@ -296,24 +309,26 @@
         except Exception as e:
             print(f"Failed to convert {f}: {e}")
 
+
 def test_order_person():
     import pandas as pd
     import openpyxl
     from pathlib import Path
-    json_path = 'output/prospecting/person_enrichment_Westbeck_Capital_Management_LLP_20250626_155955.json'
-    excel_path = 'output/excel_exports/person_enrichment_Westbeck_Capital_Management_LLP_20250626_155955_ordered.xlsx'
-    with open(json_path, 'r', encoding='utf-8') as f:
+
+    json_path = "output/prospecting/person_enrichment_Westbeck_Capital_Management_LLP_20250626_155955.json"
+    excel_path = "output/excel_exports/person_enrichment_Westbeck_Capital_Management_LLP_20250626_155955_ordered.xlsx"
+    with open(json_path, "r", encoding="utf-8") as f:
         data = json.load(f)
     # Collect all possible (category, field) pairs in order of first appearance
     all_keys = []
     seen = set()
     for person in data:
-        fused = person.get('fused_person_profile', {})
+        fused = person.get("fused_person_profile", {})
         for section in [
-            'contact_identity',
-            'professional_background',
-            'decision_making_influence',
-            'strategic_notes_personalization',
+            "contact_identity",
+            "professional_background",
+            "decision_making_influence",
+            "strategic_notes_personalization",
         ]:
             section_data = fused.get(section, {})
             for field in section_data:
@@ -321,7 +336,7 @@
                 if k not in seen:
                     all_keys.append(k)
                     seen.add(k)
-                if section == 'strategic_notes_personalization' and field == 'company_linkedin_mentions_raw':
+                if section == "strategic_notes_personalization" and field == "company_linkedin_mentions_raw":
                     break
     # Build rows in order
     rows = []
@@ -340,25 +355,28 @@
     wb.save(excel_path)
     print(f"Person enrichment Excel written to {excel_path}")
 
+
 if __name__ == "__main__":
     import argparse
+
     parser = argparse.ArgumentParser(description="Convert person/company enrichment JSON to Excel.")
-    parser.add_argument('json_path', nargs='?', help='Path to the JSON file')
-    parser.add_argument('--type', choices=['person', 'company'], help='Type of enrichment file')
-    parser.add_argument('--excel_path', help='Optional output Excel file path')
-    parser.add_argument('--batch', action='store_true', help='Convert all enrichment JSONs in output/prospecting to Excel in output/excel_exports')
-    parser.add_argument('--test_order', action='store_true', help='Test order preservation on one company file')
-    parser.add_argument('--test_order_person', action='store_true', help='Test order preservation on one person file')
+    parser.add_argument("json_path", nargs="?", help="Path to the JSON file")
+    parser.add_argument("--type", choices=["person", "company"], help="Type of enrichment file")
+    parser.add_argument("--excel_path", help="Optional output Excel file path")
+    parser.add_argument("--batch", action="store_true", help="Convert all enrichment JSONs in output/prospecting to Excel in output/excel_exports")
+    parser.add_argument("--test_order", action="store_true", help="Test order preservation on one company file")
+    parser.add_argument("--test_order_person", action="store_true", help="Test order preservation on one person file")
     args = parser.parse_args()
     if args.test_order:
         convert_company_enrichment_json_to_excel(
-            'output/prospecting/company_enrichment_Westbeck_Capital_Management_LLP_20250626_155915.json',
-            'output/excel_exports/company_enrichment_Westbeck_Capital_Management_LLP_20250626_155915_ordered.xlsx')
+            "output/prospecting/company_enrichment_Westbeck_Capital_Management_LLP_20250626_155915.json",
+            "output/excel_exports/company_enrichment_Westbeck_Capital_Management_LLP_20250626_155915_ordered.xlsx",
+        )
     elif args.test_order_person:
         test_order_person()
     elif args.batch:
         batch_convert_enrichments()
-    elif args.type == 'person':
+    elif args.type == "person":
         convert_person_enrichment_json_to_excel(args.json_path, args.excel_path)
-    elif args.type == 'company':
-        convert_company_enrichment_json_to_excel(args.json_path, args.excel_path) 
\ No newline at end of file
+    elif args.type == "company":
+        convert_company_enrichment_json_to_excel(args.json_path, args.excel_path)

--- app/tools/mcp_tools.py
+++ app/tools/mcp_tools.py
@@ -10,8 +10,10 @@
 
 load_dotenv()
 
+
 def get_server_params():
     from app.utils.config import get_brightdata_api_key
+
     return StdioServerParameters(
         command="npx",
         env={
@@ -22,5 +24,6 @@
         args=["@brightdata/mcp"],
     )
 
+
 async def async_load_mcp_tools(session):
-    return await load_mcp_tools(session)
\ No newline at end of file
+    return await load_mcp_tools(session)

--- app/tools/scrape_website_brightdata.py
+++ app/tools/scrape_website_brightdata.py
@@ -8,15 +8,12 @@
 import json
 from app.utils.chunking import chunk_web_page_data
 
+
 @tool(
     "scrape_website_brightdata",
-    args_schema={
-        "urls": list,
-        "output_file": str,
-        "mcp_tools": object
-    },
+    args_schema={"urls": list, "output_file": str, "mcp_tools": object},
     return_direct=True,
-    description="Scrape a list of web pages using Bright Data's scrape_as_markdown tool. Takes a list of URLs, an output file path, and the loaded MCP tools. Appends the scraped markdown for each URL to the output file. Returns a summary of the operation."
+    description="Scrape a list of web pages using Bright Data's scrape_as_markdown tool. Takes a list of URLs, an output file path, and the loaded MCP tools. Appends the scraped markdown for each URL to the output file. Returns a summary of the operation.",
 )
 async def scrape_website_brightdata(urls: List[str], output_file: str, mcp_tools) -> str:
     """
@@ -38,10 +35,7 @@
         if not url or not isinstance(url, str) or not url.startswith("http"):
             continue
         scrape_prompt = get_scrape_prompt_template()
-        messages = [
-            SystemMessage(content=scrape_prompt),
-            HumanMessage(content=f"Website URL: {url}")
-        ]
+        messages = [SystemMessage(content=scrape_prompt), HumanMessage(content=f"Website URL: {url}")]
         try:
             response = await llm_with_tools.ainvoke(messages)
             scrape_output = None
@@ -52,24 +46,23 @@
                     tool = {t.name: t for t in mcp_tools}[tool_name]
                     scrape_output = await tool.ainvoke(tool_args)
                     with open(output_file, "a", encoding="utf-8") as f:
-                        f.write(f"\n### Step 4: Website Scrape Output (Page {i+1})\n\n")
+                        f.write(f"\n### Step 4: Website Scrape Output (Page {i + 1})\n\n")
                         f.write(f"```\n{scrape_output}\n```\n\n---\n")
                     successes += 1
             else:
                 scrape_output = str(response)
                 with open(output_file, "a", encoding="utf-8") as f:
-                    f.write(f"\n### Step 4: Website Scrape Output (Page {i+1})\n\n")
+                    f.write(f"\n### Step 4: Website Scrape Output (Page {i + 1})\n\n")
                     f.write(f"```\n{scrape_output}\n```\n\n---\n")
                 successes += 1
         except Exception as e:
             with open(output_file, "a", encoding="utf-8") as f:
-                f.write(f"\n### Step 4: Website Scrape Output (Page {i+1})\n\nError scraping {url}: {str(e)}\n\n---\n")
+                f.write(f"\n### Step 4: Website Scrape Output (Page {i + 1})\n\nError scraping {url}: {str(e)}\n\n---\n")
             failures += 1
-    return f"Scraping complete. Successes: {successes}, Failures: {failures}." 
+    return f"Scraping complete. Successes: {successes}, Failures: {failures}."
+
 
-async def scrape_website_html_and_chunk(
-    urls, mcp_tools, output_file, company_name, run_id, chunker, user_id=None
-):
+async def scrape_website_html_and_chunk(urls, mcp_tools, output_file, company_name, run_id, chunker, user_id=None):
     from langchain_openai import ChatOpenAI
     from app.prompts.data_retrieval_prompts import get_scrape_prompt_template
     from langchain_core.messages import SystemMessage, HumanMessage
@@ -84,11 +77,8 @@
         if not url or not isinstance(url, str) or not url.startswith("http"):
             print(f"[scrape_website_html_and_chunk] Skipping invalid URL: {url}")
             continue
-        scrape_prompt_html = get_scrape_prompt_template(tool='scrape_as_html')
-        messages_html = [
-            SystemMessage(content=scrape_prompt_html),
-            HumanMessage(content=f"Website URL: {url}")
-        ]
+        scrape_prompt_html = get_scrape_prompt_template(tool="scrape_as_html")
+        messages_html = [SystemMessage(content=scrape_prompt_html), HumanMessage(content=f"Website URL: {url}")]
         try:
             response_html = await llm_with_tools.ainvoke(messages_html)
             print(f"[scrape_website_html_and_chunk] LLM response received for {url}")
@@ -110,18 +100,18 @@
                             run_id=run_id,
                             citation=url,
                             min_length=30,  # Explicitly set minimum length for filtering
-                            user_id=user_id
+                            user_id=user_id,
                         )
                         print(f"[scrape_website_html_and_chunk] Number of chunks: {len(html_chunks)}")
-                        with open(output_file, 'a', encoding='utf-8') as f:
+                        with open(output_file, "a", encoding="utf-8") as f:
                             for chunk in html_chunks:
-                                f.write(f'- {json.dumps(chunk, ensure_ascii=False)}\n')
+                                f.write(f"- {json.dumps(chunk, ensure_ascii=False)}\n")
                         successes += 1
             else:
                 print(f"[scrape_website_html_and_chunk] No tool_calls in LLM response for {url}")
         except Exception as e:
             print(f"[scrape_website_html_and_chunk] Exception for {url}: {e}")
-            with open(output_file, 'a', encoding='utf-8') as f:
+            with open(output_file, "a", encoding="utf-8") as f:
                 f.write(f"\nError scraping HTML for {url}: {str(e)}\n\n---\n")
     print(f"[scrape_website_html_and_chunk] Done. Successes: {successes}, Failures: {len(urls) - successes}")
-    return f"HTML scraping and chunking complete. Successes: {successes}, Failures: {len(urls) - successes}."
\ No newline at end of file
+    return f"HTML scraping and chunking complete. Successes: {successes}, Failures: {len(urls) - successes}."

--- app/tools/search_module.py
+++ app/tools/search_module.py
@@ -13,10 +13,10 @@
 from langchain_core.messages import SystemMessage, HumanMessage
 from app.prompts.data_retrieval_prompts import (
     get_company_info_prompt,
-    get_search_prompt, 
+    get_search_prompt,
     get_extract_urls_prompt,
     get_linkedin_prompt,
-    get_multi_domain_prompt
+    get_multi_domain_prompt,
 )
 from langchain.tools import tool
 from app.utils.langsmith_config import trace_operation, initialize_langsmith
@@ -26,6 +26,7 @@
 # Initialize LangSmith for this module
 initialize_langsmith()
 
+
 def extract_json_from_llm_output(llm_output: str) -> dict:
     """Extract JSON from LLM output, stripping markdown code blocks and language tags if present."""
     cleaned = llm_output.strip()
@@ -34,11 +35,12 @@
         cleaned = cleaned.rstrip("`").strip()
     return json.loads(cleaned)
 
+
 def filter_and_format_search_results(raw_text: str, max_results: int = 5) -> Any:
     """Filter and format search results from raw tool output."""
     # Step 1: Try to extract the section between the markers
-    start = raw_text.find('# Search Results')
-    end = raw_text.find('# Page Navigation')
+    start = raw_text.find("# Search Results")
+    end = raw_text.find("# Page Navigation")
     if start != -1 and end != -1 and end > start:
         section = raw_text[start:end]
         section = section.strip()
@@ -49,145 +51,125 @@
     lines = [line.strip() for line in raw_text.splitlines() if line.strip()]
     results = []
     for i, line in enumerate(lines):
-        md_match = re.match(r'\[(.*?)\]\((https?://[^)]+)\)', line)
+        md_match = re.match(r"\[(.*?)\]\((https?://[^)]+)\)", line)
         if md_match:
             title, url = md_match.groups()
-            snippet = lines[i+1] if i+1 < len(lines) else ''
+            snippet = lines[i + 1] if i + 1 < len(lines) else ""
             results.append({"url": url, "title": title, "snippet": snippet})
-        elif line.startswith('https://') or line.startswith('http://'):
+        elif line.startswith("https://") or line.startswith("http://"):
             url = line
-            title = lines[i-1] if i > 0 else ''
-            snippet = lines[i+1] if i+1 < len(lines) else ''
+            title = lines[i - 1] if i > 0 else ""
+            snippet = lines[i + 1] if i + 1 < len(lines) else ""
             results.append({"url": url, "title": title, "snippet": snippet})
         if len(results) >= max_results:
             break
     return results  # Option 2: Fallback
 
+
 class SearchModule:
     """
     Reusable search module that implements the two-step search process:
     1. Use search_engine tool to get search results
     2. Use LLM to extract URLs from the results
     """
-    
+
     def __init__(self, mcp_tools: List[Any]):
         self.mcp_tools = mcp_tools
         self.tools = {tool.name: tool for tool in mcp_tools}
         self.search_llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=get_openai_api_key())
         self.extract_llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=get_openai_api_key())
-        
+
         # Bind tools to search LLM
         self.llm_with_tools = self.search_llm.bind_tools(mcp_tools)
-    
-    async def search_and_extract(
-        self, 
-        search_config: Dict[str, Any], 
-        company_info: Dict[str, str],
-        debug: bool = False
-    ) -> Optional[Dict[str, Any]]:
+
+    async def search_and_extract(self, search_config: Dict[str, Any], company_info: Dict[str, str], debug: bool = False) -> Optional[Dict[str, Any]]:
         """
         Perform two-step search and extraction.
-        
+
         Args:
             search_config: Configuration with query_template, search_type_description, output_format_example, max_results
             company_info: Dict with company_name, location, focus_area
             debug: Whether to print debug information
-            
+
         Returns:
             Extracted URLs or None if failed
         """
-        
-        company_name = company_info.get('name', '')
-        location = company_info.get('location', '')
-        focus_area = company_info.get('focus_area', '')
-        
+
+        company_name = company_info.get("name", "")
+        location = company_info.get("location", "")
+        focus_area = company_info.get("focus_area", "")
+
         # Build search query
-        query = search_config["query_template"].format(
-            company_name=company_name,
-            location=location,
-            key_area_of_focus=focus_area
-        )
-        
+        query = search_config["query_template"].format(company_name=company_name, location=location, key_area_of_focus=focus_area)
+
         if debug:
             print(f"üîç Search query: {query}")
-        
+
         try:
             # Step 1: Search using LLM with tools
             search_prompt = get_search_prompt()
             messages = [
                 SystemMessage(content=search_prompt),
-                HumanMessage(content=f"Find the {search_config.get('name', 'information')} for: {query}")
+                HumanMessage(content=f"Find the {search_config.get('name', 'information')} for: {query}"),
             ]
-            
-            with trace_operation("search_module_search", {
-                "search_type": search_config.get('name', 'information'),
-                "company_name": company_name,
-                "model": "gpt-4o"
-            }):
+
+            with trace_operation(
+                "search_module_search", {"search_type": search_config.get("name", "information"), "company_name": company_name, "model": "gpt-4o"}
+            ):
                 response = await self.llm_with_tools.ainvoke(messages)
-            
+
             if debug:
                 print(f"üîß Search response: {response}")
-            
+
             # Extract tool outputs
             tool_output = None
             filtered_results = None
-            
+
             if hasattr(response, "tool_calls") and response.tool_calls:
                 for tool_call in response.tool_calls:
                     tool_name = tool_call["name"]
                     tool_args = tool_call["args"]
-                    
+
                     if debug:
                         print(f"üîß Tool call: {tool_name} with args: {tool_args}")
-                    
+
                     tool = self.tools[tool_name]
                     tool_output = await tool.ainvoke(tool_args)
-                    
+
                     # Filter and format results
-                    filtered_results = filter_and_format_search_results(
-                        str(tool_output),
-                        max_results=search_config.get("max_results", 5)
-                    )
-                    
+                    filtered_results = filter_and_format_search_results(str(tool_output), max_results=search_config.get("max_results", 5))
+
                     if debug:
                         print(f"üîç Filtered results: {filtered_results}")
-                    
+
                     break  # Use first tool call
             else:
                 if debug:
                     print("‚ö†Ô∏è No tool calls made")
                 return None
-            
+
             # Step 2: Extract URLs using LLM
-            search_type_description = search_config["search_type_description"].format(
-                key_area_of_focus=focus_area,
-                location=location
-            )
-            
+            search_type_description = search_config["search_type_description"].format(key_area_of_focus=focus_area, location=location)
+
             extract_prompt = get_extract_urls_prompt()
             formatted_extract_prompt = extract_prompt.format(
                 search_type_description=search_type_description,
                 company_info=json.dumps(company_info),
                 search_results=str(filtered_results),
-                output_format_example=search_config["output_format_example"]
+                output_format_example=search_config["output_format_example"],
             )
-            
+
             if debug:
                 print(f"üìù Extract prompt: {formatted_extract_prompt}")
-            
-            with trace_operation("search_module_extract", {
-                "search_type": search_config.get('name', 'information'),
-                "company_name": company_name,
-                "model": "gpt-4o"
-            }):
-                extract_response = await self.extract_llm.ainvoke([
-                    SystemMessage(content=formatted_extract_prompt)
-                ])
-            
+
+            with trace_operation(
+                "search_module_extract", {"search_type": search_config.get("name", "information"), "company_name": company_name, "model": "gpt-4o"}
+            ):
+                extract_response = await self.extract_llm.ainvoke([SystemMessage(content=formatted_extract_prompt)])
+
             if debug:
                 print(f"ü§ñ Extract response: {extract_response.content}")
-            
+
             # Parse extracted URLs
             try:
                 extracted_urls = extract_json_from_llm_output(extract_response.content)
@@ -196,82 +178,54 @@
                 if debug:
                     print(f"‚ùå JSON parsing failed: {e}")
                 return None
-                
+
         except Exception as e:
             if debug:
                 print(f"üí• Search failed: {e}")
-            return None 
+            return None
+
 
 @tool(
     "find_company_homepage_url_perplexity",
-    args_schema={
-        "company_name": str,
-        "location": str,
-        "focus_area": str,
-        "investor_type": str,
-        "web_search_config": dict
-    },
+    args_schema={"company_name": str, "location": str, "focus_area": str, "investor_type": str, "web_search_config": dict},
     return_direct=True,
-    description="Find the official homepage URL(s) for a company using Perplexity API. Returns a single URL string if only one domain found, or a list of URLs if multiple company domains are discovered."
+    description="Find the official homepage URL(s) for a company using Perplexity API. Returns a single URL string if only one domain found, or a list of URLs if multiple company domains are discovered.",
 )
-async def find_company_homepage_url_perplexity(company_name: str, location: str, focus_area: str, investor_type: str, web_search_config: Dict[str, Any]) -> Optional[str]:
-    search_type_description = web_search_config["search_type_description"].format(
-        company_name=company_name
-    )
-    company_info = {
-        'name': company_name,
-        'location': location,
-        'focus_area': focus_area,
-        'investor_type': investor_type
-    }
+async def find_company_homepage_url_perplexity(
+    company_name: str, location: str, focus_area: str, investor_type: str, web_search_config: Dict[str, Any]
+) -> Optional[str]:
+    search_type_description = web_search_config["search_type_description"].format(company_name=company_name)
+    company_info = {"name": company_name, "location": location, "focus_area": focus_area, "investor_type": investor_type}
     # Get the optimized multi-domain prompt from the centralized prompts module
     investor_type_param = investor_type or ""
     multi_domain_prompt = get_multi_domain_prompt(company_name, location, investor_type_param)
     investor_type_context = web_search_config.get("investor_type", "")
     investor_type_suffix = f" (this company is an {investor_type_context})" if investor_type_context else ""
-    user_prompt = web_search_config["query_template"].format(
-        company_name=company_name,
-        location=location,
-        investor_type_context=investor_type_suffix
-    )
-    chat = ChatPerplexity(
-        temperature=0, 
-        model="sonar-pro",
-        api_key=get_perplexity_api_key()
-    )
+    user_prompt = web_search_config["query_template"].format(company_name=company_name, location=location, investor_type_context=investor_type_suffix)
+    chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=get_perplexity_api_key())
     print(f"üîç Perplexity call with model: sonar-pro")
-    
+
     # API monitoring: Track Perplexity API call
     api_call_start_time = time.time()
-    
+
     try:
-        with trace_operation("find_company_homepage_url", {
-            "company_name": company_name,
-            "model": "sonar-pro",
-            "search_type": "homepage_discovery"
-        }):
+        with trace_operation("find_company_homepage_url", {"company_name": company_name, "model": "sonar-pro", "search_type": "homepage_discovery"}):
             try:
                 response = await chat.ainvoke(
-                    [
-                        ("system", multi_domain_prompt),
-                        ("user", user_prompt)
-                    ],
-                    extra_body={
-                        "web_search_options": {"search_context_size": "high"}
-                    }
+                    [("system", multi_domain_prompt), ("user", user_prompt)], extra_body={"web_search_options": {"search_context_size": "high"}}
                 )
-                
+
                 api_response_time = time.time() - api_call_start_time
-                
+
                 # API monitoring: Track successful Perplexity API call
                 print(f"‚úÖ Perplexity API call successful - Response time: {api_response_time:.2f}s")
-                
+
                 content = response.content.strip()
                 print(f"üîç Perplexity response content: {content}")
-                
+
                 # Try to parse as JSON first (for multiple domains)
                 try:
-                    if content.startswith('[') and content.endswith(']'):
+                    if content.startswith("[") and content.endswith("]"):
                         domains = json.loads(content)
                         if isinstance(domains, list) and len(domains) > 1:
                             print(f"‚úÖ Found {len(domains)} domains: {domains}")
@@ -281,9 +235,9 @@
                             return domains[0]  # Return single string
                 except json.JSONDecodeError:
                     pass
-                
+
                 # Extract single URL from response using regex (fallback)
-                url_match = re.search(r'https?://[^\s\)\"\']+', content)
+                url_match = re.search(r"https?://[^\s\)\"\']+", content)
                 if url_match:
                     homepage_url = url_match.group(0)
                     print(f"‚úÖ Found single homepage URL: {homepage_url}")
@@ -291,16 +245,16 @@
                 else:
                     print("‚ùå No valid URL found in Perplexity response")
                     return None
-                        
+
             except Exception as api_error:
                 api_response_time = time.time() - api_call_start_time
-                
+
                 # API monitoring: Track failed Perplexity API call
                 print(f"‚ùå Perplexity API call failed - Response time: {api_response_time:.2f}s - Error: {api_error}")
-                
+
                 raise api_error
-                        
+
     except Exception as e:
         print(f"‚ùå Error in find_company_homepage_url_perplexity: {e}")
-        
-    return None 
\ No newline at end of file
+
+    return None

--- app/utils/chunking.py
+++ app/utils/chunking.py
@@ -3,9 +3,20 @@
 from typing import List, Dict, Any
 from bs4 import BeautifulSoup
 
+
 def build_uniform_metadata(
-    section=None, headline=None, date=None, citation=None, source=None,
-    company=None, run_id=None, source_type=None, index=None, user_id=None, company_id=None, session_id=None
+    section=None,
+    headline=None,
+    date=None,
+    citation=None,
+    source=None,
+    company=None,
+    run_id=None,
+    source_type=None,
+    index=None,
+    user_id=None,
+    company_id=None,
+    session_id=None,
 ):
     """
     Build a uniform metadata dictionary for all chunked data.
@@ -23,9 +34,10 @@
         "user_id": user_id,
         "session_id": session_id,
         "source_type": source_type,
-        "index": index
+        "index": index,
     }
 
+
 # --- LinkedIn Chunker ---
 def chunk_linkedin_data(data, company_name=None, denylist=None, run_id=None, citation=None, user_id=None, company_id=None, session_id=None):
     """
@@ -62,7 +74,29 @@
         if isinstance(value, list):
             for idx, item in enumerate(value):
                 chunk_text = f"{key} [{idx}]: {json.dumps(item, ensure_ascii=False, indent=2) if isinstance(item, (dict, list)) else str(item)}"
-                chunks.append({
+                chunks.append(
+                    {
+                        "text": chunk_text,
+                        "metadata": build_uniform_metadata(
+                            section=key,
+                            headline=None,
+                            date=None,
+                            citation=citation,
+                            source=None,
+                            company=company_name or raw.get("name", ""),
+                            run_id=run_id,
+                            source_type="LinkedIn",
+                            index=idx,
+                            user_id=user_id,
+                            company_id=company_id,
+                            session_id=session_id,
+                        ),
+                    }
+                )
+        else:
+            chunk_text = f"{key}: {json.dumps(value, ensure_ascii=False, indent=2) if isinstance(value, (dict, list)) else str(value)}"
+            chunks.append(
+                {
                     "text": chunk_text,
                     "metadata": build_uniform_metadata(
                         section=key,
@@ -70,36 +104,19 @@
                         date=None,
                         citation=citation,
                         source=None,
-                        company=company_name or raw.get('name', ''),
+                        company=company_name or raw.get("name", ""),
                         run_id=run_id,
                         source_type="LinkedIn",
-                        index=idx,
+                        index=None,
                         user_id=user_id,
                         company_id=company_id,
-                        session_id=session_id
-                    )
-                })
-        else:
-            chunk_text = f"{key}: {json.dumps(value, ensure_ascii=False, indent=2) if isinstance(value, (dict, list)) else str(value)}"
-            chunks.append({
-                "text": chunk_text,
-                "metadata": build_uniform_metadata(
-                    section=key,
-                    headline=None,
-                    date=None,
-                    citation=citation,
-                    source=None,
-                    company=company_name or raw.get('name', ''),
-                    run_id=run_id,
-                    source_type="LinkedIn",
-                    index=None,
-                    user_id=user_id,
-                    company_id=company_id,
-                    session_id=session_id
-                )
-            })
+                        session_id=session_id,
+                    ),
+                }
+            )
     return chunks
 
+
 # --- Perplexity Chunker ---
 def chunk_perplexity_json(data, company_name=None, run_id=None, citation=None, user_id=None, company_id=None, session_id=None):
     """
@@ -134,14 +151,16 @@
                     index=idx,
                     user_id=user_id,
                     company_id=company_id,
-                    session_id=session_id
-                )
+                    session_id=session_id,
+                ),
             }
             chunks.append(chunk)
     return chunks
 
+
 # --- Bright Data HTML/Web Chunker ---
 
+
 def is_relevant_web_chunk(chunk, denylist=None, min_length=30):
     """
     Returns True if the chunk is relevant for vector DB ingestion.
@@ -149,12 +168,25 @@
     """
     if denylist is None:
         denylist = [
-            "contact", "contact us", "contact form", "subscribe", "newsletter",
-            "privacy policy", "terms of service", "legal", "cookie", "login",
-            "sign in", "register", "careers", "sitemap", "follow us", "all rights reserved"
+            "contact",
+            "contact us",
+            "contact form",
+            "subscribe",
+            "newsletter",
+            "privacy policy",
+            "terms of service",
+            "legal",
+            "cookie",
+            "login",
+            "sign in",
+            "register",
+            "careers",
+            "sitemap",
+            "follow us",
+            "all rights reserved",
         ]
-    text = chunk['text'].strip().lower()
-    headline = (chunk['metadata'].get('headline') or "").strip().lower()
+    text = chunk["text"].strip().lower()
+    headline = (chunk["metadata"].get("headline") or "").strip().lower()
     # Denylist check
     for phrase in denylist:
         if phrase in text or phrase in headline:
@@ -163,11 +195,14 @@
     if len(text) < min_length:
         return False
     # Exclude if text is just a heading (no sentence-ending punctuation) and is a heading tag
-    if not any(p in text for p in ".!?") and chunk['metadata'].get('section', '').startswith('h'):
+    if not any(p in text for p in ".!?") and chunk["metadata"].get("section", "").startswith("h"):
         return False
     return True
 
-def chunk_web_page_data(html, company_name=None, run_id=None, citation=None, chunk_size=2000, denylist=None, min_length=30, user_id=None, company_id=None, session_id=None):
+
+def chunk_web_page_data(
+    html, company_name=None, run_id=None, citation=None, chunk_size=2000, denylist=None, min_length=30, user_id=None, company_id=None, session_id=None
+):
     """
     Chunk raw HTML into semantically meaningful sections for vector DB ingestion.
     Prefers <h1>-<h6> and <p> tags for semantic chunking (headings and paragraphs).
@@ -195,10 +230,10 @@
     if not isinstance(html, str) or not html.strip():
         return []
     soup = BeautifulSoup(html, "html.parser")
-    elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p'])
+    elements = soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p"])
     idx = 0
     for el in elements:
-        text = el.get_text(separator=' ', strip=True)
+        text = el.get_text(separator=" ", strip=True)
         tag = el.name
         if text:
             chunk = {
@@ -209,14 +244,14 @@
                     date=None,
                     citation=citation,
                     source=citation,  # Set source to the URL for website chunks
-                    company=company_name or '',
+                    company=company_name or "",
                     run_id=run_id,
                     source_type="Web",
                     index=idx,
                     user_id=user_id,
                     company_id=company_id,
-                    session_id=session_id
-                )
+                    session_id=session_id,
+                ),
             }
             if is_relevant_web_chunk(chunk, denylist=denylist, min_length=min_length):
                 chunks.append(chunk)
@@ -224,7 +259,7 @@
     # Fallback: if no semantic elements found, chunk by length
     if not chunks:
         for i in range(0, len(html), chunk_size):
-            chunk_text = html[i:i+chunk_size]
+            chunk_text = html[i : i + chunk_size]
             chunk = {
                 "text": chunk_text,
                 "metadata": build_uniform_metadata(
@@ -233,14 +268,14 @@
                     date=None,
                     citation=citation,
                     source=citation,
-                    company=company_name or '',
+                    company=company_name or "",
                     run_id=run_id,
                     source_type="Web",
                     index=i // chunk_size,
                     user_id=user_id,
                     company_id=company_id,
-                    session_id=session_id
-                )
+                    session_id=session_id,
+                ),
             }
             if is_relevant_web_chunk(chunk, denylist=denylist, min_length=min_length):
                 chunks.append(chunk)
@@ -254,6 +289,7 @@
             deduped.append(chunk)
     return deduped
 
+
 # --- Coresignal Chunker ---
 def chunk_coresignal_data(data, company_name=None, run_id=None, user_id=None, company_id=None, session_id=None):
     """
@@ -270,61 +306,66 @@
         List of dicts with 'text' and uniform 'metadata'.
     """
     chunks = []
-    
+
     if not isinstance(data, dict):
         return []
-    
+
     # Extract LinkedIn URL for citation
-    linkedin_url = data.get('professional_network_url') or data.get('linkedin_url', '')
-    
+    linkedin_url = data.get("professional_network_url") or data.get("linkedin_url", "")
+
     # Chunk the description field
-    description = data.get('description', '').strip()
+    description = data.get("description", "").strip()
     if description:
-        chunks.append({
-            "text": description,
-            "metadata": build_uniform_metadata(
-                section="description",
-                headline="Company Description",
-                date=None,
-                citation=linkedin_url,
-                source=linkedin_url,
-                company=company_name or data.get('company_name', ''),
-                run_id=run_id,
-                source_type="Coresignal",
-                index=None,
-                user_id=user_id,
-                company_id=company_id,
-                session_id=session_id
-            )
-        })
-    
+        chunks.append(
+            {
+                "text": description,
+                "metadata": build_uniform_metadata(
+                    section="description",
+                    headline="Company Description",
+                    date=None,
+                    citation=linkedin_url,
+                    source=linkedin_url,
+                    company=company_name or data.get("company_name", ""),
+                    run_id=run_id,
+                    source_type="Coresignal",
+                    index=None,
+                    user_id=user_id,
+                    company_id=company_id,
+                    session_id=session_id,
+                ),
+            }
+        )
+
     # Chunk company updates (each update becomes a separate chunk)
-    company_updates = data.get('company_updates', [])
+    company_updates = data.get("company_updates", [])
     if isinstance(company_updates, list):
         for idx, update in enumerate(company_updates):
             if isinstance(update, dict):
-                update_text = update.get('description', '').strip()
+                update_text = update.get("description", "").strip()
                 if update_text:
-                    chunks.append({
-                        "text": update_text,
-                        "metadata": build_uniform_metadata(
-                            section="company_updates",
-                            headline=f"Company Update {idx + 1}",
-                            date=update.get('date'),
-                            citation=linkedin_url,
-                            source=linkedin_url,
-                            company=company_name or data.get('company_name', ''),
-                            run_id=run_id,
-                            source_type="Coresignal",
-                            index=idx,
-                            user_id=user_id,
-                            company_id=company_id,
-                            session_id=session_id
-                        )
-                    })
-    
+                    chunks.append(
+                        {
+                            "text": update_text,
+                            "metadata": build_uniform_metadata(
+                                section="company_updates",
+                                headline=f"Company Update {idx + 1}",
+                                date=update.get("date"),
+                                citation=linkedin_url,
+                                source=linkedin_url,
+                                company=company_name or data.get("company_name", ""),
+                                run_id=run_id,
+                                source_type="Coresignal",
+                                index=idx,
+                                user_id=user_id,
+                                company_id=company_id,
+                                session_id=session_id,
+                            ),
+                        }
+                    )
+
     return chunks
 
+
 # --- YouTube Transcript Chunker ---
 def chunk_youtube_transcript(
     transcript_segments: list,
@@ -335,7 +376,7 @@
     company_id: str = None,
     session_id: str = None,
     chunk_word_count: int = 180,
-    overlap_words: int = 30
+    overlap_words: int = 30,
 ) -> List[Dict[str, Any]]:
     """
     Chunk a YouTube transcript (list of segments with start/duration/text) into overlapping windows.
@@ -379,7 +420,7 @@
     idx = 0
     chunk_index = 0
     while idx < total_words:
-        window = word_timings[idx: idx + chunk_word_count]
+        window = word_timings[idx : idx + chunk_word_count]
         words_only = [w[0] for w in window]
         if not words_only:
             break
@@ -402,8 +443,8 @@
                 source_type="YouTube",
                 index=chunk_index,
                 user_id=user_id,
-                session_id=session_id
-            )
+                session_id=session_id,
+            ),
         }
         chunk["metadata"]["channel_title"] = video_meta.get("channel_title")
         chunk["metadata"]["video_id"] = video_meta.get("video_id")
@@ -417,8 +458,10 @@
 
     return chunks
 
+
 # --- LlamaIndex Chunker ---
 
+
 def chunk_with_llamaindex_text(
     text,
     company_name=None,
@@ -430,7 +473,7 @@
     chunk_size=512,
     user_id=None,
     company_id=None,
-    session_id=None
+    session_id=None,
 ):
     """
     Chunk text using LlamaIndex's SentenceSplitter and attach uniform metadata to each chunk.
@@ -451,7 +494,7 @@
     """
     from llama_index.core.node_parser import SentenceSplitter
     from llama_index.core.schema import Document
-    
+
     splitter = SentenceSplitter(chunk_size=chunk_size)
     docs = splitter.get_nodes_from_documents([Document(text=text)])
     chunks = []
@@ -468,13 +511,9 @@
             "index": idx,
             "user_id": user_id,
             "company_id": company_id,
-            "session_id": session_id
+            "session_id": session_id,
         }
         if extra_metadata:
             metadata.update(extra_metadata)
-        chunks.append({
-            "text": doc.text,
-            "metadata": metadata
-        })
+        chunks.append({"text": doc.text, "metadata": metadata})
     return chunks
- 

--- app/utils/config.py
+++ app/utils/config.py
@@ -5,6 +5,7 @@
 
 logger = logging.getLogger(__name__)
 
+
 def is_aws_secrets_enabled() -> bool:
     """
     Check if AWS Secrets Manager is enabled.
@@ -13,6 +14,7 @@
     disable_secrets = os.getenv("DISABLE_AWS_SECRETS", "false").lower()
     return disable_secrets not in ("true", "1", "yes")
 
+
 def get_config_value(key: str, default: Optional[str] = None) -> Optional[str]:
     """
     Get configuration value with priority:
@@ -26,22 +28,25 @@
         secret_value = get_secret(key)
         if secret_value:
             return secret_value
-    
+
     # Fallback to environment variable
     env_value = os.getenv(key)
     if env_value:
         return env_value
-    
+
     # Return default if provided
     return default
 
+
 # API Keys
 def get_openai_api_key() -> Optional[str]:
     return get_config_value("OPENAI_API_KEY")
 
+
 def get_anthropic_api_key() -> Optional[str]:
     return get_config_value("ANTHROPIC_API_KEY")
 
+
 def get_anthropic_model() -> str:
     raw_value = get_config_value("ANTHROPIC_MODEL", "claude-sonnet-4-5")
     model_aliases = {
@@ -57,138 +62,168 @@
         logger.debug("Resolved Anthropic model alias", extra={"alias": raw_value, "resolved": resolved})
     return resolved
 
+
 def get_perplexity_api_key() -> Optional[str]:
     return get_config_value("PERPLEXITY_API_KEY")
 
+
 def get_coresignal_api_key() -> Optional[str]:
     return get_config_value("CORESIG_API_KEY")
 
+
 def get_apollo_api_key() -> Optional[str]:
     return get_config_value("APOLLO_IO_API_KEY")
 
+
 def get_gemini_api_key() -> Optional[str]:
     return get_config_value("GEMINI_API_KEY")
 
+
 def get_google_api_key() -> Optional[str]:
     return get_config_value("GOOGLE_API_KEY")
 
+
 def get_google_client_id() -> str:
     value = get_config_value("GOOGLE_CLIENT_ID")
     if not value:
         raise ValueError("GOOGLE_CLIENT_ID not configured")
     return value
 
+
 def get_google_client_secret() -> str:
     value = get_config_value("GOOGLE_CLIENT_SECRET")
     if not value:
         raise ValueError("GOOGLE_CLIENT_SECRET not configured")
     return value
 
+
 def get_google_redirect_uri() -> str:
     value = get_config_value("GOOGLE_REDIRECT_URI")
     if not value:
         raise ValueError("GOOGLE_REDIRECT_URI not configured")
     return value
 
+
 def get_outlook_client_id() -> str:
     value = get_config_value("OUTLOOK_CLIENT_ID")
     if not value:
         raise ValueError("OUTLOOK_CLIENT_ID not configured")
     return value
 
+
 def get_outlook_client_secret() -> str:
     value = get_config_value("OUTLOOK_CLIENT_SECRET")
     if not value:
         raise ValueError("OUTLOOK_CLIENT_SECRET not configured")
     return value
 
+
 def get_outlook_redirect_uri() -> str:
     value = get_config_value("OUTLOOK_REDIRECT_URI")
     if not value:
         raise ValueError("OUTLOOK_REDIRECT_URI not configured")
     return value
 
+
 def get_outlook_tenant_id() -> str:
     value = get_config_value("OUTLOOK_TENANT_ID", "common")
     return value
 
+
 def get_hubspot_client_id() -> str:
     value = get_config_value("HUBSPOT_CLIENT_ID")
     if not value:
         raise ValueError("HUBSPOT_CLIENT_ID not configured")
     return value
 
+
 def get_hubspot_client_secret() -> str:
     value = get_config_value("HUBSPOT_CLIENT_SECRET")
     if not value:
         raise ValueError("HUBSPOT_CLIENT_SECRET not configured")
     return value
 
+
 def get_hubspot_redirect_uri() -> str:
     value = get_config_value("HUBSPOT_REDIRECT_URI")
     if not value:
         raise ValueError("HUBSPOT_REDIRECT_URI not configured")
     return value
 
+
 def get_hubspot_app_id() -> Optional[str]:
     """Optional HubSpot app ID for timeline features."""
     return get_config_value("HUBSPOT_APP_ID")
 
+
 def get_salesforce_client_id() -> str:
     value = get_config_value("SALESFORCE_CLIENT_ID")
     if not value:
         raise ValueError("SALESFORCE_CLIENT_ID not configured")
     return value
 
+
 def get_salesforce_client_secret() -> str:
     value = get_config_value("SALESFORCE_CLIENT_SECRET")
     if not value:
         raise ValueError("SALESFORCE_CLIENT_SECRET not configured")
     return value
 
+
 def get_salesforce_redirect_uri() -> str:
     value = get_config_value("SALESFORCE_REDIRECT_URI")
     if not value:
         raise ValueError("SALESFORCE_REDIRECT_URI not configured")
     return value
 
+
 def get_salesforce_login_base_url() -> str:
     """Get Salesforce login base URL (default: https://login.salesforce.com)."""
     return get_config_value("SALESFORCE_LOGIN_BASE_URL", "https://login.salesforce.com")
 
+
 def get_salesforce_default_api_version() -> str:
     """Get Salesforce API version (default: v62.0)."""
     return get_config_value("SALESFORCE_DEFAULT_API_VERSION", "v62.0")
 
+
 def is_profile_embedding_enabled() -> bool:
     """Check if profile embedding is enabled via config."""
     enabled = get_config_value("ENABLE_PROFILE_EMBEDDING", "true")
     return enabled.lower() in ("true", "1", "yes")
 
+
 def get_langsmith_api_key() -> Optional[str]:
     return get_config_value("LANGSMITH_API_KEY")
 
+
 def get_qdrant_api_key() -> Optional[str]:
     return get_config_value("QDRANT_API_KEY")
 
+
 def get_qdrant_url() -> str:
     return get_config_value("QDRANT_URL", "http://localhost:6333")
 
+
 def get_qdrant_collection() -> str:
     """Return Qdrant collection name for vector search."""
     return get_config_value("QDRANT_COLLECTION", "investors_full_local")
 
+
 def get_qdrant_api_key_search_rec() -> Optional[str]:
     """Return Qdrant API key for search/recommender cluster."""
     return get_config_value("QDRANT_API_KEY_SEARCH_REC")
 
+
 def get_qdrant_url_search_rec() -> str:
     """Return Qdrant URL for search/recommender cluster."""
     return get_config_value("QDRANT_URL_SEARCH_REC", "http://localhost:6333")
 
+
 def get_brightdata_api_key() -> Optional[str]:
     return get_config_value("BRIGHTDATA_API_KEY")
 
+
 # Database Configuration
 def get_postgres_host() -> str:
     environment = get_environment()
@@ -197,6 +232,7 @@
     else:
         return get_config_value("POSTGRES_HOST_RDS", "localhost")
 
+
 def get_postgres_port() -> int:
     environment = get_environment()
     if environment == "development":
@@ -205,6 +241,7 @@
         port_str = get_config_value("POSTGRES_PORT_RDS", "5432")
     return int(port_str)
 
+
 def get_postgres_db() -> str:
     environment = get_environment()
     if environment == "development":
@@ -212,6 +249,7 @@
     else:
         return get_config_value("POSTGRES_DB_RDS", "prospecting_db")
 
+
 def get_postgres_user() -> str:
     environment = get_environment()
     if environment == "development":
@@ -219,6 +257,7 @@
     else:
         return get_config_value("POSTGRES_USER_RDS", "prospecting_user")
 
+
 def get_postgres_password() -> str:
     environment = get_environment()
     if environment == "development":
@@ -226,37 +265,45 @@
     else:
         return get_config_value("POSTGRES_PASSWORD_RDS", "")
 
+
 # Feature Flags
 def get_enable_postgres_storage() -> bool:
     value = get_config_value("ENABLE_POSTGRES_STORAGE", "true")
     return value.lower() == "true"
 
+
 def get_enable_llm_consolidation() -> bool:
     value = get_config_value("ENABLE_LLM_CONSOLIDATION", "true")
     return value.lower() == "true"
 
+
 def get_enable_linkedin_rag() -> bool:
     value = get_config_value("ENABLE_LINKEDIN_RAG", "true")
     return value.lower() == "true"
 
+
 def get_enable_narrative_generation() -> bool:
     value = get_config_value("ENABLE_NARRATIVE_GENERATION", "true")
     return value.lower() == "true"
 
+
 def get_enable_youtube_perplexity_search() -> bool:
     """Whether to use Perplexity as an additional YouTube discovery source."""
     value = get_config_value("ENABLE_YOUTUBE_PERPLEXITY_SEARCH", "true")
     return value.lower() == "true"
 
+
 def get_enable_youtube_ingest() -> bool:
     """Whether YouTube ingestion is enabled."""
     value = get_config_value("ENABLE_YOUTUBE_INGEST", "true")
     return value.lower() == "true"
 
+
 def get_youtube_api_key() -> Optional[str]:
     """Return YouTube Data API key."""
     return get_config_value("YOUTUBE_API_KEY")
 
+
 def get_youtube_max_videos_per_run() -> int:
     """Maximum number of videos to ingest per run."""
     value = get_config_value("YOUTUBE_MAX_VIDEOS_PER_RUN", "20")
@@ -265,6 +312,7 @@
     except (TypeError, ValueError):
         return 20
 
+
 def get_youtube_max_transcribe_duration() -> int:
     """Maximum duration in seconds to transcribe (skip long streams)."""
     value = get_config_value("YOUTUBE_MAX_TRANSCRIBE_DURATION", "3600")
@@ -273,6 +321,7 @@
     except (TypeError, ValueError):
         return 3600
 
+
 def get_youtube_video_concurrency() -> int:
     """Maximum number of videos to process in parallel."""
     value = get_config_value("YOUTUBE_VIDEO_CONCURRENCY", "20")
@@ -281,6 +330,7 @@
     except (TypeError, ValueError):
         return 20
 
+
 def get_youtube_embed_concurrency() -> int:
     """Maximum number of concurrent embed calls per video."""
     value = get_config_value("YOUTUBE_EMBED_CONCURRENCY", "5")
@@ -289,6 +339,7 @@
     except (TypeError, ValueError):
         return 5
 
+
 def get_youtube_summary_concurrency() -> int:
     """Maximum number of concurrent map-stage summary calls per video."""
     value = get_config_value("YOUTUBE_SUMMARY_CONCURRENCY", "5")
@@ -297,11 +348,13 @@
     except (TypeError, ValueError):
         return 5
 
+
 def get_youtube_transcript_langs() -> list[str]:
     """Preferred transcript languages (comma-separated env)."""
     langs = get_config_value("YOUTUBE_TRANSCRIPT_LANGS", "en")
     return [lang.strip() for lang in langs.split(",") if lang.strip()]
 
+
 def get_youtube_cookies_file() -> Optional[str]:
     """
     Optional path to a YouTube cookies.txt file (NetScape format) for transcript fetches.
@@ -309,6 +362,7 @@
     """
     return get_config_value("YOUTUBE_COOKIES_FILE")
 
+
 def get_assemblyai_api_key() -> Optional[str]:
     """Optional AssemblyAI API key for third-party transcript fallback."""
     return get_config_value("ASSEMBLYAI_API_KEY")
@@ -323,6 +377,7 @@
     value = get_config_value("USE_REDIS_RATE_LIMITING", "false")
     return value.lower() == "true"
 
+
 def get_enable_debugging() -> bool:
     """
     Whether debugging output should be enabled.
@@ -335,82 +390,98 @@
         value = get_config_value("enable_debugging", "false")
     return str(value).strip().lower() in ("1", "true", "yes", "y", "on")
 
+
 # Registration and Email Verification Configuration
 def get_verification_expiry_hours() -> int:
     """Get email verification token expiry time in hours."""
     value = get_config_value("VERIFICATION_EXPIRY_HOURS", "24")
     return int(value)
 
+
 def get_max_registration_attempts() -> int:
     """Get maximum number of registration attempts per email."""
     value = get_config_value("MAX_REGISTRATION_ATTEMPTS", "3")
     return int(value)
 
+
 def get_cleanup_expired_days() -> int:
     """Get number of days to keep expired registrations before cleanup."""
     value = get_config_value("CLEANUP_EXPIRED_DAYS", "7")
     return int(value)
 
+
 def get_registration_rate_limit_per_hour() -> int:
     """Get maximum registration attempts per hour per IP."""
     value = get_config_value("REGISTRATION_RATE_LIMIT_PER_HOUR", "5")
     return int(value)
 
+
 def get_verification_resend_rate_limit_per_hour() -> int:
     """Get maximum verification email resends per hour per email."""
     value = get_config_value("VERIFICATION_RESEND_RATE_LIMIT_PER_HOUR", "3")
     return int(value)
 
+
 def get_verification_attempt_rate_limit_per_hour() -> int:
     """Get maximum verification attempts per hour per email."""
     value = get_config_value("VERIFICATION_ATTEMPT_RATE_LIMIT_PER_HOUR", "10")
     return int(value)
 
+
 def get_enable_registration_cleanup() -> bool:
     """Get whether to enable automatic cleanup of expired registrations."""
     value = get_config_value("ENABLE_REGISTRATION_CLEANUP", "true")
     return value.lower() == "true"
 
+
 def get_admin_emails() -> list:
     emails_str = get_config_value("ADMIN_EMAILS", "")
     if not emails_str:
         return []
     return [email.strip().lower() for email in emails_str.split(",") if email.strip()]
 
+
 def get_staging_additional_origins() -> list[str]:
     """Get additional CORS origins for staging from environment variables."""
     origins_str = get_config_value("STAGING_ADDITIONAL_ORIGINS", "")
     if not origins_str:
         return []
-    
+
     # Split by comma and clean up whitespace
     origins = [origin.strip() for origin in origins_str.split(",")]
     # Filter out empty strings
     return [origin for origin in origins if origin]
 
+
 # LangSmith Configuration
 def get_langsmith_project() -> str:
     return get_config_value("LANGSMITH_PROJECT", "ardessa-agent")
 
+
 def get_langsmith_tracing_v2() -> bool:
     value = get_config_value("LANGSMITH_TRACING_V2", "true")
     return value.lower() == "true"
 
+
 # Redis Configuration
 def get_redis_host() -> str:
     return get_config_value("REDIS_HOST", "localhost")
 
+
 def get_redis_port() -> int:
     port_str = get_config_value("REDIS_PORT", "6379")
     return int(port_str)
 
+
 def get_redis_password() -> str:
     return get_config_value("REDIS_PASSWORD", "")
 
+
 def get_redis_db() -> int:
     db_str = get_config_value("REDIS_DB", "0")
     return int(db_str)
 
+
 # Clerk Configuration
 def get_clerk_jwks_url() -> Optional[str]:
     """Return the JWKS URL used for verifying Clerk access tokens."""
@@ -472,13 +543,16 @@
     """Return the LP registration Clerk publishable key for frontend initialization."""
     return get_config_value("LP_CLERK_PUBLISHABLE_KEY")
 
+
 # Email Configuration
 def get_mailgun_api_key() -> Optional[str]:
     return get_config_value("MAILGUN_API_KEY")
 
+
 def get_mailgun_domain() -> Optional[str]:
     return get_config_value("MAILGUN_DOMAIN")
 
+
 def get_mailgun_region() -> str:
     return get_config_value("MAILGUN_REGION", "api")
 
@@ -502,10 +576,12 @@
     """Return the reply-to address for transactional emails if configured."""
     return get_config_value("EMAIL_REPLY_TO")
 
+
 # App Configuration
 def get_app_name() -> str:
     return get_config_value("APP_NAME", "Ardessa")
 
+
 def get_backend_url() -> str:
     """
     Get backend URL based on environment.
@@ -515,10 +591,10 @@
     explicit_url = get_config_value("BACKEND_URL")
     if explicit_url:
         return explicit_url
-    
+
     # Get current environment
     environment = get_environment()
-    
+
     # Return environment-specific URLs
     if environment == "production":
         return "https://p3363pvhdk.eu-west-1.awsapprunner.com"
@@ -534,10 +610,10 @@
     explicit_url = get_config_value("FRONTEND_URL") or get_config_value("NEXT_PUBLIC_FRONTEND_URL")
     if explicit_url:
         return explicit_url
-    
+
     # Get current environment
     environment = get_environment()
-    
+
     # Return environment-specific URLs (defaults - should be overridden with env var)
     if environment == "production":
         return "https://dashboard.ardessa.com"
@@ -558,25 +634,30 @@
         default_base = "http://localhost:5173"
     return get_config_value("APP_BASE_URL", default_base)
 
+
 # Environment Configuration
 def get_environment() -> str:
     # Check environment variable first (allows local override)
     env_value = os.getenv("ENVIRONMENT")
     if env_value:
         return env_value
-    
+
     # Fallback to Secrets Manager
     return get_config_value("ENVIRONMENT", "development")
 
+
 def get_log_level() -> str:
     return get_config_value("LOG_LEVEL", "INFO").upper()
 
+
 def get_aws_region() -> str:
     return get_config_value("AWS_REGION", "eu-west-1")
 
+
 def get_alert_email() -> str:
     return get_config_value("ALERT_EMAIL", "")
 
+
 def get_log_retention_days() -> int:
     days_str = get_config_value("LOG_RETENTION_DAYS", "7")
     return int(days_str)
@@ -626,16 +707,16 @@
 def get_tamradar_webhook_url() -> str:
     """
     Get webhook URL to use when creating radars.
-    
+
     This should point to the Lambda API Gateway endpoint that fans out to all environments.
     Set TAMRADAR_WEBHOOK_URL environment variable to your API Gateway URL.
-    
+
     Example: https://your-api-gateway-id.execute-api.eu-west-1.amazonaws.com/webhook?secret=YOUR_TAMRADAR_SECRET
     """
     explicit_url = get_config_value("TAMRADAR_WEBHOOK_URL")
     if explicit_url:
         return explicit_url
-    
+
     # If not set, return empty string (will cause radar creation to fail with clear error)
     logger.warning("TAMRADAR_WEBHOOK_URL not configured - radar creation will fail")
     return ""
@@ -671,14 +752,14 @@
     explicit_url = get_config_value("INTEGRATIONS_REDIRECT_BASE_URL")
     if explicit_url:
         return explicit_url
-    
+
     # Get current environment
     environment = get_environment()
-    
+
     # In staging, use BACKEND_URL from .env
     if environment == "staging":
         return get_backend_url()
-    
+
     # Return environment-specific URLs
     if environment == "production":
         return "https://p3363pvhdk.eu-west-1.awsapprunner.com"

--- app/utils/dashboard_cache.py
+++ app/utils/dashboard_cache.py
@@ -34,40 +34,40 @@
 def get_cached_dashboard(user_id: str) -> Optional[Dict[str, Any]]:
     """
     Get cached dashboard data for a user if it exists and is still valid.
-    
+
     Returns:
         Cached data if valid, None otherwise
     """
     with _cache_lock:
         if user_id not in _cache:
             return None
-        
+
         cache_entry = _cache[user_id]
         cache_time = cache_entry.get("timestamp")
-        
+
         if not cache_time:
             return None
-        
+
         # Check if cache is still valid (within TTL)
         if datetime.now(timezone.utc).replace(tzinfo=None) - cache_time > CACHE_TTL:
             # Cache expired
             del _cache[user_id]
             return None
-        
+
         # Check if a Tier 1/2/3 finding was added after cache was created
         last_tier1_2_finding = cache_entry.get("last_tier1_2_finding")
         if last_tier1_2_finding and last_tier1_2_finding > cache_time:
             # New Tier 1/2/3 finding added, invalidate cache
             del _cache[user_id]
             return None
-        
+
         return cache_entry.get("data")
 
 
 def set_cached_dashboard(user_id: str, data: Dict[str, Any]) -> None:
     """
     Cache dashboard data for a user.
-    
+
     Args:
         user_id: User ID
         data: Dashboard data to cache
@@ -85,7 +85,7 @@
     """
     Mark that a Tier 1, 2, or 3 finding was added for a user.
     This will invalidate their cache on next request.
-    
+
     Args:
         user_id: User ID
     """
@@ -100,4 +100,3 @@
     with _cache_lock:
         _cache.clear()
         logger.info("Cleared all dashboard cache")
-

--- app/utils/db_rate_limit.py
+++ app/utils/db_rate_limit.py
@@ -11,32 +11,33 @@
 from app.utils.global_db import get_global_db
 from app.utils.config import get_environment
 
+
 class DatabaseRateLimiter:
     """Database-based rate limiter for API requests."""
-    
+
     def __init__(self):
         """Initialize the database rate limiter."""
         self.db = None  # Will be set when needed
         self.default_daily_limit = 10
-        
+
     async def _get_db(self):
         """Get the global database instance."""
         if not self.db:
             self.db = await get_global_db()
         return self.db
-    
+
     async def get_user_limit(self, user_id: str) -> int:
         """
         Get the daily request limit for a user.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             Daily request limit for the user
         """
         db = await self._get_db()
-        
+
         async with db.pool.acquire() as conn:
             row = await conn.fetchrow(
                 "SELECT COALESCE(daily_agent_run_limit, $2) AS daily_limit FROM users WHERE user_id = $1",
@@ -44,21 +45,21 @@
                 self.default_daily_limit,
             )
             # If user not found, fall back to default
-            return int(row['daily_limit']) if row else self.default_daily_limit
-    
+            return int(row["daily_limit"]) if row else self.default_daily_limit
+
     async def set_user_limit(self, user_id: str, limit: int) -> bool:
         """
         Set the daily request limit for a user.
-        
+
         Args:
             user_id: User identifier
             limit: Daily request limit
-            
+
         Returns:
             True if successful
         """
         db = await self._get_db()
-        
+
         async with db.pool.acquire() as conn:
             result = await conn.execute(
                 "UPDATE users SET daily_agent_run_limit = $1, updated_at = NOW() WHERE user_id = $2",
@@ -67,154 +68,173 @@
             )
             # result is like 'UPDATE 1' when one row updated
             return result.startswith("UPDATE")
-    
+
     async def get_request_count(self, user_id: str) -> int:
         """
         Get the number of requests made by a user today.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             Number of requests made today
         """
         db = await self._get_db()
         today = datetime.utcnow().date()
-        
+
         async with db.pool.acquire() as conn:
-            row = await conn.fetchrow("""
+            row = await conn.fetchrow(
+                """
                 SELECT count FROM daily_request_counts 
                 WHERE user_id = $1 AND date = $2
-            """, user_id, today)
-            
-            return row['count'] if row else 0
-    
+            """,
+                user_id,
+                today,
+            )
+
+            return row["count"] if row else 0
+
     async def increment_request_count(self, user_id: str) -> int:
         """
         Increment the request count for a user.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             New request count
         """
         db = await self._get_db()
         today = datetime.utcnow().date()
-        
+
         async with db.pool.acquire() as conn:
             # Use UPSERT to handle both insert and update
-            result = await conn.fetchrow("""
+            result = await conn.fetchrow(
+                """
                 INSERT INTO daily_request_counts (user_id, date, count, updated_at)
                 VALUES ($1, $2, 1, NOW())
                 ON CONFLICT (user_id, date) DO UPDATE SET
                     count = daily_request_counts.count + 1,
                     updated_at = NOW()
                 RETURNING count
-            """, user_id, today)
-            
-            return result['count']
-    
+            """,
+                user_id,
+                today,
+            )
+
+            return result["count"]
+
     async def check_rate_limit(self, user_id: str) -> Dict[str, Any]:
         """
         Check if a user has exceeded their rate limit.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             Dictionary with limit information
         """
         limit = await self.get_user_limit(user_id)
         count = await self.get_request_count(user_id)
         remaining = max(0, limit - count)
-        
+
         # Calculate seconds until reset (midnight UTC)
         now = datetime.utcnow()
         midnight = datetime(now.year, now.month, now.day) + timedelta(days=1)
         seconds_until_reset = int((midnight - now).total_seconds())
-        
-        return {
-            "allowed": count < limit,
-            "limit": limit,
-            "remaining": remaining,
-            "reset": seconds_until_reset
-        }
-    
+
+        return {"allowed": count < limit, "limit": limit, "remaining": remaining, "reset": seconds_until_reset}
+
     async def check_window_limit(self, identifier: str, endpoint: str, limit: int, window: int) -> bool:
         """
         Check window-based rate limiting (for router-level limits).
-        
+
         Args:
             identifier: User ID or IP address
             endpoint: API endpoint path
             limit: Maximum requests per window
             window: Window size in seconds
-            
+
         Returns:
             True if request is allowed
         """
         db = await self._get_db()
         now = int(datetime.utcnow().timestamp())
         window_start = now - (now % window)
-        
+
         async with db.pool.acquire() as conn:
             # Get current count for this window
-            row = await conn.fetchrow("""
+            row = await conn.fetchrow(
+                """
                 SELECT count FROM rate_limit_windows 
                 WHERE identifier = $1 AND endpoint = $2 AND window_start = $3
-            """, identifier, endpoint, window_start)
-            
-            current_count = row['count'] if row else 0
-            
+            """,
+                identifier,
+                endpoint,
+                window_start,
+            )
+
+            current_count = row["count"] if row else 0
+
             if current_count >= limit:
                 return False  # Rate limit exceeded
-            
+
             # Increment count
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO rate_limit_windows (identifier, endpoint, window_start, count, updated_at)
                 VALUES ($1, $2, $3, 1, NOW())
                 ON CONFLICT (identifier, endpoint, window_start) DO UPDATE SET
                     count = rate_limit_windows.count + 1,
                     updated_at = NOW()
-            """, identifier, endpoint, window_start)
-            
+            """,
+                identifier,
+                endpoint,
+                window_start,
+            )
+
             return True
-    
+
     async def reset_all_counters(self) -> bool:
         """
         Reset all request counters (admin function).
-        
+
         Returns:
             True if successful
         """
         db = await self._get_db()
         today = datetime.utcnow().date()
-        
+
         async with db.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 DELETE FROM daily_request_counts WHERE date = $1
-            """, today)
-            
+            """,
+                today,
+            )
+
             return True
-    
+
     async def cleanup_old_windows(self, max_age_hours: int = 24) -> int:
         """
         Clean up old window-based rate limit data.
-        
+
         Args:
             max_age_hours: Maximum age in hours to keep data
-            
+
         Returns:
             Number of records deleted
         """
         db = await self._get_db()
         cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)
-        
+
         async with db.pool.acquire() as conn:
-            result = await conn.execute("""
+            result = await conn.execute(
+                """
                 DELETE FROM rate_limit_windows 
                 WHERE created_at < $1
-            """, cutoff_time)
-            
-            return int(result.split()[-1]) if result else 0
\ No newline at end of file
+            """,
+                cutoff_time,
+            )
+
+            return int(result.split()[-1]) if result else 0

--- app/utils/db_utils.py
+++ app/utils/db_utils.py
@@ -31,13 +31,18 @@
 from dotenv import load_dotenv
 
 # Load environment variables
-dotenv_path = Path(__file__).parent.parent / '.env'
+dotenv_path = Path(__file__).parent.parent / ".env"
 load_dotenv(dotenv_path)
 
 # Import prompts
-from app.prompts.data_process_prompts import get_executive_filtering_system_prompt, get_executive_filtering_user_prompt, pre_filter_executives_for_investment_company
+from app.prompts.data_process_prompts import (
+    get_executive_filtering_system_prompt,
+    get_executive_filtering_user_prompt,
+    pre_filter_executives_for_investment_company,
+)
 from app.utils.config import get_postgres_host, get_postgres_port, get_postgres_db, get_postgres_user, get_postgres_password, get_openai_api_key
 
+
 class ProspectingDB:
     """PostgreSQL database connection and query methods."""
 
@@ -57,21 +62,21 @@
         db = get_postgres_db()
         user = get_postgres_user()
         password = get_postgres_password()
-        
+
         return f"postgresql://{user}:{password}@{host}:{port}/{db}"
-    
+
     async def init_pool(self):
         """Initialize the database connection pool with retry logic for connection exhaustion."""
         if self.pool:
             return
-        
+
         import asyncpg
         from app.utils.logging_config import get_logger
-        
+
         logger = get_logger(__name__)
         max_retries = 10
         initial_delay = 5  # Start with 5 seconds
-        
+
         for attempt in range(max_retries):
             try:
                 self.pool = await asyncpg.create_pool(
@@ -86,7 +91,7 @@
                     max_queries=10000,  # Recycle connections after 10k queries to prevent stale connections
                     command_timeout=30,  # 30-second query timeout to prevent long-running queries from holding connections
                 )
-                
+
                 # Create tables with session_id support
                 # Use a connection from the pool, but handle TooManyConnectionsError
                 try:
@@ -101,33 +106,27 @@
                         await self.pool.close()
                         self.pool = None
                     raise
-                
+
                 await self._create_tables()
-                
-                logger.info("PostgreSQL connection pool initialized successfully", extra={
-                    "max_size": 3,
-                    "min_size": 0,
-                    "attempt": attempt + 1
-                })
+
+                logger.info("PostgreSQL connection pool initialized successfully", extra={"max_size": 3, "min_size": 0, "attempt": attempt + 1})
                 print("‚úÖ PostgreSQL connection pool initialized")
                 return
-                
+
             except asyncpg.exceptions.TooManyConnectionsError as e:
                 if attempt < max_retries - 1:
                     # Exponential backoff: 5s, 10s, 20s, 40s, 80s, 160s, 320s, 640s, 1280s, 2560s
-                    delay = initial_delay * (2 ** attempt)
+                    delay = initial_delay * (2**attempt)
                     logger.warning(
-                        f"TooManyConnectionsError on pool creation (attempt {attempt + 1}/{max_retries}), "
-                        f"retrying in {delay}s",
-                        extra={"attempt": attempt + 1, "delay": delay, "error": str(e)}
+                        f"TooManyConnectionsError on pool creation (attempt {attempt + 1}/{max_retries}), retrying in {delay}s",
+                        extra={"attempt": attempt + 1, "delay": delay, "error": str(e)},
                     )
                     await asyncio.sleep(delay)
                 else:
                     # Last attempt failed - try with even more conservative settings
                     logger.error(
-                        f"TooManyConnectionsError after {max_retries} attempts, "
-                        f"trying with min_size=0 as fallback",
-                        extra={"attempt": attempt + 1, "error": str(e)}
+                        f"TooManyConnectionsError after {max_retries} attempts, trying with min_size=0 as fallback",
+                        extra={"attempt": attempt + 1, "error": str(e)},
                     )
                     try:
                         # Final fallback: create pool with min_size=0 (no pre-allocated connections)
@@ -144,29 +143,21 @@
                             command_timeout=30,
                         )
                         await self._create_tables()
-                        logger.warning(
-                            "PostgreSQL connection pool initialized with fallback settings",
-                            extra={"max_size": 2, "min_size": 0}
-                        )
+                        logger.warning("PostgreSQL connection pool initialized with fallback settings", extra={"max_size": 2, "min_size": 0})
                         print("‚ö†Ô∏è PostgreSQL connection pool initialized with fallback settings (max_size=2)")
                         return
                     except Exception as fallback_error:
-                        logger.error(
-                            f"Failed to initialize PostgreSQL pool even with fallback settings: {fallback_error}",
-                            exc_info=True
-                        )
+                        logger.error(f"Failed to initialize PostgreSQL pool even with fallback settings: {fallback_error}", exc_info=True)
                         print(f"‚ùå Failed to initialize PostgreSQL pool: {fallback_error}")
                         raise
-                        
+
             except Exception as e:
                 logger.error(
-                    f"Failed to initialize PostgreSQL pool: {e}",
-                    extra={"attempt": attempt + 1, "error_type": type(e).__name__},
-                    exc_info=True
+                    f"Failed to initialize PostgreSQL pool: {e}", extra={"attempt": attempt + 1, "error_type": type(e).__name__}, exc_info=True
                 )
                 print(f"‚ùå Failed to initialize PostgreSQL pool: {e}")
                 raise
-    
+
     async def _create_tables(self):
         """Create all necessary tables with session_id support."""
         # Ensure pgvector extension exists
@@ -195,7 +186,7 @@
                     updated_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Create user_profiles table if not exists
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS user_profiles (
@@ -210,11 +201,11 @@
                     updated_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Ensure first_name and last_name columns exist (for existing deployments)
             await conn.execute("ALTER TABLE user_profiles ADD COLUMN IF NOT EXISTS first_name TEXT")
             await conn.execute("ALTER TABLE user_profiles ADD COLUMN IF NOT EXISTS last_name TEXT")
-            
+
             # Create user_profiles_history table if not exists
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS user_profiles_history (
@@ -228,11 +219,11 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Ensure first_name and last_name columns exist in history table (for existing deployments)
             await conn.execute("ALTER TABLE user_profiles_history ADD COLUMN IF NOT EXISTS first_name TEXT")
             await conn.execute("ALTER TABLE user_profiles_history ADD COLUMN IF NOT EXISTS last_name TEXT")
-            
+
             # Create index on user_id for efficient history retrieval
             await conn.execute("""
                 CREATE INDEX IF NOT EXISTS idx_user_profiles_history_user_id
@@ -244,7 +235,7 @@
                 CREATE INDEX IF NOT EXISTS idx_user_profiles_investor_profile_gin
                 ON user_profiles USING GIN (investor_profile)
             """)
-            
+
             # Create profiles_embeddings table for storing profile embeddings
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS profiles_embeddings (
@@ -259,7 +250,7 @@
                     PRIMARY KEY (user_id, embedding_purpose)
                 )
             """)
-            
+
             # Create indexes for profiles_embeddings
             await conn.execute("""
                 CREATE INDEX IF NOT EXISTS profiles_embeddings_purpose_version_idx
@@ -269,7 +260,7 @@
                 CREATE INDEX IF NOT EXISTS idx_profiles_embeddings_updated_at
                 ON profiles_embeddings(updated_at)
             """)
-            
+
             # Create sessions table
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS sessions (
@@ -284,7 +275,7 @@
                     updated_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Create session_interactions table
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS session_interactions (
@@ -300,7 +291,7 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Create approved_users table with enhanced fields
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS approved_users (
@@ -311,7 +302,7 @@
                     is_active BOOLEAN DEFAULT TRUE
                 )
             """)
-            
+
             # Create ria_user_activity table for tracking RIA search and view activity
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS ria_user_activity (
@@ -329,7 +320,7 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Create indexes for ria_user_activity for efficient querying
             await conn.execute("""
                 CREATE INDEX IF NOT EXISTS idx_ria_user_activity_user_id
@@ -351,7 +342,7 @@
                 CREATE INDEX IF NOT EXISTS idx_ria_user_activity_company_crd
                 ON ria_user_activity(company_crd)
             """)
-            
+
             # Create prospecting_runs table with session_id
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS prospecting_runs (
@@ -378,7 +369,7 @@
                 CREATE INDEX IF NOT EXISTS idx_runs_user_time
                 ON prospecting_runs(user_id, start_time)
             """)
-            
+
             # Create agent_results table with session_id
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS agent_results (
@@ -394,7 +385,7 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Create company_search_results table with session_id
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS company_search_results (
@@ -409,7 +400,7 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             # Create person_enrichment_results table with session_id
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS person_enrichment_results (
@@ -494,7 +485,7 @@
                 CREATE INDEX IF NOT EXISTS idx_youtube_embeddings_video
                 ON youtube_transcript_embeddings(video_id)
             """)
-            
+
             # Create companies table with session_id
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS companies (
@@ -512,7 +503,7 @@
                     UNIQUE(run_id, company_id, user_id)
                 )
             """)
-            
+
             # Create company_search_feedback table
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS company_search_feedback (
@@ -534,7 +525,7 @@
                     metadata JSONB
                 )
             """)
-            
+
             # Create user_saved_companies table
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS user_saved_companies (
@@ -555,20 +546,22 @@
                     metadata JSONB
                 )
             """)
-            
+
             # Create indexes for company_search_feedback
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_feedback_user_id ON company_search_feedback(user_id)")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_feedback_run_id ON company_search_feedback(run_id)")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_feedback_selected_at ON company_search_feedback(selected_at)")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_feedback_types ON company_search_feedback USING GIN (feedback_types)")
-            
+
             # Create indexes for user_saved_companies
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_saved_companies_user_id ON user_saved_companies(user_id)")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_saved_companies_category ON user_saved_companies(category) WHERE is_archived = FALSE")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_saved_companies_saved_at ON user_saved_companies(saved_at) WHERE is_archived = FALSE")
             # Unique partial index to prevent duplicate active saves (replaces UNIQUE constraint with WHERE)
-            await conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_saved_companies_unique_active ON user_saved_companies(user_id, company_domain) WHERE is_archived = FALSE")
-            
+            await conn.execute(
+                "CREATE UNIQUE INDEX IF NOT EXISTS idx_saved_companies_unique_active ON user_saved_companies(user_id, company_domain) WHERE is_archived = FALSE"
+            )
+
             # Create user_feedback table
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS user_feedback (
@@ -582,7 +575,7 @@
                     metadata JSONB
                 )
             """)
-            
+
             # Create rate limiting tables
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS user_rate_limits (
@@ -594,7 +587,7 @@
                     UNIQUE(user_id)
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS daily_request_counts (
                     id SERIAL PRIMARY KEY,
@@ -606,7 +599,7 @@
                     UNIQUE(user_id, date)
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS rate_limit_windows (
                     id SERIAL PRIMARY KEY,
@@ -619,23 +612,23 @@
                     UNIQUE(identifier, endpoint, window_start)
                 )
             """)
-            
+
             # Create indexes for rate limiting
             await conn.execute("""
                 CREATE INDEX IF NOT EXISTS idx_user_rate_limits_user_id 
                 ON user_rate_limits(user_id)
             """)
-            
+
             await conn.execute("""
                 CREATE INDEX IF NOT EXISTS idx_daily_request_counts_user_date 
                 ON daily_request_counts(user_id, date)
             """)
-            
+
             await conn.execute("""
                 CREATE INDEX IF NOT EXISTS idx_rate_limit_windows_identifier_endpoint 
                 ON rate_limit_windows(identifier, endpoint)
             """)
-            
+
             # Create TAMradar tables
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS tamradar_radars (
@@ -656,7 +649,7 @@
                     deactivated_at TIMESTAMP
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS tamradar_user_radars (
                     id SERIAL PRIMARY KEY,
@@ -671,7 +664,7 @@
                     UNIQUE(user_id, radar_id)
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS tamradar_webhook_events (
                     id SERIAL PRIMARY KEY,
@@ -686,7 +679,7 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS tamradar_findings (
                     id SERIAL PRIMARY KEY,
@@ -700,7 +693,7 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS tamradar_user_findings (
                     id SERIAL PRIMARY KEY,
@@ -716,7 +709,7 @@
                     UNIQUE(user_id, finding_id)
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS tamradar_balance_alerts (
                     id SERIAL PRIMARY KEY,
@@ -727,7 +720,7 @@
                     created_at TIMESTAMP DEFAULT NOW()
                 )
             """)
-            
+
             await conn.execute("""
                 CREATE TABLE IF NOT EXISTS tamradar_user_deletions (
                     id SERIAL PRIMARY KEY,
@@ -737,7 +730,7 @@
                     UNIQUE(user_id, radar_id, deleted_at)
                 )
             """)
-            
+
             # Create indexes for TAMradar tables
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_tamradar_radars_category ON tamradar_radars(radar_category)")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_tamradar_radars_status ON tamradar_radars(status)")
@@ -749,9 +742,9 @@
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_tamradar_findings_radar_id ON tamradar_findings(radar_id)")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_tamradar_user_findings_user_id ON tamradar_user_findings(user_id)")
             await conn.execute("CREATE INDEX IF NOT EXISTS idx_tamradar_user_findings_radar_id ON tamradar_user_findings(radar_id)")
-            
+
             print("‚úÖ Database tables created with session_id support")
-            
+
             # Now create indexes conditionally
             await self._create_indexes_conditionally()
 
@@ -765,7 +758,7 @@
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_sessions_last_activity ON sessions(last_activity)")
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_session_interactions_session_id ON session_interactions(session_id)")
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_session_interactions_run_id ON session_interactions(run_id)")
-                
+
                 # Check if session_id column exists before creating indexes
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
@@ -773,56 +766,58 @@
                 """)
                 if columns:
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_prospecting_runs_session_id ON prospecting_runs(session_id)")
-                
+
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_prospecting_runs_user_id ON prospecting_runs(user_id)")
-                
+
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'agent_results' AND column_name = 'session_id'
                 """)
                 if columns:
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_agent_results_session_id ON agent_results(session_id)")
-                
+
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_agent_results_run_id ON agent_results(run_id)")
-                
+
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'company_search_results' AND column_name = 'session_id'
                 """)
                 if columns:
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_company_search_results_session_id ON company_search_results(session_id)")
-                
+
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_company_search_results_run_id ON company_search_results(run_id)")
-                
+
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'person_enrichment_results' AND column_name = 'session_id'
                 """)
                 if columns:
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_person_enrichment_results_session_id ON person_enrichment_results(session_id)")
-                
+
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_person_enrichment_results_run_id ON person_enrichment_results(run_id)")
-                
+
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'companies' AND column_name = 'session_id'
                 """)
                 if columns:
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_companies_session_id ON companies(session_id)")
-                
+
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_companies_run_id ON companies(run_id)")
-                
+
                 # Create indexes for rate limiting tables
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_daily_request_counts_user_date ON daily_request_counts(user_id, date)")
-                await conn.execute("CREATE INDEX IF NOT EXISTS idx_rate_limit_windows_identifier_endpoint ON rate_limit_windows(identifier, endpoint)")
+                await conn.execute(
+                    "CREATE INDEX IF NOT EXISTS idx_rate_limit_windows_identifier_endpoint ON rate_limit_windows(identifier, endpoint)"
+                )
                 await conn.execute("CREATE INDEX IF NOT EXISTS idx_rate_limit_windows_created_at ON rate_limit_windows(created_at)")
-                
+
                 print("‚úÖ Database indexes created")
-                
+
             except Exception as e:
                 print(f"‚ö†Ô∏è Index creation failed: {e}")
                 # Continue anyway - indexes are not critical for basic functionality
-    
+
     async def _migrate_existing_tables(self):
         """Migrate existing tables to add session_id columns if they don't exist."""
         async with self.pool.acquire() as conn:
@@ -832,37 +827,37 @@
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'prospecting_runs' AND column_name = 'session_id'
                 """)
-                
+
                 if not columns:
                     print("üîÑ Adding session_id column to prospecting_runs table...")
                     await conn.execute("ALTER TABLE prospecting_runs ADD COLUMN session_id VARCHAR(255)")
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_prospecting_runs_session_id ON prospecting_runs(session_id)")
                     print("‚úÖ Added session_id column to prospecting_runs table")
-                
+
                 # Check if agent_results table has session_id column
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'agent_results' AND column_name = 'session_id'
                 """)
-                
+
                 if not columns:
                     print("üîÑ Adding session_id column to agent_results table...")
                     await conn.execute("ALTER TABLE agent_results ADD COLUMN session_id VARCHAR(255)")
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_agent_results_session_id ON agent_results(session_id)")
                     print("‚úÖ Added session_id column to agent_results table")
-                
+
                 # Check if company_search_results table has session_id column
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'company_search_results' AND column_name = 'session_id'
                 """)
-                
+
                 if not columns:
                     print("üîÑ Adding session_id column to company_search_results table...")
                     await conn.execute("ALTER TABLE company_search_results ADD COLUMN session_id VARCHAR(255)")
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_company_search_results_session_id ON company_search_results(session_id)")
                     print("‚úÖ Added session_id column to company_search_results table")
-                
+
                 # Check if company_search_results table has search_query column
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
@@ -872,7 +867,7 @@
                     print("üîÑ Adding search_query column to company_search_results table...")
                     await conn.execute("ALTER TABLE company_search_results ADD COLUMN search_query TEXT")
                     print("‚úÖ Added search_query column to company_search_results table")
-                
+
                 # Check if company_search_results table has companies column (to remove it)
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
@@ -882,41 +877,50 @@
                     print("üîÑ Removing redundant companies column from company_search_results table...")
                     await conn.execute("ALTER TABLE company_search_results DROP COLUMN companies")
                     print("‚úÖ Removed redundant companies column from company_search_results table")
-                
+
                 # Check if person_enrichment_results table has session_id column
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'person_enrichment_results' AND column_name = 'session_id'
                 """)
-                
+
                 if not columns:
                     print("üîÑ Adding session_id column to person_enrichment_results table...")
                     await conn.execute("ALTER TABLE person_enrichment_results ADD COLUMN session_id VARCHAR(255)")
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_person_enrichment_results_session_id ON person_enrichment_results(session_id)")
                     print("‚úÖ Added session_id column to person_enrichment_results table")
-                
+
                 # Check if companies table has session_id column
                 columns = await conn.fetch("""
                     SELECT column_name FROM information_schema.columns 
                     WHERE table_name = 'companies' AND column_name = 'session_id'
                 """)
-                
+
                 if not columns:
                     print("üîÑ Adding session_id column to companies table...")
                     await conn.execute("ALTER TABLE companies ADD COLUMN session_id VARCHAR(255)")
                     await conn.execute("CREATE INDEX IF NOT EXISTS idx_companies_session_id ON companies(session_id)")
                     print("‚úÖ Added session_id column to companies table")
-                
+
                 print("‚úÖ Database migration completed")
-                
+
             except Exception as e:
                 print(f"‚ö†Ô∏è Database migration failed: {e}")
                 # Continue anyway - the tables might already be in the correct state
-    
-    async def start_run(self, run_id: str, user_id: str, session_id: str, search_params: Dict[str, Any] = None, company_name: str = None, company_id: str = None, workflow_type: str = None) -> bool:
+
+    async def start_run(
+        self,
+        run_id: str,
+        user_id: str,
+        session_id: str,
+        search_params: Dict[str, Any] = None,
+        company_name: str = None,
+        company_id: str = None,
+        workflow_type: str = None,
+    ) -> bool:
         """
         Start a new prospecting run.
-        
+
         Args:
             run_id: Unique run identifier
             user_id: User identifier
@@ -924,13 +928,13 @@
             search_params: Search parameters (investor_type, location, etc.) - for search workflows
             company_name: Company name being researched (NULL during search, filled when company selected)
             company_id: Company identifier (NULL during search, filled when company selected)
-            
+
         Returns:
             True if run was started successfully
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
             await conn.execute(
                 """
@@ -938,19 +942,25 @@
                 VALUES ($1, $2, $3, $4, $5, $6, 'searching', $7, NOW(), NOW(), NOW())
                 ON CONFLICT (run_id) DO NOTHING
                 """,
-                run_id, user_id, session_id, json.dumps(search_params) if search_params else None, company_name, company_id, workflow_type
+                run_id,
+                user_id,
+                session_id,
+                json.dumps(search_params) if search_params else None,
+                company_name,
+                company_id,
+                workflow_type,
             )
             return True
 
     async def update_run_company(self, run_id: str, company_name: str, company_id: str) -> bool:
         """
         Update prospecting run with selected company information.
-        
+
         Args:
             run_id: Run identifier
             company_name: Selected company name
             company_id: Selected company identifier
-            
+
         Returns:
             True if run was updated successfully
         """
@@ -958,23 +968,28 @@
             await self.init_pool()
 
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 UPDATE prospecting_runs 
                 SET company_name = $1, company_id = $2, status = 'company_selected', updated_at = NOW()
                 WHERE run_id = $3
-            """, company_name, company_id, run_id)
-            
+            """,
+                company_name,
+                company_id,
+                run_id,
+            )
+
             print(f"‚úÖ Updated prospecting run {run_id} with company: {company_name} (ID: {company_id})")
             return True
 
     async def update_run_status(self, run_id: str, status: str) -> bool:
         """
         Update prospecting run status.
-        
+
         Args:
             run_id: Run identifier
             status: New status ('searching', 'company_selected', 'processing', 'completed')
-            
+
         Returns:
             True if run was updated successfully
         """
@@ -982,21 +997,32 @@
             await self.init_pool()
 
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 UPDATE prospecting_runs 
                 SET status = $1, updated_at = NOW()
                 WHERE run_id = $2
-            """, status, run_id)
-            
+            """,
+                status,
+                run_id,
+            )
+
             print(f"‚úÖ Updated prospecting run {run_id} status to: {status}")
             return True
 
-    async def store_agent_result(self, run_id: str, user_id: str, session_id: str, agent_name: str, 
-                                result_data: Dict[str, Any], company_id: str = None, 
-                                execution_time_ms: int = None) -> bool:
+    async def store_agent_result(
+        self,
+        run_id: str,
+        user_id: str,
+        session_id: str,
+        agent_name: str,
+        result_data: Dict[str, Any],
+        company_id: str = None,
+        execution_time_ms: int = None,
+    ) -> bool:
         """
         Store agent execution result.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
@@ -1005,40 +1031,50 @@
             result_data: Agent result data
             company_id: Company identifier (optional)
             execution_time_ms: Execution time in milliseconds (optional)
-        
+
         Returns:
             True if result was stored successfully
         """
         if not self.pool:
             await self.init_pool()
-        
+
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO agent_results (run_id, session_id, user_id, agent_name, company_id, result_data, execution_time_ms)
                 VALUES ($1, $2, $3, $4, $5, $6, $7)
-            """, run_id, session_id, user_id, agent_name, company_id, json.dumps(result_data, default=str), execution_time_ms)
-            
+            """,
+                run_id,
+                session_id,
+                user_id,
+                agent_name,
+                company_id,
+                json.dumps(result_data, default=str),
+                execution_time_ms,
+            )
+
             print(f"‚úÖ Stored {agent_name} result for run: {run_id} (session: {session_id})")
             return True
 
-    async def get_agent_data_by_company_id(self, company_id: str, agent_name: str, run_id: str, 
-                                          data_key: str = None, user_id: str = None) -> Dict[str, Any]:
+    async def get_agent_data_by_company_id(
+        self, company_id: str, agent_name: str, run_id: str, data_key: str = None, user_id: str = None
+    ) -> Dict[str, Any]:
         """
         Get agent data by company_id, agent_name, and optional data_key.
-        
+
         Args:
             company_id: Company identifier
             agent_name: Name of the agent
             run_id: Run identifier
             data_key: Specific key within result_data to extract (optional)
             user_id: User identifier (optional, for additional filtering)
-        
+
         Returns:
             Dictionary containing agent data or None if not found
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
             # Build query with optional user_id filter
             query = """
@@ -1046,37 +1082,37 @@
                 WHERE company_id = $1 AND agent_name = $2 AND run_id = $3
             """
             params = [company_id, agent_name, run_id]
-            
+
             if user_id:
                 query += " AND user_id = $4"
                 params.append(user_id)
-            
+
             # Use the correct column name based on the schema
             query += " ORDER BY id DESC LIMIT 1"
-            
+
             row = await conn.fetchrow(query, *params)
-            
-            if row and row['result_data']:
+
+            if row and row["result_data"]:
                 # Parse the JSON string into a dictionary
-                if isinstance(row['result_data'], str):
-                    result_data = json.loads(row['result_data'])
+                if isinstance(row["result_data"], str):
+                    result_data = json.loads(row["result_data"])
                 else:
-                    result_data = row['result_data']
-                
+                    result_data = row["result_data"]
+
                 # If data_key is specified, extract that specific key
                 if data_key and isinstance(result_data, dict):
                     return result_data.get(data_key)
-                
+
                 return result_data
-            
+
             return None
 
-    async def store_company_search_results(self, run_id: str, user_id: str, session_id: str, 
-                                          search_query: str, search_results: Dict[str, Any], 
-                                          execution_time_ms: int = None) -> bool:
+    async def store_company_search_results(
+        self, run_id: str, user_id: str, session_id: str, search_query: str, search_results: Dict[str, Any], execution_time_ms: int = None
+    ) -> bool:
         """
         Store company search results.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
@@ -1084,59 +1120,76 @@
             search_query: Original search query
             search_results: Full search results (including companies)
             execution_time_ms: Execution time in milliseconds (optional)
-        
+
         Returns:
             True if results were stored successfully
         """
         if not self.pool:
             await self.init_pool()
-        
+
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO company_search_results (run_id, session_id, user_id, search_query, search_results, execution_time_ms)
                 VALUES ($1, $2, $3, $4, $5, $6)
-            """, run_id, session_id, user_id, search_query, json.dumps(search_results), execution_time_ms)
-            
+            """,
+                run_id,
+                session_id,
+                user_id,
+                search_query,
+                json.dumps(search_results),
+                execution_time_ms,
+            )
+
             print(f"‚úÖ Stored company search results for run: {run_id} (session: {session_id})")
             return True
 
     async def get_company_search_results(self, user_id: str, run_id: str, session_id: str = None) -> List[Dict[str, Any]]:
         """
         Get company search results.
-        
+
         Args:
             user_id: User identifier
             run_id: Run identifier
             session_id: Session identifier (optional)
-            
+
         Returns:
             List of search result dictionaries
         """
         if not self.pool:
             await self.init_pool()
-        
+
         async with self.pool.acquire() as conn:
             # First try to find results with exact session_id match
             if session_id:
-                rows = await conn.fetch("""
+                rows = await conn.fetch(
+                    """
                     SELECT * FROM company_search_results 
                     WHERE user_id = $1 AND run_id = $2 AND session_id = $3
                         ORDER BY created_at DESC
-                """, user_id, run_id, session_id)
-                
+                """,
+                    user_id,
+                    run_id,
+                    session_id,
+                )
+
                 if rows:
                     print(f"‚úÖ Found {len(rows)} search results with session_id: {session_id}")
                     return [dict(row) for row in rows]
                 else:
                     print(f"‚ö†Ô∏è No results found with session_id: {session_id}, trying without session_id")
-            
+
             # Fallback: try without session_id (for backward compatibility)
-            rows = await conn.fetch("""
+            rows = await conn.fetch(
+                """
                 SELECT * FROM company_search_results 
                 WHERE user_id = $1 AND run_id = $2
                 ORDER BY created_at DESC
-            """, user_id, run_id)
-            
+            """,
+                user_id,
+                run_id,
+            )
+
             if rows:
                 print(f"‚úÖ Found {len(rows)} search results without session_id")
                 return [dict(row) for row in rows]
@@ -1144,13 +1197,22 @@
                 print(f"‚ùå No search results found for user_id: {user_id}, run_id: {run_id}")
                 return []
 
-    async def store_person_enrichment_result(self, run_id: str, user_id: str, session_id: str, 
-                                           person_id: str, person_name: str, company_name: str, 
-                                           company_id: str, enrichment_data: Dict[str, Any], 
-                                           person_rank: int = None, execution_time_ms: int = None) -> bool:
+    async def store_person_enrichment_result(
+        self,
+        run_id: str,
+        user_id: str,
+        session_id: str,
+        person_id: str,
+        person_name: str,
+        company_name: str,
+        company_id: str,
+        enrichment_data: Dict[str, Any],
+        person_rank: int = None,
+        execution_time_ms: int = None,
+    ) -> bool:
         """
         Store person enrichment result.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
@@ -1162,99 +1224,117 @@
             enrichment_data: Enrichment result data
             person_rank: Person rank/priority (optional)
             execution_time_ms: Execution time in milliseconds (optional)
-            
+
         Returns:
             True if result was stored successfully
         """
         if not self.pool:
             await self.init_pool()
-        
+
         # Validate required NOT NULL fields
         if not run_id:
             print("‚ùå run_id is required but not provided")
             return False
-        
+
         if not user_id:
             print("‚ùå user_id is required but not provided")
             return False
-        
+
         if not session_id:
             print("‚ùå session_id is required but not provided")
             return False
-        
+
         if not company_id:
             print("‚ùå company_id is required but not provided")
             return False
-        
+
         if not person_id:
             print("‚ùå person_id is required but not provided")
             return False
-        
+
         # Ensure person_name has a default value
         if not person_name:
             person_name = "Unknown"
-        
+
         # Ensure company_name has a default value
         if not company_name:
             company_name = "Unknown"
-        
+
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO person_enrichment_results (run_id, session_id, user_id, person_id, person_name, company_name, company_id, person_rank, enrichment_data, execution_time_ms)
                 VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
-            """, run_id, session_id, user_id, person_id, 
-                person_name, company_name, company_id, person_rank, json.dumps(enrichment_data), execution_time_ms)
-            
+            """,
+                run_id,
+                session_id,
+                user_id,
+                person_id,
+                person_name,
+                company_name,
+                company_id,
+                person_rank,
+                json.dumps(enrichment_data),
+                execution_time_ms,
+            )
+
             print(f"‚úÖ Stored person enrichment result for run: {run_id} (session: {session_id})")
             return True
 
-    async def store_company(self, run_id: str, user_id: str, session_id: str, company_data: Dict[str, Any], 
-                           company_id: str) -> bool:
+    async def store_company(self, run_id: str, user_id: str, session_id: str, company_data: Dict[str, Any], company_id: str) -> bool:
         """
         Store company information.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
             session_id: Session identifier
             company_data: Company data dictionary
             company_id: Company identifier
-            
+
         Returns:
             True if company was stored successfully
         """
         if not self.pool:
             await self.init_pool()
-        
+
         # Validate required NOT NULL fields
         if not run_id:
             print("‚ùå run_id is required but not provided")
             return False
-        
+
         if not user_id:
             print("‚ùå user_id is required but not provided")
             return False
-        
+
         if not session_id:
             print("‚ùå session_id is required but not provided")
             return False
-        
+
         if not company_id:
             print("‚ùå company_id is required but not provided")
             return False
-        
+
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO companies (run_id, session_id, user_id, company_id, name, website_url, location, focus_area, company_data)
                 VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                 ON CONFLICT (run_id, company_id, user_id) DO UPDATE SET
                     company_data = EXCLUDED.company_data,
                     updated_at = NOW()
-            """, run_id, session_id, user_id, company_id, 
-                company_data.get('name'), company_data.get('website_url'), 
-                company_data.get('location'), company_data.get('focus_area'), 
-                json.dumps(company_data))
-            
+            """,
+                run_id,
+                session_id,
+                user_id,
+                company_id,
+                company_data.get("name"),
+                company_data.get("website_url"),
+                company_data.get("location"),
+                company_data.get("focus_area"),
+                json.dumps(company_data),
+            )
+
             print(f"‚úÖ Stored company {company_data.get('name')} for run: {run_id} (session: {session_id})")
             return True
 
@@ -1277,75 +1357,96 @@
 
         Args:
             session_id: Session identifier
-            
+
         Returns:
             Dictionary containing all session data
         """
         if not self.pool:
             await self.init_pool()
-        
+
         async with self.pool.acquire() as conn:
             # Get session info
-            session = await conn.fetchrow("""
+            session = await conn.fetchrow(
+                """
                 SELECT * FROM sessions WHERE session_id = $1
-            """, session_id)
-            
+            """,
+                session_id,
+            )
+
             if not session:
                 return None
-            
+
             # Get session interactions
-            interactions = await conn.fetch("""
+            interactions = await conn.fetch(
+                """
                 SELECT * FROM session_interactions WHERE session_id = $1 ORDER BY interaction_number
-            """, session_id)
-            
+            """,
+                session_id,
+            )
+
             # Get prospecting runs
-            runs = await conn.fetch("""
+            runs = await conn.fetch(
+                """
                 SELECT * FROM prospecting_runs WHERE session_id = $1 ORDER BY start_time
-            """, session_id)
-            
+            """,
+                session_id,
+            )
+
             # Get agent results
-            agent_results = await conn.fetch("""
+            agent_results = await conn.fetch(
+                """
                 SELECT * FROM agent_results WHERE session_id = $1 ORDER BY created_at
-            """, session_id)
-            
+            """,
+                session_id,
+            )
+
             # Get company search results
-            search_results = await conn.fetch("""
+            search_results = await conn.fetch(
+                """
                 SELECT * FROM company_search_results WHERE session_id = $1 ORDER BY created_at
-            """, session_id)
-            
+            """,
+                session_id,
+            )
+
             # Get person enrichment results
-            person_results = await conn.fetch("""
+            person_results = await conn.fetch(
+                """
                 SELECT * FROM person_enrichment_results WHERE session_id = $1 ORDER BY created_at
-            """, session_id)
-            
+            """,
+                session_id,
+            )
+
             # Get companies
-            companies = await conn.fetch("""
+            companies = await conn.fetch(
+                """
                 SELECT * FROM companies WHERE session_id = $1 ORDER BY created_at
-            """, session_id)
-            
+            """,
+                session_id,
+            )
+
             return {
-                'session': dict(session),
-                'interactions': [dict(row) for row in interactions],
-                'runs': [dict(row) for row in runs],
-                'agent_results': [dict(row) for row in agent_results],
-                'search_results': [dict(row) for row in search_results],
-                'person_results': [dict(row) for row in person_results],
-                'companies': [dict(row) for row in companies]
+                "session": dict(session),
+                "interactions": [dict(row) for row in interactions],
+                "runs": [dict(row) for row in runs],
+                "agent_results": [dict(row) for row in agent_results],
+                "search_results": [dict(row) for row in search_results],
+                "person_results": [dict(row) for row in person_results],
+                "companies": [dict(row) for row in companies],
             }
 
     async def cleanup_session_data(self, session_id: str) -> bool:
         """
         Clean up all data for a specific session.
-        
+
         Args:
             session_id: Session identifier
-            
+
         Returns:
             True if cleanup was successful
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
             try:
                 # Delete all session-related data
@@ -1356,79 +1457,90 @@
                 await conn.execute("DELETE FROM prospecting_runs WHERE session_id = $1", session_id)
                 await conn.execute("DELETE FROM session_interactions WHERE session_id = $1", session_id)
                 await conn.execute("DELETE FROM sessions WHERE session_id = $1", session_id)
-                
+
                 print(f"‚úÖ Cleaned up all data for session: {session_id}")
                 return True
-            
+
             except Exception as e:
                 print(f"‚ùå Failed to cleanup session data: {e}")
                 return False
 
-    async def store_person_enrichment_batch(self, run_id: str, user_id: str, session_id: str, 
-                                          company_id: str, person_enrichment_results: List[Dict[str, Any]]) -> List[str]:
+    async def store_person_enrichment_batch(
+        self, run_id: str, user_id: str, session_id: str, company_id: str, person_enrichment_results: List[Dict[str, Any]]
+    ) -> List[str]:
         """
         Store multiple person enrichment results in batch.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
             session_id: Session identifier
             company_id: Company identifier
             person_enrichment_results: List of person enrichment result dictionaries
-            
+
         Returns:
             List of stored result IDs
         """
         if not self.pool:
             await self.init_pool()
-            
+
         stored_ids = []
-        
+
         async with self.pool.acquire() as conn:
             for person_result in person_enrichment_results:
                 try:
                     # Extract fields from person enrichment result structure
                     # The person enrichment agent uses 'member_full_name' not 'full_name'
-                    person_name = person_result.get('member_full_name', person_result.get('full_name', 'Unknown'))
-                    person_rank = person_result.get('rank')
-                    
+                    person_name = person_result.get("member_full_name", person_result.get("full_name", "Unknown"))
+                    person_rank = person_result.get("rank")
+
                     # Generate a unique person_id if not provided
-                    person_id = person_result.get('person_id')
+                    person_id = person_result.get("person_id")
                     if not person_id:
                         # Try to extract from parent_id or generate a new one
-                        parent_id = person_result.get('parent_id')
+                        parent_id = person_result.get("parent_id")
                         if parent_id:
                             person_id = f"person_{parent_id}"
                         else:
                             person_id = f"person_{uuid.uuid4().hex[:8]}"
-                    
+
                     # Ensure company_id is provided
                     if not company_id:
                         print(f"‚ö†Ô∏è company_id is required but not provided, skipping person: {person_name}")
                         continue
-                    
+
                     # Ensure run_id is provided
                     if not run_id:
                         print(f"‚ö†Ô∏è run_id is required but not provided, skipping person: {person_name}")
                         continue
-                    
+
                     # Ensure user_id is provided
                     if not user_id:
                         print(f"‚ö†Ô∏è user_id is required but not provided, skipping person: {person_name}")
                         continue
-                    
-                    result = await conn.fetchval("""
+
+                    result = await conn.fetchval(
+                        """
                         INSERT INTO person_enrichment_results 
                         (run_id, session_id, user_id, person_id, person_name, company_name, company_id, person_rank, enrichment_data, execution_time_ms)
                         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                         RETURNING id
-                    """, run_id, session_id, user_id, person_id, 
-                        person_name, None, company_id, person_rank,  # company_name is None since it's not in the result
-                        json.dumps(person_result), person_result.get('processing_time_ms'))
-                    
+                    """,
+                        run_id,
+                        session_id,
+                        user_id,
+                        person_id,
+                        person_name,
+                        None,
+                        company_id,
+                        person_rank,  # company_name is None since it's not in the result
+                        json.dumps(person_result),
+                        person_result.get("processing_time_ms"),
+                    )
+
                     stored_ids.append(str(result))
                     print(f"‚úÖ Stored person enrichment result: {person_name} (ID: {person_id})")
-                
+
                 except Exception as e:
                     print(f"‚ö†Ô∏è Failed to store person enrichment result for {person_name}: {e}")
                     continue
@@ -1439,18 +1551,19 @@
     async def store_session(self, session_data: Dict[str, Any]) -> bool:
         """
         Store or update session data.
-        
+
         Args:
             session_data: Dictionary containing session information
-            
+
         Returns:
             True if session was stored successfully
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO sessions (session_id, user_id, start_time, last_activity, interaction_count, status)
                 VALUES ($1, $2, $3, $4, $5, $6)
                 ON CONFLICT (session_id) DO UPDATE SET
@@ -1458,91 +1571,109 @@
                     interaction_count = EXCLUDED.interaction_count,
                     status = EXCLUDED.status,
                     updated_at = NOW()
-            """, session_data['session_id'], session_data['user_id'], 
-                session_data['start_time'], session_data['last_activity'],
-                session_data['interaction_count'], session_data['status'])
-            
+            """,
+                session_data["session_id"],
+                session_data["user_id"],
+                session_data["start_time"],
+                session_data["last_activity"],
+                session_data["interaction_count"],
+                session_data["status"],
+            )
+
             print(f"‚úÖ Stored session: {session_data['session_id']}")
             return True
 
     async def store_session_interaction(self, interaction_data: Dict[str, Any]) -> bool:
         """
         Store session interaction data.
-        
+
         Args:
             interaction_data: Dictionary containing interaction information
-            
+
         Returns:
             True if interaction was stored successfully
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO session_interactions 
                 (interaction_id, session_id, run_id, interaction_number, prompt, workflow_type, start_time)
                 VALUES ($1, $2, $3, $4, $5, $6, $7)
-            """, interaction_data['interaction_id'], interaction_data['session_id'],
-                interaction_data['run_id'], interaction_data['interaction_number'],
-                interaction_data['prompt'], interaction_data['workflow_type'],
-                interaction_data['start_time'])
-            
+            """,
+                interaction_data["interaction_id"],
+                interaction_data["session_id"],
+                interaction_data["run_id"],
+                interaction_data["interaction_number"],
+                interaction_data["prompt"],
+                interaction_data["workflow_type"],
+                interaction_data["start_time"],
+            )
+
             print(f"‚úÖ Stored session interaction: {interaction_data['interaction_id']}")
             return True
 
     async def get_original_search_prompt(self, run_id: str) -> Optional[str]:
         """
         Get the original search prompt from session_interactions for a given run_id.
-        
+
         Args:
             run_id: The run identifier to look up
-            
+
         Returns:
             The original prompt text if found, None otherwise
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
-            row = await conn.fetchrow("""
+            row = await conn.fetchrow(
+                """
                 SELECT prompt FROM session_interactions 
                 WHERE run_id = $1 AND workflow_type = 'prospecting_orchestration'
                 ORDER BY created_at DESC
                 LIMIT 1
-            """, run_id)
-            
+            """,
+                run_id,
+            )
+
             if row:
-                return row['prompt']
+                return row["prompt"]
             return None
 
     async def get_search_parameters(self, run_id: str) -> Optional[Dict[str, Any]]:
         """
         Get the search parameters from prospecting_runs for a given run_id.
-        
+
         Args:
             run_id: The run identifier to look up
-            
+
         Returns:
             The search parameters dictionary if found, None otherwise
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
-            row = await conn.fetchrow("""
+            row = await conn.fetchrow(
+                """
                 SELECT search_params FROM prospecting_runs 
                 WHERE run_id = $1 AND workflow_type IN ('general_search', 'company_selection')
                 ORDER BY created_at DESC
                 LIMIT 1
-            """, run_id)
-            
-            if row and row['search_params']:
-                search_params = row['search_params']
+            """,
+                run_id,
+            )
+
+            if row and row["search_params"]:
+                search_params = row["search_params"]
                 # Handle case where JSONB is returned as string
                 if isinstance(search_params, str):
                     try:
                         import json
+
                         return json.loads(search_params)
                     except json.JSONDecodeError:
                         return None
@@ -1552,21 +1683,22 @@
     async def complete_prospecting_run(self, run_id: str, user_id: str, session_id: str = None, execution_time_ms: int = None) -> bool:
         """
         Mark a prospecting run as completed and update final metadata.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
             session_id: Session identifier (optional)
             execution_time_ms: Execution time in milliseconds (optional)
-            
+
         Returns:
             True if run was updated successfully
         """
         if not self.pool:
             await self.init_pool()
-                
+
         async with self.pool.acquire() as conn:
-            await conn.execute("""
+            await conn.execute(
+                """
                 UPDATE prospecting_runs 
                 SET status = 'completed', 
                     end_time = NOW(), 
@@ -1574,8 +1706,13 @@
                     session_id = $4,
                     updated_at = NOW()
                 WHERE run_id = $1 AND user_id = $2
-            """, run_id, user_id, execution_time_ms, session_id)
-                
+            """,
+                run_id,
+                user_id,
+                execution_time_ms,
+                session_id,
+            )
+
             print(f"‚úÖ Completed prospecting run {run_id} for user {user_id} with execution time {execution_time_ms}ms (session: {session_id})")
             return True
 
@@ -1592,7 +1729,7 @@
                 WHERE company_id = $1 AND (published_at IS NULL OR published_at >= $2)
                 """,
                 company_id,
-                since
+                since,
             )
             return [r["video_id"] for r in rows]
 
@@ -1751,7 +1888,9 @@
                         emb.get("start_seconds"),
                         emb.get("end_seconds"),
                         emb.get("text"),
-                        "[" + ",".join(str(x) for x in emb.get("embedding", [])) + "]" if isinstance(emb.get("embedding"), list) else emb.get("embedding"),
+                        "[" + ",".join(str(x) for x in emb.get("embedding", [])) + "]"
+                        if isinstance(emb.get("embedding"), list)
+                        else emb.get("embedding"),
                         emb.get("company_id"),
                         emb.get("person_name"),
                         emb.get("run_id"),
@@ -1759,7 +1898,7 @@
                         emb.get("session_id"),
                     )
                     for emb in embeddings
-                ]
+                ],
             )
 
     async def search_youtube_embeddings(
@@ -1820,7 +1959,7 @@
                 ORDER BY e.embedding <-> $2::vector
                 LIMIT {limit}
                 """,
-                *params
+                *params,
             )
             return [dict(row) for row in rows]
 
@@ -1841,7 +1980,7 @@
                 LIMIT 1
                 """,
                 run_id,
-                user_id
+                user_id,
             )
 
         if not row:
@@ -1863,11 +2002,11 @@
     async def get_company_enrichment_by_run(self, run_id: str, user_id: str) -> Dict[str, Any]:
         """
         Get company enrichment data for a specific run and user.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
-        
+
         Returns:
             Dictionary containing company enrichment data
         """
@@ -1875,7 +2014,8 @@
             await self.init_pool()
 
         async with self.pool.acquire() as conn:
-            result = await conn.fetchrow("""
+            result = await conn.fetchrow(
+                """
                 SELECT result_data->'company_enrichment_result' as enrichment_data
                 FROM agent_results 
                 WHERE run_id = $1 
@@ -1883,21 +2023,25 @@
                 AND agent_name = 'Company Enrichment Agent'
                 ORDER BY created_at DESC 
                 LIMIT 1
-            """, run_id, user_id)
-            
+            """,
+                run_id,
+                user_id,
+            )
+
             if not result:
                 print(f"‚ùå No company enrichment data found for run: {run_id} (user: {user_id})")
                 return None
-            enrichment_data = result['enrichment_data']
-            #print(f"[DEBUG] Raw enrichment_data type: {type(enrichment_data)} value: {enrichment_data}")
+            enrichment_data = result["enrichment_data"]
+            # print(f"[DEBUG] Raw enrichment_data type: {type(enrichment_data)} value: {enrichment_data}")
             # Handle asyncpg.Record or similar
-            if hasattr(enrichment_data, 'items') and not isinstance(enrichment_data, dict):
+            if hasattr(enrichment_data, "items") and not isinstance(enrichment_data, dict):
                 enrichment_data = dict(enrichment_data)
                 print(f"[DEBUG] Converted enrichment_data to dict via items: {type(enrichment_data)}")
             # Handle string
             if isinstance(enrichment_data, str):
                 try:
                     import json
+
                     enrichment_data = json.loads(enrichment_data)
                     print(f"[DEBUG] Parsed enrichment_data from string to dict.")
                 except Exception as e:
@@ -1907,21 +2051,21 @@
             if not enrichment_data or not isinstance(enrichment_data, dict):
                 print(f"‚ùå enrichment_data is empty or not a dict after conversion: {enrichment_data}")
                 return None
-            
+
             # Parse nested JSON strings (e.g., awards_events_press stored as string in JSONB)
             enrichment_data = _parse_nested_json_strings(enrichment_data)
-            
+
             print(f"[DEBUG] Final enrichment_data type: {type(enrichment_data)} keys: {list(enrichment_data.keys())}")
             return enrichment_data
 
     async def get_person_enrichment_by_run(self, run_id: str, user_id: str) -> List[Dict[str, Any]]:
         """
         Get person enrichment data for a specific run and user.
-        
+
         Args:
             run_id: Run identifier
             user_id: User identifier
-        
+
         Returns:
             List of dictionaries containing person enrichment data
         """
@@ -1929,23 +2073,28 @@
             await self.init_pool()
 
         async with self.pool.acquire() as conn:
-            results = await conn.fetch("""
+            results = await conn.fetch(
+                """
                 SELECT enrichment_data
                 FROM person_enrichment_results 
                 WHERE run_id = $1 
                 AND user_id = $2
                 ORDER BY person_rank ASC
-            """, run_id, user_id)
-            
+            """,
+                run_id,
+                user_id,
+            )
+
             if not results:
                 print(f"‚ùå No person enrichment data found for run: {run_id} (user: {user_id})")
                 return None
 
             parsed = []
             for row in results:
-                data = row['enrichment_data']
+                data = row["enrichment_data"]
                 if isinstance(data, str):
                     import json
+
                     data = json.loads(data)
                 parsed.append(data)
             return parsed
@@ -1953,67 +2102,74 @@
     async def get_user_profile(self, user_id: str) -> Dict[str, Any]:
         """
         Get user profile data from user_profiles table.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             Dictionary containing user profile data or None if not found
         """
         if not self.pool:
             await self.init_pool()
-        
+
         async with self.pool.acquire() as conn:
-            result = await conn.fetchrow(
-                "SELECT * FROM user_profiles WHERE user_id = $1",
-                user_id
-            )
-            
+            result = await conn.fetchrow("SELECT * FROM user_profiles WHERE user_id = $1", user_id)
+
             return dict(result) if result else None
-            
-    async def create_user_profile(self, user_id: str, firm_description: str, 
-                                 key_differentiators: str, key_objectives: str) -> Dict[str, Any]:
+
+    async def create_user_profile(self, user_id: str, firm_description: str, key_differentiators: str, key_objectives: str) -> Dict[str, Any]:
         """
         Create a new user profile.
-        
+
         Args:
             user_id: User identifier
             firm_description: Description of the user's firm
             key_differentiators: Key differentiators of the user's firm
             key_objectives: Key objectives of the user's firm
-            
+
         Returns:
             Dictionary containing the created user profile
         """
         if not self.pool:
             await self.init_pool()
-            
+
         timestamp = datetime.now()
-        
+
         async with self.pool.acquire() as conn:
             # Check if profile already exists
-            existing = await conn.fetchval(
-                "SELECT user_id FROM user_profiles WHERE user_id = $1",
-                user_id
-            )
-            
+            existing = await conn.fetchval("SELECT user_id FROM user_profiles WHERE user_id = $1", user_id)
+
             if existing:
                 raise ValueError(f"Profile already exists for user {user_id}")
-                
+
             # Create new profile
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO user_profiles 
                 (user_id, firm_description, key_differentiators, key_objectives, created_at, updated_at)
                 VALUES ($1, $2, $3, $4, $5, $5)
-            """, user_id, firm_description, key_differentiators, key_objectives, timestamp)
-            
+            """,
+                user_id,
+                firm_description,
+                key_differentiators,
+                key_objectives,
+                timestamp,
+            )
+
             # Insert initial version in history
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO user_profiles_history
                 (user_id, firm_description, key_differentiators, key_objectives, created_at, version)
                 VALUES ($1, $2, $3, $4, $5, 1)
-            """, user_id, firm_description, key_differentiators, key_objectives, timestamp)
-            
+            """,
+                user_id,
+                firm_description,
+                key_differentiators,
+                key_objectives,
+                timestamp,
+            )
+
             # Return the created profile
             return {
                 "user_id": user_id,
@@ -2021,58 +2177,72 @@
                 "key_differentiators": key_differentiators,
                 "key_objectives": key_objectives,
                 "created_at": timestamp,
-                "updated_at": timestamp
+                "updated_at": timestamp,
             }
-            
-    async def update_user_profile(self, user_id: str, firm_description: str, 
-                                 key_differentiators: str, key_objectives: str) -> Dict[str, Any]:
+
+    async def update_user_profile(self, user_id: str, firm_description: str, key_differentiators: str, key_objectives: str) -> Dict[str, Any]:
         """
         Update an existing user profile and archive the previous version.
-        
+
         Args:
             user_id: User identifier
             firm_description: Description of the user's firm
             key_differentiators: Key differentiators of the user's firm
             key_objectives: Key objectives of the user's firm
-            
+
         Returns:
             Dictionary containing the updated user profile
         """
         if not self.pool:
             await self.init_pool()
-            
+
         timestamp = datetime.now()
-        
+
         async with self.pool.acquire() as conn:
             # Check if profile exists
-            existing = await conn.fetchrow(
-                "SELECT * FROM user_profiles WHERE user_id = $1",
-                user_id
-            )
-            
+            existing = await conn.fetchrow("SELECT * FROM user_profiles WHERE user_id = $1", user_id)
+
             if not existing:
                 raise ValueError(f"No profile found for user {user_id}")
-                
+
             # Get the latest version number
-            latest_version = await conn.fetchval("""
+            latest_version = await conn.fetchval(
+                """
                 SELECT COALESCE(MAX(version), 0) FROM user_profiles_history
                 WHERE user_id = $1
-                """, user_id)
-            
+                """,
+                user_id,
+            )
+
             # Archive current profile to history
-            await conn.execute("""
+            await conn.execute(
+                """
                 INSERT INTO user_profiles_history
                 (user_id, firm_description, key_differentiators, key_objectives, created_at, version)
                 VALUES ($1, $2, $3, $4, $5, $6)
-            """, user_id, firm_description, key_differentiators, key_objectives, timestamp, latest_version + 1)
-            
+            """,
+                user_id,
+                firm_description,
+                key_differentiators,
+                key_objectives,
+                timestamp,
+                latest_version + 1,
+            )
+
             # Update the profile
-            await conn.execute("""
+            await conn.execute(
+                """
                 UPDATE user_profiles
                 SET firm_description = $2, key_differentiators = $3, key_objectives = $4, updated_at = $5
                 WHERE user_id = $1
-            """, user_id, firm_description, key_differentiators, key_objectives, timestamp)
-            
+            """,
+                user_id,
+                firm_description,
+                key_differentiators,
+                key_objectives,
+                timestamp,
+            )
+
             # Return the updated profile
             return {
                 "user_id": user_id,
@@ -2080,29 +2250,32 @@
                 "key_differentiators": key_differentiators,
                 "key_objectives": key_objectives,
                 "created_at": existing["created_at"],
-                "updated_at": timestamp
+                "updated_at": timestamp,
             }
-            
+
     async def get_user_profile_history(self, user_id: str) -> List[Dict[str, Any]]:
         """
         Get history of user profile changes.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             List of dictionaries containing historical user profile data
         """
         if not self.pool:
             await self.init_pool()
-            
+
         async with self.pool.acquire() as conn:
-            results = await conn.fetch("""
+            results = await conn.fetch(
+                """
                 SELECT * FROM user_profiles_history
                 WHERE user_id = $1
                 ORDER BY version DESC
-            """, user_id)
-            
+            """,
+                user_id,
+            )
+
             return [dict(row) for row in results] if results else []
 
     async def close(self):
@@ -2191,36 +2364,45 @@
                 admin_user_id,
                 action,
                 target_resource,
-                details if details is not None else None
+                details if details is not None else None,
             )
 
     async def update_session_status(self, session_id: str, status: str, end_time: str = None) -> bool:
         """
         Update session status and end time.
-        
+
         Args:
             session_id: Session identifier
             status: New status
             end_time: End time (optional)
-            
+
         Returns:
             True if session was updated successfully
         """
         if not self.pool:
             await self.init_pool()
-        
+
         async with self.pool.acquire() as conn:
             if end_time:
-                await conn.execute("""
+                await conn.execute(
+                    """
                     UPDATE sessions SET status = $1, end_time = $2, updated_at = NOW()
                     WHERE session_id = $3
-                """, status, end_time, session_id)
+                """,
+                    status,
+                    end_time,
+                    session_id,
+                )
             else:
-                await conn.execute("""
+                await conn.execute(
+                    """
                     UPDATE sessions SET status = $1, updated_at = NOW()
                     WHERE session_id = $2
-                """, status, session_id)
-            
+                """,
+                    status,
+                    session_id,
+                )
+
             print(f"‚úÖ Updated session {session_id} status to: {status}")
 
     async def store_ria_activity(
@@ -2230,15 +2412,15 @@
         activity_type: str,
         search_id: str = None,
         search_params: str = None,
-        search_timestamp = None,
+        search_timestamp=None,
         company_crd: str = None,
         company_name: str = None,
         company_action: str = None,
-        ip_address: str = None
+        ip_address: str = None,
     ) -> int:
         """
         Store RIA activity (search or company view) in the database.
-        
+
         Args:
             user_id: User identifier
             session_id: Session identifier
@@ -2250,15 +2432,16 @@
             company_name: Name of the company
             company_action: Action taken on the company
             ip_address: User's IP address
-            
+
         Returns:
             Activity ID of the stored record
         """
         if not self.pool:
             await self.init_pool()
-        
+
         async with self.pool.acquire() as conn:
-            result = await conn.fetchrow("""
+            result = await conn.fetchrow(
+                """
                 INSERT INTO ria_user_activity (
                     user_id, session_id, activity_type,
                     search_id, search_params, search_timestamp,
@@ -2267,13 +2450,19 @@
                 ) VALUES (
                     $1, $2, $3, $4, $5, $6, $7, $8, $9, $10
                 ) RETURNING id
-            """, 
-                user_id, session_id, activity_type,
-                search_id, search_params, search_timestamp,
-                company_crd, company_name, company_action,
-                ip_address
+            """,
+                user_id,
+                session_id,
+                activity_type,
+                search_id,
+                search_params,
+                search_timestamp,
+                company_crd,
+                company_name,
+                company_action,
+                ip_address,
             )
-            
+
             activity_id = result["id"]
             print(f"‚úÖ Stored RIA activity: {activity_type} for user {user_id}, activity_id: {activity_id}")
             return activity_id
@@ -2286,11 +2475,11 @@
         from_date: str = None,
         to_date: str = None,
         limit: int = 50,
-        offset: int = 0
+        offset: int = 0,
     ) -> list:
         """
         Retrieve RIA activity records with optional filtering.
-        
+
         Args:
             user_id: Filter by specific user
             search_id: Filter by specific search
@@ -2299,61 +2488,65 @@
             to_date: Filter by end date
             limit: Maximum number of records to return
             offset: Number of records to skip
-            
+
         Returns:
             List of activity records
         """
         if not self.pool:
             await self.init_pool()
-        
+
         # Build WHERE clause dynamically
         where_conditions = []
         params = []
         param_count = 0
-        
+
         if user_id:
             param_count += 1
             where_conditions.append(f"user_id = ${param_count}")
             params.append(user_id)
-            
+
         if search_id:
             param_count += 1
             where_conditions.append(f"search_id = ${param_count}")
             params.append(search_id)
-            
+
         if activity_type:
             param_count += 1
             where_conditions.append(f"activity_type = ${param_count}")
             params.append(activity_type)
-            
+
         if from_date:
             param_count += 1
             where_conditions.append(f"created_at >= ${param_count}")
             params.append(from_date)
-            
+
         if to_date:
             param_count += 1
             where_conditions.append(f"created_at <= ${param_count}")
             params.append(to_date)
-        
+
         where_clause = " AND ".join(where_conditions) if where_conditions else "TRUE"
-        
+
         async with self.pool.acquire() as conn:
             # Get total count
-            count_result = await conn.fetchrow(f"""
+            count_result = await conn.fetchrow(
+                f"""
                 SELECT COUNT(*) as total
                 FROM ria_user_activity
                 WHERE {where_clause}
-            """, *params)
+            """,
+                *params,
+            )
             total_count = count_result["total"]
-            
+
             # Get paginated results
             param_count += 1
             params.append(limit)
             param_count += 1
             params.append(offset)
-            
-            records = await conn.fetch(f"""
+
+            records = await conn.fetch(
+                f"""
                 SELECT 
                     id, user_id, session_id, activity_type,
                     search_id, search_params, search_timestamp,
@@ -2363,20 +2556,17 @@
                 WHERE {where_clause}
                 ORDER BY search_id, created_at ASC
                 LIMIT ${param_count - 1} OFFSET ${param_count}
-            """, *params)
-            
-            return {
-                "records": [dict(record) for record in records],
-                "total_count": total_count,
-                "limit": limit,
-                "offset": offset
-            }
+            """,
+                *params,
+            )
+
+            return {"records": [dict(record) for record in records], "total_count": total_count, "limit": limit, "offset": offset}
 
 
 def extract_coresignal_fields(data: Dict[str, Any]) -> Dict[str, Any]:
     """
     Extract specific fields from Coresignal data for structured querying.
-    
+
     Fields: twitter_url, youtube_url, description, categories_and_keywords, type, status,
     founded_year, employees_count, hq_full_address, company_locations_full, company_updates,
     parent_company_information, "revenue_annual", "funding_rounds", "ownership_status"
@@ -2388,26 +2578,49 @@
     "active_job_postings_count_change", active_job_postings_count_by_month", "linkedin_followers_count_change"
     """
     extracted = {}
-    
+
     # Handle nested data structure - check if data has 'data' key (from Coresignal API response)
-    actual_data = data.get('data', data) if isinstance(data, dict) and 'data' in data else data
-    
+    actual_data = data.get("data", data) if isinstance(data, dict) and "data" in data else data
+
     # Direct field mappings
     direct_fields = [
-        "id", "company_legal_name", "company_name",
-        "website", "linkedin_url", "twitter_url", "youtube_url", "description", "categories_and_keywords", 
-        "type", "status", "founded_year", "employees_count", "hq_full_address",
-        "company_locations_full", "company_updates", "parent_company_information",
-        "revenue_annual", "funding_rounds", "ownership_status", "competitors",
-        "num_acquisitions_source_1", "acquisition_list_source_1",
-        "num_acquisitions_source_2", "acquisition_list_source_2", 
-        "num_acquisitions_source_5", "acquisition_list_source_5",
-        "key_executives", "key_employee_change_events",
-        "employees_count_change", "employees_count_by_country",
-        "active_job_postings_count", "active_job_postings_titles",
-        "active_job_postings_count_change", "linkedin_followers_count_change"
+        "id",
+        "company_legal_name",
+        "company_name",
+        "website",
+        "linkedin_url",
+        "twitter_url",
+        "youtube_url",
+        "description",
+        "categories_and_keywords",
+        "type",
+        "status",
+        "founded_year",
+        "employees_count",
+        "hq_full_address",
+        "company_locations_full",
+        "company_updates",
+        "parent_company_information",
+        "revenue_annual",
+        "funding_rounds",
+        "ownership_status",
+        "competitors",
+        "num_acquisitions_source_1",
+        "acquisition_list_source_1",
+        "num_acquisitions_source_2",
+        "acquisition_list_source_2",
+        "num_acquisitions_source_5",
+        "acquisition_list_source_5",
+        "key_executives",
+        "key_employee_change_events",
+        "employees_count_change",
+        "employees_count_by_country",
+        "active_job_postings_count",
+        "active_job_postings_titles",
+        "active_job_postings_count_change",
+        "linkedin_followers_count_change",
     ]
-    
+
     for field in direct_fields:
         value = actual_data.get(field)
         if value is not None:
@@ -2433,42 +2646,42 @@
             for exec_data in value:
                 if isinstance(exec_data, dict):
                     exec_copy = exec_data.copy()
-                    exec_copy.pop('parent_id', None)  # Remove parent_id if present
+                    exec_copy.pop("parent_id", None)  # Remove parent_id if present
                     cleaned_executives.append(exec_copy)
             extracted[field] = cleaned_executives
-    
+
     # Add computed fields
     extracted["coresignal_company_id"] = actual_data.get("id")
     extracted["has_twitter"] = bool(actual_data.get("twitter_url"))
     extracted["has_youtube"] = bool(actual_data.get("youtube_url"))
     extracted["has_linkedin"] = bool(extracted.get("linkedin_url"))
-    
+
     # Fix description_length to handle None values properly
     description = actual_data.get("description")
     extracted["description_length"] = len(description) if description is not None else 0
-    
+
     return extracted
 
 
 async def extract_coresignal_fields_with_filtering(data: Dict[str, Any], company_name: str = "") -> Dict[str, Any]:
     """
     Extract specific fields from Coresignal data for structured querying with GPT-4o-mini filtering.
-    
+
     This is the async version that applies intelligent filtering to executive fields with >10 people.
-    
+
     Args:
         data: Coresignal data dictionary
         company_name: Company name for context in executive filtering
     """
     # First extract all fields using the original function
     extracted = extract_coresignal_fields(data)
-    
+
     # Build company context for executive filtering
     company_context = f"Investment company: {company_name}" if company_name else "Investment company"
-    
+
     # Define executive fields to check for filtering
     executive_fields = ["key_executives", "key_executive_arrivals", "key_executive_departures"]
-    
+
     # Apply filtering to each executive field if needed
     for field_name in executive_fields:
         field_data = extracted.get(field_name)
@@ -2478,7 +2691,7 @@
             extracted[field_name] = filtered_data
         elif field_data and isinstance(field_data, list):
             print(f"‚úÖ {field_name} has {len(field_data)} people (‚â§10), no filtering needed")
-    
+
     # Filter competitors to top 5 if more than 5 exist
     competitors = extracted.get("competitors")
     if competitors and isinstance(competitors, list) and len(competitors) > 5:
@@ -2487,121 +2700,115 @@
         print(f"‚úÖ Filtered competitors from {len(competitors)} to 5")
     elif competitors and isinstance(competitors, list):
         print(f"‚úÖ Competitors has {len(competitors)} entries (‚â§5), no filtering needed")
-    
+
     # Filter employees_count_by_country - keep top 15 by employee count
     employees_by_country = extracted.get("employees_count_by_country")
     if employees_by_country and isinstance(employees_by_country, list) and len(employees_by_country) > 15:
         print(f"üîç Found {len(employees_by_country)} countries in employee distribution, filtering to top 15...")
-        sorted_countries = sorted(employees_by_country, key=lambda x: x['employee_count'], reverse=True)[:15]
+        sorted_countries = sorted(employees_by_country, key=lambda x: x["employee_count"], reverse=True)[:15]
         extracted["employees_count_by_country"] = sorted_countries
         print(f"‚úÖ Filtered employee distribution from {len(employees_by_country)} to 15 countries")
     elif employees_by_country and isinstance(employees_by_country, list):
         print(f"‚úÖ Employee distribution has {len(employees_by_country)} countries (‚â§15), no filtering needed")
-    
+
     return extracted
 
 
 async def filter_executive_field_with_gpt4o_mini(people: List[Dict[str, Any]], field_name: str, company_context: str = "") -> List[Dict[str, Any]]:
     """
     Filter executive fields using enhanced pre-filtering + GPT-4o-mini for investment companies.
-    
+
     Args:
         people: List of executive dictionaries
         field_name: Name of the field being filtered for logging
         company_context: Company context for better filtering
-    
+
     Returns:
         List of filtered executives (max 10)
     """
     if len(people) <= 10:
         print(f"‚úÖ {field_name} has {len(people)} people, no filtering needed")
         return people
-    
+
     try:
         print(f"üîç Pre-filtering {field_name}: {len(people)} executives for investment company")
-        
+
         # Step 1: Pre-filter using rule-based logic
-        pre_filtered_executives, original_indices = pre_filter_executives_for_investment_company(
-            people, max_output=30
-        )
-        
+        pre_filtered_executives, original_indices = pre_filter_executives_for_investment_company(people, max_output=30)
+
         print(f"‚úÖ Pre-filtering reduced {len(people)} ‚Üí {len(pre_filtered_executives)} executives")
-        
+
         # If pre-filtering gives us 10 or fewer, return them
         if len(pre_filtered_executives) <= 10:
             print(f"‚úÖ Pre-filtering sufficient, returning {len(pre_filtered_executives)} executives")
             return pre_filtered_executives
-        
+
         # Step 2: Use LLM for final selection from pre-filtered list
         # Import OpenAI client
         from openai import AsyncOpenAI
-        
+
         # Get API key
         api_key = get_openai_api_key()
         if not api_key:
             print(f"‚ö†Ô∏è OPENAI_API_KEY not found, returning top 10 from pre-filtering for {field_name}")
             return pre_filtered_executives[:10]
-        
+
         # Initialize OpenAI client
         client = AsyncOpenAI(api_key=api_key)
-        
+
         # Get prompts from data_process_prompts.py
         system_prompt = get_executive_filtering_system_prompt()
         user_prompt = get_executive_filtering_user_prompt(
-            json.dumps(pre_filtered_executives, ensure_ascii=False), 
-            len(pre_filtered_executives),
-            company_context
+            json.dumps(pre_filtered_executives, ensure_ascii=False), len(pre_filtered_executives), company_context
         )
-        
+
         print(f"ü§ñ LLM filtering {field_name}: {len(pre_filtered_executives)} ‚Üí 10 executives")
-        
+
         # Call GPT-4o-mini
         response = await client.chat.completions.create(
             model="gpt-4o-mini",
-            messages=[
-                {"role": "system", "content": system_prompt},
-                {"role": "user", "content": user_prompt}
-            ],
+            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
             temperature=0.1,
-            max_tokens=1000
+            max_tokens=1000,
         )
-        
+
         # Extract response content
         response_content = response.choices[0].message.content.strip()
-        
+
         # Parse JSON response to get indices
         try:
             # Try to extract JSON from markdown code blocks if present
             import re
-            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response_content, re.DOTALL)
+
+            json_match = re.search(r"```(?:json)?\s*(\[.*?\])\s*```", response_content, re.DOTALL)
             if json_match:
                 selected_indices = json.loads(json_match.group(1))
             else:
                 # Try to parse as direct JSON
                 selected_indices = json.loads(response_content)
-            
+
             # Validate that we got a list of integers
             if not isinstance(selected_indices, list) or not all(isinstance(i, int) for i in selected_indices):
                 print(f"‚ö†Ô∏è LLM returned invalid indices for {field_name}, using top 10 from pre-filtering")
                 return pre_filtered_executives[:10]
-            
+
             # Validate indices are within bounds of pre-filtered list
             valid_indices = [i for i in selected_indices if 0 <= i < len(pre_filtered_executives)]
             if len(valid_indices) != 10:
                 print(f"‚ö†Ô∏è LLM returned {len(valid_indices)} valid indices (expected 10), using top 10 from pre-filtering")
                 return pre_filtered_executives[:10]
-            
+
             # Select the executives by their indices from the pre-filtered list
             final_executives = [pre_filtered_executives[i] for i in valid_indices]
-            
+
             print(f"‚úÖ Successfully filtered {field_name}: {len(people)} ‚Üí pre-filter({len(pre_filtered_executives)}) ‚Üí LLM({len(final_executives)})")
             return final_executives
-            
+
         except json.JSONDecodeError as e:
             print(f"‚ö†Ô∏è Failed to parse LLM response for {field_name}: {e}")
             print(f"Response: {response_content[:200]}...")
             return pre_filtered_executives[:10]  # Fallback to top 10 from pre-filtering
-            
+
     except Exception as e:
         print(f"‚ö†Ô∏è Error in enhanced filtering for {field_name}: {e}")
         # Fallback to simple pre-filtering
@@ -2618,7 +2825,7 @@
     Handles cases where JSONB fields contain JSON strings that need parsing.
     """
     import json
-    
+
     if isinstance(obj, str):
         # Try to parse as JSON
         try:
@@ -2642,7 +2849,7 @@
 def extract_web_research_fields(data: Dict[str, Any]) -> Dict[str, Any]:
     """
     Extract the 32 investment firm fields from web research data (v2 prompts).
-    
+
     Fields organized by category:
     - Core Profile (7): firm_name, legal_entity_name, website, year_founded, aum, headquarters_location, additional_office_locations
     - Investment Strategy & Mandate (6): primary_investment_strategy, primary_investment_asset_class, sector_focus, geographic_focus, stage_preference, typical_check_size
@@ -2654,100 +2861,101 @@
     - Activity & Sentiment Signals (5): recent_hires, new_offices_expansion, M&A_fund_spinouts, awards_events_press, public_fund_strategy_commentary
     """
     extracted = {}
-    
+
     # Helper function to extract value from nested structure
     def extract_value(obj, default=None):
         # Handle JSON strings that need parsing
         if isinstance(obj, str):
             try:
                 import json
+
                 obj = json.loads(obj)
             except (json.JSONDecodeError, TypeError):
                 # If parsing fails, return the string as-is or default
                 return obj if obj is not None else default
-        
+
         if isinstance(obj, dict):
-            return obj.get('value', default)
+            return obj.get("value", default)
         return obj if obj is not None else default
-    
+
     # Helper function to extract citation from nested structure
     def extract_citation(obj, default=None):
         if isinstance(obj, dict):
-            return obj.get('citation', default)
+            return obj.get("citation", default)
         return default
-    
+
     # Core Profile (7 fields)
-    core_profile = data.get('core_profile', {})
-    extracted['firm_name'] = extract_value(core_profile.get('firm_name'))
-    extracted['legal_entity_name'] = extract_value(core_profile.get('legal_entity_name'))
-    extracted['website'] = extract_value(core_profile.get('website'))
-    extracted['year_founded'] = extract_value(core_profile.get('year_founded'))
-    extracted['aum'] = extract_value(core_profile.get('aum'))
-    extracted['headquarters_location'] = extract_value(core_profile.get('headquarters_location'))
-    extracted['additional_office_locations'] = extract_value(core_profile.get('additional_office_locations'), [])
-    
+    core_profile = data.get("core_profile", {})
+    extracted["firm_name"] = extract_value(core_profile.get("firm_name"))
+    extracted["legal_entity_name"] = extract_value(core_profile.get("legal_entity_name"))
+    extracted["website"] = extract_value(core_profile.get("website"))
+    extracted["year_founded"] = extract_value(core_profile.get("year_founded"))
+    extracted["aum"] = extract_value(core_profile.get("aum"))
+    extracted["headquarters_location"] = extract_value(core_profile.get("headquarters_location"))
+    extracted["additional_office_locations"] = extract_value(core_profile.get("additional_office_locations"), [])
+
     # Investment Strategy & Mandate (6 fields)
-    investment_strategy = data.get('investment_strategy_mandate', {})
-    extracted['primary_investment_strategy'] = extract_value(investment_strategy.get('primary_investment_strategy'))
-    extracted['primary_investment_asset_class'] = extract_value(investment_strategy.get('primary_investment_asset_class'))
-    extracted['sector_focus'] = extract_value(investment_strategy.get('sector_focus'), [])
-    extracted['geographic_focus'] = extract_value(investment_strategy.get('geographic_focus'), [])
-    extracted['stage_preference'] = extract_value(investment_strategy.get('stage_preference'), [])
-    extracted['typical_check_size'] = extract_value(investment_strategy.get('typical_check_size'))
-    
+    investment_strategy = data.get("investment_strategy_mandate", {})
+    extracted["primary_investment_strategy"] = extract_value(investment_strategy.get("primary_investment_strategy"))
+    extracted["primary_investment_asset_class"] = extract_value(investment_strategy.get("primary_investment_asset_class"))
+    extracted["sector_focus"] = extract_value(investment_strategy.get("sector_focus"), [])
+    extracted["geographic_focus"] = extract_value(investment_strategy.get("geographic_focus"), [])
+    extracted["stage_preference"] = extract_value(investment_strategy.get("stage_preference"), [])
+    extracted["typical_check_size"] = extract_value(investment_strategy.get("typical_check_size"))
+
     # Fund Information (5 fields)
-    fund_info = data.get('fund_information', {})
-    extracted['current_funds_active'] = extract_value(fund_info.get('current_funds_active'), [])
-    extracted['historical_fund_performance'] = extract_value(fund_info.get('historical_fund_performance'))
-    extracted['recent_fund_launches'] = extract_value(fund_info.get('recent_fund_launches'), [])
-    extracted['recent_fund_closures'] = extract_value(fund_info.get('recent_fund_closures'), [])
-    extracted['recent_fund_activities'] = extract_value(fund_info.get('recent_fund_activities'), [])
-    
+    fund_info = data.get("fund_information", {})
+    extracted["current_funds_active"] = extract_value(fund_info.get("current_funds_active"), [])
+    extracted["historical_fund_performance"] = extract_value(fund_info.get("historical_fund_performance"))
+    extracted["recent_fund_launches"] = extract_value(fund_info.get("recent_fund_launches"), [])
+    extracted["recent_fund_closures"] = extract_value(fund_info.get("recent_fund_closures"), [])
+    extracted["recent_fund_activities"] = extract_value(fund_info.get("recent_fund_activities"), [])
+
     # Capital Commitments (4 fields)
-    capital_commitments = data.get('capital_commitments', {})
-    extracted['recent_LP_commitments'] = extract_value(capital_commitments.get('recent_LP_commitments'), [])
-    extracted['co_investment_activity'] = extract_value(capital_commitments.get('co_investment_activity'))
-    extracted['history_investing_similar_funds'] = extract_value(capital_commitments.get('history_investing_similar_funds'))
-    extracted['public_RFPs_allocation_shifts'] = extract_value(capital_commitments.get('public_RFPs_allocation_shifts'))
-    
+    capital_commitments = data.get("capital_commitments", {})
+    extracted["recent_LP_commitments"] = extract_value(capital_commitments.get("recent_LP_commitments"), [])
+    extracted["co_investment_activity"] = extract_value(capital_commitments.get("co_investment_activity"))
+    extracted["history_investing_similar_funds"] = extract_value(capital_commitments.get("history_investing_similar_funds"))
+    extracted["public_RFPs_allocation_shifts"] = extract_value(capital_commitments.get("public_RFPs_allocation_shifts"))
+
     # Assets Under Management (2 fields)
-    aum_data = data.get('assets_under_management', {})
-    extracted['breakdown_by_asset_class'] = extract_value(aum_data.get('breakdown_by_asset_class'), [])
-    extracted['growth_decline_trajectory'] = extract_value(aum_data.get('growth_decline_trajectory'))
-    extracted['esg_dei_strategy'] = extract_value(aum_data.get('esg_dei_strategy'))
-    
+    aum_data = data.get("assets_under_management", {})
+    extracted["breakdown_by_asset_class"] = extract_value(aum_data.get("breakdown_by_asset_class"), [])
+    extracted["growth_decline_trajectory"] = extract_value(aum_data.get("growth_decline_trajectory"))
+    extracted["esg_dei_strategy"] = extract_value(aum_data.get("esg_dei_strategy"))
+
     # Organization & Decision-Making (4 fields)
-    org_decision = data.get('organization_decision_making', {})
-    extracted['c_suite_members'] = extract_value(org_decision.get('c_suite_members'), [])
-    extracted['key_decision_makers'] = extract_value(org_decision.get('key_decision_makers'), [])
-    extracted['firm_ownership_type'] = extract_value(org_decision.get('firm_ownership_type'))
-    extracted['recent_leadership_changes'] = extract_value(org_decision.get('recent_leadership_changes'), [])
-    
+    org_decision = data.get("organization_decision_making", {})
+    extracted["c_suite_members"] = extract_value(org_decision.get("c_suite_members"), [])
+    extracted["key_decision_makers"] = extract_value(org_decision.get("key_decision_makers"), [])
+    extracted["firm_ownership_type"] = extract_value(org_decision.get("firm_ownership_type"))
+    extracted["recent_leadership_changes"] = extract_value(org_decision.get("recent_leadership_changes"), [])
+
     # Distribution & Coverage Relevance (3 fields)
-    distribution = data.get('distribution_coverage_relevance', {})
-    extracted['target_investor_type'] = extract_value(distribution.get('target_investor_type'), [])
-    extracted['regulated_by'] = extract_value(distribution.get('regulated_by'), [])
-    extracted['litigation_regulatory_risk'] = extract_value(distribution.get('litigation_regulatory_risk'), [])
-    
+    distribution = data.get("distribution_coverage_relevance", {})
+    extracted["target_investor_type"] = extract_value(distribution.get("target_investor_type"), [])
+    extracted["regulated_by"] = extract_value(distribution.get("regulated_by"), [])
+    extracted["litigation_regulatory_risk"] = extract_value(distribution.get("litigation_regulatory_risk"), [])
+
     # Activity & Sentiment Signals (5 fields)
-    activity_signals = data.get('activity_sentiment_signals', {})
-    extracted['recent_hires'] = extract_value(activity_signals.get('recent_hires'), [])
-    extracted['new_offices_expansion'] = extract_value(activity_signals.get('new_offices_expansion'), [])
-    extracted['M&A_fund_spinouts'] = extract_value(activity_signals.get('M&A_fund_spinouts'), [])
-    extracted['awards_events_press'] = extract_value(activity_signals.get('awards_events_press'), [])
-    extracted['public_fund_strategy_commentary'] = extract_value(activity_signals.get('public_fund_strategy_commentary'))
-    extracted['recent_news'] = extract_value(activity_signals.get('recent_news'), [])
-    
+    activity_signals = data.get("activity_sentiment_signals", {})
+    extracted["recent_hires"] = extract_value(activity_signals.get("recent_hires"), [])
+    extracted["new_offices_expansion"] = extract_value(activity_signals.get("new_offices_expansion"), [])
+    extracted["M&A_fund_spinouts"] = extract_value(activity_signals.get("M&A_fund_spinouts"), [])
+    extracted["awards_events_press"] = extract_value(activity_signals.get("awards_events_press"), [])
+    extracted["public_fund_strategy_commentary"] = extract_value(activity_signals.get("public_fund_strategy_commentary"))
+    extracted["recent_news"] = extract_value(activity_signals.get("recent_news"), [])
+
     # Add computed fields for easier querying
-    extracted['has_aum'] = bool(extracted.get('aum'))
-    extracted['has_year_founded'] = bool(extracted.get('year_founded'))
-    extracted['office_locations_count'] = len(extracted.get('additional_office_locations', []))
-    extracted['sector_focus_count'] = len(extracted.get('sector_focus', []))
-    extracted['geographic_focus_count'] = len(extracted.get('geographic_focus', []))
-    extracted['current_funds_count'] = len(extracted.get('current_funds_active', []))
-    extracted['recent_hires_count'] = len(extracted.get('recent_hires', []))
-    extracted['c_suite_members_count'] = len(extracted.get('c_suite_members', []))
-    extracted['key_decision_makers_count'] = len(extracted.get('key_decision_makers', []))
-    extracted['recent_news_count'] = len(extracted.get('recent_news', []))
-    
+    extracted["has_aum"] = bool(extracted.get("aum"))
+    extracted["has_year_founded"] = bool(extracted.get("year_founded"))
+    extracted["office_locations_count"] = len(extracted.get("additional_office_locations", []))
+    extracted["sector_focus_count"] = len(extracted.get("sector_focus", []))
+    extracted["geographic_focus_count"] = len(extracted.get("geographic_focus", []))
+    extracted["current_funds_count"] = len(extracted.get("current_funds_active", []))
+    extracted["recent_hires_count"] = len(extracted.get("recent_hires", []))
+    extracted["c_suite_members_count"] = len(extracted.get("c_suite_members", []))
+    extracted["key_decision_makers_count"] = len(extracted.get("key_decision_makers", []))
+    extracted["recent_news_count"] = len(extracted.get("recent_news", []))
+
     return extracted

--- app/utils/dependencies.py
+++ app/utils/dependencies.py
@@ -13,12 +13,14 @@
 from app.models.current_user import CurrentUser
 import time
 
+
 async def get_db() -> ProspectingDB:
     """FastAPI dependency to get the global database instance."""
     db = await get_global_db()
     print(f"üîó Dependencies: Providing global database instance: {id(db)}")
     return db
 
+
 async def get_client_ip(request: Request) -> str:
     # Try to get real client IP from headers, fallback to request.client.host
     x_forwarded_for = request.headers.get("x-forwarded-for")
@@ -26,6 +28,7 @@
         return x_forwarded_for.split(",")[0].strip()
     return request.client.host
 
+
 def rate_limit_dependency(limit: int, window: int):
     """
     Flexible rate limiting dependency for FastAPI endpoints.
@@ -43,14 +46,12 @@
         identifier = getattr(current_user, "identifier", None) or getattr(current_user, "user_id", None)
         if not identifier:
             identifier = await get_client_ip(request)
-        
+
         # Use the new check_window_limit method
         allowed = await rate_limiter.check_window_limit(identifier, request.url.path, limit, window)
         if not allowed:
-            raise HTTPException(
-                status_code=429,
-                detail=f"Rate limit exceeded: {limit} requests per {window} seconds."
-            )
+            raise HTTPException(status_code=429, detail=f"Rate limit exceeded: {limit} requests per {window} seconds.")
+
     return dependency
 
 
@@ -60,6 +61,7 @@
     Uses user_id if available in request.state.user, else falls back to client IP.
     Usage: dependencies=[Depends(public_rate_limit_dependency(5, 60))]
     """
+
     async def dependency(request: Request):
         rate_limiter = RateLimiter()
         identifier = None
@@ -69,14 +71,12 @@
             identifier = user.user_id
         if not identifier:
             identifier = await get_client_ip(request)
-        
+
         # Use the new check_window_limit method
         allowed = await rate_limiter.check_window_limit(identifier, request.url.path, limit, window)
         if not allowed:
-            raise HTTPException(
-                status_code=429,
-                detail=f"Rate limit exceeded: {limit} requests per {window} seconds."
-            )
+            raise HTTPException(status_code=429, detail=f"Rate limit exceeded: {limit} requests per {window} seconds.")
+
     return dependency
 
 
@@ -93,12 +93,10 @@
         identifier = getattr(current_user, "identifier", None) or getattr(current_user, "user_id", None)
         if not identifier:
             identifier = await get_client_ip(request)
-        
+
         # Use the new check_window_limit method
         allowed = await rate_limiter.check_window_limit(identifier, request.url.path, limit, window)
         if not allowed:
-            raise HTTPException(
-                status_code=429,
-                detail=f"Rate limit exceeded: {limit} requests per {window} seconds."
-            )
-    return dependency 
+            raise HTTPException(status_code=429, detail=f"Rate limit exceeded: {limit} requests per {window} seconds.")
+
+    return dependency

--- app/utils/domain_utils.py
+++ app/utils/domain_utils.py
@@ -10,43 +10,43 @@
 def normalize_domain(domain: str) -> str:
     """
     Normalize a domain by removing protocol, path, and common subdomains.
-    
+
     Args:
         domain: Domain string (may include protocol, path, etc.)
-        
+
     Returns:
         Normalized root domain (e.g., 'company.com')
     """
     if not domain:
         return ""
-    
+
     # Remove protocol if present
     domain = domain.strip()
-    if domain.startswith(('http://', 'https://')):
+    if domain.startswith(("http://", "https://")):
         parsed = urlparse(domain)
         domain = parsed.netloc or parsed.path
     else:
         # Remove path if present
-        if '/' in domain:
-            domain = domain.split('/')[0]
-    
+        if "/" in domain:
+            domain = domain.split("/")[0]
+
     # Remove port if present
-    if ':' in domain:
-        domain = domain.split(':')[0]
-    
+    if ":" in domain:
+        domain = domain.split(":")[0]
+
     # Convert to lowercase
     domain = domain.lower().strip()
-    
+
     # Remove leading/trailing dots
-    domain = domain.strip('.')
-    
+    domain = domain.strip(".")
+
     return domain
 
 
 def clean_domain(domain: str) -> str:
     """
     Alias for normalize_domain for backward compatibility.
-    
+
     This function exists to maintain compatibility with existing code
     that uses clean_domain().
     """
@@ -56,104 +56,158 @@
 def extract_root_domain(domain: str) -> str:
     """
     Extract root domain from a potentially subdomain.
-    
+
     Examples:
         'blog.company.com' -> 'company.com'
         'www.company.co.uk' -> 'company.co.uk'
         'company.com' -> 'company.com'
-    
+
     Args:
         domain: Domain string (may be a subdomain)
-        
+
     Returns:
         Root domain
     """
     domain = normalize_domain(domain)
-    
+
     if not domain:
         return ""
-    
+
     # Split by dots
-    parts = domain.split('.')
-    
+    parts = domain.split(".")
+
     # Handle common cases
     if len(parts) <= 2:
         # Already a root domain (e.g., 'company.com')
         return domain
-    
+
     # Check for common subdomain prefixes
     common_subdomains = {
-        'www', 'blog', 'news', 'mail', 'email', 'webmail', 'ftp', 'smtp',
-        'pop', 'imap', 'admin', 'dashboard', 'app', 'api', 'cdn', 'static',
-        'assets', 'media', 'images', 'img', 'js', 'css', 'dev', 'staging',
-        'test', 'prod', 'production', 'secure', 'ssl', 'vpn', 'remote'
+        "www",
+        "blog",
+        "news",
+        "mail",
+        "email",
+        "webmail",
+        "ftp",
+        "smtp",
+        "pop",
+        "imap",
+        "admin",
+        "dashboard",
+        "app",
+        "api",
+        "cdn",
+        "static",
+        "assets",
+        "media",
+        "images",
+        "img",
+        "js",
+        "css",
+        "dev",
+        "staging",
+        "test",
+        "prod",
+        "production",
+        "secure",
+        "ssl",
+        "vpn",
+        "remote",
     }
-    
+
     # If first part is a common subdomain, remove it
     if parts[0].lower() in common_subdomains:
-        return '.'.join(parts[1:])
-    
+        return ".".join(parts[1:])
+
     # For domains like 'company.co.uk', we want to keep the last 2 parts
     # For domains like 'blog.company.com', we want to keep the last 2 parts
     # This is a simple heuristic - for more accuracy, use a public suffix list
-    
+
     # Simple approach: if more than 2 parts and first part is short (likely subdomain)
     if len(parts) > 2 and len(parts[0]) <= 4:
         # Likely a subdomain, return last 2 parts
-        return '.'.join(parts[-2:])
-    
+        return ".".join(parts[-2:])
+
     # Default: return last 2 parts (handles most cases)
-    return '.'.join(parts[-2:])
+    return ".".join(parts[-2:])
 
 
 def is_subdomain(domain: str) -> bool:
     """
     Check if a domain appears to be a subdomain.
-    
+
     Args:
         domain: Domain string
-        
+
     Returns:
         True if domain appears to be a subdomain
     """
     domain = normalize_domain(domain)
-    parts = domain.split('.')
-    
+    parts = domain.split(".")
+
     # If more than 2 parts, likely a subdomain
     if len(parts) > 2:
         return True
-    
+
     # Check for common subdomain patterns
     common_subdomains = {
-        'www', 'blog', 'news', 'mail', 'email', 'webmail', 'ftp', 'smtp',
-        'pop', 'imap', 'admin', 'dashboard', 'app', 'api', 'cdn', 'static',
-        'assets', 'media', 'images', 'img', 'js', 'css', 'dev', 'staging',
-        'test', 'prod', 'production', 'secure', 'ssl', 'vpn', 'remote'
+        "www",
+        "blog",
+        "news",
+        "mail",
+        "email",
+        "webmail",
+        "ftp",
+        "smtp",
+        "pop",
+        "imap",
+        "admin",
+        "dashboard",
+        "app",
+        "api",
+        "cdn",
+        "static",
+        "assets",
+        "media",
+        "images",
+        "img",
+        "js",
+        "css",
+        "dev",
+        "staging",
+        "test",
+        "prod",
+        "production",
+        "secure",
+        "ssl",
+        "vpn",
+        "remote",
     }
-    
+
     if len(parts) == 2 and parts[0].lower() in common_subdomains:
         return True
-    
+
     return False
 
 
 def check_domain_reachability(domain: str) -> Tuple[bool, Optional[str]]:
     """
     Check if a domain is reachable via DNS lookup.
-    
+
     Args:
         domain: Domain string to check
-        
+
     Returns:
         Tuple of (is_reachable, error_message)
     """
     import socket
-    
+
     domain = normalize_domain(domain)
-    
+
     if not domain:
         return False, "Empty domain"
-    
+
     try:
         # Try to resolve the domain
         socket.gethostbyname(domain)
@@ -167,93 +221,93 @@
 def suggest_parent_domain(domain: str) -> Optional[str]:
     """
     Suggest a parent domain if the input appears to be a subdomain.
-    
+
     Args:
         domain: Domain string (may be a subdomain)
-        
+
     Returns:
         Suggested parent domain, or None if not a subdomain
     """
     if not is_subdomain(domain):
         return None
-    
+
     root = extract_root_domain(domain)
     if root != domain:
         return root
-    
+
     return None
 
 
 def generate_company_id_from_domain(domain_or_url: str) -> str:
     """
     Generate a company_id from a domain or URL.
-    
+
     Normalizes the domain and converts it to a company_id format
     by replacing dots with underscores.
-    
+
     Args:
         domain_or_url: Domain string or URL (e.g., 'company.com' or 'https://www.company.com')
-        
+
     Returns:
         Company ID string (e.g., 'company_com')
     """
     # Normalize the domain first
     normalized = normalize_domain(domain_or_url)
-    
+
     if not normalized:
         # Fallback: try to extract domain from URL if normalization failed
-        if domain_or_url.startswith(('http://', 'https://')):
+        if domain_or_url.startswith(("http://", "https://")):
             try:
                 parsed = urlparse(domain_or_url)
                 normalized = parsed.netloc or parsed.path
             except Exception:
                 normalized = domain_or_url
-    
+
     if not normalized:
         return "unknown_company"
-    
+
     # Remove leading/trailing dots and convert to lowercase
-    normalized = normalized.strip('.').lower()
-    
+    normalized = normalized.strip(".").lower()
+
     # Replace dots with underscores to create company_id
-    company_id = normalized.replace('.', '_')
-    
+    company_id = normalized.replace(".", "_")
+
     return company_id
 
 
 def create_company_id_from_name(company_name: str) -> str:
     """
     Generate a company_id from a company name.
-    
+
     Converts company name to lowercase, removes special characters,
     and replaces spaces with underscores.
-    
+
     Args:
         company_name: Company name string (e.g., 'Acme Corp' or 'Acme, Inc.')
-        
+
     Returns:
         Company ID string (e.g., 'acme_corp')
     """
     if not company_name:
         return "unknown_company"
-    
+
     # Convert to lowercase and strip whitespace
     company_id = company_name.lower().strip()
-    
+
     # Remove common suffixes and special characters
-    company_id = re.sub(r'[^\w\s-]', '', company_id)  # Remove special chars except word chars, spaces, hyphens
-    company_id = re.sub(r'\s+', '_', company_id)  # Replace spaces with underscores
-    company_id = re.sub(r'_+', '_', company_id)  # Replace multiple underscores with single
-    company_id = company_id.strip('_')  # Remove leading/trailing underscores
-    
+    company_id = re.sub(r"[^\w\s-]", "", company_id)  # Remove special chars except word chars, spaces, hyphens
+    company_id = re.sub(r"\s+", "_", company_id)  # Replace spaces with underscores
+    company_id = re.sub(r"_+", "_", company_id)  # Replace multiple underscores with single
+    company_id = company_id.strip("_")  # Remove leading/trailing underscores
+
     # Remove common company suffixes
-    suffixes = ['_inc', '_llc', '_ltd', '_corp', '_corporation', '_limited', '_company', '_co']
+    suffixes = ["_inc", "_llc", "_ltd", "_corp", "_corporation", "_limited", "_company", "_co"]
     for suffix in suffixes:
         if company_id.endswith(suffix):
-            company_id = company_id[:-len(suffix)]
+            company_id = company_id[: -len(suffix)]
             break
-    
+
     if not company_id:
         return "unknown_company"
-    
+
     return company_id

--- app/utils/global_db.py
+++ app/utils/global_db.py
@@ -23,13 +23,14 @@
 
 logger = get_logger(__name__)
 
+
 class GlobalDB:
     """Singleton database manager for the entire application."""
-    
+
     _instance: Optional[ProspectingDB] = None
     _lock = threading.Lock()
     _initialized = False
-    
+
     @classmethod
     async def get_instance(cls) -> ProspectingDB:
         """Get the global database instance, creating it if necessary."""
@@ -43,7 +44,7 @@
         else:
             print(f"üîó GlobalDB: Returning existing database instance: {id(cls._instance)}")
         return cls._instance
-    
+
     @classmethod
     async def initialize(cls) -> ProspectingDB:
         """Initialize the global database instance at application startup."""
@@ -59,7 +60,7 @@
         else:
             print(f"‚úÖ Global database instance already initialized: {id(cls._instance)}")
         return cls._instance
-    
+
     @classmethod
     async def close(cls):
         """Close the global database instance at application shutdown."""
@@ -75,12 +76,12 @@
                     print(f"‚úÖ Global database instance closed: {instance_id}")
         else:
             print("‚úÖ Global database instance already closed or never initialized")
-    
+
     @classmethod
     def is_initialized(cls) -> bool:
         """Check if the global database instance is initialized."""
         return cls._initialized
-    
+
     @classmethod
     def reset_for_testing(cls):
         """Reset the singleton for testing purposes."""
@@ -89,9 +90,10 @@
             cls._instance = None
             cls._initialized = False
 
+
 # Convenience function for getting the global database instance
 async def get_global_db() -> ProspectingDB:
     """Get the global database instance."""
     db = await GlobalDB.get_instance()
     print(f"üîó get_global_db: Returning database instance: {id(db)}")
-    return db 
\ No newline at end of file
+    return db

--- app/utils/langsmith_config.py
+++ app/utils/langsmith_config.py
@@ -25,10 +25,11 @@
 # Thread-local storage for current run context
 _run_context = threading.local()
 
+
 def initialize_langsmith():
     """Initialize LangSmith client if API key is available."""
     global langsmith_client
-    
+
     if LANGSMITH_API_KEY:
         try:
             langsmith_client = Client(api_key=LANGSMITH_API_KEY)
@@ -38,39 +39,38 @@
     else:
         print("‚ö†Ô∏è LangSmith API key not found - tracing disabled")
 
+
 def set_run_context(run_id: str, user_id: str = None, session_id: str = None, parent_run_id: str = None):
     """Set the current run context for LangSmith tracing."""
-    if not hasattr(_run_context, 'current_run'):
+    if not hasattr(_run_context, "current_run"):
         _run_context.current_run = {}
-    _run_context.current_run = {
-        'run_id': run_id,
-        'user_id': user_id,
-        'session_id': session_id,
-        'parent_run_id': parent_run_id
-    }
+    _run_context.current_run = {"run_id": run_id, "user_id": user_id, "session_id": session_id, "parent_run_id": parent_run_id}
+
 
 def get_run_context() -> Optional[Dict[str, str]]:
     """Get the current run context."""
-    if hasattr(_run_context, 'current_run'):
+    if hasattr(_run_context, "current_run"):
         return _run_context.current_run
     return None
 
+
 def clear_run_context():
     """Clear the current run context."""
-    if hasattr(_run_context, 'current_run'):
-        delattr(_run_context, 'current_run')
+    if hasattr(_run_context, "current_run"):
+        delattr(_run_context, "current_run")
+
 
 @contextmanager
 def create_main_run(run_id: str, user_id: str, session_id: str, metadata: Dict[str, Any] = None):
     """
     Create a main run context for LangSmith tracing.
-    
+
     Args:
         run_id: Unique run identifier
         user_id: User identifier
         session_id: Session identifier
         metadata: Additional metadata for the run
-        
+
     Yields:
         The trace object for the main run
     """
@@ -78,30 +78,20 @@
         print("‚ö†Ô∏è LangSmith not initialized - skipping main run creation")
         yield None
         return
-    
+
     # Create main run metadata
-    run_metadata = {
-        'run_id': run_id,
-        'user_id': user_id,
-        'session_id': session_id,
-        'project': LANGSMITH_PROJECT,
-        **(metadata or {})
-    }
-    
+    run_metadata = {"run_id": run_id, "user_id": user_id, "session_id": session_id, "project": LANGSMITH_PROJECT, **(metadata or {})}
+
     try:
         # Create main run using the trace class
         with trace(
-            name=f"prospecting_run_{run_id}",
-            run_type="chain",
-            metadata=run_metadata,
-            project_name=LANGSMITH_PROJECT,
-            client=langsmith_client
+            name=f"prospecting_run_{run_id}", run_type="chain", metadata=run_metadata, project_name=LANGSMITH_PROJECT, client=langsmith_client
         ) as main_trace:
             print(f"‚úÖ Created main LangSmith run: {main_trace.id if main_trace else 'None'}")
-            
+
             # Set the run context with the main trace ID
             set_run_context(run_id, user_id, session_id, main_trace.id if main_trace else None)
-            
+
             yield main_trace
     except Exception as e:
         print(f"‚ö†Ô∏è LangSmith main run creation error: {e}")
@@ -110,16 +100,17 @@
         # Clear the run context
         clear_run_context()
 
+
 @contextmanager
 def trace_operation(operation_name: str, metadata: Dict[str, Any] = None, parent_run_id: str = None):
     """
     Trace an operation with LangSmith.
-    
+
     Args:
         operation_name: Name of the operation being traced
         metadata: Additional metadata for the operation
         parent_run_id: Optional parent run ID for linking
-        
+
     Yields:
         The trace object for the operation
     """
@@ -127,40 +118,35 @@
         print("‚ö†Ô∏è LangSmith not initialized - skipping operation tracing")
         yield None
         return
-    
+
     # Get current run context
     run_context = get_run_context()
-    
+
     # Determine parent run ID
     if parent_run_id:
         # Use explicitly provided parent run ID
         actual_parent_run_id = parent_run_id
-    elif run_context and run_context.get('parent_run_id'):
+    elif run_context and run_context.get("parent_run_id"):
         # Use parent run ID from context
-        actual_parent_run_id = run_context.get('parent_run_id')
+        actual_parent_run_id = run_context.get("parent_run_id")
     else:
         # No parent run ID available
         actual_parent_run_id = None
-    
+
     # Prepare operation metadata
-    operation_metadata = {
-        'operation_name': operation_name,
-        **(metadata or {})
-    }
-    
+    operation_metadata = {"operation_name": operation_name, **(metadata or {})}
+
     # If we have a run context, add it to metadata
     if run_context:
-        operation_metadata.update({
-            'parent_run_id': run_context.get('run_id'),
-            'user_id': run_context.get('user_id'),
-            'session_id': run_context.get('session_id')
-        })
-    
+        operation_metadata.update(
+            {"parent_run_id": run_context.get("run_id"), "user_id": run_context.get("user_id"), "session_id": run_context.get("session_id")}
+        )
+
     # Create operation trace with parent linking
     # Handle trace setup errors separately to avoid interfering with exception propagation
     trace_context = None
     exc_info = (None, None, None)  # Track exception info for proper __exit__ handling
-    
+
     try:
         trace_context = trace(
             name=operation_name,
@@ -168,7 +154,7 @@
             metadata=operation_metadata,
             project_name=LANGSMITH_PROJECT,
             client=langsmith_client,
-            parent=actual_parent_run_id  # Link to parent run
+            parent=actual_parent_run_id,  # Link to parent run
         )
         operation_trace = trace_context.__enter__()
         print(f"‚úÖ Created LangSmith operation trace: {operation_trace.id if operation_trace else 'None'}")
@@ -179,7 +165,7 @@
         print(f"‚ö†Ô∏è LangSmith tracing setup error: {e}")
         operation_trace = None
         trace_context = None
-    
+
     try:
         # Yield and let exceptions from inside propagate naturally
         # @contextmanager will handle exception propagation correctly
@@ -187,6 +173,7 @@
     except Exception:
         # Capture exception info for proper cleanup
         import sys
+
         exc_info = sys.exc_info()
         # Re-raise to let @contextmanager handle it
         raise
@@ -198,4 +185,4 @@
                 trace_context.__exit__(*exc_info)
             except Exception as cleanup_error:
                 # Don't let cleanup errors mask the original exception
-                print(f"‚ö†Ô∏è LangSmith trace cleanup error: {cleanup_error}") 
\ No newline at end of file
+                print(f"‚ö†Ô∏è LangSmith trace cleanup error: {cleanup_error}")

--- app/utils/logging_config.py
+++ app/utils/logging_config.py
@@ -18,12 +18,13 @@
 ENVIRONMENT = get_environment()
 LOG_LEVEL = get_log_level()
 
+
 class StructuredFormatter(logging.Formatter):
     """
     Custom formatter that outputs structured JSON logs for CloudWatch.
     Includes contextual information like user_id, session_id, run_id when available.
     """
-    
+
     def format(self, record: logging.LogRecord) -> str:
         # Base log structure
         log_entry = {
@@ -34,39 +35,57 @@
             "module": record.module,
             "function": record.funcName,
             "line": record.lineno,
-            "environment": ENVIRONMENT
+            "environment": ENVIRONMENT,
         }
-        
+
         # Add exception info if present
         if record.exc_info:
             log_entry["exception"] = self.formatException(record.exc_info)
-        
+
         # Add custom fields from extra kwargs
         for key, value in record.__dict__.items():
-            if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 'pathname', 
-                          'filename', 'module', 'lineno', 'funcName', 'created', 
-                          'msecs', 'relativeCreated', 'thread', 'threadName', 
-                          'processName', 'process', 'message', 'exc_info', 'exc_text', 'stack_info']:
+            if key not in [
+                "name",
+                "msg",
+                "args",
+                "levelname",
+                "levelno",
+                "pathname",
+                "filename",
+                "module",
+                "lineno",
+                "funcName",
+                "created",
+                "msecs",
+                "relativeCreated",
+                "thread",
+                "threadName",
+                "processName",
+                "process",
+                "message",
+                "exc_info",
+                "exc_text",
+                "stack_info",
+            ]:
                 log_entry[key] = value
-        
+
         return json.dumps(log_entry, default=str, ensure_ascii=False)
 
+
 class DevelopmentFormatter(logging.Formatter):
     """Human-readable formatter for development environment."""
-    
+
     def __init__(self):
-        super().__init__(
-            fmt='%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s',
-            datefmt='%Y-%m-%d %H:%M:%S'
-        )
+        super().__init__(fmt="%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
+
 
 def setup_logging() -> None:
     """
     Configure logging for the entire application.
-    Uses JSON structured logging for production (CloudWatch) and 
+    Uses JSON structured logging for production (CloudWatch) and
     human-readable format for development.
     """
-    
+
     # Determine formatter based on environment
     if ENVIRONMENT == "production":
         formatter_class = StructuredFormatter
@@ -74,7 +93,7 @@
     else:
         formatter_class = DevelopmentFormatter
         handler_class = logging.StreamHandler
-    
+
     # Logging configuration
     config = {
         "version": 1,
@@ -85,14 +104,14 @@
             },
             "development": {
                 "()": DevelopmentFormatter,
-            }
+            },
         },
         "handlers": {
             "console": {
                 "class": "logging.StreamHandler",
                 "stream": "ext://sys.stdout",
                 "formatter": "structured" if ENVIRONMENT == "production" else "development",
-                "level": LOG_LEVEL
+                "level": LOG_LEVEL,
             },
             "file": {
                 "class": "logging.handlers.RotatingFileHandler",
@@ -100,126 +119,61 @@
                 "maxBytes": 10485760,  # 10MB
                 "backupCount": 5,
                 "formatter": "development",
-                "level": LOG_LEVEL
-            }
+                "level": LOG_LEVEL,
+            },
         },
         "loggers": {
             # Application loggers
-            "app": {
-                "level": LOG_LEVEL,
-                "handlers": ["console", "file"],
-                "propagate": False
-            },
+            "app": {"level": LOG_LEVEL, "handlers": ["console", "file"], "propagate": False},
             # Specific component loggers
-            "app.agents": {
-                "level": LOG_LEVEL,
-                "handlers": ["console", "file"],
-                "propagate": False
-            },
-            "app.routers": {
-                "level": LOG_LEVEL,
-                "handlers": ["console", "file"],
-                "propagate": False
-            },
-            "app.middleware": {
-                "level": LOG_LEVEL,
-                "handlers": ["console", "file"],
-                "propagate": False
-            },
-            "app.utils": {
-                "level": LOG_LEVEL,
-                "handlers": ["console", "file"],
-                "propagate": False
-            },
-            "app.utils.secrets_manager": {
-                "level": "WARNING",
-                "handlers": ["console", "file"],
-                "propagate": False
-            },
-            "app.tasks": {
-                "level": LOG_LEVEL,
-                "handlers": ["console", "file"],
-                "propagate": False
-            },
+            "app.agents": {"level": LOG_LEVEL, "handlers": ["console", "file"], "propagate": False},
+            "app.routers": {"level": LOG_LEVEL, "handlers": ["console", "file"], "propagate": False},
+            "app.middleware": {"level": LOG_LEVEL, "handlers": ["console", "file"], "propagate": False},
+            "app.utils": {"level": LOG_LEVEL, "handlers": ["console", "file"], "propagate": False},
+            "app.utils.secrets_manager": {"level": "WARNING", "handlers": ["console", "file"], "propagate": False},
+            "app.tasks": {"level": LOG_LEVEL, "handlers": ["console", "file"], "propagate": False},
             # Third-party loggers (reduced verbosity)
-            "uvicorn": {
-                "level": "INFO",
-                "handlers": ["console"],
-                "propagate": False
-            },
-            "uvicorn.access": {
-                "level": "WARNING",
-                "handlers": ["console"],
-                "propagate": False
-            },
-            "asyncpg": {
-                "level": "WARNING",
-                "handlers": ["console"],
-                "propagate": False
-            },
-            "httpx": {
-                "level": "WARNING",
-                "handlers": ["console"],
-                "propagate": False
-            },
+            "uvicorn": {"level": "INFO", "handlers": ["console"], "propagate": False},
+            "uvicorn.access": {"level": "WARNING", "handlers": ["console"], "propagate": False},
+            "asyncpg": {"level": "WARNING", "handlers": ["console"], "propagate": False},
+            "httpx": {"level": "WARNING", "handlers": ["console"], "propagate": False},
             # Celery loggers - ensure they output to console
-            "celery": {
-                "level": "INFO",
-                "handlers": ["console"],
-                "propagate": False
-            },
-            "celery.worker": {
-                "level": "INFO",
-                "handlers": ["console"],
-                "propagate": False
-            },
-            "celery.task": {
-                "level": "INFO",
-                "handlers": ["console"],
-                "propagate": False
-            },
-            "kombu": {
-                "level": "WARNING",
-                "handlers": ["console"],
-                "propagate": False
-            }
+            "celery": {"level": "INFO", "handlers": ["console"], "propagate": False},
+            "celery.worker": {"level": "INFO", "handlers": ["console"], "propagate": False},
+            "celery.task": {"level": "INFO", "handlers": ["console"], "propagate": False},
+            "kombu": {"level": "WARNING", "handlers": ["console"], "propagate": False},
         },
-        "root": {
-            "level": LOG_LEVEL,
-            "handlers": ["console"]
-        }
+        "root": {"level": LOG_LEVEL, "handlers": ["console"]},
     }
-    
+
     # Apply configuration
     logging.config.dictConfig(config)
-    
+
     # Log initialization
     logger = logging.getLogger("app.logging")
     logger.info(
         "Logging initialized",
-        extra={
-            "environment": ENVIRONMENT,
-            "log_level": LOG_LEVEL,
-            "formatter": "structured" if ENVIRONMENT == "production" else "development"
-        }
+        extra={"environment": ENVIRONMENT, "log_level": LOG_LEVEL, "formatter": "structured" if ENVIRONMENT == "production" else "development"},
     )
 
+
 def get_logger(name: str) -> logging.Logger:
     """
     Get a logger instance with the specified name.
-    
+
     Args:
         name: Logger name, typically __name__
-        
+
     Returns:
         Configured logger instance
     """
     return logging.getLogger(name)
 
+
 def log_with_context(logger: logging.Logger, level: str, message: str, **context) -> None:
     """
     Log a message with additional context.
-    
+
     Args:
         logger: Logger instance
         level: Log level (info, warning, error, etc.)
@@ -229,31 +183,25 @@
     log_method = getattr(logger, level.lower())
     log_method(message, extra=context)
 
+
 # Performance monitoring context
 class PerformanceLogger:
     """Context manager for logging performance metrics."""
-    
+
     def __init__(self, logger: logging.Logger, operation: str, **context):
         self.logger = logger
         self.operation = operation
         self.context = context
         self.start_time = None
-    
+
     def __enter__(self):
         self.start_time = datetime.now()
-        log_with_context(
-            self.logger, 
-            "info", 
-            f"Started {self.operation}",
-            operation=self.operation,
-            operation_status="started",
-            **self.context
-        )
+        log_with_context(self.logger, "info", f"Started {self.operation}", operation=self.operation, operation_status="started", **self.context)
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         duration = (datetime.now() - self.start_time).total_seconds()
-        
+
         if exc_type is None:
             log_with_context(
                 self.logger,
@@ -262,7 +210,7 @@
                 operation=self.operation,
                 operation_status="completed",
                 duration_seconds=duration,
-                **self.context
+                **self.context,
             )
         else:
             log_with_context(
@@ -274,44 +222,32 @@
                 duration_seconds=duration,
                 error_type=exc_type.__name__,
                 error_message=str(exc_val),
-                **self.context
+                **self.context,
             )
 
+
 # Error logging utilities
 def log_error(logger: logging.Logger, error: Exception, message: str = None, **context) -> None:
     """
     Log an error with full context.
-    
+
     Args:
         logger: Logger instance
         error: Exception object
         message: Custom error message
         **context: Additional context
     """
-    log_with_context(
-        logger,
-        "error",
-        message or str(error),
-        error_type=type(error).__name__,
-        error_message=str(error),
-        **context
-    )
+    log_with_context(logger, "error", message or str(error), error_type=type(error).__name__, error_message=str(error), **context)
+
 
 def log_security_event(logger: logging.Logger, event_type: str, message: str, **context) -> None:
     """
     Log security-related events.
-    
+
     Args:
         logger: Logger instance
         event_type: Type of security event (auth_failure, rate_limit, etc.)
         message: Event message
         **context: Additional context
     """
-    log_with_context(
-        logger,
-        "warning",
-        message,
-        security_event=True,
-        event_type=event_type,
-        **context
-    ) 
+    log_with_context(logger, "warning", message, security_event=True, event_type=event_type, **context)

--- app/utils/perplexity_utils.py
+++ app/utils/perplexity_utils.py
@@ -1,6 +1,7 @@
 """
 Utility functions for parsing and merging Perplexity API outputs.
 """
+
 import json
 from typing import Dict, Any, List, Union
 
@@ -9,11 +10,11 @@
     """
     Append all information from both website and external sources.
     Simple additive approach - keep everything from both sources.
-    
+
     Args:
         website_value: Value from website data
         external_value: Value from external data
-        
+
     Returns:
         Combined value with all information from both sources
     """
@@ -22,33 +23,33 @@
         return external_value
     if external_value is None:
         return website_value
-        
+
     # Handle different data types
     if isinstance(website_value, list) and isinstance(external_value, list):
         # For lists, simply combine all items from both sources
         combined = []
-        
+
         # Add all website items first
         for item in website_value:
             if item:  # Only add non-empty items
                 combined.append(item)
-        
+
         # Add all external items
         for item in external_value:
             if item:  # Only add non-empty items
                 combined.append(item)
-        
+
         return combined if combined else None
-    
+
     elif isinstance(website_value, dict) and isinstance(external_value, dict):
         # For dictionaries, combine all keys from both sources
         result = {}
-        
+
         # Add all website fields
         for key, value in website_value.items():
             if value is not None:
                 result[key] = value
-        
+
         # Add all external fields
         for key, value in external_value.items():
             if value is not None:
@@ -57,14 +58,14 @@
                     result[key] = append_all_information(result[key], value)
                 else:
                     result[key] = value
-        
+
         return result
-    
+
     elif isinstance(website_value, str) and isinstance(external_value, str):
         # For strings, combine with separator if they're different
         website_str = website_value.strip()
         external_str = external_value.strip()
-        
+
         if website_str and external_str:
             if website_str.lower() == external_str.lower():
                 # Same content, return one
@@ -78,12 +79,12 @@
             return external_str
         else:
             return None
-    
+
     else:
         # For mixed types or other cases, convert to strings and combine
         website_str = str(website_value).strip() if website_value is not None else ""
         external_str = str(external_value).strip() if external_value is not None else ""
-        
+
         if website_str and external_str:
             return f"{website_str}; {external_str}"
         elif website_str:
@@ -100,79 +101,77 @@
     Each field should have a 'value' and optional 'citation'.
     """
     result = {}
-    
-    # Handle completely empty categories  
+
+    # Handle completely empty categories
     if not website_category and not external_category:
         return {}
-    
+
     # If one category is missing, return the other
     if not website_category:
         return external_category
     if not external_category:
         return website_category
-    
+
     # Get all unique field names from both categories
     all_fields = set(website_category.keys()) | set(external_category.keys())
-    
+
     for field in all_fields:
         # Extract field data from each source
         website_field = website_category.get(field)
         external_field = external_category.get(field)
-        
-        # Extract values and citations 
+
+        # Extract values and citations
         if website_field is not None:
             if isinstance(website_field, dict):
-                website_value = website_field.get('value')
-                website_citations = website_field.get('citation', [])
+                website_value = website_field.get("value")
+                website_citations = website_field.get("citation", [])
             else:
                 website_value = website_field
                 website_citations = []
         else:
             website_value = None
             website_citations = []
-            
+
         if external_field is not None:
             if isinstance(external_field, dict):
-                external_value = external_field.get('value')
-                external_citations = external_field.get('citation', [])
+                external_value = external_field.get("value")
+                external_citations = external_field.get("citation", [])
             else:
                 external_value = external_field
                 external_citations = []
         else:
             external_value = None
             external_citations = []
-        
+
         # Combine values and citations
         merged_value = append_all_information(website_value, external_value)
-        
+
         # Combine citations (ensure it's a list and remove duplicates)
         all_citations = []
         if isinstance(website_citations, list):
             all_citations.extend(website_citations)
         elif website_citations:
             all_citations.append(website_citations)
-        
+
         if isinstance(external_citations, list):
             all_citations.extend(external_citations)
         elif external_citations:
             all_citations.append(external_citations)
-        
+
         # Remove duplicates while preserving order
         unique_citations = []
         for citation in all_citations:
             if citation and citation not in unique_citations:
                 unique_citations.append(citation)
-        
+
         # Store the merged field if there's meaningful content
         if merged_value is not None and str(merged_value).strip():
-            result[field] = {
-                'value': merged_value
-            }
-            
+            result[field] = {"value": merged_value}
+
             # Add citations if available
             if unique_citations:
                 result[field]["citation"] = unique_citations
-    
+
     return result
 
 
@@ -180,67 +179,72 @@
     """
     Merge data from website and external Perplexity outputs.
     Uses V2 structured format following the investment firm schema.
-    
+
     Args:
         website_data: JSON data from website OSINT (V2 format expected)
         external_data: JSON data from external OSINT (V2 format expected)
-        
+
     Returns:
         Merged JSON data following V2 schema structure with proper citations
     """
     # Guard clauses for None or empty inputs
     if website_data is None and external_data is None:
         return {}
-    
+
     if website_data is None or not isinstance(website_data, dict):
         return external_data if isinstance(external_data, dict) else {}
-    
+
     if external_data is None or not isinstance(external_data, dict):
         return website_data
-    
+
     # Initialize result with V2 schema structure
     result = {}
-    
+
     # Define the expected V2 schema categories
     v2_categories = [
-        "core_profile", "investment_strategy_mandate", "fund_information",
-        "capital_commitments", "assets_under_management", "organization_decision_making",
-        "distribution_coverage_relevance", "activity_sentiment_signals"
+        "core_profile",
+        "investment_strategy_mandate",
+        "fund_information",
+        "capital_commitments",
+        "assets_under_management",
+        "organization_decision_making",
+        "distribution_coverage_relevance",
+        "activity_sentiment_signals",
     ]
-    
+
     # Process each V2 category
     for category in v2_categories:
         website_category = website_data.get(category, {})
         external_category = external_data.get(category, {})
-        
+
         # Merge this category
         merged_category = merge_category_fields(website_category, external_category)
-        
+
         # Add to result if it has any content
         if merged_category and isinstance(merged_category, dict) and len(merged_category) > 0:
             result[category] = merged_category
-    
+
     return result
 
 
 def safe_parse_perplexity_content(content: Union[str, Dict, List, None]) -> Union[Dict, List]:
     """
     Safely parse JSON content from Perplexity API response.
-    
+
     Args:
         content: String or dict content from Perplexity API
-        
+
     Returns:
         Parsed JSON as dict/list or empty dict on failure
     """
     # Handle None case
     if content is None:
         return {}
-        
+
     # If already a dict or list, return as is
     if isinstance(content, (dict, list)):
         return content
-        
+
     # Try to parse if it's a string
     if isinstance(content, str):
         try:
@@ -249,31 +253,31 @@
             return parsed
         except json.JSONDecodeError:
             # If the content looks like JSON but has issues, try to clean it up
-            if '{' in content and '}' in content:
+            if "{" in content and "}" in content:
                 try:
                     # Find what looks like the start and end of a JSON object
-                    start = content.find('{')
-                    end = content.rfind('}') + 1
+                    start = content.find("{")
+                    end = content.rfind("}") + 1
                     if start < end:
                         cleaned_content = content[start:end]
                         return json.loads(cleaned_content)
                 except:
                     pass
-                    
+
             # If the content looks like a list but has issues
-            if '[' in content and ']' in content:
+            if "[" in content and "]" in content:
                 try:
                     # Find what looks like the start and end of a JSON list
-                    start = content.find('[')
-                    end = content.rfind(']') + 1
+                    start = content.find("[")
+                    end = content.rfind("]") + 1
                     if start < end:
                         cleaned_content = content[start:end]
                         return json.loads(cleaned_content)
                 except:
                     pass
-                    
+
             # If we couldn't parse it, return empty dict
             return {}
-            
+
     # For any other type, return empty dict
-    return {} 
\ No newline at end of file
+    return {}

--- app/utils/progress_store.py
+++ app/utils/progress_store.py
@@ -87,18 +87,21 @@
                 await self._redis.delete(key)
             except Exception:
                 # Fall back to setting a minimal reset map
-                await self._redis.hset(key, mapping={
-                    "progress_pct": 0,
-                    "last_progress_pct": 0,
-                    "display_progress": 0,
-                    "web_progress": 0.0,
-                    "coresignal_progress": 0.0,
-                    "company_enrichment_progress": 0.0,
-                    "person_enrichment_progress": 0.0,
-                    "next_checkpoint_idx": 0,
-                    "last_tick_at": now_iso,
-                    "updated_at": now_iso,
-                })
+                await self._redis.hset(
+                    key,
+                    mapping={
+                        "progress_pct": 0,
+                        "last_progress_pct": 0,
+                        "display_progress": 0,
+                        "web_progress": 0.0,
+                        "coresignal_progress": 0.0,
+                        "company_enrichment_progress": 0.0,
+                        "person_enrichment_progress": 0.0,
+                        "next_checkpoint_idx": 0,
+                        "last_tick_at": now_iso,
+                        "updated_at": now_iso,
+                    },
+                )
                 await self._redis.expire(key, 60 * 60 * 48)
             return
 
@@ -130,7 +133,7 @@
 
     # --- Simple checkpoint-based smoothing (server-side) ---
     _RATE_PCT_PER_SEC: float = 0.5  # 0.5% per second between checkpoints
-    _SAFETY_MARGIN_PCT: int = 1     # never show at/above next checkpoint until actually reached
+    _SAFETY_MARGIN_PCT: int = 1  # never show at/above next checkpoint until actually reached
 
     _CHECKPOINTS_BY_WORKFLOW: Dict[str, List[int]] = {
         # Tune as desired; must be increasing and end with 100
@@ -422,5 +425,3 @@
                 run_id,
                 payload,
             )
-
-

--- app/utils/qdrant_utils.py
+++ app/utils/qdrant_utils.py
@@ -11,10 +11,7 @@
 
 
 def get_openai_embedding(text: str, model: str = "text-embedding-3-small") -> List[float]:
-    response = openai.embeddings.create(
-        input=text,
-        model=model
-    )
+    response = openai.embeddings.create(input=text, model=model)
     return response.data[0].embedding
 
 
@@ -54,14 +51,14 @@
 
     # Embed all chunks
     for chunk in chunks:
-        text = chunk.get('text') or chunk.get('content')
+        text = chunk.get("text") or chunk.get("content")
         if text is None:
             raise ValueError(f"Chunk missing both 'text' and 'content': {chunk}")
-        chunk['embedding'] = embedding_func(text)
+        chunk["embedding"] = embedding_func(text)
 
     # Infer vector size if not provided
     if not vector_size:
-        vector_size = len(chunks[0]['embedding'])
+        vector_size = len(chunks[0]["embedding"])
 
     # Create collection if needed
     if collection_name not in [c.name for c in qdrant_client.get_collections().collections]:
@@ -69,43 +66,27 @@
             collection_name=collection_name,
             vectors_config=VectorParams(size=vector_size, distance=distance),
         )
-        
+
         # Create indexes for filtering fields after collection creation
         # Index for company_id filtering
-        qdrant_client.create_payload_index(
-            collection_name=collection_name,
-            field_name="company_id",
-            field_schema="keyword"
-        )
-        
-        # Index for run_id filtering  
-        qdrant_client.create_payload_index(
-            collection_name=collection_name,
-            field_name="run_id", 
-            field_schema="keyword"
-        )
-        
+        qdrant_client.create_payload_index(collection_name=collection_name, field_name="company_id", field_schema="keyword")
+
+        # Index for run_id filtering
+        qdrant_client.create_payload_index(collection_name=collection_name, field_name="run_id", field_schema="keyword")
+
         # Index for user_id filtering
-        qdrant_client.create_payload_index(
-            collection_name=collection_name,
-            field_name="user_id",
-            field_schema="keyword"
-        )
-        
+        qdrant_client.create_payload_index(collection_name=collection_name, field_name="user_id", field_schema="keyword")
+
         # Index for section filtering (useful for additional filters)
-        qdrant_client.create_payload_index(
-            collection_name=collection_name,
-            field_name="section",
-            field_schema="keyword"
-        )
-        
+        qdrant_client.create_payload_index(collection_name=collection_name, field_name="section", field_schema="keyword")
+
         print(f"‚úÖ Created collection '{collection_name}' with indexes for company_id, run_id, user_id, and section")
 
     # Prepare and upsert in batches
     points = []
     for idx, chunk in enumerate(chunks):
-        text = chunk.get('text') or chunk.get('content')
-        meta = chunk.get('metadata', {})
+        text = chunk.get("text") or chunk.get("content")
+        meta = chunk.get("metadata", {})
         # Ensure required fields are present for filtering
         payload = {"text": text}
         # Always include company_id, run_id, user_id (even if None)
@@ -121,24 +102,14 @@
         # Add company_id for consistent filtering across collections if not present
         if not payload["company_id"] and "company" in payload:
             from app.utils.domain_utils import create_company_id_from_name
+
             payload["company_id"] = create_company_id_from_name(payload["company"])
-        points.append(
-            PointStruct(
-                id=idx,
-                vector=chunk['embedding'],
-                payload=payload
-            )
-        )
+        points.append(PointStruct(id=idx, vector=chunk["embedding"], payload=payload))
         # Upsert in batches
         if len(points) == batch_size or idx == len(chunks) - 1:
-            qdrant_client.upsert(
-                collection_name=collection_name,
-                points=points
-            )
+            qdrant_client.upsert(collection_name=collection_name, points=points)
             points = []
 
-    
-
 
 def search_chunks_in_qdrant(
     query_text: str,
@@ -156,7 +127,7 @@
 ) -> List[Dict[str, Any]]:
     """
     Search for chunks in a specific Qdrant collection with filtering by company_id, run_id, and user_id.
-    
+
     Args:
         query_text: Text to search for semantically
         collection_name: Qdrant collection to search (e.g., 'linkedin_posts', 'web_research')
@@ -168,66 +139,57 @@
         limit: Maximum number of results to return
         embedding_model: OpenAI embedding model for query
         qdrant_url, qdrant_api_key, openai_api_key: Credentials (from env if not provided)
-    
+
     Returns:
         List of matching chunks with their text, metadata, and similarity scores
     """
     # Load credentials
     qdrant_url = qdrant_url or get_qdrant_url()
-    qdrant_api_key = qdrant_api_key or get_qdrant_api_key() 
+    qdrant_api_key = qdrant_api_key or get_qdrant_api_key()
     openai_api_key = openai_api_key or get_openai_api_key()
     openai.api_key = openai_api_key
     qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
-    
+
     # Generate embedding for query
     query_embedding = get_openai_embedding(query_text, model=embedding_model)
-    
+
     # Build filters
     filter_conditions = []
-    
+
     # Company filtering (required)
     if company_id:
-        filter_conditions.append(
-            FieldCondition(key="company_id", match=MatchValue(value=company_id))
-        )
+        filter_conditions.append(FieldCondition(key="company_id", match=MatchValue(value=company_id)))
     elif company_name:
         # Convert company name to company_id for consistent filtering
         from app.utils.domain_utils import create_company_id_from_name
+
         company_id = create_company_id_from_name(company_name)
-        filter_conditions.append(
-            FieldCondition(key="company_id", match=MatchValue(value=company_id))
-        )
+        filter_conditions.append(FieldCondition(key="company_id", match=MatchValue(value=company_id)))
     else:
         print("‚ö†Ô∏è Warning: No company_id or company_name provided for filtering")
-    
+
     # Run filtering (required for run isolation)
     if run_id:
-        filter_conditions.append(
-            FieldCondition(key="run_id", match=MatchValue(value=run_id))
-        )
+        filter_conditions.append(FieldCondition(key="run_id", match=MatchValue(value=run_id)))
     else:
         print("‚ö†Ô∏è Warning: No run_id provided for filtering")
-    
+
     # User filtering (required for user isolation)
     if user_id:
-        filter_conditions.append(
-            FieldCondition(key="user_id", match=MatchValue(value=user_id))
-        )
+        filter_conditions.append(FieldCondition(key="user_id", match=MatchValue(value=user_id)))
     else:
         print("‚ö†Ô∏è Warning: No user_id provided for filtering")
-    
+
     # Additional filters
     if additional_filters:
         for field, value in additional_filters.items():
-            filter_conditions.append(
-                FieldCondition(key=field, match=MatchValue(value=value))
-            )
-    
+            filter_conditions.append(FieldCondition(key=field, match=MatchValue(value=value)))
+
     # Create filter object
     query_filter = None
     if filter_conditions:
         query_filter = Filter(must=filter_conditions)
-    
+
     # Perform search
     try:
         search_results = qdrant_client.search(
@@ -236,22 +198,24 @@
             query_filter=query_filter,
             limit=limit,
             with_payload=True,
-            with_vectors=False
+            with_vectors=False,
         )
-        
+
         # Format results
         formatted_results = []
         for result in search_results:
-            formatted_results.append({
-                "text": result.payload.get("text", ""),
-                "metadata": {k: v for k, v in result.payload.items() if k != "text"},
-                "score": result.score,
-                "id": result.id
-            })
-        
+            formatted_results.append(
+                {
+                    "text": result.payload.get("text", ""),
+                    "metadata": {k: v for k, v in result.payload.items() if k != "text"},
+                    "score": result.score,
+                    "id": result.id,
+                }
+            )
+
         print(f"üîç Found {len(formatted_results)} chunks in '{collection_name}' collection")
         return formatted_results
-        
+
     except Exception as e:
         print(f"‚ùå Error searching Qdrant collection '{collection_name}': {e}")
         return []
@@ -271,7 +235,7 @@
 ) -> int:
     """
     Extract LinkedIn posts from CoreSignal data and write them to a dedicated collection.
-    
+
     Args:
         coresignal_data: CoreSignal API response data
         company_name: Company name for metadata
@@ -281,45 +245,45 @@
         session_id: Session ID for data isolation
         collection_name: Qdrant collection name (default: 'linkedin_posts')
         qdrant_url, qdrant_api_key, openai_api_key: Credentials
-    
+
     Returns:
         Number of LinkedIn posts processed
     """
     from app.utils.chunking import build_uniform_metadata
-    
+
     if not isinstance(coresignal_data, dict):
         print("‚ö†Ô∏è Invalid CoreSignal data format")
         return 0
-    
+
     # Extract LinkedIn URL for citation
-    linkedin_url = coresignal_data.get('professional_network_url') or coresignal_data.get('linkedin_url', '')
-    
+    linkedin_url = coresignal_data.get("professional_network_url") or coresignal_data.get("linkedin_url", "")
+
     # Extract company updates (LinkedIn posts)
-    company_updates = coresignal_data.get('company_updates', [])
+    company_updates = coresignal_data.get("company_updates", [])
     if not isinstance(company_updates, list):
         print("‚ö†Ô∏è No company_updates found in CoreSignal data")
         return 0
-    
+
     # Create LinkedIn post chunks with enhanced metadata
     linkedin_chunks = []
     for idx, update in enumerate(company_updates):
         if isinstance(update, dict):
             # Handle None values properly
-            description = update.get('description', '')
-            update_text = description.strip() if description is not None else ''
+            description = update.get("description", "")
+            update_text = description.strip() if description is not None else ""
             if update_text:
                 # Enhanced metadata for LinkedIn posts
                 enhanced_metadata = build_uniform_metadata(
                     section="company_updates",
                     headline=f"LinkedIn Post {idx + 1}",
-                    date=update.get('date'),
+                    date=update.get("date"),
                     citation=linkedin_url,
                     source=linkedin_url,
                     company=company_name,
                     run_id=run_id,
                     source_type="Coresignal",
                     index=idx,
-                    user_id=user_id
+                    user_id=user_id,
                 )
                 # Always include company_id, run_id, user_id, session_id in metadata
                 enhanced_metadata["company_id"] = company_id
@@ -327,9 +291,9 @@
                 enhanced_metadata["user_id"] = user_id
                 enhanced_metadata["session_id"] = session_id
                 # Add LinkedIn-specific metadata with safe numeric handling
-                reactions_count = update.get('reactions_count') or 0
-                comments_count = update.get('comments_count') or 0
-                followers = update.get('followers') or 0
+                reactions_count = update.get("reactions_count") or 0
+                comments_count = update.get("comments_count") or 0
+                followers = update.get("followers") or 0
                 # Ensure numeric values are integers
                 try:
                     reactions_count = int(reactions_count) if reactions_count is not None else 0
@@ -339,34 +303,29 @@
                     reactions_count = 0
                     comments_count = 0
                     followers = 0
-                enhanced_metadata.update({
-                    "post_date": update.get('date'),
-                    "reactions_count": reactions_count,
-                    "comments_count": comments_count,
-                    "engagement_score": reactions_count + comments_count,
-                    "followers": followers,
-                    "reshared_post_author": update.get('reshared_post_author'),
-                    "reshared_post_date": update.get('reshared_post_date')
-                })
-                
-                linkedin_chunks.append({
-                    "text": update_text,
-                    "metadata": enhanced_metadata
-                })
-    
+                enhanced_metadata.update(
+                    {
+                        "post_date": update.get("date"),
+                        "reactions_count": reactions_count,
+                        "comments_count": comments_count,
+                        "engagement_score": reactions_count + comments_count,
+                        "followers": followers,
+                        "reshared_post_author": update.get("reshared_post_author"),
+                        "reshared_post_date": update.get("reshared_post_date"),
+                    }
+                )
+
+                linkedin_chunks.append({"text": update_text, "metadata": enhanced_metadata})
+
     if not linkedin_chunks:
         print("‚ö†Ô∏è No valid LinkedIn posts found to chunk")
         return 0
-    
+
     # Write to dedicated LinkedIn posts collection
     write_chunks_to_qdrant(
-        chunks=linkedin_chunks,
-        collection_name=collection_name,
-        qdrant_url=qdrant_url,
-        qdrant_api_key=qdrant_api_key,
-        openai_api_key=openai_api_key
+        chunks=linkedin_chunks, collection_name=collection_name, qdrant_url=qdrant_url, qdrant_api_key=qdrant_api_key, openai_api_key=openai_api_key
     )
-    
+
     print(f"üì± Processed {len(linkedin_chunks)} LinkedIn posts to '{collection_name}' collection")
     return len(linkedin_chunks)
 
@@ -381,7 +340,7 @@
     """
     Write JSON data to Qdrant without embedding, storing only the payload.
     This is used for storing structured JSON data that doesn't need vector search.
-    
+
     Args:
         chunks: List of dictionaries to store in Qdrant
         collection_name: Name of the collection to store data in
@@ -393,35 +352,26 @@
     qdrant_url = qdrant_url or get_qdrant_url()
     qdrant_api_key = qdrant_api_key or get_qdrant_api_key()
     qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
-    
+
     # Create collection if needed - create a dummy vector of size 1 since Qdrant requires vectors
     if collection_name not in [c.name for c in qdrant_client.get_collections().collections]:
         qdrant_client.recreate_collection(
             collection_name=collection_name,
             vectors_config=VectorParams(size=1, distance=Distance.COSINE),
         )
-    
+
     # Prepare and upsert in batches
     points = []
     for idx, chunk in enumerate(chunks):
         # Create a dummy vector of size 1 - this won't be used for search but is required by Qdrant
         dummy_vector = [0.0]
-        
+
         # Use the entire chunk as the payload
-        points.append(
-            PointStruct(
-                id=idx,
-                vector=dummy_vector,
-                payload=chunk
-            )
-        )
-        
+        points.append(PointStruct(id=idx, vector=dummy_vector, payload=chunk))
+
         # Upsert in batches
         if len(points) == batch_size or idx == len(chunks) - 1:
-            qdrant_client.upsert(
-                collection_name=collection_name,
-                points=points
-            )
+            qdrant_client.upsert(collection_name=collection_name, points=points)
             points = []
-    
-    print(f"‚úÖ Upserted {len(chunks)} JSON objects to Qdrant collection '{collection_name}' without embedding.") 
\ No newline at end of file
+
+    print(f"‚úÖ Upserted {len(chunks)} JSON objects to Qdrant collection '{collection_name}' without embedding.")

--- app/utils/rate_limit.py
+++ app/utils/rate_limit.py
@@ -31,13 +31,14 @@
 # Default rate limits
 DEFAULT_DAILY_LIMIT = 10  # Hardcoded default daily runs for users without an explicit override
 
+
 class RateLimiter:
     """Rate limiter for API requests with support for both Redis and database."""
-    
+
     def __init__(self):
         """Initialize the rate limiter."""
         self.use_redis = get_use_redis_rate_limiting()
-        
+
         if self.use_redis:
             # Initialize Redis connection
             self.redis = redis.Redis(
@@ -47,28 +48,29 @@
                 db=REDIS_DB,
                 decode_responses=True,
                 ssl=True,  # Required for ElastiCache Serverless
-                ssl_cert_reqs=None  # Required for ElastiCache
+                ssl_cert_reqs=None,  # Required for ElastiCache
             )
             self.db_limiter = None
         else:
             # Initialize database rate limiter
             self.redis = None
             self.db_limiter = None  # Will be imported when needed
-    
+
     async def _get_db_limiter(self):
         """Get the database rate limiter instance (lazy loading)."""
         if not self.db_limiter:
             from app.utils.db_rate_limit import DatabaseRateLimiter
+
             self.db_limiter = DatabaseRateLimiter()
         return self.db_limiter
-        
+
     async def get_user_limit(self, user_id: str) -> int:
         """
         Get the daily request limit for a user.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             Daily request limit for the user
         """
@@ -81,15 +83,15 @@
             # Database implementation
             db_limiter = await self._get_db_limiter()
             return await db_limiter.get_user_limit(user_id)
-        
+
     async def set_user_limit(self, user_id: str, limit: int) -> bool:
         """
         Set the daily request limit for a user.
-        
+
         Args:
             user_id: User identifier
             limit: Daily request limit
-            
+
         Returns:
             True if successful, False otherwise
         """
@@ -102,14 +104,14 @@
             # Database implementation
             db_limiter = await self._get_db_limiter()
             return await db_limiter.set_user_limit(user_id, limit)
-        
+
     async def get_request_count(self, user_id: str) -> int:
         """
         Get the number of requests made by a user today.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             Number of requests made today
         """
@@ -123,14 +125,14 @@
             # Database implementation
             db_limiter = await self._get_db_limiter()
             return await db_limiter.get_request_count(user_id)
-        
+
     async def increment_request_count(self, user_id: str) -> int:
         """
         Increment the request count for a user.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             New request count
         """
@@ -138,33 +140,33 @@
             # Redis implementation
             today = datetime.utcnow().strftime("%Y-%m-%d")
             count_key = f"request_count:{user_id}:{today}"
-            
+
             # Increment the count
             new_count = await self.redis.incr(count_key)
-            
+
             # Set expiry to end of day if not already set
             if new_count == 1:
                 # Calculate seconds until midnight
                 now = datetime.utcnow()
                 midnight = datetime(now.year, now.month, now.day) + timedelta(days=1)
                 seconds_until_midnight = (midnight - now).total_seconds()
-                
+
                 # Set expiry
                 await self.redis.expire(count_key, int(seconds_until_midnight))
-                
+
             return new_count
         else:
             # Database implementation
             db_limiter = await self._get_db_limiter()
             return await db_limiter.increment_request_count(user_id)
-        
+
     async def check_rate_limit(self, user_id: str) -> Dict[str, Any]:
         """
         Check if a user has exceeded their rate limit.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             Dictionary with limit information:
             {
@@ -179,33 +181,28 @@
             limit = await self.get_user_limit(user_id)
             count = await self.get_request_count(user_id)
             remaining = max(0, limit - count)
-            
+
             # Calculate seconds until reset (midnight UTC)
             now = datetime.utcnow()
             midnight = datetime(now.year, now.month, now.day) + timedelta(days=1)
             seconds_until_reset = int((midnight - now).total_seconds())
-            
-            return {
-                "allowed": count < limit,
-                "limit": limit,
-                "remaining": remaining,
-                "reset": seconds_until_reset
-            }
+
+            return {"allowed": count < limit, "limit": limit, "remaining": remaining, "reset": seconds_until_reset}
         else:
             # Database implementation
             db_limiter = await self._get_db_limiter()
             return await db_limiter.check_rate_limit(user_id)
-    
+
     async def check_window_limit(self, identifier: str, endpoint: str, limit: int, window: int) -> bool:
         """
         Check window-based rate limiting (for router-level limits).
-        
+
         Args:
             identifier: User ID or IP address
             endpoint: API endpoint path
             limit: Maximum requests per window
             window: Window size in seconds
-            
+
         Returns:
             True if request is allowed
         """
@@ -217,11 +214,11 @@
             # Database implementation
             db_limiter = await self._get_db_limiter()
             return await db_limiter.check_window_limit(identifier, endpoint, limit, window)
-        
+
     async def reset_all_counters(self) -> bool:
         """
         Reset all request counters (admin function).
-        
+
         Returns:
             True if successful, False otherwise
         """
@@ -230,23 +227,23 @@
             today = datetime.utcnow().strftime("%Y-%m-%d")
             pattern = f"request_count:*:{today}"
             keys = await self.redis.keys(pattern)
-            
+
             if keys:
                 await self.redis.delete(*keys)
-                
-            return True 
+
+            return True
         else:
             # Database implementation
             db_limiter = await self._get_db_limiter()
             return await db_limiter.reset_all_counters()
-    
+
     async def cleanup_old_windows(self, max_age_hours: int = 24) -> int:
         """
         Clean up old window-based rate limit data.
-        
+
         Args:
             max_age_hours: Maximum age in hours to keep data
-            
+
         Returns:
             Number of records deleted
         """
@@ -256,4 +253,4 @@
         else:
             # Database implementation
             db_limiter = await self._get_db_limiter()
-            return await db_limiter.cleanup_old_windows(max_age_hours) 
\ No newline at end of file
+            return await db_limiter.cleanup_old_windows(max_age_hours)

--- app/utils/secrets_manager.py
+++ app/utils/secrets_manager.py
@@ -7,6 +7,7 @@
 
 logger = logging.getLogger(__name__)
 
+
 class SecretsManager:
     def __init__(self, secret_name: str = None, region_name: str = "eu-west-1"):
         # Auto-detect secret name based on environment if not provided
@@ -28,32 +29,29 @@
         self.secrets_client = None
         self._secrets_cache = {}
         self._aws_enabled = True
-        
+
     def _disable_aws(self, reason: str) -> None:
         """Disable AWS Secrets Manager lookups for the current process."""
         if self._aws_enabled:
             self._aws_enabled = False
             logger.info(f"‚ÑπÔ∏è SecretsManager: Disabled AWS lookups ({reason}). Falling back to env variables only.")
-        
+
         print(f"üîê SecretsManager: Initialized for environment '{self.environment}' using secret '{self.secret_name}'")
         logger.info(f"üîê SecretsManager: Initialized for environment '{self.environment}' using secret '{self.secret_name}'")
-        
+
     def _get_secrets_client(self):
         """Get or create boto3 secrets client"""
         if not self._aws_enabled:
             return None
         if self.secrets_client is None:
             try:
-                self.secrets_client = boto3.client(
-                    'secretsmanager',
-                    region_name=self.region_name
-                )
+                self.secrets_client = boto3.client("secretsmanager", region_name=self.region_name)
                 logger.info(f"üîó SecretsManager: Initialized client for region {self.region_name}")
             except Exception as e:
                 logger.warning(f"‚ö†Ô∏è SecretsManager: Failed to initialize client: {e}")
                 self.secrets_client = None
         return self.secrets_client
-    
+
     def get_secret(self, secret_key: str) -> Optional[str]:
         """
         Get a secret value with fallback priority:
@@ -66,17 +64,17 @@
         if secrets_value:
             logger.info(f"üîê SecretsManager: Retrieved {secret_key} from AWS Secrets Manager")
             return secrets_value
-        
+
         # Fallback to environment variable
         env_value = os.getenv(secret_key)
         if env_value:
             logger.info(f"üîê SecretsManager: Retrieved {secret_key} from environment variable")
             return env_value
-        
+
         # Fallback to .env file (handled by python-dotenv)
         logger.warning(f"‚ö†Ô∏è SecretsManager: {secret_key} not found in Secrets Manager or environment variables")
         return None
-    
+
     def _get_secret_from_aws(self, secret_key: str) -> Optional[str]:
         """Get secret from AWS Secrets Manager"""
         if not self._aws_enabled:
@@ -85,24 +83,24 @@
             client = self._get_secrets_client()
             if not client:
                 return None
-            
+
             # Check cache first
             if self.secret_name in self._secrets_cache:
                 secrets_data = self._secrets_cache[self.secret_name]
             else:
                 # Get secret from AWS
                 response = client.get_secret_value(SecretId=self.secret_name)
-                secrets_data = json.loads(response['SecretString'])
+                secrets_data = json.loads(response["SecretString"])
                 self._secrets_cache[self.secret_name] = secrets_data
                 logger.info(f"üîê SecretsManager: Retrieved secret {self.secret_name} from AWS")
-            
+
             return secrets_data.get(secret_key)
-            
+
         except ClientError as e:
-            error_code = e.response['Error']['Code']
-            if error_code == 'ResourceNotFoundException':
+            error_code = e.response["Error"]["Code"]
+            if error_code == "ResourceNotFoundException":
                 logger.warning(f"‚ö†Ô∏è SecretsManager: Secret {self.secret_name} not found in AWS")
-            elif error_code == 'AccessDeniedException':
+            elif error_code == "AccessDeniedException":
                 logger.warning(f"‚ö†Ô∏è SecretsManager: Access denied to secret {self.secret_name}")
             else:
                 logger.warning(f"‚ö†Ô∏è SecretsManager: AWS error for {secret_key}: {e}")
@@ -113,31 +111,34 @@
         except Exception as e:
             logger.warning(f"‚ö†Ô∏è SecretsManager: Unexpected error for {secret_key}: {e}")
             return None
-    
+
     def get_all_secrets(self) -> Dict[str, Any]:
         """Get all secrets from AWS Secrets Manager"""
         try:
             client = self._get_secrets_client()
             if not client:
                 return {}
-            
+
             response = client.get_secret_value(SecretId=self.secret_name)
-            secrets_data = json.loads(response['SecretString'])
+            secrets_data = json.loads(response["SecretString"])
             self._secrets_cache[self.secret_name] = secrets_data
             logger.info(f"üîê SecretsManager: Retrieved all secrets from {self.secret_name}")
             return secrets_data
-            
+
         except Exception as e:
             logger.warning(f"‚ö†Ô∏è SecretsManager: Failed to get all secrets: {e}")
             return {}
 
+
 # Global secrets manager instance
 secrets_manager = SecretsManager()
 
+
 def get_secret(secret_key: str) -> Optional[str]:
     """Global function to get a secret value"""
     return secrets_manager.get_secret(secret_key)
 
+
 def get_all_secrets() -> Dict[str, Any]:
     """Global function to get all secrets"""
-    return secrets_manager.get_all_secrets() 
+    return secrets_manager.get_all_secrets()

--- archive/BD_url_coresig_agent.py
+++ archive/BD_url_coresig_agent.py
@@ -15,7 +15,7 @@
 from app.utils.db_utils import ProspectingDB, extract_coresignal_fields
 
 # Load environment variables from .env file
-dotenv_path = Path(__file__).parent.parent.parent / '.env'
+dotenv_path = Path(__file__).parent.parent.parent / ".env"
 load_dotenv(dotenv_path)
 
 
@@ -28,7 +28,6 @@
     def __init__(self, mcp_tools: List[Any], output_dir: str = "app/data"):
         super().__init__(output_dir)
         self.mcp_tools = mcp_tools
-
 
     @property
     def agent_name(self) -> str:
@@ -37,20 +36,20 @@
     async def execute(self, company_data: Dict[str, Any], run_id: str = None, company_id: str = None) -> Dict[str, Any]:
         """
         Find company website URL and query Coresignal API.
-        
+
         Args:
             company_data: Dict with company information and shared output_file
-            run_id: Unique identifier for the run 
+            run_id: Unique identifier for the run
             company_id: Pre-generated domain-based company ID (optional)
-        
+
         Returns:
             Coresignal API results
         """
         start_time = time.time()
-        
+
         # Initialize PostgreSQL connection if enabled
         db = None
-        postgres_enabled = os.getenv('ENABLE_POSTGRES_STORAGE', 'false').lower() == 'true'
+        postgres_enabled = os.getenv("ENABLE_POSTGRES_STORAGE", "false").lower() == "true"
         if postgres_enabled:
             try:
                 db = ProspectingDB()
@@ -58,73 +57,73 @@
             except Exception as e:
                 print(f"‚ö†Ô∏è PostgreSQL disabled due to connection error: {e}")
                 postgres_enabled = False
-        
+
         try:
-            company_name = company_data.get('name', '')
-            location = company_data.get('location', '')
-            focus_area = company_data.get('focus_area', '')
-            website_url = company_data.get('website_url', '')
-            shared_output_file = company_data.get('output_file')
+            company_name = company_data.get("name", "")
+            location = company_data.get("location", "")
+            focus_area = company_data.get("focus_area", "")
+            website_url = company_data.get("website_url", "")
+            shared_output_file = company_data.get("output_file")
 
             # Step 1: Find company website URL if not provided
             if not website_url:
                 if shared_output_file:
-                    await self.append_markdown(shared_output_file, "Step 5: Coresignal - Finding Website URL", f"Searching for {company_name} website URL...")
+                    await self.append_markdown(
+                        shared_output_file, "Step 5: Coresignal - Finding Website URL", f"Searching for {company_name} website URL..."
+                    )
 
                 # Try Perplexity first
-                found_url = await find_company_homepage_url_perplexity.ainvoke({
-                    "company_name": company_name,
-                    "location": location,
-                    "focus_area": focus_area,
-                    "web_search_config": {
-                        "name": "company_website_homepage_url",
-                        "query_template": "{company_name} official website",
-                        "max_results": 1,
-                        "search_type_description": (
-                            "Perform a web search to extract the company's main homepage url only. Return Null if it cannot be found. The homepage must be a valid, public, official company website (not a social media page, not a directory, not a login page, not a news article, not a subpage). Example output: [\"https://company.com\"] or [null]"
-                        ),
-                        "output_format_example": "[\"https://company.com\"]",
+                found_url = await find_company_homepage_url_perplexity.ainvoke(
+                    {
+                        "company_name": company_name,
+                        "location": location,
+                        "focus_area": focus_area,
+                        "web_search_config": {
+                            "name": "company_website_homepage_url",
+                            "query_template": "{company_name} official website",
+                            "max_results": 1,
+                            "search_type_description": (
+                                'Perform a web search to extract the company\'s main homepage url only. Return Null if it cannot be found. The homepage must be a valid, public, official company website (not a social media page, not a directory, not a login page, not a news article, not a subpage). Example output: ["https://company.com"] or [null]'
+                            ),
+                            "output_format_example": '["https://company.com"]',
+                        },
                     }
-                })
+                )
 
                 if found_url:
                     website_url = found_url
-                    company_data['website_url'] = website_url
+                    company_data["website_url"] = website_url
                     if shared_output_file:
                         await self.append_markdown(shared_output_file, "Step 5: Coresignal - Website URL Found (Perplexity)", website_url)
                 else:
                     # Fallback: Try Bright Data search
                     if shared_output_file:
-                        await self.append_markdown(shared_output_file, "Step 5: Coresignal - Perplexity Failed, Trying Bright Data", "Perplexity could not find URL, trying Bright Data search...")
+                        await self.append_markdown(
+                            shared_output_file,
+                            "Step 5: Coresignal - Perplexity Failed, Trying Bright Data",
+                            "Perplexity could not find URL, trying Bright Data search...",
+                        )
 
                     web_search_config = {
                         "name": "company_website_urls",
                         "query_template": "{company_name} official website",
                         "max_results": 1,
                         "search_type_description": (
-                            "Extract the company's main homepage URL only. Return Null if it cannot be found. The homepage must be a valid, public, official company website. Example output: [\"https://company.com\"] or [null]"
+                            'Extract the company\'s main homepage URL only. Return Null if it cannot be found. The homepage must be a valid, public, official company website. Example output: ["https://company.com"] or [null]'
                         ),
-                        "output_format_example": "[\"https://company.com\"]",
+                        "output_format_example": '["https://company.com"]',
                     }
-                    
-                    company_info = {
-                        'name': company_name,
-                        'location': location,
-                        'focus_area': focus_area
-                    }
-                    
-                    if not hasattr(self, 'search_module'):
+
+                    company_info = {"name": company_name, "location": location, "focus_area": focus_area}
+
+                    if not hasattr(self, "search_module"):
                         self.search_module = SearchModule(self.mcp_tools)
-                    
-                    urls = await self.search_module.search_and_extract(
-                        search_config=web_search_config,
-                        company_info=company_info,
-                        debug=False
-                    )
-                    
+
+                    urls = await self.search_module.search_and_extract(search_config=web_search_config, company_info=company_info, debug=False)
+
                     if isinstance(urls, list) and urls and urls[0]:
                         website_url = urls[0]
-                        company_data['website_url'] = website_url
+                        company_data["website_url"] = website_url
                         if shared_output_file:
                             await self.append_markdown(shared_output_file, "Step 5: Coresignal - Website URL Found (Bright Data)", website_url)
                     else:
@@ -132,37 +131,39 @@
                         if shared_output_file:
                             await self.append_markdown(shared_output_file, "Step 5: Coresignal - Error", error_msg)
                         return {
-                            'success': False,
-                            'error': error_msg,
-                            'output_file': shared_output_file,
-                            'agent_name': self.agent_name,
-                            'company_name': company_name
+                            "success": False,
+                            "error": error_msg,
+                            "output_file": shared_output_file,
+                            "agent_name": self.agent_name,
+                            "company_name": company_name,
                         }
 
             # Step 2: Query Coresignal API with the website URL (two-step process)
             if shared_output_file:
-                await self.append_markdown(shared_output_file, "Step 5: Coresignal - Querying API", f"Searching Coresignal database for website: {website_url}")
+                await self.append_markdown(
+                    shared_output_file, "Step 5: Coresignal - Querying API", f"Searching Coresignal database for website: {website_url}"
+                )
 
             coresignal_result = await self._query_coresignal_api(website_url, company_name, shared_output_file, run_id)
 
             # Store in PostgreSQL if enabled and successful
-            if postgres_enabled and db and coresignal_result.get('success'):
+            if postgres_enabled and db and coresignal_result.get("success"):
                 try:
                     # Use provided company_id or create one if not provided
                     storage_company_id = company_id
                     if not storage_company_id:
-                        storage_company_id = await db.store_company(run_id or 'default', company_data)
+                        storage_company_id = await db.store_company(run_id or "default", company_data)
                         print(f"‚ö†Ô∏è No company_id provided to CoreSignal agent, created new one: {storage_company_id}")
                     else:
                         print(f"‚úÖ Using provided domain-based company_id: {storage_company_id}")
-                    
+
                     # Extract structured fields
-                    extracted_fields = extract_coresignal_fields(coresignal_result.get('data', {}))
-                    
+                    extracted_fields = extract_coresignal_fields(coresignal_result.get("data", {}))
+
                     # Add website URL to raw_data for proper citation
                     coresignal_result_with_website = coresignal_result.copy()
-                    coresignal_result_with_website['website_url'] = website_url
-                    
+                    coresignal_result_with_website["website_url"] = website_url
+
                     # Store agent result
                     execution_time_ms = int((time.time() - start_time) * 1000)
                     await db.store_agent_result(
@@ -171,48 +172,46 @@
                         success=True,
                         raw_data=coresignal_result_with_website,
                         extracted_fields=extracted_fields,
-                        execution_time_ms=execution_time_ms
+                        execution_time_ms=execution_time_ms,
                     )
-                    
+
                     if shared_output_file:
                         await self.append_markdown(
                             shared_output_file,
                             "Step 5: Coresignal - PostgreSQL Storage",
-                            f"‚úÖ Stored in PostgreSQL - Company ID: {storage_company_id}, Extracted {len(extracted_fields)} fields"
+                            f"‚úÖ Stored in PostgreSQL - Company ID: {storage_company_id}, Extracted {len(extracted_fields)} fields",
                         )
-                        
+
                 except Exception as e:
                     print(f"‚ö†Ô∏è PostgreSQL storage failed: {e}")
                     if shared_output_file:
                         await self.append_markdown(
-                            shared_output_file,
-                            "Step 5: Coresignal - PostgreSQL Error",
-                            f"‚ùå PostgreSQL storage failed: {str(e)}"
+                            shared_output_file, "Step 5: Coresignal - PostgreSQL Error", f"‚ùå PostgreSQL storage failed: {str(e)}"
                         )
 
             return {
-                'success': coresignal_result['success'],
-                'website_url': website_url,
-                'coresignal_data': coresignal_result.get('data'),
-                'company_id': coresignal_result.get('company_id'),
-                'error': coresignal_result.get('error'),
-                'output_file': shared_output_file,
-                'agent_name': self.agent_name,
-                'company_name': company_name,
-                'postgres_stored': postgres_enabled and coresignal_result.get('success')
+                "success": coresignal_result["success"],
+                "website_url": website_url,
+                "coresignal_data": coresignal_result.get("data"),
+                "company_id": coresignal_result.get("company_id"),
+                "error": coresignal_result.get("error"),
+                "output_file": shared_output_file,
+                "agent_name": self.agent_name,
+                "company_name": company_name,
+                "postgres_stored": postgres_enabled and coresignal_result.get("success"),
             }
 
         except Exception as e:
             print(f"\n[CoreSignalSubAgent ERROR] Exception in execute():\n{traceback.format_exc()}")
-            
+
             # Store error in PostgreSQL if enabled
             if postgres_enabled and db:
                 try:
                     # Use provided company_id or create one if not provided
                     storage_company_id = company_id
                     if not storage_company_id:
-                        storage_company_id = await db.store_company(run_id or 'default', company_data)
-                    
+                        storage_company_id = await db.store_company(run_id or "default", company_data)
+
                     execution_time_ms = int((time.time() - start_time) * 1000)
                     await db.store_agent_result(
                         company_id=storage_company_id,
@@ -220,18 +219,18 @@
                         success=False,
                         raw_data={},
                         error_message=str(e),
-                        execution_time_ms=execution_time_ms
+                        execution_time_ms=execution_time_ms,
                     )
                 except Exception as db_error:
                     print(f"‚ö†Ô∏è PostgreSQL error storage failed: {db_error}")
-            
+
             return {
-                'success': False,
-                'error': str(e),
-                'output_file': company_data.get('output_file'),
-                'agent_name': self.agent_name,
-                'company_name': company_data.get('name', ''),
-                'postgres_stored': False
+                "success": False,
+                "error": str(e),
+                "output_file": company_data.get("output_file"),
+                "agent_name": self.agent_name,
+                "company_name": company_data.get("name", ""),
+                "postgres_stored": False,
             }
 
     async def _query_coresignal_api(self, website_url: str, company_name: str, shared_output_file: str = None, run_id: str = None) -> Dict[str, Any]:
@@ -239,120 +238,87 @@
         Query the Coresignal company multi-source API using two-step process:
         1. Search by website to get company ID
         2. Collect full company data using the ID
-        
+
         Args:
             website_url: The company website URL to search for
             company_name: Company name for logging
             shared_output_file: Optional output file for logging
             run_id: Unique identifier for the run
-            
+
         Returns:
             Dict with API results or error
         """
         try:
-            api_key = os.getenv('CORESIG_API_KEY')
+            api_key = os.getenv("CORESIG_API_KEY")
             if not api_key:
                 error_msg = "CORESIG_API_KEY not found in environment variables"
                 if shared_output_file:
                     await self.append_markdown(shared_output_file, "Step 5: Coresignal - API Key Error", error_msg)
-                return {
-                    'success': False,
-                    'error': error_msg
-                }
+                return {"success": False, "error": error_msg}
 
             # Step 1: Search by website to get company ID
             # Try to extract domain from URL for better matching
             domain_url = website_url
-            if website_url.startswith('http://'):
+            if website_url.startswith("http://"):
                 domain_url = website_url[7:]
-            elif website_url.startswith('https://'):
+            elif website_url.startswith("https://"):
                 domain_url = website_url[8:]
-            
+
             # Remove www. prefix if present
-            if domain_url.startswith('www.'):
+            if domain_url.startswith("www."):
                 domain_url = domain_url[4:]
-            
+
             # Try multiple query variations to find the company
             search_query = {
                 "bool": {
                     "should": [
                         # Exact website match
-                        {
-                            "term": {
-                                "website.exact": website_url
-                            }
-                        },
+                        {"term": {"website.exact": website_url}},
                         # Domain-only match
-                        {
-                            "match": {
-                                "website.domain_only": domain_url
-                            }
-                        },
+                        {"match": {"website.domain_only": domain_url}},
                         # Query string search with quotes
-                        {
-                            "query_string": {
-                                "default_field": "website",
-                                "query": f'"{website_url}"'
-                            }
-                        },
+                        {"query_string": {"default_field": "website", "query": f'"{website_url}"'}},
                         # Query string search for domain only
-                        {
-                            "query_string": {
-                                "default_field": "website.domain_only", 
-                                "query": f'"{domain_url}"'
-                            }
-                        }
+                        {"query_string": {"default_field": "website.domain_only", "query": f'"{domain_url}"'}},
                     ],
-                    "minimum_should_match": 1
+                    "minimum_should_match": 1,
                 }
             }
-            
+
             # API endpoint and headers for search
             search_url = "https://api.coresignal.com/cdapi/v2/company_multi_source/search/es_dsl"
-            
+
             # Use correct Coresignal authentication format
-            headers = {
-                "apikey": api_key,
-                "Content-Type": "application/json"
-            }
-            
+            headers = {"apikey": api_key, "Content-Type": "application/json"}
 
-            
             # Search payload - multi-source API only accepts query object
-            search_payload = {
-                "query": search_query
-            }
-            
+            search_payload = {"query": search_query}
+
             if shared_output_file:
                 await self.append_markdown(
-                    shared_output_file,
-                    "Step 5: Coresignal - Step 1: Searching by Website",
-                    f"Searching for company by website: {website_url}"
+                    shared_output_file, "Step 5: Coresignal - Step 1: Searching by Website", f"Searching for company by website: {website_url}"
                 )
-            
+
             async with aiohttp.ClientSession() as session:
                 # Step 1: Search for company ID
                 async with session.post(search_url, headers=headers, json=search_payload, timeout=30) as response:
                     if response.status != 200:
                         error_text = await response.text()
                         error_msg = f"Coresignal search API error: {response.status} - {error_text}"
-                        
+
                         if shared_output_file:
                             await self.append_markdown(shared_output_file, "Step 5: Coresignal - Search API Error", error_msg)
-                        return {
-                            'success': False,
-                            'error': error_msg
-                        }
-                    
+                        return {"success": False, "error": error_msg}
+
                     search_data = await response.json()
-                
+
                 if shared_output_file:
                     await self.append_markdown(
                         shared_output_file,
                         "Step 5: Coresignal - Step 1: Search Results",
-                        f"Found {len(search_data) if isinstance(search_data, list) else len(search_data.get('companies', []))} results"
+                        f"Found {len(search_data) if isinstance(search_data, list) else len(search_data.get('companies', []))} results",
                     )
-                
+
                 # Extract company ID - handle both list and dict response formats
                 companies = []
                 if isinstance(search_data, list):
@@ -360,112 +326,85 @@
                     companies = search_data
                 elif isinstance(search_data, dict):
                     # Response is a dict with companies key
-                    companies = search_data.get('companies', [])
-                
+                    companies = search_data.get("companies", [])
+
                 if not companies:
                     error_msg = f"No company found for website: {website_url}"
                     if shared_output_file:
                         await self.append_markdown(shared_output_file, "Step 5: Coresignal - No Results", error_msg)
-                    return {
-                        'success': False,
-                        'error': error_msg
-                    }
-                
+                    return {"success": False, "error": error_msg}
+
                 # Get the first company's ID
                 first_company = companies[0]
                 if isinstance(first_company, dict):
-                    company_id = first_company.get('id')
+                    company_id = first_company.get("id")
                 elif isinstance(first_company, int):
                     # If the response is just a list of IDs
                     company_id = first_company
                 else:
                     company_id = None
-                
+
                 if not company_id:
                     error_msg = "Company ID not found in search results"
                     if shared_output_file:
                         await self.append_markdown(shared_output_file, "Step 5: Coresignal - No ID", error_msg)
-                    return {
-                        'success': False,
-                        'error': error_msg
-                    }
-                
+                    return {"success": False, "error": error_msg}
+
                 if shared_output_file:
-                    await self.append_markdown(
-                        shared_output_file,
-                        "Step 5: Coresignal - Company ID Found",
-                        f"Company ID: {company_id}"
-                    )
-                
+                    await self.append_markdown(shared_output_file, "Step 5: Coresignal - Company ID Found", f"Company ID: {company_id}")
+
                 # Step 2: Collect full company data using the ID
                 collect_url = f"https://api.coresignal.com/cdapi/v2/company_multi_source/collect/{company_id}"
-                
+
                 if shared_output_file:
                     await self.append_markdown(
-                        shared_output_file,
-                        "Step 5: Coresignal - Step 2: Collecting Full Data", 
-                        "Retrieving complete company profile..."
+                        shared_output_file, "Step 5: Coresignal - Step 2: Collecting Full Data", "Retrieving complete company profile..."
                     )
-                
+
                 async with session.get(collect_url, headers=headers, timeout=30) as response:
                     if response.status != 200:
                         error_text = await response.text()
                         error_msg = f"Coresignal collect API error: {response.status} - {error_text}"
-                        
+
                         if shared_output_file:
                             await self.append_markdown(shared_output_file, "Step 5: Coresignal - Collect API Error", error_msg)
-                        
-                        return {
-                            'success': False,
-                            'error': error_msg,
-                            'company_id': company_id
-                        }
-                    
+
+                        return {"success": False, "error": error_msg, "company_id": company_id}
+
                     collect_data = await response.json()
-                    
+
                     # Chunk the Coresignal data for vector database
-                    coresignal_chunks = chunk_coresignal_data(
-                        collect_data, 
-                        company_name=company_name, 
-                        run_id=run_id
-                    )
-                    
+                    coresignal_chunks = chunk_coresignal_data(collect_data, company_name=company_name, run_id=run_id)
+
                     # Write chunks to vector database
                     if coresignal_chunks:
                         write_chunks_to_qdrant(coresignal_chunks)
-                    
+
                     if shared_output_file:
                         # Write the full data
                         await self.append_markdown(
                             shared_output_file,
                             "Step 5: Coresignal - Company Data Retrieved",
-                            f"```json\n{json.dumps(collect_data, indent=2, ensure_ascii=False)}\n```"
+                            f"```json\n{json.dumps(collect_data, indent=2, ensure_ascii=False)}\n```",
                         )
-                        
+
                         # Write the chunked data
                         await self.append_markdown(
                             shared_output_file,
                             "Step 5: Coresignal - Chunked Data for Vector DB",
-                            f"**Processed {len(coresignal_chunks)} chunks:**\n\n" + 
-                            "\n".join([f"- **{chunk['metadata']['section']}**: {chunk['text'][:100]}..." 
-                                     for chunk in coresignal_chunks[:5]]) +  # Show first 5 chunks
-                            (f"\n- ... and {len(coresignal_chunks) - 5} more chunks" if len(coresignal_chunks) > 5 else "")
+                            f"**Processed {len(coresignal_chunks)} chunks:**\n\n"
+                            + "\n".join(
+                                [f"- **{chunk['metadata']['section']}**: {chunk['text'][:100]}..." for chunk in coresignal_chunks[:5]]
+                            )  # Show first 5 chunks
+                            + (f"\n- ... and {len(coresignal_chunks) - 5} more chunks" if len(coresignal_chunks) > 5 else ""),
                         )
-                    
-                    return {
-                        'success': True,
-                        'data': collect_data,
-                        'company_id': company_id,
-                        'chunks_processed': len(coresignal_chunks)
-                    }
-                        
+
+                    return {"success": True, "data": collect_data, "company_id": company_id, "chunks_processed": len(coresignal_chunks)}
+
         except Exception as e:
             error_msg = f"Coresignal API query failed: {str(e)}"
-            
+
             if shared_output_file:
                 await self.append_markdown(shared_output_file, "Step 5: Coresignal - Exception", error_msg)
-            
-            return {
-                'success': False,
-                'error': error_msg
-            }
+
+            return {"success": False, "error": error_msg}

--- check_agent_results_by_run.py
+++ check_agent_results_by_run.py
@@ -3,22 +3,17 @@
 import os
 import json
 
-DB_HOST = os.environ.get('POSTGRES_HOST', 'localhost')
-DB_PORT = os.environ.get('POSTGRES_PORT', '5432')
-DB_NAME = os.environ.get('POSTGRES_DB', 'prospecting_db')
-DB_USER = os.environ.get('POSTGRES_USER', 'prospecting_user')
-DB_PASS = os.environ.get('POSTGRES_PASSWORD', 'your_password_here')
+DB_HOST = os.environ.get("POSTGRES_HOST", "localhost")
+DB_PORT = os.environ.get("POSTGRES_PORT", "5432")
+DB_NAME = os.environ.get("POSTGRES_DB", "prospecting_db")
+DB_USER = os.environ.get("POSTGRES_USER", "prospecting_user")
+DB_PASS = os.environ.get("POSTGRES_PASSWORD", "your_password_here")
 
-RUN_ID = '0da9c370-ca6f-4e96-a906-d3b49c7f516c_run_001'
+RUN_ID = "0da9c370-ca6f-4e96-a906-d3b49c7f516c_run_001"
+
 
 async def main():
-    conn = await asyncpg.connect(
-        host=DB_HOST,
-        port=DB_PORT,
-        database=DB_NAME,
-        user=DB_USER,
-        password=DB_PASS
-    )
+    conn = await asyncpg.connect(host=DB_HOST, port=DB_PORT, database=DB_NAME, user=DB_USER, password=DB_PASS)
     query = """
         SELECT run_id, user_id, agent_name, created_at, result_data
         FROM agent_results
@@ -36,7 +31,7 @@
             print(f"  created_at: {row['created_at']}")
             # Print top-level keys in result_data if possible
             try:
-                data = row['result_data']
+                data = row["result_data"]
                 if isinstance(data, str):
                     data = json.loads(data)
                 print(f"  result_data keys: {list(data.keys()) if isinstance(data, dict) else type(data)}")
@@ -45,5 +40,6 @@
             print()
     await conn.close()
 
+
 if __name__ == "__main__":
-    asyncio.run(main()) 
\ No newline at end of file
+    asyncio.run(main())

--- check_company_enrich_output.py
+++ check_company_enrich_output.py
@@ -4,23 +4,18 @@
 import json
 
 # --- CONFIGURE THESE ---
-DB_HOST = os.environ.get('POSTGRES_HOST', 'localhost')
-DB_PORT = os.environ.get('POSTGRES_PORT', '5432')
-DB_NAME = os.environ.get('POSTGRES_DB', 'prospecting_db')
-DB_USER = os.environ.get('POSTGRES_USER', 'prospecting_user')
-DB_PASS = os.environ.get('POSTGRES_PASSWORD', 'your_password_here')  # Set or export this
+DB_HOST = os.environ.get("POSTGRES_HOST", "localhost")
+DB_PORT = os.environ.get("POSTGRES_PORT", "5432")
+DB_NAME = os.environ.get("POSTGRES_DB", "prospecting_db")
+DB_USER = os.environ.get("POSTGRES_USER", "prospecting_user")
+DB_PASS = os.environ.get("POSTGRES_PASSWORD", "your_password_here")  # Set or export this
+
+RUN_ID = "0da9c370-ca6f-4e96-a906-d3b49c7f516c_run_001"
+USER_ID = "cfa56b08-98bf-4eef-9bed-e579d8ebe7e4"
 
-RUN_ID = '0da9c370-ca6f-4e96-a906-d3b49c7f516c_run_001'
-USER_ID = 'cfa56b08-98bf-4eef-9bed-e579d8ebe7e4'
 
 async def main():
-    conn = await asyncpg.connect(
-        host=DB_HOST,
-        port=DB_PORT,
-        database=DB_NAME,
-        user=DB_USER,
-        password=DB_PASS
-    )
+    conn = await asyncpg.connect(host=DB_HOST, port=DB_PORT, database=DB_NAME, user=DB_USER, password=DB_PASS)
     query = """
         SELECT result_data->'company_enrichment_result' as enrichment_data
         FROM agent_results 
@@ -31,12 +26,13 @@
         LIMIT 1
     """
     row = await conn.fetchrow(query, RUN_ID, USER_ID)
-    if row and row['enrichment_data']:
+    if row and row["enrichment_data"]:
         print("‚úÖ Company enrichment data found:")
-        print(json.dumps(row['enrichment_data'], indent=2, ensure_ascii=False))
+        print(json.dumps(row["enrichment_data"], indent=2, ensure_ascii=False))
     else:
         print("‚ùå No company enrichment data found for this run/user.")
     await conn.close()
 
+
 if __name__ == "__main__":
-    asyncio.run(main()) 
\ No newline at end of file
+    asyncio.run(main())

--- check_session_for_run.py
+++ check_session_for_run.py
@@ -2,23 +2,18 @@
 import asyncpg
 import os
 
-DB_HOST = os.environ.get('POSTGRES_HOST', 'localhost')
-DB_PORT = os.environ.get('POSTGRES_PORT', '5432')
-DB_NAME = os.environ.get('POSTGRES_DB', 'prospecting_db')
-DB_USER = os.environ.get('POSTGRES_USER', 'prospecting_user')
-DB_PASS = os.environ.get('POSTGRES_PASSWORD', 'your_password_here')
+DB_HOST = os.environ.get("POSTGRES_HOST", "localhost")
+DB_PORT = os.environ.get("POSTGRES_PORT", "5432")
+DB_NAME = os.environ.get("POSTGRES_DB", "prospecting_db")
+DB_USER = os.environ.get("POSTGRES_USER", "prospecting_user")
+DB_PASS = os.environ.get("POSTGRES_PASSWORD", "your_password_here")
+
+SESSION_ID = "0da9c370-ca6f-4e96-a906-d3b49c7f516c"
+USER_ID = "cfa56b08-98bf-4eef-9bed-e579d8ebe7e4"
 
-SESSION_ID = '0da9c370-ca6f-4e96-a906-d3b49c7f516c'
-USER_ID = 'cfa56b08-98bf-4eef-9bed-e579d8ebe7e4'
 
 async def main():
-    conn = await asyncpg.connect(
-        host=DB_HOST,
-        port=DB_PORT,
-        database=DB_NAME,
-        user=DB_USER,
-        password=DB_PASS
-    )
+    conn = await asyncpg.connect(host=DB_HOST, port=DB_PORT, database=DB_NAME, user=DB_USER, password=DB_PASS)
     query = """
         SELECT * FROM sessions WHERE session_id = $1 AND user_id = $2
     """
@@ -31,5 +26,6 @@
         print("‚ùå No session found for this session_id and user_id.")
     await conn.close()
 
+
 if __name__ == "__main__":
-    asyncio.run(main()) 
\ No newline at end of file
+    asyncio.run(main())

--- companies_house_api.py
+++ companies_house_api.py
@@ -16,6 +16,7 @@
 # Example for Windows: pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
 # Example for Linux/macOS: pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract' # or /usr/local/bin/tesseract
 
+
 class CompaniesHouseAPI:
     def __init__(self, api_key):
         self.api_key = api_key
@@ -27,13 +28,10 @@
         """Get the filing history for a company"""
         url = f"{self.base_url}/company/{company_number}/filing-history"
         print(f"\nRequesting filing history from: {url}")
-        
-        headers = {
-            'Authorization': f'Basic {self.auth_header}',
-            'Accept': 'application/json'
-        }
+
+        headers = {"Authorization": f"Basic {self.auth_header}", "Accept": "application/json"}
         # print("Request headers:", headers) # Redundant for clean version
-        
+
         response = requests.get(url, headers=headers)
         print(f"Response status: {response.status_code}")
         # print(f"Response headers: {dict(response.headers)}") # Redundant
@@ -41,7 +39,7 @@
         if response.status_code != 200:
             print(f"Error getting filing history: {response.status_code} - {response.text}")
             return None
-            
+
         return response.json()
 
     def get_filing_document(self, company_number, transaction_id):
@@ -49,94 +47,91 @@
         print(f"\nFetching document details for transaction: {transaction_id}")
 
         filing_item_url = f"{self.base_url}/company/{company_number}/filing-history/{transaction_id}"
-        json_headers = {
-            'Authorization': f'Basic {self.auth_header}',
-            'Accept': 'application/json'
-        }
+        json_headers = {"Authorization": f"Basic {self.auth_header}", "Accept": "application/json"}
         filing_item_response = requests.get(filing_item_url, headers=json_headers)
         # print(f"Filing item response status: {filing_item_response.status_code}") # Covered by error check
 
         if filing_item_response.status_code != 200:
             print(f"Error getting filing item details for {transaction_id}: {filing_item_response.status_code} - {filing_item_response.text}")
             return None
-        
+
         filing_item_details = filing_item_response.json()
 
-        if not ('links' in filing_item_details and 'document_metadata' in filing_item_details['links']):
+        if not ("links" in filing_item_details and "document_metadata" in filing_item_details["links"]):
             print(f"Error: 'links.document_metadata' not found for {transaction_id}.")
-            if 'document_status' in filing_item_details:
-                 print(f"Document status: {filing_item_details['document_status']}. This might be a CHIPS document or otherwise unavailable via API.")
+            if "document_status" in filing_item_details:
+                print(f"Document status: {filing_item_details['document_status']}. This might be a CHIPS document or otherwise unavailable via API.")
             return None
-            
-        document_api_metadata_url = filing_item_details['links']['document_metadata']
+
+        document_api_metadata_url = filing_item_details["links"]["document_metadata"]
         # print(f"Document API Metadata URL: {document_api_metadata_url}") # Can be noisy
 
         doc_api_meta_response = requests.get(document_api_metadata_url, headers=json_headers)
         # print(f"Document API metadata response status: {doc_api_meta_response.status_code}")
 
         if doc_api_meta_response.status_code != 200:
-            print(f"Error fetching document API metadata from {document_api_metadata_url}: {doc_api_meta_response.status_code} - {doc_api_meta_response.text}")
+            print(
+                f"Error fetching document API metadata from {document_api_metadata_url}: {doc_api_meta_response.status_code} - {doc_api_meta_response.text}"
+            )
             return None
-            
+
         doc_api_meta = doc_api_meta_response.json()
 
-        if not ('links' in doc_api_meta and 'document' in doc_api_meta['links']):
+        if not ("links" in doc_api_meta and "document" in doc_api_meta["links"]):
             print(f"Error: 'links.document' (actual document URL) not found in document API metadata for {transaction_id}.")
             return None
-            
-        actual_document_url = doc_api_meta['links']['document']
+
+        actual_document_url = doc_api_meta["links"]["document"]
         print(f"Fetching PDF content from: {actual_document_url}")
-        
-        pdf_headers = {
-            'Authorization': f'Basic {self.auth_header}',
-            'Accept': 'application/pdf'
-        }
+
+        pdf_headers = {"Authorization": f"Basic {self.auth_header}", "Accept": "application/pdf"}
         doc_response = requests.get(actual_document_url, headers=pdf_headers)
-        
+
         # print(f"Actual document content response status: {doc_response.status_code}")
-        
+
         if doc_response.status_code != 200:
             print(f"Error getting actual document content from {actual_document_url}: {doc_response.status_code}")
             # print(f"Error response (first 500 chars): {doc_response.text[:500]}...") # Potentially large binary data
             return None
-            
+
         return doc_response.content
 
+
 def save_to_markdown(company_number, filings_data, n, output_file="test.md"):
     """Save filing information to markdown file, including extracted text from PDFs."""
-    with open(output_file, 'w') as f:
+    with open(output_file, "w") as f:
         f.write(f"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
         f.write(f"# Filing History for Company {company_number}\n\n")
-        
-        if not filings_data or 'items' not in filings_data or not filings_data['items']:
+
+        if not filings_data or "items" not in filings_data or not filings_data["items"]:
             f.write("No filing data available or no items in filings data.\n")
             print("No filing data items to process.")
             return
-            
+
         print(f"\nFound {len(filings_data['items'])} filings, processing the latest {n}")
-        
-        for filing in filings_data['items'][:n]:
-            transaction_id = filing.get('transaction_id', 'N/A')
+
+        for filing in filings_data["items"][:n]:
+            transaction_id = filing.get("transaction_id", "N/A")
             print(f"\nProcessing filing: {filing.get('description', 'No description')} (ID: {transaction_id})")
-            
+
             f.write(f"## {filing.get('description', 'No description')}\n")
             f.write(f"Filed on: {filing.get('date', 'No date')}\n")
             f.write(f"Type: {filing.get('type', 'No type')}\n")
             f.write(f"Transaction ID: {transaction_id}\n")
-            
+
             f.write("\n### Filing Details (Metadata):\n")
             f.write("```json\n")
             f.write(json.dumps(filing, indent=2))
             f.write("\n```\n\n")
-            
-            if 'transaction_id' not in filing:
+
+            if "transaction_id" not in filing:
                 f.write("Skipping document retrieval: No transaction_id available.\n\n")
                 continue
 
             try:
                 print(f"Attempting to get document for transaction {transaction_id}")
                 doc_content = api.get_filing_document(company_number, transaction_id)
-                
+
                 if doc_content:
                     print(f"Document content fetched for transaction {transaction_id}. Attempting text extraction...")
                     extracted_text_content = ""
@@ -147,11 +142,11 @@
                                 for i, page in enumerate(pdf.pages):
                                     # print(f"Document {transaction_id} - Processing page {i+1}/{len(pdf.pages)}") # Removed DEBUG
                                     text = page.extract_text(layout=True)
-                                    
+
                                     if text:
-                                        # debug_text_snippet = text[:100].replace('\n', ' ') 
+                                        # debug_text_snippet = text[:100].replace('\n', ' ')
                                         # print(f"Document {transaction_id} - Page {i+1} - Extracted text (pdfplumber): '{debug_text_snippet}...'") # Removed DEBUG
-                                        extracted_text_content += f"--- Page {i+1} (Text via pdfplumber) ---\n{text}\n\n"
+                                        extracted_text_content += f"--- Page {i + 1} (Text via pdfplumber) ---\n{text}\n\n"
                                     else:
                                         # print(f"Document {transaction_id} - Page {i+1} - No text extracted by pdfplumber. Attempting OCR...") # Removed DEBUG
                                         try:
@@ -160,14 +155,14 @@
                                             if ocr_text.strip():
                                                 # debug_ocr_snippet = ocr_text[:100].replace('\n', ' ')
                                                 # print(f"Document {transaction_id} - Page {i+1} - Extracted text (OCR): '{debug_ocr_snippet}...'") # Removed DEBUG
-                                                extracted_text_content += f"--- Page {i+1} (Text via OCR) ---\n{ocr_text}\n\n"
+                                                extracted_text_content += f"--- Page {i + 1} (Text via OCR) ---\n{ocr_text}\n\n"
                                             else:
                                                 # print(f"Document {transaction_id} - Page {i+1} - OCR returned no text.") # Removed DEBUG
-                                                extracted_text_content += f"--- Page {i+1} ---\nNo text extracted by pdfplumber or OCR.\n\n"
+                                                extracted_text_content += f"--- Page {i + 1} ---\nNo text extracted by pdfplumber or OCR.\n\n"
                                         except Exception as e_ocr:
-                                            ocr_error_msg = f"OCR attempt failed for page {i+1} of {transaction_id}: {str(e_ocr)}"
-                                            print(ocr_error_msg) # Keep this error log
-                                            extracted_text_content += f"--- Page {i+1} ---\nNo text extracted by pdfplumber. OCR Error: {str(e_ocr)}\n(Ensure Tesseract OCR is installed and configured, and pytesseract & Pillow libraries are installed.)\n\n"
+                                            ocr_error_msg = f"OCR attempt failed for page {i + 1} of {transaction_id}: {str(e_ocr)}"
+                                            print(ocr_error_msg)  # Keep this error log
+                                            extracted_text_content += f"--- Page {i + 1} ---\nNo text extracted by pdfplumber. OCR Error: {str(e_ocr)}\n(Ensure Tesseract OCR is installed and configured, and pytesseract & Pillow libraries are installed.)\n\n"
 
                                 if extracted_text_content.strip():
                                     f.write("### Extracted Text from Document:\n")
@@ -180,38 +175,39 @@
                                     print(f"No text extracted or written for transaction {transaction_id}.")
                     except Exception as e_extract:
                         extract_error_msg = f"Error during text extraction for transaction {transaction_id}: {str(e_extract)}"
-                        print(extract_error_msg) # Keep this error log
+                        print(extract_error_msg)  # Keep this error log
                         f.write(f"{extract_error_msg}\nEnsure pdfplumber is installed ('pip install pdfplumber').\n\n")
                 else:
                     f.write(f"Error: Could not retrieve document content for transaction {transaction_id}\n\n")
                     print(f"Failed to retrieve document content for transaction {transaction_id}.")
-                    
+
             except Exception as e_process:
                 error_msg = f"Error during document processing for transaction {transaction_id}: {str(e_process)}"
-                print(error_msg) # Keep this error log
+                print(error_msg)  # Keep this error log
                 f.write(f"{error_msg}\n\n")
 
+
 if __name__ == "__main__":
     # Get API key from .env file
-    api_key = os.getenv('COMPANIES_HOUSE_API_KEY')
+    api_key = os.getenv("COMPANIES_HOUSE_API_KEY")
     if not api_key:
         print("Error: COMPANIES_HOUSE_API_KEY not found in .env file")
         exit(1)
-    
+
     print("Using Companies House API")
     print("Ensure you have installed the required libraries: pip install requests python-dotenv pdfplumber pytesseract Pillow")
     print("Also ensure Tesseract OCR engine is installed on your system and accessible via pytesseract (see script comments for configuration).")
     # print(f"API key length: {len(api_key)}") # Can be removed, less critical now
 
     company_number = "OC123456"  # EXAMPLE CAPITAL LLP
-    num_recent_filings_to_fetch = 2 # Define how many recent filings to fetch
-    
+    num_recent_filings_to_fetch = 2  # Define how many recent filings to fetch
+
     try:
         api = CompaniesHouseAPI(api_key)
-        
+
         print(f"\nGetting filing history for company {company_number}...")
         filings = api.get_filing_history(company_number)
-        
+
         if filings:
             print(f"Saving results for the latest {num_recent_filings_to_fetch} filings to test.md...")
             save_to_markdown(company_number, filings, num_recent_filings_to_fetch)
@@ -219,4 +215,4 @@
         else:
             print("Failed to get filing history")
     except Exception as e:
-        print(f"An unexpected error occurred in the main execution block: {str(e)}") 
\ No newline at end of file
+        print(f"An unexpected error occurred in the main execution block: {str(e)}")

--- config/environment.py
+++ config/environment.py
@@ -12,9 +12,10 @@
 # Environment types
 EnvironmentType = Literal["development", "staging", "production"]
 
+
 class EnvironmentConfig:
     """Centralized environment configuration."""
-    
+
     def __init__(self):
         # Load from environment variables with defaults
         self.environment: EnvironmentType = get_environment()
@@ -24,71 +25,72 @@
         self.alert_email: str = get_alert_email()
         self.log_retention_days: int = get_log_retention_days()
         self.backend_url: str = get_backend_url()
-        
+
         # Validate environment
         if self.environment not in ["development", "staging", "production"]:
             raise ValueError(f"Invalid environment: {self.environment}")
-    
+
     @property
     def is_development(self) -> bool:
         """Check if running in development environment."""
         return self.environment == "development"
-    
+
     @property
     def is_staging(self) -> bool:
         """Check if running in staging environment."""
         return self.environment == "staging"
-    
+
     @property
     def is_production(self) -> bool:
         """Check if running in production environment."""
         return self.environment == "production"
-    
+
     @property
     def cloudwatch_log_group_name(self) -> str:
         """Get CloudWatch log group name for this environment."""
         return f"/aws/apprunner/{self.app_name}-{self.environment}/application"
-    
+
     @property
     def cloudwatch_system_log_group_name(self) -> str:
         """Get CloudWatch system log group name for this environment."""
         return f"/aws/apprunner/{self.app_name}-{self.environment}/system"
-    
+
     @property
     def sns_topic_name(self) -> str:
         """Get SNS topic name for this environment."""
         return f"{self.app_name}-{self.environment}-alerts"
-    
+
     @property
     def dashboard_name(self) -> str:
         """Get CloudWatch dashboard name for this environment."""
         return f"{self.app_name}-{self.environment}-dashboard"
-    
+
     @property
     def namespace(self) -> str:
         """Get CloudWatch namespace for this environment."""
         return f"{self.app_name}/{self.environment}"
-    
+
     def get_terraform_vars(self) -> dict:
         """Get Terraform variables for this environment."""
         return {
             "app_name": self.app_name,
             "environment": self.environment,
             "retention_in_days": self.log_retention_days,
-            "alert_email": self.alert_email
+            "alert_email": self.alert_email,
         }
-    
+
     def get_apprunner_env_vars(self) -> dict:
         """Get App Runner environment variables for this environment."""
         return {
             "ENVIRONMENT": self.environment,
             "LOG_LEVEL": self.log_level,
             "PYTHONPATH": "/opt/render/project/src",
-            "BACKEND_URL": self.backend_url
+            "BACKEND_URL": self.backend_url,
         }
-    
+
     def __str__(self) -> str:
         return f"EnvironmentConfig(environment={self.environment}, app_name={self.app_name})"
 
+
 # Global instance
-config = EnvironmentConfig() 
\ No newline at end of file
+config = EnvironmentConfig()

--- examples/prospecting_example.py
+++ examples/prospecting_example.py
@@ -14,52 +14,50 @@
 from app.agents.prospecting_orchestrator import ProspectingOrchestrator
 from langchain_openai import ChatOpenAI
 
+
 async def main():
     """Main execution function demonstrating prospecting with MCP tools."""
-    
+
     print("üöÄ Prospecting Agent Demo with MCP Tools")
     print("=" * 50)
-    
+
     # Check environment variables
     required_env_vars = ["BRIGHTDATA_API_KEY", "OPENAI_API_KEY"]
     missing_vars = [var for var in required_env_vars if not os.getenv(var)]
-    
+
     if missing_vars:
         print(f"‚ùå Missing required environment variables: {', '.join(missing_vars)}")
         print("Please set these environment variables and try again.")
         return
-    
+
     # Initialize LLM for extraction
     extractor_llm = ChatOpenAI(model="gpt-4o", temperature=0)
-    
+
     # Initialize orchestrator (MCP tools will be loaded automatically)
     print("üîß Initializing orchestrator with MCP tools...")
-    orchestrator = ProspectingOrchestrator(
-        extractor_llm=extractor_llm,
-        output_dir="app/data"
-    )
-    
+    orchestrator = ProspectingOrchestrator(extractor_llm=extractor_llm, output_dir="app/data")
+
     # Initialize MCP tools and sub-agents
     await orchestrator.initialize_mcp_tools()
-    
+
     print(f"‚úÖ Orchestrator ready with {len(orchestrator.sub_agents)} sub-agents")
-    
+
     # Test 1: Natural language user prompt
     print(f"\n" + "=" * 70)
     print("üîç TEST 1: NATURAL LANGUAGE PROMPT")
     print("=" * 70)
-    
+
     user_prompt = """
     I am prospecting on a company called NorthEdge Capital in Manchester. 
     I am particularly interested in their private wealth services.
     """
-    
+
     print(f"üìù User Prompt: {user_prompt.strip()}")
-    
+
     try:
         results = await orchestrator.execute_from_prompt(user_prompt.strip())
-        
-        if results.get('success', False):
+
+        if results.get("success", False):
             print(f"\n‚úÖ PROMPT PROCESSING COMPLETED")
             print(f"üìä Company: {results['company_data']['name']}")
             print(f"üìç Location: {results['company_data']['location']}")
@@ -69,88 +67,80 @@
         else:
             print(f"\n‚ùå PROMPT PROCESSING FAILED")
             print(f"   Error: {results.get('error', 'Unknown error')}")
-            
+
     except Exception as e:
         print(f"\nüí• EXCEPTION: {str(e)}")
-    
+
     # Test 2: Structured company data
     print(f"\n" + "=" * 70)
     print("üîç TEST 2: STRUCTURED DATA")
     print("=" * 70)
-    
-    company_data = {
-        'name': 'Acme Corp',
-        'website': 'https://acmecorp.com',
-        'location': 'San Francisco, CA',
-        'focus_area': 'AI technology'
-    }
-    
+
+    company_data = {"name": "Acme Corp", "website": "https://acmecorp.com", "location": "San Francisco, CA", "focus_area": "AI technology"}
+
     print(f"üìä Company Data: {company_data}")
-    
+
     try:
         results = await orchestrator.execute_from_data(company_data)
-        
-        if results.get('success', False):
+
+        if results.get("success", False):
             print(f"\n‚úÖ STRUCTURED DATA PROCESSING COMPLETED")
             print(f"üìä Company: {results['company_data']['name']}")
             print(f"üìÑ Report: {results['shared_output_file']}")
             print(f"ü§ñ Agents Run: {results['total_agents_run']}")
-            
-            prospecting = results['prospecting_result']
+
+            prospecting = results["prospecting_result"]
             if prospecting:
-                successful = len(prospecting.get('successful_agents', []))
-                total = prospecting.get('total_agents', 0)
+                successful = len(prospecting.get("successful_agents", []))
+                total = prospecting.get("total_agents", 0)
                 print(f"‚úÖ Success Rate: {successful}/{total} agents")
-                
-                if prospecting.get('failed_agents'):
+
+                if prospecting.get("failed_agents"):
                     print(f"‚ùå Failed: {', '.join(prospecting['failed_agents'])}")
         else:
             print(f"\n‚ùå STRUCTURED DATA PROCESSING FAILED")
             print(f"   Error: {results.get('error', 'Unknown error')}")
-            
+
     except Exception as e:
         print(f"\nüí• EXCEPTION: {str(e)}")
-    
+
     # Test 3: Hybrid mode
     print(f"\n" + "=" * 70)
     print("üîç TEST 3: HYBRID MODE")
     print("=" * 70)
-    
+
     test_inputs = [
         "Research Tesla in Austin, focusing on manufacturing",
-        {
-            'name': 'Apple Inc',
-            'location': 'Cupertino, CA',
-            'focus_area': 'consumer electronics'
-        }
+        {"name": "Apple Inc", "location": "Cupertino, CA", "focus_area": "consumer electronics"},
     ]
-    
+
     for i, test_input in enumerate(test_inputs, 1):
         print(f"\nüìù Input {i}: {type(test_input).__name__}")
         if isinstance(test_input, str):
             print(f"   Content: {test_input}")
         else:
             print(f"   Content: {test_input}")
-        
+
         try:
             result = await orchestrator.execute_hybrid(test_input)
-            
-            if result.get('success', False):
+
+            if result.get("success", False):
                 print(f"   ‚úÖ Processed successfully")
-                if 'extraction_result' in result:
+                if "extraction_result" in result:
                     print(f"   üìä Extracted: {result['company_data'].get('name', 'N/A')}")
                 else:
                     print(f"   üìä Company: {result['company_data'].get('name', 'N/A')}")
                 print(f"   üìÑ Report: {result.get('shared_output_file', 'N/A')}")
             else:
                 print(f"   ‚ùå Processing failed: {result.get('error', 'Unknown')}")
-                
+
         except Exception as e:
             print(f"   üí• Exception: {str(e)}")
-    
+
     print(f"\n" + "=" * 70)
     print("üéâ PROSPECTING DEMO COMPLETED")
     print("=" * 70)
 
+
 if __name__ == "__main__":
-    asyncio.run(main()) 
\ No newline at end of file
+    asyncio.run(main())

--- experiments/investors_search_recommender/generate_embeddings.py
+++ experiments/investors_search_recommender/generate_embeddings.py
@@ -17,9 +17,10 @@
 
 from experiments.investors_search_recommender.utils.env_utils import load_repo_env
 
+
 class EmbeddingGenerator:
     """Memory-efficient, resumable OpenAI embedding generator."""
-    
+
     def __init__(self, model: str = "text-embedding-3-large", batch_size: int = 70, checkpoint_dir: str = "data/processed/checkpoints"):
         """Initialize the embedding generator."""
         self.model = model
@@ -27,33 +28,29 @@
         self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
         self.checkpoint_dir = Path(checkpoint_dir)
         self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
-        
-        self.cost_tracker = {
-            "total_tokens": 0,
-            "total_requests": 0,
-            "total_cost": 0.0
-        }
-        
+
+        self.cost_tracker = {"total_tokens": 0, "total_requests": 0, "total_cost": 0.0}
+
     def count_total_records(self, input_path: str) -> int:
         """Count total records without loading them into memory."""
         data_path = Path(input_path)
         if not data_path.exists():
             raise FileNotFoundError(f"Payload data not found at {data_path}")
-        
+
         count = 0
-        with open(data_path, 'r', encoding='utf-8') as f:
+        with open(data_path, "r", encoding="utf-8") as f:
             for line in f:
                 if line.strip():
                     count += 1
         return count
-    
+
     def stream_payloads(self, input_path: str, limit: int = None) -> Iterator[Tuple[str, Dict[str, Any]]]:
         """Stream payloads one at a time to avoid loading everything into memory."""
         data_path = Path(input_path)
         if not data_path.exists():
             raise FileNotFoundError(f"Payload data not found at {data_path}")
-        
-        with open(data_path, 'r', encoding='utf-8') as f:
+
+        with open(data_path, "r", encoding="utf-8") as f:
             for i, line in enumerate(f):
                 if limit and i >= limit:
                     break
@@ -61,16 +58,16 @@
                     record = json.loads(line)
                     key = record.get("metadata", {}).get("company_name", f"record_{i}")
                     yield key, record
-    
+
     def create_batches_streaming(self, input_path: str, limit: int = None) -> Iterator[List[Tuple[str, str]]]:
         """Create batches by streaming data instead of loading everything into memory."""
         batch = []
         batch_count = 0
-        
+
         for company_id, payload in self.stream_payloads(input_path, limit):
             text_block = payload["semantic"]["text_block"]
             batch.append((company_id, text_block))
-            
+
             if len(batch) >= self.batch_size:
                 batch_count += 1
                 yield batch
@@ -78,14 +75,14 @@
                 # Force garbage collection every 10 batches
                 if batch_count % 10 == 0:
                     gc.collect()
-        
+
         # Add remaining items
         if batch:
             batch_count += 1
             yield batch
-        
+
         print(f"üì¶ Created {batch_count} batches (batch size: {self.batch_size})")
-    
+
     def save_checkpoint(self, batch_index: int, total_batches: int, embeddings_file: Path):
         """Save progress checkpoint."""
         checkpoint_data = {
@@ -93,80 +90,77 @@
             "total_batches": total_batches,
             "embeddings_file": str(embeddings_file),
             "cost_tracker": self.cost_tracker,
-            "timestamp": time.time()
+            "timestamp": time.time(),
         }
-        
+
         checkpoint_path = self.checkpoint_dir / f"checkpoint_{batch_index}.json"
-        with open(checkpoint_path, 'w') as f:
+        with open(checkpoint_path, "w") as f:
             json.dump(checkpoint_data, f, indent=2)
-        
+
         # Keep only the latest checkpoint
         for old_checkpoint in self.checkpoint_dir.glob("checkpoint_*.json"):
             if old_checkpoint != checkpoint_path:
                 old_checkpoint.unlink()
-    
+
     def load_checkpoint(self) -> Dict[str, Any]:
         """Load the latest checkpoint if it exists."""
         checkpoints = list(self.checkpoint_dir.glob("checkpoint_*.json"))
         if not checkpoints:
             return None
-        
+
         latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)
-        with open(latest_checkpoint, 'r') as f:
+        with open(latest_checkpoint, "r") as f:
             return json.load(f)
-    
+
     def embed_batch(self, batch: List[Tuple[str, str]]) -> List[Tuple[str, List[float]]]:
         """Embed a single batch using OpenAI API."""
         texts = [item[1] for item in batch]  # Extract text blocks
-        
+
         try:
-            response = self.client.embeddings.create(
-                model=self.model,
-                input=texts
-            )
-            
+            response = self.client.embeddings.create(model=self.model, input=texts)
+
             # Track costs
             self.cost_tracker["total_requests"] += 1
             self.cost_tracker["total_tokens"] += response.usage.total_tokens
             self.cost_tracker["total_cost"] += response.usage.total_tokens * 0.00013 / 1000
-            
+
             # Extract embeddings
             embeddings = []
             for i, embedding_data in enumerate(response.data):
                 company_id = batch[i][0]
                 embedding = embedding_data.embedding
                 embeddings.append((company_id, embedding))
-            
+
             return embeddings
-            
+
         except Exception as e:
             print(f"‚ùå Error embedding batch: {e}")
             return []
-    
+
     def generate_embeddings_streaming(self, input_path: str = None, sample_size: int = None, output_prefix: str = None) -> Path:
         """Generate embeddings with streaming to disk and resume capability."""
         if input_path is None:
             input_path = "data/processed/investors_payload.jsonl"
-        
+
         # Count total records
         total_records = self.count_total_records(input_path)
         if sample_size:
             total_records = min(total_records, sample_size)
-        
+
         print(f"üöÄ Generating embeddings for {total_records:,} records")
         print(f"üìä Using model: {self.model}")
         print(f"üì¶ Batch size: {self.batch_size}")
-        
+
         # Check for existing checkpoint
         checkpoint = self.load_checkpoint()
         start_batch = 0
         embeddings_file = None
-        
+
         if checkpoint:
             print(f"üîÑ Resuming from checkpoint: batch {checkpoint['batch_index']}")
-            start_batch = checkpoint['batch_index']
-            self.cost_tracker = checkpoint['cost_tracker']
-            embeddings_file = Path(checkpoint['embeddings_file'])
+            start_batch = checkpoint["batch_index"]
+            self.cost_tracker = checkpoint["cost_tracker"]
+            embeddings_file = Path(checkpoint["embeddings_file"])
         else:
             # Create new embeddings file
             timestamp = time.strftime("%Y%m%d_%H%M%S")
@@ -175,149 +169,142 @@
             else:
                 suffix = f"_sample_{sample_size}" if sample_size else "_full"
                 embeddings_file = Path(f"data/processed/embeddings{suffix}_{timestamp}.jsonl")
-        
+
         # Calculate total batches
         total_batches = (total_records + self.batch_size - 1) // self.batch_size
-        
+
         start_time = time.time()
         processed_records = 0
-        
+
         # Open embeddings file for appending
-        mode = 'a' if checkpoint else 'w'
-        with open(embeddings_file, mode, encoding='utf-8') as f:
+        mode = "a" if checkpoint else "w"
+        with open(embeddings_file, mode, encoding="utf-8") as f:
             batch_iter = self.create_batches_streaming(input_path, sample_size)
-            
+
             for batch_index, batch in enumerate(batch_iter):
                 if batch_index < start_batch:
                     continue  # Skip already processed batches
-                
-                print(f"Processing batch {batch_index+1}/{total_batches} ({len(batch)} records)")
-                
+
+                print(f"Processing batch {batch_index + 1}/{total_batches} ({len(batch)} records)")
+
                 embeddings = self.embed_batch(batch)
                 for company_id, embedding in embeddings:
                     # Write each embedding immediately to disk
-                    embedding_record = {
-                        "company_id": company_id,
-                        "embedding": embedding
-                    }
-                    f.write(json.dumps(embedding_record) + '\n')
+                    embedding_record = {"company_id": company_id, "embedding": embedding}
+                    f.write(json.dumps(embedding_record) + "\n")
                     f.flush()  # Ensure data is written
-                
+
                 processed_records += len(batch)
-                
+
                 # Save checkpoint every 50 batches
                 if batch_index % 50 == 0:
                     self.save_checkpoint(batch_index, total_batches, embeddings_file)
-                
+
                 # Rate limiting - OpenAI allows 3000 requests per minute
                 if batch_index < total_batches - 1:
                     time.sleep(0.02)  # ~50 requests per second
-                
+
                 # Force garbage collection every 100 batches
                 if batch_index % 100 == 0:
                     gc.collect()
-        
+
         end_time = time.time()
         duration = end_time - start_time
-        
+
         print(f"‚úÖ Generated embeddings for {processed_records:,} records in {duration:.2f} seconds")
         print(f"üí∞ Total cost: ${self.cost_tracker['total_cost']:.4f}")
         print(f"üíæ Embeddings saved to: {embeddings_file}")
-        
+
         # Clean up checkpoint
         for checkpoint_file in self.checkpoint_dir.glob("checkpoint_*.json"):
             checkpoint_file.unlink()
-        
+
         return embeddings_file
-    
+
     def validate_embeddings_lightweight(self, embeddings_file: Path) -> Dict[str, Any]:
         """Lightweight validation without loading all embeddings into memory."""
         print("\nüîç VALIDATION RESULTS")
         print("=" * 50)
-        
+
         validation_results = {}
-        
+
         # Sample first 100 embeddings for validation
         sample_embeddings = []
         sample_count = 0
         max_sample = 100
-        
-        with open(embeddings_file, 'r', encoding='utf-8') as f:
+
+        with open(embeddings_file, "r", encoding="utf-8") as f:
             for line in f:
                 if sample_count >= max_sample:
                     break
                 if line.strip():
                     record = json.loads(line)
-                    sample_embeddings.append(record['embedding'])
+                    sample_embeddings.append(record["embedding"])
                     sample_count += 1
-        
+
         # 1. Dimensionality check
         embedding_dims = [len(emb) for emb in sample_embeddings]
         unique_dims = set(embedding_dims)
-        
+
         validation_results["dimensionality"] = {
             "expected_dim": 3072,
             "actual_dims": list(unique_dims),
             "all_same_dim": len(unique_dims) == 1,
             "correct_dim": 3072 in unique_dims,
-            "sample_size": len(sample_embeddings)
+            "sample_size": len(sample_embeddings),
         }
-        
+
         print(f"üìè Dimensionality: {unique_dims}")
         if len(unique_dims) == 1 and 3072 in unique_dims:
             print("‚úÖ All embeddings have correct dimension (3072)")
         else:
             print("‚ùå Dimensionality issues detected")
-        
+
         # 2. Basic quality metrics on sample
         if sample_embeddings:
             sample_matrix = np.array(sample_embeddings)
             mean_norm = np.mean([np.linalg.norm(emb) for emb in sample_embeddings])
-            
+
             validation_results["quality_metrics"] = {
                 "mean_norm": float(mean_norm),
                 "sample_size": len(sample_embeddings),
-                "total_embeddings": "See embeddings file"
+                "total_embeddings": "See embeddings file",
             }
-            
+
             print(f"üìè Mean embedding norm: {mean_norm:.3f}")
             print(f"üìä Sample size: {len(sample_embeddings)}")
-        
+
         # 3. File statistics
         file_size = embeddings_file.stat().st_size
-        validation_results["file_stats"] = {
-            "file_size_mb": file_size / (1024 * 1024),
-            "file_path": str(embeddings_file)
-        }
-        
+        validation_results["file_stats"] = {"file_size_mb": file_size / (1024 * 1024), "file_path": str(embeddings_file)}
+
         print(f"üíæ File size: {file_size / (1024 * 1024):.1f} MB")
-        
+
         return validation_results
-    
-    
+
     def save_validation_results(self, embeddings_file: Path, validation_results: Dict[str, Any], output_prefix: str = None):
         """Save validation results."""
         timestamp = time.strftime("%Y%m%d_%H%M%S")
-        
+
         if output_prefix:
             prefix = output_prefix
         else:
             prefix = "validation"
-        
+
         validation_path = Path(f"data/processed/{prefix}_validation_{timestamp}.json")
-        
+
         # Add metadata to validation results
         validation_results["metadata"] = {
             "model": self.model,
             "batch_size": self.batch_size,
             "cost": self.cost_tracker["total_cost"],
             "embeddings_file": str(embeddings_file),
-            "timestamp": timestamp
+            "timestamp": timestamp,
         }
-        
-        with open(validation_path, 'w') as f:
+
+        with open(validation_path, "w") as f:
             json.dump(validation_results, f, indent=2)
-        
+
         print(f"\nüíæ Validation results saved: {validation_path}")
         return validation_path
 
@@ -325,44 +312,37 @@
 def main():
     """Main function to run memory-efficient embedding generation."""
     import argparse
-    
+
     parser = argparse.ArgumentParser(description="Generate OpenAI embeddings (memory-efficient)")
     parser.add_argument("--input", help="Input payload JSONL file path")
     parser.add_argument("--sample", type=int, help="Sample size for testing")
     parser.add_argument("--output", help="Output file prefix (e.g., 'investors_test')")
     parser.add_argument("--resume", action="store_true", help="Resume from checkpoint if available")
-    
+
     args = parser.parse_args()
-    
+
     print("üöÄ Memory-Efficient OpenAI Embedding Generator")
     print("=" * 60)
-    
+
     # Load environment variables
     load_repo_env()
-    
+
     # Initialize generator
-    generator = EmbeddingGenerator(
-        model="text-embedding-3-large",
-        batch_size=70
-    )
-    
+    generator = EmbeddingGenerator(model="text-embedding-3-large", batch_size=70)
+
     # Generate embeddings with streaming
-    embeddings_file = generator.generate_embeddings_streaming(
-        input_path=args.input,
-        sample_size=args.sample,
-        output_prefix=args.output
-    )
-    
+    embeddings_file = generator.generate_embeddings_streaming(input_path=args.input, sample_size=args.sample, output_prefix=args.output)
+
     if not embeddings_file.exists():
         print("‚ùå No embeddings generated")
         return
-    
+
     # Validate embeddings (lightweight)
     validation_results = generator.validate_embeddings_lightweight(embeddings_file)
-    
+
     # Save validation results
     validation_path = generator.save_validation_results(embeddings_file, validation_results, args.output)
-    
+
     print(f"\n‚úÖ Embedding generation complete!")
     print(f"üíæ Embeddings file: {embeddings_file}")
     print(f"üí∞ Total cost: ${generator.cost_tracker['total_cost']:.4f}")

--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 1
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 1
@@ -1,5 +1,5 @@
 import pandas as pd
 import polars as pl
 
-pd.set_option('display.max_rows', None)
-pd.set_option('display.max_columns', None)
+pd.set_option("display.max_rows", None)
+pd.set_option("display.max_columns", None)
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 2
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 2
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 3
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 3
@@ -1,3 +1 @@
-(df_pl.filter(
-    pl.col("Company Name").str.to_lowercase().str.contains("1752".lower())
-)).to_pandas()
+(df_pl.filter(pl.col("Company Name").str.to_lowercase().str.contains("1752".lower()))).to_pandas()
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 4
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 4
@@ -1,5 +1 @@
-(
-    df_pl.filter(
-        pl.col("Company Name").str.to_lowercase().str.starts_with("Alpha Universal Management".lower())
-    )
-).to_pandas()['Description'].values[0]
+(df_pl.filter(pl.col("Company Name").str.to_lowercase().str.starts_with("Alpha Universal Management".lower()))).to_pandas()["Description"].values[0]
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 5
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 5
@@ -1,3 +1 @@
-(df_pl.filter(
-    pl.col("Website").str.to_lowercase().str.contains("walespensionpartnershi".lower())
-)).to_pandas()
+(df_pl.filter(pl.col("Website").str.to_lowercase().str.contains("walespensionpartnershi".lower()))).to_pandas()
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 6
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 6
@@ -1,3 +1 @@
-(df_pl.filter(
-    pl.col("Company URL").str.to_lowercase().str.contains("brunel".lower())
-)).to_pandas()
+(df_pl.filter(pl.col("Company URL").str.to_lowercase().str.contains("brunel".lower()))).to_pandas()
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 7
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 7
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 8
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 8
@@ -7,12 +7,7 @@
 cols_no_index = [c for c in df_indexed.columns if c != "row_number"]
 
 # 3Ô∏è‚É£ Count how many times each full row (excluding index) appears
-dup_counts = (
-    df_indexed
-    .group_by(cols_no_index)
-    .len()
-    .rename({"len": "count"})
-)
+dup_counts = df_indexed.group_by(cols_no_index).len().rename({"len": "count"})
 
 # 4Ô∏è‚É£ Join counts back to tag each row with its repetition count
 df_with_counts = df_indexed.join(dup_counts, on=cols_no_index, how="left")
@@ -24,4 +19,4 @@
 duplicates_df = duplicates_df.sort(cols_no_index)
 
 # 7Ô∏è‚É£ Display a preview (e.g. first 50 duplicates)
-duplicates_df.head(10)
+duplicates_df.head(10)
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 10
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 10
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 11
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 11
@@ -1,18 +1,13 @@
 cols = ["Company Name", "Website"]
 
 # 1Ô∏è‚É£ Count how many times each (Company Name, Website) pair appears
-counts = (
-    df_pl
-    .group_by(cols)
-    .len()
-    .rename({"len": "count"})
-)
+counts = df_pl.group_by(cols).len().rename({"len": "count"})
 
 # 2Ô∏è‚É£ Join back to the original DataFrame
 df_shared = (
     df_pl.join(counts, on=cols)
-    .filter(pl.col("count") > 1)   # keep only pairs appearing more than once
-    .sort(cols)                    # sort for readability
+    .filter(pl.col("count") > 1)  # keep only pairs appearing more than once
+    .sort(cols)  # sort for readability
 )
 
 # 3Ô∏è‚É£ Show result
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 12
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 12
@@ -1,9 +1,3 @@
 # Convert "NA" tp null
-df_pl = df_pl.with_columns([
-    pl.when(pl.col(c) == "N/A")
-      .then(None)
-      .otherwise(pl.col(c))
-      .alias(c)
-    for c in df_pl.columns
-])
+df_pl = df_pl.with_columns([pl.when(pl.col(c) == "N/A").then(None).otherwise(pl.col(c)).alias(c) for c in df_pl.columns])
 df_pl
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 13
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 13
@@ -2,16 +2,10 @@
 n_rows = df_pl.height
 
 # Compute null counts
-null_summary = df_pl.select([
-    pl.col(c).null_count().alias(c) for c in df_pl.columns
-])
+null_summary = df_pl.select([pl.col(c).null_count().alias(c) for c in df_pl.columns])
 
 # Transpose and convert to Pandas safely
-null_df = (
-    null_summary.transpose(include_header=True)
-    .to_pandas()
-    .reset_index(drop=True)
-)
+null_df = null_summary.transpose(include_header=True).to_pandas().reset_index(drop=True)
 
 # The first column is always the original column name
 # The second column (the counts) may be unnamed ‚Üí rename both properly
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 14
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 14
@@ -1,8 +1,3 @@
 # Count number of rows per HQ Global Region
-region_counts = (
-    df_pl
-    .group_by("HQ Global Region")
-    .agg(pl.count().alias("row_count"))
-    .sort("row_count", descending=True)
-)
-region_counts
+region_counts = df_pl.group_by("HQ Global Region").agg(pl.count().alias("row_count")).sort("row_count", descending=True)
+region_counts
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 15
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 15
@@ -9,16 +9,10 @@
     n_rows = df_region.height
 
     # Compute null counts per column
-    null_summary = df_region.select([
-        pl.col(c).null_count().alias(c) for c in df_region.columns
-    ])
+    null_summary = df_region.select([pl.col(c).null_count().alias(c) for c in df_region.columns])
 
     # Convert to Pandas-friendly format
-    null_df = (
-        null_summary.transpose(include_header=True)
-        .to_pandas()
-        .reset_index(drop=True)
-    )
+    null_df = null_summary.transpose(include_header=True).to_pandas().reset_index(drop=True)
 
     null_df.columns = ["column", "null_count"]
     null_df[f"% nulls ({region})"] = (null_df["null_count"] / n_rows * 100).round(2)
@@ -34,4 +28,4 @@
 
 # Display
 print(" Null percentage per column, split by region:")
-display(null_compare)
+display(null_compare)
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 16
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 16
@@ -1,9 +1,5 @@
 # ---------- Distinct Values ----------
 cat_cols = [c for c, dtype in zip(df_pl.columns, df_pl.dtypes) if dtype == pl.Utf8]
-cat_summary = (
-    df_pl.select([
-        pl.col(c).n_unique().alias(c) for c in cat_cols
-    ])
-)
+cat_summary = df_pl.select([pl.col(c).n_unique().alias(c) for c in cat_cols])
 print("\n Distinct Values:")
 pd.DataFrame(cat_summary.transpose(include_header=True))
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 17
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 17
@@ -1,15 +1,13 @@
-cols_to_convert = [
-    "Total Investments",
-    "Total Active Portfolio",
-    "Total Exits"
-]
+cols_to_convert = ["Total Investments", "Total Active Portfolio", "Total Exits"]
 
 # Clean and convert
-df_pl = df_pl.with_columns([
-    pl.col(c)
-      .cast(pl.Utf8)                                   # ensure string type
-      .str.replace_all(r"[^\d\.\-]", "")               # remove non-numeric chars (commas, $, etc.)
-      .cast(pl.Float64)                                # convert to numeric
-      .alias(c)
-    for c in cols_to_convert
-])
+df_pl = df_pl.with_columns(
+    [
+        pl.col(c)
+        .cast(pl.Utf8)  # ensure string type
+        .str.replace_all(r"[^\d\.\-]", "")  # remove non-numeric chars (commas, $, etc.)
+        .cast(pl.Float64)  # convert to numeric
+        .alias(c)
+        for c in cols_to_convert
+    ]
+)
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 18
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 18
@@ -1,10 +1,12 @@
-#Convert AUM column by increasing by an order of x1,000,000
-df_pl = df_pl.with_columns([
-    pl.col("AUM")
-    .cast(pl.Utf8)                                # ensure string
-    .str.replace_all(" ", "")                     # remove spaces (thousands separators)
-    .str.replace_all(",", ".")                    # replace comma decimal with dot
-    .cast(pl.Float64)                             # convert to number
-    .mul(1_000_000)                               # multiply by 1,000,000
-    .alias("AUM")                                 # overwrite same column
-])
+# Convert AUM column by increasing by an order of x1,000,000
+df_pl = df_pl.with_columns(
+    [
+        pl.col("AUM")
+        .cast(pl.Utf8)  # ensure string
+        .str.replace_all(" ", "")  # remove spaces (thousands separators)
+        .str.replace_all(",", ".")  # replace comma decimal with dot
+        .cast(pl.Float64)  # convert to number
+        .mul(1_000_000)  # multiply by 1,000,000
+        .alias("AUM")  # overwrite same column
+    ]
+)
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 19
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 19
@@ -2,27 +2,24 @@
 QUANTILES = [0.25, 0.5, 0.75, 0.9]  # customize as needed
 
 # Identify numeric columns
-num_cols = [c for c, dtype in zip(df_pl.columns, df_pl.dtypes)
-            if dtype in pl.NUMERIC_DTYPES]
+num_cols = [c for c, dtype in zip(df_pl.columns, df_pl.dtypes) if dtype in pl.NUMERIC_DTYPES]
 
 # Basic numeric stats
-num_stats = df_pl.select([
-    expr
-    for c in num_cols
-    for expr in [
-        pl.col(c).mean().alias(f"{c}_mean"),
-        pl.col(c).std().alias(f"{c}_std"),
-        pl.col(c).min().alias(f"{c}_min"),
-        pl.col(c).max().alias(f"{c}_max")
+num_stats = df_pl.select(
+    [
+        expr
+        for c in num_cols
+        for expr in [
+            pl.col(c).mean().alias(f"{c}_mean"),
+            pl.col(c).std().alias(f"{c}_std"),
+            pl.col(c).min().alias(f"{c}_min"),
+            pl.col(c).max().alias(f"{c}_max"),
+        ]
     ]
-])
+)
 
 # Quantile stats
-quantile_stats = df_pl.select([
-    pl.col(c).quantile(q).alias(f"{c}_q{int(q*100)}")
-    for c in num_cols
-    for q in QUANTILES
-])
+quantile_stats = df_pl.select([pl.col(c).quantile(q).alias(f"{c}_q{int(q * 100)}") for c in num_cols for q in QUANTILES])
 
 print("\n Numeric Distributions (basic stats):")
 pd.DataFrame(num_stats.transpose(include_header=True))
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 20
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 20
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 21
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 21
@@ -2,10 +2,10 @@
 
 # Define numeric columns and how many bins you want for each
 bin_config = {
-    "AUM": 8,                     # use quantile bins (skewed continuous variable)
-    "Total Investments": 5,       # use log bins (discrete count data)
+    "AUM": 8,  # use quantile bins (skewed continuous variable)
+    "Total Investments": 5,  # use log bins (discrete count data)
     "Total Active Portfolio": 5,  # use log bins
-    "Total Exits": 5,             # use log bins
+    "Total Exits": 5,  # use log bins
 }
 
 SKEW_THRESHOLD = 1  # decides quantile vs equal-width if not explicitly overridden
@@ -14,13 +14,15 @@
 # ---- STEP 1: Compute bins for each specified column ----
 for c, N_BINS in bin_config.items():
     # compute stats
-    stats = df_pl.select([
-        pl.col(c).min().alias("min"),
-        pl.col(c).max().alias("max"),
-        pl.col(c).mean().alias("mean"),
-        pl.col(c).std().alias("std"),
-        pl.col(c).skew().alias("skew"),
-    ]).row(0)
+    stats = df_pl.select(
+        [
+            pl.col(c).min().alias("min"),
+            pl.col(c).max().alias("max"),
+            pl.col(c).mean().alias("mean"),
+            pl.col(c).std().alias("std"),
+            pl.col(c).skew().alias("skew"),
+        ]
+    ).row(0)
 
     col_min, col_max, col_mean, col_std, skewness = stats
 
@@ -33,10 +35,7 @@
     # ---- QUANTILE BINNING (for AUM) ----
     if method == "quantile":
         quantiles = [i / N_BINS for i in range(N_BINS + 1)]
-        edges = (
-            df_pl.select([pl.col(c).quantile(q).alias(f"q{int(q*100)}") for q in quantiles])
-            .row(0)
-        )
+        edges = df_pl.select([pl.col(c).quantile(q).alias(f"q{int(q * 100)}") for q in quantiles]).row(0)
         edges = sorted([round(float(e), 6) if e is not None else None for e in edges])
 
     # ---- LOGARITHMIC BINNING (for count-based columns) ----
@@ -59,17 +58,14 @@
         "edges": edges,
     }
 
+
 # ---- STEP 2: Assign bin index per row for each column ----
 def assign_bin(col, edges):
     """Return Polars expression that assigns bin index based on edges"""
-    return pl.col(col).map_elements(
-        lambda x: int(np.digitize(x, edges, right=False)), return_dtype=pl.Int32
-    )
+    return pl.col(col).map_elements(lambda x: int(np.digitize(x, edges, right=False)), return_dtype=pl.Int32)
+
 
-df_binned = df_pl.with_columns([
-    assign_bin(c, bucket_suggestions[c]["edges"]).alias(f"{c}_bin")
-    for c in bin_config.keys()
-])
+df_binned = df_pl.with_columns([assign_bin(c, bucket_suggestions[c]["edges"]).alias(f"{c}_bin") for c in bin_config.keys()])
 
 # ---- STEP 3: Display results ----
 print("\nüßÆ Auto Bucket Suggestions:")
@@ -78,4 +74,4 @@
     print(f"   edges={info['edges']}")
 
 print("\n‚úÖ Added bin columns:")
-print([f"{c}_bin" for c in bin_config.keys()])
+print([f"{c}_bin" for c in bin_config.keys()])
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 23
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 23
@@ -12,44 +12,30 @@
     100_000_000,
     1_000_000_000,
     100_000_000_000,
-    float("inf")  # everything above 100B
+    float("inf"),  # everything above 100B
 ]
 
-labels = [
-    "<100K",
-    "100K‚Äì500K",
-    "500K‚Äì1M",
-    "1M‚Äì100M",
-    "100M‚Äì1B",
-    "1B‚Äì100B",
-    "100B+"
-]
+labels = ["<100K", "100K‚Äì500K", "500K‚Äì1M", "1M‚Äì100M", "100M‚Äì1B", "1B‚Äì100B", "100B+"]
 
 # 2Ô∏è‚É£ Filter out nulls / invalids
 df_valid = df_pl.filter(pl.col(col).is_not_null() & (pl.col(col) > 0))
 
+
 # 3Ô∏è‚É£ Assign bins using np.digitize
 def assign_manual_bin(x):
     if x is None or np.isnan(x):
         return None
     return int(np.digitize(x, edges, right=False))
 
-df_binned = df_valid.with_columns([
-    pl.col(col)
-    .map_elements(assign_manual_bin, return_dtype=pl.Int32)
-    .alias(f"{col}_bin")
-])
 
+df_binned = df_valid.with_columns([pl.col(col).map_elements(assign_manual_bin, return_dtype=pl.Int32).alias(f"{col}_bin")])
+
 # 4Ô∏è‚É£ Count how many rows fall in each bin
-counts = (
-    df_binned.group_by(f"{col}_bin")
-    .agg(pl.count().alias("row_count"))
-    .sort(f"{col}_bin")
-).to_pandas()
+counts = (df_binned.group_by(f"{col}_bin").agg(pl.count().alias("row_count")).sort(f"{col}_bin")).to_pandas()
 
 # 5Ô∏è‚É£ Add readable labels
-counts["range"] = [labels[i-1] if 1 <= i <= len(labels) else "Unknown" for i in counts["AUM_bin"]]
+counts["range"] = [labels[i - 1] if 1 <= i <= len(labels) else "Unknown" for i in counts["AUM_bin"]]
 
 # 6Ô∏è‚É£ Display
 print("\nüìä Rows per AUM Range:")
-print(counts[["range", "row_count"]])
+print(counts[["range", "row_count"]])
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 24
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 24
@@ -14,46 +14,30 @@
     1_000,
     50_000,
     100_000,
-    float("inf")  # everything above 100,000
+    float("inf"),  # everything above 100,000
 ]
 
-labels = [
-    "0‚Äì1",
-    "1‚Äì10",
-    "10‚Äì50",
-    "50‚Äì100",
-    "100‚Äì500",
-    "500‚Äì1,000",
-    "1,000‚Äì50,000",
-    "50,000‚Äì100,000",
-    "100,000+"
-]
+labels = ["0‚Äì1", "1‚Äì10", "10‚Äì50", "50‚Äì100", "100‚Äì500", "500‚Äì1,000", "1,000‚Äì50,000", "50,000‚Äì100,000", "100,000+"]
 
 # 2Ô∏è‚É£ Filter out nulls / invalids
 df_valid = df_pl.filter(pl.col(col).is_not_null() & (pl.col(col) > 0))
 
+
 # 3Ô∏è‚É£ Assign bins using np.digitize
 def assign_manual_bin(x):
     if x is None or np.isnan(x):
         return None
     return int(np.digitize(x, edges, right=False))
 
-df_binned = df_valid.with_columns([
-    pl.col(col)
-    .map_elements(assign_manual_bin, return_dtype=pl.Int32)
-    .alias(f"{col}_bin")
-])
 
+df_binned = df_valid.with_columns([pl.col(col).map_elements(assign_manual_bin, return_dtype=pl.Int32).alias(f"{col}_bin")])
+
 # 4Ô∏è‚É£ Count how many rows fall in each bin
-counts = (
-    df_binned.group_by(f"{col}_bin")
-    .agg(pl.count().alias("row_count"))
-    .sort(f"{col}_bin")
-).to_pandas()
+counts = (df_binned.group_by(f"{col}_bin").agg(pl.count().alias("row_count")).sort(f"{col}_bin")).to_pandas()
 
 # 5Ô∏è‚É£ Add readable labels
-counts["range"] = [labels[i-1] if 1 <= i <= len(labels) else "Unknown" for i in counts[f"{col}_bin"]]
+counts["range"] = [labels[i - 1] if 1 <= i <= len(labels) else "Unknown" for i in counts[f"{col}_bin"]]
 
 # 6Ô∏è‚É£ Display
 print("\nüìä Rows per AUM Range:")
-print(counts[["range", "row_count"]])
+print(counts[["range", "row_count"]])
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 25
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 25
@@ -1,2 +1,2 @@
 non_null_count = df_pl.filter(pl.col("AUM").is_not_null()).height
-print(f"Total rows with non-null AUM: {non_null_count:,}")
+print(f"Total rows with non-null AUM: {non_null_count:,}")
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 28
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 28
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 29
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 29
@@ -1,4 +1,5 @@
 import json
+
 file_path = "../../../data/processed/investors_payload.jsonl"
 
 with open(file_path, "r") as f:
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 30
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 30
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 31
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 31
@@ -1 +1 @@
-item['filters']
+item["filters"]
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 32
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 32
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 33
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 33
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 34
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 34
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 35
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 35
@@ -1 +1 @@
-len(df_pl['Preferred Industry'].unique())
+len(df_pl["Preferred Industry"].unique())
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 36
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 36
@@ -1,2 +1 @@
-
-len(df_pl['Preferred Verticals'].unique())
+len(df_pl["Preferred Verticals"].unique())
--- experiments/investors_search_recommender/notebooks/eda.ipynb:cell 37
+++ experiments/investors_search_recommender/notebooks/eda.ipynb:cell 37

--- experiments/investors_search_recommender/pipelines/__init__.py
+++ experiments/investors_search_recommender/pipelines/__init__.py
@@ -4,4 +4,3 @@
 from .embed_payloads import build_embeddings
 
 __all__ = ["build_dataset", "build_embeddings"]
-

--- experiments/investors_search_recommender/pipelines/build_dataset.py
+++ experiments/investors_search_recommender/pipelines/build_dataset.py
@@ -102,9 +102,7 @@
 
 
 SEMANTIC_TEMPLATE = SemanticTemplate(
-    sections=_build_section_templates(
-        SCHEMA_CONFIG.get("semantic_text", {}).get("template_sections", [])
-    ),
+    sections=_build_section_templates(SCHEMA_CONFIG.get("semantic_text", {}).get("template_sections", [])),
     separator=SCHEMA_CONFIG.get("semantic_text", {}).get("separator", " "),
     weight_field=SCHEMA_CONFIG.get("semantic_text", {}).get("weight_field"),
 )
@@ -125,10 +123,7 @@
     def default(cls) -> "PipelinePaths":
         raw_files = sorted(RAW_DATA_DIR.glob("dataset_investors-*.xlsx"))
         if not raw_files:
-            raise FileNotFoundError(
-                "No Excel dataset found in data/raw/. Expected a file named like "
-                "'dataset_investors-*.xlsx'."
-            )
+            raise FileNotFoundError("No Excel dataset found in data/raw/. Expected a file named like 'dataset_investors-*.xlsx'.")
         latest = raw_files[-1]
 
         processed_dir = PROCESSED_DATA_DIR
@@ -157,43 +152,43 @@
     if not AUM_ENRICHMENT_PATH.exists():
         print(f"AUM enrichment file not found: {AUM_ENRICHMENT_PATH}")
         return {}
-    
-    with open(AUM_ENRICHMENT_PATH, 'r', encoding='utf-8') as f:
+
+    with open(AUM_ENRICHMENT_PATH, "r", encoding="utf-8") as f:
         enrichment_data = json.load(f)
-    
+
     # The JSON has a "results" key containing the array
     if "results" in enrichment_data:
         results_array = enrichment_data["results"]
     else:
         results_array = enrichment_data
-    
+
     print(f"Loading AUM enrichment: {len(results_array)} raw entries")
-    
+
     # Deduplicate by company name (keep first occurrence)
     seen_companies = set()
     deduplicated_results = []
-    
+
     for item in results_array:
         if isinstance(item, dict):
             company_name = item.get("Company Name", "").strip()
             aum_value = item.get("AUM")
-            
+
             if company_name and aum_value is not None and company_name not in seen_companies:
                 seen_companies.add(company_name)
                 deduplicated_results.append(item)
-    
+
     print(f"AUM enrichment deduplicated: {len(deduplicated_results)} unique companies")
     print(f"Removed {len(results_array) - len(deduplicated_results)} duplicate entries")
-    
+
     # Convert to dictionary: company_name -> AUM_value
     aum_dict = {}
     for item in deduplicated_results:
         company_name = item.get("Company Name", "").strip()
         aum_value = item.get("AUM")
-        
+
         if company_name and aum_value is not None:
             aum_dict[company_name] = float(aum_value)
-    
+
     print(f"AUM enrichment loaded: {len(aum_dict)} companies with AUM data")
     return aum_dict
 
@@ -203,45 +198,39 @@
     if not aum_dict:
         print("No AUM enrichment data available, skipping enrichment")
         return df
-    
+
     # Get companies that exist in both datasets
     main_companies = set(df["company_name"].to_list())
     enrichment_companies = set(aum_dict.keys())
     matched_companies = main_companies.intersection(enrichment_companies)
-    
+
     print(f"AUM enrichment: {len(matched_companies)} companies matched for enrichment")
-    
+
     if not matched_companies:
         print("No companies matched for AUM enrichment")
         return df
-    
+
     # Count companies with null AUM before enrichment
-    companies_with_null_aum = df.filter(
-        pl.col("company_name").is_in(matched_companies) & 
-        pl.col("aum").is_null()
-    ).height
-    
+    companies_with_null_aum = df.filter(pl.col("company_name").is_in(matched_companies) & pl.col("aum").is_null()).height
+
     print(f"AUM enrichment: {companies_with_null_aum} companies with null AUM ready for enrichment")
-    
+
     # Apply AUM enrichment
     enriched_df = df.with_columns(
-        pl.when(
-            pl.col("company_name").is_in(matched_companies) &
-            pl.col("aum").is_null()
-        )
+        pl.when(pl.col("company_name").is_in(matched_companies) & pl.col("aum").is_null())
         .then(pl.col("company_name").map_elements(lambda x: aum_dict.get(x), return_dtype=pl.Float64))
         .otherwise(pl.col("aum"))
         .alias("aum")
     )
-    
+
     # Count companies with AUM after enrichment
     companies_with_aum_after = enriched_df.filter(pl.col("aum").is_not_null()).height
     companies_with_aum_before = df.filter(pl.col("aum").is_not_null()).height
     aum_increase = companies_with_aum_after - companies_with_aum_before
-    
+
     print(f"AUM enrichment: {aum_increase} companies enriched with AUM data")
     print(f"AUM enrichment: Total companies with AUM: {companies_with_aum_after:,}")
-    
+
     return enriched_df
 
 
@@ -250,33 +239,25 @@
 
     missing_columns = [col for col in COLUMN_MAPPING if col not in df.columns]
     if missing_columns:
-        raise ValueError(
-            "Dataset is missing expected columns: " + ", ".join(sorted(missing_columns))
-        )
+        raise ValueError("Dataset is missing expected columns: " + ", ".join(sorted(missing_columns)))
 
     selected = df.select([pl.col(col) for col in COLUMN_MAPPING])
     renamed = selected.rename(COLUMN_MAPPING)
 
     string_columns = [col for col in renamed.columns if col not in NUMERIC_COLUMNS]
-    cleaned_strings = renamed.with_columns([
-        pl.when(pl.col(col).cast(pl.Utf8).str.strip_chars().str.to_lowercase().is_in(
-            list(NULL_STRINGS_LOWER)
-        ))
-        .then(None)
-        .otherwise(pl.col(col))
-        .alias(col)
-        for col in string_columns
-    ])
+    cleaned_strings = renamed.with_columns(
+        [
+            pl.when(pl.col(col).cast(pl.Utf8).str.strip_chars().str.to_lowercase().is_in(list(NULL_STRINGS_LOWER)))
+            .then(None)
+            .otherwise(pl.col(col))
+            .alias(col)
+            for col in string_columns
+        ]
+    )
 
-    trimmed_strings = cleaned_strings.with_columns([
-        pl.col(col).cast(pl.Utf8).str.strip_chars().alias(col)
-        for col in string_columns
-    ])
+    trimmed_strings = cleaned_strings.with_columns([pl.col(col).cast(pl.Utf8).str.strip_chars().alias(col) for col in string_columns])
 
-    cleaned_numeric = trimmed_strings.with_columns([
-        _parse_numeric_column(col)
-        for col in NUMERIC_COLUMNS
-    ])
+    cleaned_numeric = trimmed_strings.with_columns([_parse_numeric_column(col) for col in NUMERIC_COLUMNS])
 
     return cleaned_numeric
 
@@ -301,7 +282,7 @@
 
 def derive_investor_category_flags(df: pl.DataFrame, batch_size: int = 50000) -> pl.DataFrame:
     """Derive boolean flags for investor categories using company_name + description.
-    
+
     Processes in batches to avoid memory issues with large datasets.
 
     Adds:
@@ -310,11 +291,9 @@
     """
     total_rows = len(df)
     print(f"  Processing {total_rows:,} rows in batches of {batch_size:,}...")
-    
+
     # Define regex patterns once
-    foundation_re = (
-        r"\bfoundation(s)?\b|\bgrant[-\s]?making\b|\bcharitable\s+trust\b"
-    )
+    foundation_re = r"\bfoundation(s)?\b|\bgrant[-\s]?making\b|\bcharitable\s+trust\b"
     pension_re = (
         r"\bpension(s)?\b|\bretirement\s+fund(s)?\b|\bsuperannuation\b|\bsuper\s+fund(s)?\b|"
         r"\blgps\b|\bteachers'?\s+retirement\b|\bpublic\s+employees'?\s+retirement\b|\boccupational\s+pension\b"
@@ -325,64 +304,67 @@
         r"\bgeneral\s+insurer\b|\bproperty\s+and\s+casualty\b|\bp\s*&\s*c\b|\blloyd'?s\s+syndicate\b"
     )
     charity_re = r"\bcharit(y|ies|able)\b"
-    
+
     # Process in batches and collect results
     batch_results = []
-    
+
     for batch_start in range(0, total_rows, batch_size):
         batch_end = min(batch_start + batch_size, total_rows)
         batch_df = df.slice(batch_start, batch_end - batch_start)
-        
+
         if batch_start % (batch_size * 5) == 0 or batch_end == total_rows:
-            print(f"    Processing rows {batch_start:,} - {batch_end:,} ({batch_end/total_rows*100:.1f}%)")
-        
+            print(f"    Processing rows {batch_start:,} - {batch_end:,} ({batch_end / total_rows * 100:.1f}%)")
+
         # Build combined text for this batch (using expressions)
-        combined_text_expr = (
-            pl.concat_str([
+        combined_text_expr = pl.concat_str(
+            [
                 pl.col("company_name").cast(pl.Utf8),
                 pl.lit(" "),
                 pl.col("description").cast(pl.Utf8),
             ],
             separator="",
-            ignore_nulls=True)
-            .str.to_lowercase()
-        )
-        
+            ignore_nulls=True,
+        ).str.to_lowercase()
+
         # Apply regex patterns as expressions
         is_foundation_expr = combined_text_expr.str.contains(foundation_re)
         is_pension_expr = combined_text_expr.str.contains(pension_re)
         is_endowment_expr = combined_text_expr.str.contains(endowment_re)
         is_insurance_expr = combined_text_expr.str.contains(insurance_re)
         is_charity_expr = combined_text_expr.str.contains(charity_re)
-        
-        tags_expr = pl.concat_list([
-            pl.when(is_foundation_expr).then(pl.lit("foundation")).otherwise(None),
-            pl.when(is_pension_expr).then(pl.lit("pension")).otherwise(None),
-            pl.when(is_endowment_expr).then(pl.lit("endowment")).otherwise(None),
-            pl.when(is_insurance_expr).then(pl.lit("insurance")).otherwise(None),
-            pl.when(is_charity_expr).then(pl.lit("charity")).otherwise(None),
-        ]).list.drop_nulls()
-        
+
+        tags_expr = pl.concat_list(
+            [
+                pl.when(is_foundation_expr).then(pl.lit("foundation")).otherwise(None),
+                pl.when(is_pension_expr).then(pl.lit("pension")).otherwise(None),
+                pl.when(is_endowment_expr).then(pl.lit("endowment")).otherwise(None),
+                pl.when(is_insurance_expr).then(pl.lit("insurance")).otherwise(None),
+                pl.when(is_charity_expr).then(pl.lit("charity")).otherwise(None),
+            ]
+        ).list.drop_nulls()
+
         # Add flags to batch
-        batch_with_flags = batch_df.with_columns([
-            is_foundation_expr.alias("is_foundation"),
-            is_pension_expr.alias("is_pension"),
-            is_endowment_expr.alias("is_endowment"),
-            is_insurance_expr.alias("is_insurance"),
-            is_charity_expr.alias("is_charity"),
-            tags_expr.alias("investor_category_tags"),
-        ])
-        
+        batch_with_flags = batch_df.with_columns(
+            [
+                is_foundation_expr.alias("is_foundation"),
+                is_pension_expr.alias("is_pension"),
+                is_endowment_expr.alias("is_endowment"),
+                is_insurance_expr.alias("is_insurance"),
+                is_charity_expr.alias("is_charity"),
+                tags_expr.alias("investor_category_tags"),
+            ]
+        )
+
         batch_results.append(batch_with_flags)
-        
+
         # Free memory (expressions are lightweight, but clear references)
         del batch_df, batch_with_flags
-    
+
     # Concatenate all batches
     print(f"  Combining {len(batch_results)} batches...")
     result_df = pl.concat(batch_results)
     del batch_results
-    
+
     print(f"  Completed flag derivation for {total_rows:,} rows")
     return result_df
 
@@ -411,9 +393,7 @@
 def _write_payload_jsonl(df: pl.DataFrame, destination: Path, batch_size: int = 10000) -> None:
     """Write payload JSONL in batches to avoid memory issues with large datasets."""
     identifier_cols = [col for col in IDENTIFIER_FIELDS if col in df.columns]
-    filter_cols = (
-        FILTER_FIELDS.get("categorical", []) + FILTER_FIELDS.get("numeric", [])
-    )
+    filter_cols = FILTER_FIELDS.get("categorical", []) + FILTER_FIELDS.get("numeric", [])
     filter_cols = [col for col in filter_cols if col in df.columns]
 
     total_rows = len(df)
@@ -423,10 +403,10 @@
         for batch_start in range(0, total_rows, batch_size):
             batch_end = min(batch_start + batch_size, total_rows)
             batch_df = df.slice(batch_start, batch_end - batch_start)
-            
+
             if batch_start % (batch_size * 10) == 0 or batch_end == total_rows:
-                print(f"  Processing rows {batch_start:,} - {batch_end:,} ({batch_end/total_rows*100:.1f}%)")
-            
+                print(f"  Processing rows {batch_start:,} - {batch_end:,} ({batch_end / total_rows * 100:.1f}%)")
+
             for row in batch_df.iter_rows(named=True):
                 metadata = {key: row.get(key) for key in identifier_cols if row.get(key) is not None}
                 filters = {key: row.get(key) for key in filter_cols if row.get(key) is not None}
@@ -458,7 +438,7 @@
                     section_text = segment.get("text", "")
                     # Add section heading as [section_name] followed by the text
                     text_block_parts.append(f"[{section_name}]\n{section_text}")
-                
+
                 text_block = "\n\n".join(text_block_parts)
 
                 record = {
@@ -471,10 +451,10 @@
                 }
                 fp.write(json.dumps(record, ensure_ascii=False))
                 fp.write("\n")
-            
+
             # Explicitly free batch memory
             del batch_df
-    
+
     print(f"Completed writing JSONL payload: {total_rows:,} rows")
 
 
@@ -484,10 +464,10 @@
     paths = paths or PipelinePaths.default()
     raw_df = load_raw_dataset(paths.raw_dataset)
     print(f"Loaded raw dataset: {len(raw_df):,} rows")
-    
+
     normalized_df = normalize_columns(raw_df)
     print(f"Normalized dataset: {len(normalized_df):,} rows")
-    
+
     # Deduplicate the main dataset by company_name (name-level uniqueness)
     print("Deduplicating dataset by company_name...")
     original_count = len(normalized_df)
@@ -496,7 +476,7 @@
     duplicates_removed = original_count - deduplicated_count
     print(f"Removed {duplicates_removed:,} rows via name-level deduplication")
     print(f"Unique companies: {deduplicated_count:,}")
-    
+
     # Load and apply AUM enrichment
     aum_dict = load_aum_enrichment_data()
     # AUM non-null count before enrichment
@@ -508,20 +488,20 @@
     # AUM non-null count after enrichment
     aum_non_null_after = enriched_df.filter(pl.col("aum").is_not_null()).height
     print(f"AUM non-null after enrichment:  {aum_non_null_after:,}")
-    
+
     # Free memory from intermediate dataframes
     del deduplicated_df
-    
+
     print("Deriving feature columns (bucketing)...")
     featured_df = derive_feature_columns(enriched_df)
     # Free memory
     del enriched_df
-    
+
     print("Deriving investor category flags...")
     categorized_df = derive_investor_category_flags(featured_df)
     # Free memory
     del featured_df
-    
+
     print("Saving processed outputs...")
     save_processed_outputs(categorized_df, paths)
     return categorized_df
@@ -529,26 +509,26 @@
 
 def build_dataset_custom(input_path: str, output_prefix: str) -> pl.DataFrame:
     """Build dataset with custom input and output paths."""
-    
+
     print(f"üöÄ Building custom dataset")
     print(f"üìÇ Input: {input_path}")
     print(f"üìÅ Output prefix: {output_prefix}")
     print("=" * 50)
-    
+
     # Create custom paths
     input_file = Path(input_path)
     if not input_file.exists():
         raise FileNotFoundError(f"Input file not found: {input_file}")
-    
+
     processed_dir = PROCESSED_DATA_DIR
     processed_dir.mkdir(parents=True, exist_ok=True)
-    
+
     custom_paths = PipelinePaths(
         raw_dataset=input_file,
         processed_table=processed_dir / f"{output_prefix}_normalized.parquet",
         payload_jsonl=processed_dir / f"{output_prefix}_payload.jsonl",
     )
-    
+
     print(f"üìä Processing steps:")
     print(f"  1. Load raw dataset from {input_file}")
     print(f"  2. Normalize columns and clean data")
@@ -558,7 +538,7 @@
     print(f"  6. Generate semantic text payloads")
     print(f"  7. Save to {custom_paths.processed_table} and {custom_paths.payload_jsonl}")
     print()
-    
+
     return build_dataset(custom_paths)
 
 
@@ -567,9 +547,7 @@
 
     paths = paths or PipelinePaths.default()
     if not paths.payload_jsonl.exists():
-        raise FileNotFoundError(
-            f"Processed payload not found at {paths.payload_jsonl}. Run build_dataset() first."
-        )
+        raise FileNotFoundError(f"Processed payload not found at {paths.payload_jsonl}. Run build_dataset() first.")
 
     with paths.payload_jsonl.open("r", encoding="utf-8") as fp:
         for line in fp:
@@ -589,15 +567,15 @@
     expr = (
         pl.col(column_name)
         .cast(pl.Utf8)
-        .str.replace_all(" ", "")           # Remove spaces (thousands separators)
-        .str.replace_all(",", ".")          # Replace comma decimal with dot
-        .cast(pl.Float64, strict=False)     # Convert to number
+        .str.replace_all(" ", "")  # Remove spaces (thousands separators)
+        .str.replace_all(",", ".")  # Replace comma decimal with dot
+        .cast(pl.Float64, strict=False)  # Convert to number
     )
-    
+
     # Special handling for AUM column - scale by 1 million
     if column_name == "aum":
         expr = expr.mul(1_000_000)
-    
+
     return expr.alias(column_name)
 
 
@@ -623,9 +601,7 @@
     return with_bucket.with_columns(
         pl.when(pl.col(bucket_col).is_null())
         .then(None)
-        .otherwise(
-            pl.concat_str([pl.lit(phrase_prefix), pl.col(bucket_col)], separator=": ")
-        )
+        .otherwise(pl.concat_str([pl.lit(phrase_prefix), pl.col(bucket_col)], separator=": "))
         .alias(phrase_col)
     )
 
@@ -666,7 +642,7 @@
         included = [field for field in section.fields if field in df.columns]
         if not included:
             return pl.lit(None)
-        
+
         # Apply contextual prefixes to each field
         contextual_exprs = []
         for field in included:
@@ -680,14 +656,11 @@
                     contextual_expr = pl.format(context_template, formatted_aum)
                 else:
                     # Create expression that applies context template
-                    contextual_expr = pl.format(
-                        context_template,
-                        pl.col(field).cast(pl.Utf8).str.strip_chars()
-                    )
+                    contextual_expr = pl.format(context_template, pl.col(field).cast(pl.Utf8).str.strip_chars())
                 contextual_exprs.append(contextual_expr)
             else:
                 contextual_exprs.append(pl.col(field).cast(pl.Utf8).str.strip_chars())
-        
+
         combined = pl.concat_str(
             contextual_exprs,
             separator="; ",
@@ -711,9 +684,7 @@
         ]
     ).list.drop_nulls()
 
-    text_block_expr = text_segments_expr.list.eval(pl.element().struct.field("text")).list.join(
-        SEMANTIC_TEMPLATE.separator
-    )
+    text_block_expr = text_segments_expr.list.eval(pl.element().struct.field("text")).list.join(SEMANTIC_TEMPLATE.separator)
 
     return df.with_columns(
         text_segments_expr.alias("text_segments"),
@@ -758,17 +729,16 @@
 
 if __name__ == "__main__":
     import argparse
-    
+
     parser = argparse.ArgumentParser(description="Build investors dataset")
     parser.add_argument("--input", help="Input Excel file path")
     parser.add_argument("--output", help="Output file prefix (e.g., 'investors_test')")
-    
+
     args = parser.parse_args()
-    
+
     if args.input and args.output:
         # Custom build
         build_dataset_custom(args.input, args.output)
     else:
         # Default build
         build_dataset()
-

--- experiments/investors_search_recommender/pipelines/full_dataset_aum_enrichment.py
+++ experiments/investors_search_recommender/pipelines/full_dataset_aum_enrichment.py
@@ -45,25 +45,25 @@
 @dataclass
 class CostTracker:
     """Track costs for Perplexity API usage."""
-    
+
     total_requests: int = 0
     total_input_tokens: int = 0
     total_output_tokens: int = 0
     total_request_cost: float = 0.0
     total_token_cost: float = 0.0
     total_cost: float = 0.0
-    
+
     def add_request(self, input_tokens: int, output_tokens: int):
         """Add a request to the cost tracker."""
         self.total_requests += 1
         self.total_input_tokens += input_tokens
         self.total_output_tokens += output_tokens
-        
+
         # Calculate costs
         input_cost = (input_tokens / 1_000_000) * SONAR_PRICING["input_tokens_per_million"]
         output_cost = (output_tokens / 1_000_000) * SONAR_PRICING["output_tokens_per_million"]
         request_cost = SONAR_PRICING["request_fee"]
-        
+
         self.total_request_cost += request_cost
         self.total_token_cost += input_cost + output_cost
         self.total_cost = self.total_request_cost + self.total_token_cost
@@ -72,7 +72,7 @@
 @dataclass
 class ProgressState:
     """Track progress state for resume capability."""
-    
+
     last_processed_batch: int = 0
     total_companies_processed: int = 0
     companies_with_aum: int = 0
@@ -85,7 +85,7 @@
     """Load the full dataset."""
     if not DATASET_PATH.exists():
         raise FileNotFoundError(f"Dataset not found at {DATASET_PATH}")
-    
+
     df = pl.read_excel(DATASET_PATH)
     print(f"Loaded {len(df):,} companies from dataset")
     return df
@@ -104,14 +104,18 @@
     """Save progress state to file."""
     PROGRESS_PATH.parent.mkdir(parents=True, exist_ok=True)
     with PROGRESS_PATH.open("w", encoding="utf-8") as f:
-        json.dump({
-            "last_processed_batch": progress.last_processed_batch,
-            "total_companies_processed": progress.total_companies_processed,
-            "companies_with_aum": progress.companies_with_aum,
-            "total_cost": progress.total_cost,
-            "last_update": progress.last_update,
-            "start_time": progress.start_time
-        }, f, indent=2)
+        json.dump(
+            {
+                "last_processed_batch": progress.last_processed_batch,
+                "total_companies_processed": progress.total_companies_processed,
+                "companies_with_aum": progress.companies_with_aum,
+                "total_cost": progress.total_cost,
+                "last_update": progress.last_update,
+                "start_time": progress.start_time,
+            },
+            f,
+            indent=2,
+        )
 
 
 def load_results() -> List[Dict]:
@@ -134,7 +138,7 @@
     """Format website URL to remove www. and everything before the domain."""
     if not website or website.strip() == "":
         return ""
-    
+
     # Remove common prefixes
     website = website.strip()
     if website.startswith("http://"):
@@ -143,30 +147,30 @@
         website = website[8:]
     if website.startswith("www."):
         website = website[4:]
-    
+
     # Remove trailing slash
     if website.endswith("/"):
         website = website[:-1]
-    
+
     return website
 
 
 def create_batch_prompt(companies: List[Dict]) -> str:
     """Create a brief prompt for batch AUM enrichment."""
-    
+
     company_list = []
     for i, company in enumerate(companies, 1):
-        company_name = company.get('Company Name', company.get('company_name', ''))
-        website = company.get('Website', company.get('website', ''))
+        company_name = company.get("Company Name", company.get("company_name", ""))
+        website = company.get("Website", company.get("website", ""))
         formatted_website = format_website(website)
-        
+
         if formatted_website:
             company_list.append(f"{i}. {company_name} ({formatted_website})")
         else:
             company_list.append(f"{i}. {company_name}")
-    
+
     companies_text = "\n".join(company_list)
-    
+
     prompt = f"""Find the Assets Under Management (AUM) for these investment companies. Focus on company websites and public disclosures from the country of the company (SEC, Companies House, etc.).
 
 Companies:
@@ -195,88 +199,80 @@
 
 def call_perplexity_api(prompt: str) -> Tuple[Dict, int, int]:
     """Call Perplexity API and return response with token usage."""
-    
+
     if not PERPLEXITY_API_KEY:
         raise ValueError("PERPLEXITY_API_KEY environment variable not set")
-    
-    headers = {
-        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
-        "Content-Type": "application/json"
-    }
-    
+
+    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
+
     payload = {
         "model": "sonar",
-        "messages": [
-            {
-                "role": "user",
-                "content": prompt
-            }
-        ],
+        "messages": [{"role": "user", "content": prompt}],
         "max_tokens": 2000,
         "temperature": 0.1,
         "top_p": 0.9,
         "return_citations": False,
-        "search_recency_filter": "year"
+        "search_recency_filter": "year",
     }
-    
+
     response = requests.post(PERPLEXITY_API_URL, headers=headers, json=payload)
     response.raise_for_status()
-    
+
     data = response.json()
-    
+
     # Extract token usage
     usage = data.get("usage", {})
     input_tokens = usage.get("prompt_tokens", 0)
     output_tokens = usage.get("completion_tokens", 0)
-    
+
     return data, input_tokens, output_tokens
 
 
 def parse_aum_response(response_text: str, companies: List[Dict]) -> List[Dict]:
     """Parse the Perplexity response and extract AUM results."""
-    
+
     results = []
-    
+
     try:
         # Handle responses wrapped in code blocks
-        if '```json' in response_text:
+        if "```json" in response_text:
             # Extract JSON from code block
-            json_start = response_text.find('```json') + 7
-            json_end = response_text.find('```', json_start)
+            json_start = response_text.find("```json") + 7
+            json_end = response_text.find("```", json_start)
             if json_end == -1:
-                json_end = response_text.rfind('}') + 1
+                json_end = response_text.rfind("}") + 1
             json_text = response_text[json_start:json_end].strip()
         else:
             # Try to extract JSON from the response
-            json_start = response_text.find('{')
-            json_end = response_text.rfind('}') + 1
+            json_start = response_text.find("{")
+            json_end = response_text.rfind("}") + 1
             json_text = response_text[json_start:json_end]
-        
+
         if json_text:
             # Clean up common JSON issues
             json_text = json_text.strip()
             # Remove any trailing commas before closing braces
-            json_text = json_text.replace(',}', '}').replace(',]', ']')
-            
+            json_text = json_text.replace(",}", "}").replace(",]", "]")
+
             try:
                 parsed_data = json.loads(json_text)
             except json.JSONDecodeError as json_err:
                 # Try to fix common JSON issues
                 print(f"JSON parsing error, attempting to fix: {json_err}")
                 # Remove any text after the last }
-                if json_text.count('}') > 0:
-                    last_brace = json_text.rfind('}')
-                    json_text = json_text[:last_brace + 1]
+                if json_text.count("}") > 0:
+                    last_brace = json_text.rfind("}")
+                    json_text = json_text[: last_brace + 1]
                 parsed_data = json.loads(json_text)
-            
+
             # Map results back to original companies
             response_results = parsed_data.get("results", [])
             company_map = {company.get("Company Name", company.get("company_name", "")): company for company in companies}
-            
+
             for result in response_results:
                 company_name = result.get("Company Name", "")
                 aum_value = result.get("AUM")
-                
+
                 # Try exact match first
                 matched_original_name = None
                 if company_name in company_map:
@@ -284,17 +280,13 @@
                 else:
                     # Try fuzzy matching for truncated names
                     for original_name in company_map.keys():
-                        if (company_name.lower() in original_name.lower() or 
-                            original_name.lower().startswith(company_name.lower())):
+                        if company_name.lower() in original_name.lower() or original_name.lower().startswith(company_name.lower()):
                             matched_original_name = original_name
                             break
-                
+
                 if matched_original_name and aum_value is not None:
-                    results.append({
-                        "Company Name": matched_original_name,
-                        "AUM": float(aum_value)
-                    })
-        
+                    results.append({"Company Name": matched_original_name, "AUM": float(aum_value)})
+
         else:
             # If no JSON found, check if it's a text response explaining no results
             if any(phrase in response_text.lower() for phrase in ["unable to find", "no reliable", "does not contain", "no data about"]):
@@ -302,39 +294,39 @@
                 pass
             else:
                 print(f"Warning: Could not parse response: {response_text[:100]}...")
-    
+
     except json.JSONDecodeError as e:
         print(f"JSON parsing error: {str(e)}")
-    
+
     return results
 
 
 def process_batch(companies: List[Dict], batch_num: int, total_batches: int) -> Tuple[List[Dict], CostTracker, int]:
     """Process a single batch of companies."""
-    
+
     print(f"Processing batch {batch_num}/{total_batches} ({len(companies)} companies)...")
-    
+
     # Create prompt
     prompt = create_batch_prompt(companies)
-    
+
     # Call API
     start_time = time.time()
     try:
         response_data, input_tokens, output_tokens = call_perplexity_api(prompt)
         api_time = time.time() - start_time
-        
+
         # Parse response
         response_text = response_data["choices"][0]["message"]["content"]
         results = parse_aum_response(response_text, companies)
-        
+
         # Track costs
         cost_tracker = CostTracker()
         cost_tracker.add_request(input_tokens, output_tokens)
-        
+
         print(f"Batch {batch_num}: Found AUM for {len(results)} companies, ${cost_tracker.total_cost:.4f}")
-        
+
         return results, cost_tracker, batch_num
-        
+
     except Exception as e:
         print(f"Error in batch {batch_num}: {str(e)}")
         # Return empty results and zero cost for failed batch
@@ -343,16 +335,15 @@
 
 def process_batches_parallel(batch_data: List[Tuple[List[Dict], int, int]]) -> List[Tuple[List[Dict], CostTracker, int]]:
     """Process multiple batches in parallel."""
-    
+
     results = []
-    
+
     with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
         # Submit all batches
         future_to_batch = {
-            executor.submit(process_batch, companies, batch_num, total_batches): batch_num
-            for companies, batch_num, total_batches in batch_data
+            executor.submit(process_batch, companies, batch_num, total_batches): batch_num for companies, batch_num, total_batches in batch_data
         }
-        
+
         # Collect results as they complete
         for future in as_completed(future_to_batch):
             batch_num = future_to_batch[future]
@@ -362,7 +353,7 @@
             except Exception as e:
                 print(f"Batch {batch_num} failed: {str(e)}")
                 results.append(([], CostTracker(), batch_num))
-    
+
     # Sort results by batch number to maintain order
     results.sort(key=lambda x: x[2])
     return results
@@ -372,66 +363,66 @@
     """Calculate estimated time to completion."""
     if progress.total_companies_processed == 0:
         return "Unknown"
-    
+
     elapsed_time = time.time() - start_time
     rate = progress.total_companies_processed / elapsed_time  # companies per second
     remaining_companies = total_companies - progress.total_companies_processed
     remaining_seconds = remaining_companies / rate if rate > 0 else 0
-    
+
     hours = int(remaining_seconds // 3600)
     minutes = int((remaining_seconds % 3600) // 60)
-    
+
     return f"{hours}h {minutes}m"
 
 
 def main():
     """Main function to run the full dataset AUM enrichment."""
-    
+
     print("=== Full Dataset AUM Enrichment ===")
-    
+
     # Load dataset
     df = load_dataset()
     total_companies = len(df)
-    
+
     # Load progress and results
     progress = load_progress()
     results = load_results()
-    
+
     # Initialize start time if this is a new run
     if not progress.start_time:
         progress.start_time = time.strftime("%Y-%m-%d %H:%M:%S")
         save_progress(progress)
-    
+
     print(f"Total companies: {total_companies:,}")
     print(f"Already processed: {progress.total_companies_processed:,}")
     print(f"Companies with AUM found: {progress.companies_with_aum:,}")
     print(f"Total cost so far: ${progress.total_cost:.2f}")
-    
+
     if progress.total_companies_processed >= total_companies:
         print("All companies have already been processed!")
         return
-    
+
     # Convert to list of dictionaries
     all_companies = df.to_dicts()
-    
+
     # Calculate batches
     total_batches = (total_companies + BATCH_SIZE - 1) // BATCH_SIZE
     start_batch = progress.last_processed_batch + 1
-    
+
     print(f"Processing {total_batches - start_batch + 1} remaining batches...")
-    
+
     # Process batches in parallel groups
     start_time = time.time()
     cost_tracker = CostTracker()
     cost_tracker.total_cost = progress.total_cost
     cost_tracker.total_requests = progress.last_processed_batch
-    
+
     # Process in groups of MAX_WORKERS batches at a time
     for group_start in range(start_batch, total_batches + 1, MAX_WORKERS):
         group_end = min(group_start + MAX_WORKERS, total_batches + 1)
-        
-        print(f"\n=== Processing batches {group_start}-{group_end-1} in parallel ===")
-        
+
+        print(f"\n=== Processing batches {group_start}-{group_end - 1} in parallel ===")
+
         # Prepare batch data for parallel processing
         batch_data = []
         for batch_num in range(group_start, group_end):
@@ -439,44 +430,44 @@
             end_idx = min(start_idx + BATCH_SIZE, total_companies)
             batch_companies = all_companies[start_idx:end_idx]
             batch_data.append((batch_companies, batch_num, total_batches))
-        
+
         # Process batches in parallel
         parallel_results = process_batches_parallel(batch_data)
-        
+
         # Process results from parallel execution
         for batch_results, batch_cost, batch_num in parallel_results:
             # Update results
             results.extend(batch_results)
-            
+
             # Update progress
             progress.last_processed_batch = batch_num
             progress.total_companies_processed = min((batch_num) * BATCH_SIZE, total_companies)
             progress.companies_with_aum = len(results)
             progress.total_cost = cost_tracker.total_cost + batch_cost.total_cost
             progress.last_update = time.strftime("%Y-%m-%d %H:%M:%S")
-            
+
             # Update cost tracker
             cost_tracker.total_cost += batch_cost.total_cost
             cost_tracker.total_requests += batch_cost.total_requests
             cost_tracker.total_input_tokens += batch_cost.total_input_tokens
             cost_tracker.total_output_tokens += batch_cost.total_output_tokens
-        
+
         # Save results and progress after each group
         save_results(results)
         save_progress(progress)
-        
+
         # Calculate and display progress
         completion_pct = (progress.total_companies_processed / total_companies) * 100
         eta = calculate_eta(progress, total_companies, start_time)
-        
+
         print(f"\nProgress: {completion_pct:.1f}% ({progress.total_companies_processed:,}/{total_companies:,})")
         print(f"Success rate: {(progress.companies_with_aum / progress.total_companies_processed) * 100:.1f}%")
         print(f"Total cost: ${progress.total_cost:.2f}")
         print(f"ETA: {eta}")
-        
+
         # Add small delay between groups to respect rate limits
         time.sleep(3)  # Conservative delay for 500/min limit
-    
+
     # Save final summary
     final_summary = {
         "completion_date": time.strftime("%Y-%m-%d %H:%M:%S"),
@@ -488,13 +479,13 @@
         "total_input_tokens": cost_tracker.total_input_tokens,
         "total_output_tokens": cost_tracker.total_output_tokens,
         "cost_per_company": progress.total_cost / total_companies,
-        "processing_time_hours": (time.time() - start_time) / 3600
+        "processing_time_hours": (time.time() - start_time) / 3600,
     }
-    
+
     SUMMARY_PATH.parent.mkdir(parents=True, exist_ok=True)
     with SUMMARY_PATH.open("w", encoding="utf-8") as f:
         json.dump(final_summary, f, indent=2)
-    
+
     print(f"\n=== Enrichment Complete ===")
     print(f"Total companies processed: {total_companies:,}")
     print(f"Companies with AUM found: {progress.companies_with_aum:,}")

--- experiments/investors_search_recommender/test_fund_lp_engine.py
+++ experiments/investors_search_recommender/test_fund_lp_engine.py
@@ -17,6 +17,7 @@
 
 # Load environment variables from .env file
 from dotenv import load_dotenv
+
 load_dotenv()
 
 from app.services.vector_search.fund_lp_engine import FundLPMatchingEngine
@@ -24,7 +25,7 @@
 
 def get_test_funds():
     """Get the two test fund profiles."""
-    
+
     return [
         {
             "fund_id": "fund_001",
@@ -41,8 +42,8 @@
                 "geographic_focus": "United Kingdom",
                 "sector_focus": "Real Estate",
                 "check_size_range": "¬£50M - ¬£200M",
-                "lp_target_types": ["Pension Funds", "Sovereign Wealth Funds", "Family Offices"]
-            }
+                "lp_target_types": ["Pension Funds", "Sovereign Wealth Funds", "Family Offices"],
+            },
         },
         {
             "fund_id": "fund_002",
@@ -59,91 +60,67 @@
                 "geographic_focus": "United States",
                 "sector_focus": "Technology, SaaS",
                 "check_size_range": "$10M - $50M",
-                "lp_target_types": ["Endowments", "Pension Funds", "Fund of Funds"]
-            }
-        }
+                "lp_target_types": ["Endowments", "Pension Funds", "Fund of Funds"],
+            },
+        },
     ]
 
 
 def get_test_prompts():
     """Get 25 diverse test prompts: 10 non-semantic anchor types, 10 semantic anchor types, and 5 mixed prompts with realistic GP fundraiser scenarios."""
-    
+
     return [
         # === 10 PROMPTS: NON-SEMANTIC ANCHOR PRIMARY INVESTOR TYPES (Mapped Types) ===
         # 1. Limited Partners (general)
         "Limited partners in North America and Europe with AUM over $500M interested in private equity opportunities",
-        
         # 2. Venture Capital firms
         "Venture capital firms in Silicon Valley and Boston focused on early-stage technology and SaaS startups",
-        
         # 3. Private Equity firms
         "Private equity firms in Europe and North America with expertise in technology and healthcare buyouts",
-        
         # 4. Family Offices
         "Family offices in Switzerland, Monaco, and New York with interest in technology and healthcare investments",
-        
         # 5. Fund of Funds
         "Fund of funds targeting healthcare and biotech sectors with AUM over $1B in North America",
-        
         # 6. Sovereign Wealth Funds
         "Sovereign wealth funds in the Middle East and Asia focused on technology and infrastructure investments",
-        
         # 7. Corporate Venture Capital
         "Corporate venture capital firms in the United States and Europe targeting enterprise software and AI",
-        
         # 8. Growth/Expansion funds
         "Growth and expansion capital firms in Europe interested in B2B SaaS and fintech companies",
-        
         # 9. Mezzanine funds
         "Mezzanine capital providers in North America focused on buyout opportunities and leveraged transactions",
-        
         # 10. Real Estate Investment Trusts / Real Estate funds
         "Real estate investment funds and REITs in the UK and Germany focused on commercial and residential properties",
-        
         # === 10 PROMPTS: SEMANTIC ANCHOR PRIMARY INVESTOR TYPES (Flag-based) ===
         # 11. Public sector pension (subtype)
         "Public sector pension funds in the United Kingdom interested in infrastructure and renewable energy investments",
-        
         # 12. Corporate pension (subtype)
         "Corporate pension funds in Germany focused on renewable energy and sustainable infrastructure projects",
-        
         # 13. Local government pension (subtype/LGPS)
         "Local government pension schemes (LGPS) in the UK prioritizing impact investing, ESG, and sustainable finance",
-        
         # 14. University endowments (subtype)
         "University endowments based in the United States targeting technology and healthcare sectors",
-        
         # 15. Life insurance (subtype)
         "Life insurance companies in Europe focused on infrastructure and buyout opportunities",
-        
         # 16. General insurance
         "Insurance companies in North America with interest in technology and renewable energy investments",
-        
         # 17. Charities (merged anchor)
         "Charities in the United States focused on education technology and healthcare innovation",
-        
         # 18. Foundations (merged anchor)
         "Private foundations in Europe and North America prioritizing climate tech and social impact investments",
-        
         # 19. Charities and Foundations (merged anchor explicit)
         "Charities and foundations in the United States focused on education technology and healthcare innovation",
-        
         # 20. Multiple pension types
         "Pension funds in North America and Europe with AUM over $1B targeting infrastructure and real estate",
-        
         # === 5 PROMPTS: MIXED INVESTOR TYPES ===
         # 21. Mix: Semantic anchor + Non-semantic anchor (TWO TYPES)
         "Pension funds and venture capital firms in the US seeking growth equity and technology investments",
-        
         # 22. Mix: Semantic anchor + Non-semantic anchor (TWO TYPES)
         "University endowments and family offices in North America targeting real estate and private equity investments",
-        
         # 23. Multiple non-semantic anchor types (TWO TYPES)
         "Venture capital firms and family offices in Europe and North America focused on technology and healthcare",
-        
         # 24. Multiple non-semantic anchor types (TWO TYPES)
         "Private equity firms and fund of funds in North America and Europe with infrastructure and buyout focus",
-        
         # 25. Multiple semantic anchor types (TWO TYPES)
         "Pension funds and endowments in the United States with AUM over $500M targeting technology and healthcare",
     ]
@@ -151,88 +128,88 @@
 
 async def test_fund_prompts(engine, fund, prompts):
     """Test a fund with various prompts."""
-    
+
     print(f"\nüè¢ Testing Fund: {fund['firm_name']}")
     print(f"üåç Geographic Focus: {fund['investor_profile']['geographic_focus']}")
     print(f"üè≠ Sector Focus: {fund['investor_profile']['sector_focus']}")
     print(f"üéØ Target LP Types: {', '.join(fund['investor_profile']['lp_target_types'])}")
     print(f"üí∞ Fund Size: {fund['investor_profile']['fund_size']}")
-    print("="*80)
-    
+    print("=" * 80)
+
     for i, prompt in enumerate(prompts, 1):
         print(f"\nüìù Test {i}/{len(prompts)}: {prompt}")
         print("-" * 60)
-        
+
         try:
             # Search for LPs
             lp_results = await engine.search_lps_for_fund(fund, prompt, limit=5)
-            
+
             # Present results
             engine.present_results(fund, prompt, lp_results)
-            
+
         except Exception as e:
             print(f"‚ùå Error testing prompt: {e}")
-        
+
         # Wait for user input to continue
         if i < len(prompts):
             input(f"\n‚è∏Ô∏è  Press Enter to continue to next prompt...")
-    
+
     print(f"\n‚úÖ Completed testing {fund['firm_name']} with {len(prompts)} prompts")
 
 
 def test_performance_summary(engine):
     """Print performance summary."""
-    
+
     stats = engine.get_performance_stats()
-    
+
     print(f"\nüìä PERFORMANCE SUMMARY")
-    print("="*50)
+    print("=" * 50)
     print(f"Total LLM calls: {stats['total_calls']}")
     print(f"Extraction calls: {stats['extraction_calls']}")
     print(f"Mapping calls: {stats['mapping_calls']}")
     print(f"Total time: {stats['total_time']:.2f}s")
     print(f"Average time per call: {stats['average_time']:.2f}s")
     print(f"Total cost: ${stats['total_cost']:.4f}")
-    
-    if stats['total_calls'] > 0:
-        print(f"Average cost per call: ${stats['total_cost']/stats['total_calls']:.4f}")
 
+    if stats["total_calls"] > 0:
+        print(f"Average cost per call: ${stats['total_cost'] / stats['total_calls']:.4f}")
+
 
 async def validation_mode(engine, test_funds, test_prompts):
     """Run validation mode with detailed output for manual evaluation."""
-    
+
     print("üîç VALIDATION MODE - Manual Evaluation")
-    print("="*80)
+    print("=" * 80)
     print("This will run all test prompts and show:")
     print("1. Extracted filter information from prompts")
     print("2. Complete semantic block (Anchored Prompt + Profile Query) used for vectorization")
     print("3. Top 5 LP results with their semantic blocks for comparison")
     print("4. How semantic blocks are formed and matched")
     print("Results grouped by prompt for easier comparison")
-    print("="*80)
-    
+    print("=" * 80)
+
     print(f"\nüìä Validation Setup:")
     print(f"   Prompts: {len(test_prompts)}")
     print(f"   Funds: {len(test_funds)}")
     print(f"   Total evaluations: {len(test_prompts) * len(test_funds)}")
-    
+
     # Create output file
     output_file = f"validation_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt"
-    
-    with open(output_file, 'w', encoding='utf-8') as f:
+
+    with open(output_file, "w", encoding="utf-8") as f:
         f.write("FUND-TO-LP MATCHING VALIDATION RESULTS\n")
-        f.write("="*80 + "\n")
+        f.write("=" * 80 + "\n")
         f.write(f"Generated: {pd.Timestamp.now()}\n")
         f.write(f"Prompts: {len(test_prompts)}\n")
         f.write(f"Funds: {len(test_funds)}\n")
         f.write(f"Total evaluations: {len(test_prompts) * len(test_funds)}\n\n")
-        
+
         # Display both fund profiles at the top
         f.write("FUND PROFILES\n")
-        f.write("="*80 + "\n")
+        f.write("=" * 80 + "\n")
         for fund_idx, fund in enumerate(test_funds, 1):
             f.write(f"\nFUND {fund_idx}: {fund['firm_name']}\n")
-            f.write(f"{'-'*50}\n")
+            f.write(f"{'-' * 50}\n")
             f.write(f"Firm Description: {fund['investor_profile'].get('firm_description', 'N/A')}\n")
             f.write(f"Key Differentiators: {fund['investor_profile'].get('key_differentiators', 'N/A')}\n")
             f.write(f"Key Objectives: {fund['investor_profile'].get('key_objectives', 'N/A')}\n")
@@ -242,53 +219,53 @@
             f.write(f"Geographic Focus: {fund['investor_profile'].get('geographic_focus', 'N/A')}\n")
             f.write(f"Sector Focus: {fund['investor_profile'].get('sector_focus', 'N/A')}\n")
             f.write(f"LP Target Types: {', '.join(fund['investor_profile'].get('lp_target_types', []))}\n")
-        
-        f.write(f"\n{'='*80}\n")
+
+        f.write(f"\n{'=' * 80}\n")
         f.write("VALIDATION RESULTS\n")
-        f.write("="*80 + "\n\n")
-        
+        f.write("=" * 80 + "\n\n")
+
         # Run validation grouped by prompt
         for prompt_idx, prompt in enumerate(test_prompts, 1):
-            f.write(f"\n{'='*100}\n")
+            f.write(f"\n{'=' * 100}\n")
             f.write(f"PROMPT {prompt_idx}/{len(test_prompts)}: {prompt}\n")
-            f.write(f"{'='*100}\n")
-            
+            f.write(f"{'=' * 100}\n")
+
             # Show normalized version of the prompt
             try:
                 normalized_prompt = engine.enhance_query(prompt)
             except Exception:
                 normalized_prompt = prompt.lower()
             f.write(f"Normalized Prompt: {normalized_prompt}\n")
-            
+
             # Extract filters once for this prompt (same for all funds)
             try:
                 # Step 1: Extract filters and track timing/cost
                 extraction_start = time.time()
                 extracted_data = await engine.extract_filters_with_llm(prompt)
                 extraction_time = time.time() - extraction_start
-                
+
                 # Process filters
                 filters = {}
                 geo_filters = engine.process_geographic_filters(extracted_data.get("geographic", {}))
                 filters.update(geo_filters)
-                
+
                 # Get investor type mapping with timing/cost (after flag detection)
                 investor_type_mapping_time = 0
                 investor_type_mapping_cost = 0
-                
+
                 # Apply flag detection to filter out pension/endowment/foundation/insurance/charity before mapping
                 raw_types = extracted_data.get("investor_types", []) or []
                 categories_or, cleaned_types = engine._detect_flag_categories(raw_types)
-                
+
                 # Store flag categories in extracted_data for display
                 if categories_or:
                     extracted_data["flag_categories"] = categories_or
-                
+
                 if cleaned_types:
                     mapping_start = time.time()
                     investor_type_ids = await engine.match_investor_types(cleaned_types)
                     investor_type_mapping_time = time.time() - mapping_start
-                    
+
                     if investor_type_ids:
                         filters["primary_investor_types"] = investor_type_ids
                         # Get actual investor type names
@@ -303,45 +280,45 @@
                 else:
                     # All types were flags, no mapping needed
                     extracted_data["investor_type_names"] = []
-                
+
                 aum_filters = engine.process_aum_filters(extracted_data.get("aum_filters", {}))
                 if aum_filters:
                     filters["aum"] = aum_filters
-                
+
                 # Calculate costs (simplified)
                 extraction_cost = 0.0002  # Approximate cost
                 mapping_cost = 0.0001 if investor_type_mapping_time > 0 else 0
-                
+
                 f.write(f"\n1. EXTRACTED INFORMATION FROM PROMPT:\n")
                 f.write(f"   Geographic: {extracted_data.get('geographic', {})}\n")
-                
+
                 # Show flag categories if detected
-                flag_categories = extracted_data.get('flag_categories', [])
+                flag_categories = extracted_data.get("flag_categories", [])
                 if flag_categories:
                     f.write(f"   Flag Categories (filtered out before mapping): {flag_categories}\n")
-                
+
                 # Show mapped investor types (or raw if no mapping occurred)
-                investor_type_names = extracted_data.get('investor_type_names', [])
+                investor_type_names = extracted_data.get("investor_type_names", [])
                 if investor_type_names:
                     f.write(f"   Investor Types (mapped): {investor_type_names}\n")
                 else:
-                    raw_types = extracted_data.get('investor_types', [])
+                    raw_types = extracted_data.get("investor_types", [])
                     if raw_types and not flag_categories:
                         f.write(f"   Investor Types (raw, not mapped): {raw_types}\n")
                     elif flag_categories:
                         f.write(f"   Investor Types: All were flag categories (filtered)\n")
-                
+
                 f.write(f"   Investor Focus: {extracted_data.get('investor_focus', [])}\n")
                 f.write(f"   AUM Filters: {extracted_data.get('aum_filters', {})}\n")
                 f.write(f"\n2. COST AND TIME TAKEN:\n")
                 f.write(f"   Filter Extraction: {extraction_time:.2f}s, ${extraction_cost:.4f}\n")
                 if investor_type_mapping_time > 0:
                     f.write(f"   Investor Type Mapping: {investor_type_mapping_time:.2f}s, ${mapping_cost:.4f}\n")
-                
+
                 # Build a preview of the anchored prompt for visibility in tests
-                must_types_preview = [t.strip() for t in (extracted_data.get('investor_types') or []) if t and t.strip()]
-                focus_list_preview = extracted_data.get('investor_focus') or []
-                geo_countries = (extracted_data.get('geographic') or {}).get('countries') or []
+                must_types_preview = [t.strip() for t in (extracted_data.get("investor_types") or []) if t and t.strip()]
+                focus_list_preview = extracted_data.get("investor_focus") or []
+                geo_countries = (extracted_data.get("geographic") or {}).get("countries") or []
                 preferred_focus = None
                 if focus_list_preview:
                     preferred_focus = "; ".join(sorted({str(t).strip().lower() for t in focus_list_preview if t}))
@@ -356,45 +333,45 @@
                     anchored_lines.append(f"PREFERRED: investor focus ‚âà {preferred_focus}")
                 anchored_lines.append(f"Primary Query: {prompt}")
                 f.write(f"\nAnchored Prompt Preview:\n{chr(10).join(anchored_lines)}\n")
-                
+
                 # Test this prompt with each fund
                 for fund_idx, fund in enumerate(test_funds, 1):
-                    f.write(f"\n{'-'*80}\n")
+                    f.write(f"\n{'-' * 80}\n")
                     f.write(f"FUND {fund_idx}/{len(test_funds)}: {fund['firm_name']}\n")
-                    f.write(f"{'-'*80}\n")
-                    
+                    f.write(f"{'-' * 80}\n")
+
                     # Step 2: Show combined prompt + profile string
                     enhanced_query = engine.build_enhanced_query(prompt, fund)
                     f.write(f"\n3. COMBINED TEXT BLOCK FOR VECTORIZATION:\n")
                     f.write(f"{enhanced_query}\n")
-                    
+
                     # Perform search to get the actual semantic block
                     lp_results = await engine.search_lps_for_fund(fund, prompt, limit=5)
-                    
+
                     # Get the stored semantic block from the engine
-                    if hasattr(engine, 'last_semantic_block'):
+                    if hasattr(engine, "last_semantic_block"):
                         semantic_block = engine.last_semantic_block
                         f.write(f"\nA. ANCHORED PROMPT (70% weight) - What actually gets embedded:\n")
-                        f.write(f"{'‚îÄ'*80}\n")
+                        f.write(f"{'‚îÄ' * 80}\n")
                         # Show both the original and enhanced version
-                        anchored_prompt = semantic_block.get('anchored_prompt', 'N/A')
-                        enhanced_prompt = semantic_block.get('enhanced_anchored_prompt', anchored_prompt)
+                        anchored_prompt = semantic_block.get("anchored_prompt", "N/A")
+                        enhanced_prompt = semantic_block.get("enhanced_anchored_prompt", anchored_prompt)
                         f.write(f"{enhanced_prompt}\n")
                         if enhanced_prompt != anchored_prompt:
                             f.write(f"\n   (Note: Financial terms were expanded from original anchored prompt)\n")
-                        f.write(f"{'‚îÄ'*80}\n")
+                        f.write(f"{'‚îÄ' * 80}\n")
                         f.write(f"\nB. PROFILE QUERY (30% weight):\n")
-                        f.write(f"{'‚îÄ'*80}\n")
+                        f.write(f"{'‚îÄ' * 80}\n")
                         f.write(f"{semantic_block.get('profile_query', 'N/A')}\n")
-                        f.write(f"{'‚îÄ'*80}\n")
+                        f.write(f"{'‚îÄ' * 80}\n")
                         f.write(f"\nC. WEIGHTING:\n")
                         f.write(f"   {semantic_block.get('weighting', 'N/A')}\n")
                         f.write(f"   (Embeddings are created separately, then combined)\n")
                     else:
                         f.write(f"\n‚ö†Ô∏è Semantic block not available (engine may not have stored it)\n")
-                    
-                    f.write(f"\n{'='*80}\n")
-                    
+
+                    f.write(f"\n{'=' * 80}\n")
+
                     # Get filter counts from the search results
                     filter_counts_info = ""
                     if lp_results and isinstance(lp_results[0], dict) and "filter_counts" in lp_results[0]:
@@ -405,7 +382,7 @@
                         filter_counts_info = f"   Meta filtering results: {meta}\n   Semantic search results: {semantic}\n"
                     else:
                         filter_counts_info = f"   Meta filtering results: 0\n   Semantic search results: {len(lp_results) if lp_results else 0}\n"
-                    
+
                     f.write(f"\n3. TOP 5 MOST SIMILAR RESULTS (with Semantic Blocks):\n")
                     if filter_counts_info:
                         f.write(f"\n{filter_counts_info}\n")
@@ -413,17 +390,17 @@
                         f.write("   No LPs found matching the criteria\n")
                     else:
                         for i, lp in enumerate(lp_results, 1):
-                            f.write(f"\n   {'='*70}\n")
+                            f.write(f"\n   {'=' * 70}\n")
                             f.write(f"   RESULT {i}: {lp['lp_name']} ({lp['lp_type']})\n")
                             f.write(f"   Similarity Score: {lp['score']:.4f}\n")
-                            f.write(f"   {'='*70}\n")
-                            
+                            f.write(f"   {'=' * 70}\n")
+
                             # Show company semantic block
-                            semantic_data = lp['raw_payload'].get('semantic', {})
-                            company_semantic_block = semantic_data.get('text_block', 'N/A')
+                            semantic_data = lp["raw_payload"].get("semantic", {})
+                            company_semantic_block = semantic_data.get("text_block", "N/A")
                             f.write(f"\n   COMPANY SEMANTIC BLOCK (what was embedded for this company):\n")
-                            f.write(f"   {'-'*70}\n")
-                            if company_semantic_block and company_semantic_block != 'N/A':
+                            f.write(f"   {'-' * 70}\n")
+                            if company_semantic_block and company_semantic_block != "N/A":
                                 # Show first 500 chars, then indicate if truncated
                                 if len(company_semantic_block) > 500:
                                     f.write(f"   {company_semantic_block[:500]}...\n")
@@ -432,30 +409,30 @@
                                     f.write(f"   {company_semantic_block}\n")
                             else:
                                 f.write(f"   ‚ö†Ô∏è No semantic block found in payload\n")
-                            f.write(f"   {'-'*70}\n")
-                            
+                            f.write(f"   {'-' * 70}\n")
+
                             # Show all metadata fields
-                            metadata = lp['raw_payload'].get('metadata', {})
-                            filters = lp['raw_payload'].get('filters', {})
-                            
+                            metadata = lp["raw_payload"].get("metadata", {})
+                            filters = lp["raw_payload"].get("filters", {})
+
                             f.write(f"\n   METADATA:\n")
                             for key, value in metadata.items():
                                 if value and value != "Unknown" and value != "N/A":
                                     f.write(f"      {key}: {value}\n")
-                            
+
                             # Show flag columns if true
-                            flag_columns = ['is_foundation', 'is_pension', 'is_endowment', 'is_insurance', 'is_charity']
+                            flag_columns = ["is_foundation", "is_pension", "is_endowment", "is_insurance", "is_charity"]
                             active_flags = []
                             for flag in flag_columns:
                                 flag_value = filters.get(flag)
-                                if flag_value is True or (isinstance(flag_value, str) and flag_value.lower() in ['true', '1', 'yes']):
+                                if flag_value is True or (isinstance(flag_value, str) and flag_value.lower() in ["true", "1", "yes"]):
                                     active_flags.append(flag)
-                            
+
                             if active_flags:
                                 f.write(f"\n   ACTIVE FLAGS:\n")
                                 for flag in active_flags:
                                     f.write(f"      {flag}: True\n")
-                            
+
                             f.write(f"\n   FILTERS:\n")
                             for key, value in filters.items():
                                 # Skip flag columns in general filters section (they're shown above)
@@ -463,37 +440,34 @@
                                     continue
                                 if value and value != "Unknown" and value != "N/A":
                                     f.write(f"      {key}: {value}\n")
-                            
+
                             f.write(f"\n")
-                
-                f.write(f"\n{'='*50}\n")
+
+                f.write(f"\n{'=' * 50}\n")
                 f.write(f"END OF PROMPT {prompt_idx}\n")
-                f.write(f"{'='*50}\n\n")
-                
+                f.write(f"{'=' * 50}\n\n")
+
             except Exception as e:
                 f.write(f"ERROR processing prompt: {e}\n")
                 import traceback
+
                 f.write(f"Traceback: {traceback.format_exc()}\n")
-    
+
     print(f"\n‚úÖ Validation completed! Results written to: {output_file}")
     test_performance_summary(engine)
 
 
 async def main():
     """Main test function."""
-    
+
     # Parse command-line arguments
     parser = argparse.ArgumentParser(description="Test Fund-to-LP Matching Engine")
-    parser.add_argument(
-        "--local-qdrant",
-        action="store_true",
-        help="Override QDRANT_URL to use localhost:6333 for local testing"
-    )
+    parser.add_argument("--local-qdrant", action="store_true", help="Override QDRANT_URL to use localhost:6333 for local testing")
     args = parser.parse_args()
-    
+
     print("üöÄ Fund-to-LP Matching Engine Test")
-    print("="*50)
-    
+    print("=" * 50)
+
     # Check for OpenAI API key
     api_key = os.getenv("OPENAI_API_KEY")
     if not api_key:
@@ -501,13 +475,13 @@
         print("Please set the OPENAI_API_KEY environment variable")
         print("Example: export OPENAI_API_KEY='your-api-key-here'")
         return
-    
+
     # Override Qdrant URL if local_qdrant flag is set
     qdrant_url = None
     if args.local_qdrant:
         qdrant_url = "http://localhost:6333"
         print(f"üîß Using local Qdrant: {qdrant_url}")
-    
+
     # Initialize engine
     try:
         engine = FundLPMatchingEngine(qdrant_url=qdrant_url)
@@ -515,23 +489,23 @@
     except Exception as e:
         print(f"‚ùå Failed to initialize engine: {e}")
         return
-    
+
     # Get test data
     test_funds = get_test_funds()
     test_prompts = get_test_prompts()
-    
+
     print(f"üìä Test Setup:")
     print(f"   Funds: {len(test_funds)}")
     print(f"   Prompts: {len(test_prompts)}")
     print(f"   Total tests: {len(test_funds) * len(test_prompts)}")
-    
+
     # Run validation mode by default
     print(f"\nRunning validation mode with detailed output...")
     await validation_mode(engine, test_funds, test_prompts)
-    
+
     # Print performance summary
     test_performance_summary(engine)
-    
+
     print(f"\nüéâ Testing completed!")
 
 

--- experiments/investors_search_recommender/upload_to_qdrant.py
+++ experiments/investors_search_recommender/upload_to_qdrant.py
@@ -25,64 +25,59 @@
 
 class QdrantUploader:
     """Memory-efficient Qdrant uploader with resume capability."""
-    
+
     def __init__(self, config_path: str = "config/search_recommender/qdrant.yaml", checkpoint_dir: str = "data/processed/qdrant_checkpoints"):
         """Initialize the Qdrant uploader."""
         self.config_path = Path(config_path)
         self.checkpoint_dir = Path(checkpoint_dir)
         self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
-        
+
         self.config = self._load_config()
         self.client = self._init_client()
-        
-        self.upload_stats = {
-            "total_points": 0,
-            "successful_uploads": 0,
-            "failed_uploads": 0,
-            "skipped_points": 0
-        }
-    
+
+        self.upload_stats = {"total_points": 0, "successful_uploads": 0, "failed_uploads": 0, "skipped_points": 0}
+
     def _load_config(self) -> Dict[str, Any]:
         """Load Qdrant configuration."""
         if not self.config_path.exists():
             raise FileNotFoundError(f"Config file not found: {self.config_path}")
-        
-        with open(self.config_path, 'r') as f:
+
+        with open(self.config_path, "r") as f:
             config = yaml.safe_load(f)
-        
+
         # Set defaults
         config.setdefault("batch_size", 500)  # Reduced from 1000 to 500
         config.setdefault("vector_size", 3072)
         config.setdefault("metric", "cosine")
         config.setdefault("collection", "investors")
-        
+
         return config
-    
+
     def _init_client(self) -> QdrantClient:
         """Initialize Qdrant client with timeout settings."""
         client_kwargs = {
             "url": self.config["url"],
-            "timeout": 60  # 60 second timeout
+            "timeout": 60,  # 60 second timeout
         }
-        
+
         # Add API key if configured
         api_key_env = self.config.get("api_key_env")
         if api_key_env:
             api_key = os.getenv(api_key_env)
             if api_key:
                 client_kwargs["api_key"] = api_key
-        
+
         return QdrantClient(**client_kwargs)
-    
+
     def ensure_collection(self) -> None:
         """Create or update Qdrant collection."""
         collection_name = self.config["collection"]
-        
+
         try:
             # Check if collection exists
             collections = self.client.get_collections()
             existing_names = {col.name for col in collections.collections}
-            
+
             if collection_name not in existing_names:
                 print(f"üì¶ Creating collection: {collection_name}")
                 self._create_collection()
@@ -90,64 +85,60 @@
                 print(f"üì¶ Collection exists: {collection_name}")
                 # Update collection configuration if needed
                 self._update_collection()
-                
+
         except Exception as e:
             print(f"‚ùå Error managing collection: {e}")
             raise
-    
+
     def _create_collection(self) -> None:
         """Create new Qdrant collection."""
         collection_name = self.config["collection"]
         vector_size = self.config["vector_size"]
         metric = self.config["metric"]
-        
-        vector_params = qmodels.VectorParams(
-            size=vector_size,
-            distance=qmodels.Distance.COSINE if metric == "cosine" else qmodels.Distance.EUCLIDEAN
-        )
-        
-        self.client.create_collection(
-            collection_name=collection_name,
-            vectors_config=vector_params
-        )
-        
+
+        vector_params = qmodels.VectorParams(size=vector_size, distance=qmodels.Distance.COSINE if metric == "cosine" else qmodels.Distance.EUCLIDEAN)
+
+        self.client.create_collection(collection_name=collection_name, vectors_config=vector_params)
+
         # Create payload indexes for efficient filtering
         self._create_payload_indexes()
-    
+
     def _update_collection(self) -> None:
         """Update existing collection configuration."""
         collection_name = self.config["collection"]
-        
+
         # Check if collection configuration matches what we need
         try:
             collection_info = self.client.get_collection(collection_name)
             current_size = collection_info.config.params.vectors.size
             expected_size = self.config["vector_size"]
-            
+
             if current_size != expected_size:
                 print(f"‚ö†Ô∏è  Collection vector size mismatch: {current_size} vs {expected_size}")
                 print(f"‚ùå Cannot update vector size of existing collection")
                 print(f"üí° Solutions:")
-                print(f"   1. Delete collection: python -c \"from qdrant_client import QdrantClient; QdrantClient().delete_collection('{collection_name}')\"")
+                print(
+                    f"   1. Delete collection: python -c \"from qdrant_client import QdrantClient; QdrantClient().delete_collection('{collection_name}')\""
+                )
                 print(f"   2. Use different collection name with --collection flag")
                 raise ValueError(f"Vector size mismatch: {current_size} != {expected_size}")
-            
+
             print(f"‚úÖ Collection configuration is correct")
-            
+
         except Exception as e:
             print(f"‚ùå Error checking collection configuration: {e}")
             raise
-    
+
     def _create_payload_indexes(self) -> None:
         """Create payload indexes for efficient filtering."""
         collection_name = self.config["collection"]
         indexes = self.config.get("payload_indexes", [])
-        
+
         for index_config in indexes:
             try:
                 field_name = index_config["field"]
                 field_type = index_config.get("type", "keyword")
-                
+
                 # Map field_type string to PayloadSchemaType enum
                 if field_type == "keyword":
                     schema_type = qmodels.PayloadSchemaType.KEYWORD
@@ -160,24 +151,20 @@
                 else:
                     # Default to KEYWORD for unknown types
                     schema_type = qmodels.PayloadSchemaType.KEYWORD
-                
-                self.client.create_payload_index(
-                    collection_name=collection_name,
-                    field_name=field_name,
-                    field_schema=schema_type
-                )
+
+                self.client.create_payload_index(collection_name=collection_name, field_name=field_name, field_schema=schema_type)
                 print(f"üìä Created index: {field_name} ({field_type})")
-                
+
             except Exception as e:
                 print(f"‚ö†Ô∏è  Warning: Could not create index for {index_config.get('field', 'unknown')}: {e}")
-    
+
     def stream_embeddings(self, embeddings_path: str) -> Iterator[Tuple[str, list[float]]]:
         """Stream embeddings from JSONL file."""
         embeddings_file = Path(embeddings_path)
         if not embeddings_file.exists():
             raise FileNotFoundError(f"Embeddings file not found: {embeddings_file}")
-        
-        with open(embeddings_file, 'r', encoding='utf-8') as f:
+
+        with open(embeddings_file, "r", encoding="utf-8") as f:
             for line_num, line in enumerate(f, 1):
                 if line.strip():
                     try:
@@ -188,14 +175,14 @@
                     except (json.JSONDecodeError, KeyError) as e:
                         print(f"‚ö†Ô∏è  Warning: Skipping malformed line {line_num}: {e}")
                         continue
-    
+
     def stream_payloads(self, payload_path: str) -> Iterator[Tuple[str, Dict[str, Any]]]:
         """Stream payloads from JSONL file."""
         payload_file = Path(payload_path)
         if not payload_file.exists():
             raise FileNotFoundError(f"Payload file not found: {payload_file}")
-        
-        with open(payload_file, 'r', encoding='utf-8') as f:
+
+        with open(payload_file, "r", encoding="utf-8") as f:
             for line_num, line in enumerate(f, 1):
                 if line.strip():
                     try:
@@ -205,29 +192,29 @@
                     except (json.JSONDecodeError, KeyError) as e:
                         print(f"‚ö†Ô∏è  Warning: Skipping malformed line {line_num}: {e}")
                         continue
-    
+
     def merge_data_streams(self, embeddings_path: str, payload_path: str) -> Iterator[Tuple[str, Dict[str, Any], list[float]]]:
         """Merge embeddings and payloads by company_id - truly memory efficient streaming approach."""
         print("üîó Merging embeddings with payloads...")
-        
+
         # Create a temporary SQLite database for efficient lookups
         import sqlite3
         import pickle
-        
+
         temp_dir = Path(tempfile.gettempdir()) / "qdrant_upload"
         temp_dir.mkdir(exist_ok=True)
-        
+
         db_file = temp_dir / "embeddings.db"
-        
+
         # Check if database already exists and has data
         if db_file.exists():
             conn = sqlite3.connect(str(db_file))
             cursor = conn.cursor()
-            
+
             # Verify database has data
             cursor.execute("SELECT COUNT(*) FROM embeddings")
             count = cursor.fetchone()[0]
-            
+
             if count > 0:
                 print(f"üìä Found existing embeddings database with {count:,} embeddings, skipping re-indexing...")
                 conn.close()
@@ -235,11 +222,11 @@
                 print("üìä Found empty database, rebuilding...")
                 conn.close()
                 db_file.unlink()  # Delete empty database
-        
+
         # Build database if it doesn't exist or was empty
         if not db_file.exists():
             print("üìä Building embeddings database...")
-            
+
             # Create database and table
             conn = sqlite3.connect(str(db_file))
             cursor = conn.cursor()
@@ -249,41 +236,40 @@
                     embedding BLOB
                 )
             """)
-            
+
             # Stream embeddings to database
             count = 0
             duplicates_handled = 0
-            
+
             for company_id, embedding in self.stream_embeddings(embeddings_path):
                 embedding_blob = pickle.dumps(embedding)
-                cursor.execute("INSERT OR REPLACE INTO embeddings (company_id, embedding) VALUES (?, ?)", 
-                             (company_id, embedding_blob))
+                cursor.execute("INSERT OR REPLACE INTO embeddings (company_id, embedding) VALUES (?, ?)", (company_id, embedding_blob))
                 count += 1
-                
+
                 if count % 10000 == 0:
                     print(f"   üìä Indexed {count:,} embeddings...")
                     conn.commit()  # Commit every 10K to prevent memory buildup
-            
+
             conn.commit()
             conn.close()
             print(f"üìä Indexed {count:,} embeddings total")
-        
+
         # Reconnect to database for matching
         if not db_file.exists():
             raise FileNotFoundError("Database should exist at this point")
-        
+
         conn = sqlite3.connect(str(db_file))
         cursor = conn.cursor()
-        
+
         # Stream payloads and match with embeddings
         matched_count = 0
         skipped_count = 0
-        
+
         for company_id, payload in self.stream_payloads(payload_path):
             # Query database for specific embedding
             cursor.execute("SELECT embedding FROM embeddings WHERE company_id = ?", (company_id,))
             result = cursor.fetchone()
-            
+
             if result:
                 embedding = pickle.loads(result[0])
                 yield company_id, payload, embedding
@@ -292,51 +278,46 @@
                 skipped_count += 1
                 if skipped_count <= 5:  # Show first few skipped items
                     print(f"‚ö†Ô∏è  No embedding found for: {company_id}")
-        
+
         conn.close()
-        
+
         # Clean up database file
         db_file.unlink()
-        
+
         print(f"‚úÖ Matched: {matched_count}, Skipped: {skipped_count}")
         self.upload_stats["skipped_points"] = skipped_count
-    
+
     def save_checkpoint(self, batch_index: int, total_batches: int):
         """Save upload progress checkpoint."""
-        checkpoint_data = {
-            "batch_index": batch_index,
-            "total_batches": total_batches,
-            "upload_stats": self.upload_stats,
-            "timestamp": time.time()
-        }
-        
+        checkpoint_data = {"batch_index": batch_index, "total_batches": total_batches, "upload_stats": self.upload_stats, "timestamp": time.time()}
+
         checkpoint_path = self.checkpoint_dir / f"upload_checkpoint_{batch_index}.json"
-        with open(checkpoint_path, 'w') as f:
+        with open(checkpoint_path, "w") as f:
             json.dump(checkpoint_data, f, indent=2)
-        
+
         # Keep only the latest checkpoint
         for old_checkpoint in self.checkpoint_dir.glob("upload_checkpoint_*.json"):
             if old_checkpoint != checkpoint_path:
                 old_checkpoint.unlink()
-    
+
     def load_checkpoint(self) -> Optional[Dict[str, Any]]:
         """Load the latest upload checkpoint if it exists."""
         checkpoints = list(self.checkpoint_dir.glob("upload_checkpoint_*.json"))
         if not checkpoints:
             return None
-        
+
         latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)
-        with open(latest_checkpoint, 'r') as f:
+        with open(latest_checkpoint, "r") as f:
             return json.load(f)
-    
+
     def upload_batch(self, batch: list[Tuple[str, Dict[str, Any], list[float]]]) -> bool:
         """Upload a batch of points to Qdrant with retry logic."""
         if not batch:
             return True
-        
+
         max_retries = 3
         retry_delay = 1  # Start with 1 second delay
-        
+
         # Helpers for deterministic, human-readable string IDs
         def _normalize_name(name: str) -> str:
             return (name or "").strip().lower()
@@ -369,8 +350,8 @@
                 for item in batch:
                     company_name = item[0]
                     payload = item[1].copy()  # Copy the original payload
-                    meta = payload.get('metadata', {})
-                    website = meta.get('website') or meta.get('Website') or ""
+                    meta = payload.get("metadata", {})
+                    website = meta.get("website") or meta.get("Website") or ""
                     domain = _extract_domain(website)
                     name_norm = _normalize_name(company_name)
                     key_str = f"company:{name_norm}|{domain}" if domain else f"company:{name_norm}"
@@ -378,27 +359,20 @@
                     point_uuid = str(uuid.uuid5(UUID_NAMESPACE, key_str))
 
                     # Ensure point id and helpful fields are present in payload
-                    payload['company_id'] = company_name
-                    payload['domain'] = domain
-                    payload['point_id'] = key_str
+                    payload["company_id"] = company_name
+                    payload["domain"] = domain
+                    payload["point_id"] = key_str
                     payloads.append(payload)
                     ids.append(point_uuid)
-                
+
                 print(f"   üîç Debug: Uploading {len(batch)} points to collection '{self.config['collection']}'")
-                
+
                 # Upload to Qdrant
-                self.client.upsert(
-                    collection_name=self.config["collection"],
-                    points=qmodels.Batch(
-                        ids=ids,
-                        vectors=vectors,
-                        payloads=payloads
-                    )
-                )
-                
+                self.client.upsert(collection_name=self.config["collection"], points=qmodels.Batch(ids=ids, vectors=vectors, payloads=payloads))
+
                 self.upload_stats["successful_uploads"] += len(batch)
                 return True
-                
+
             except Exception as e:
                 if attempt < max_retries - 1:
                     print(f"‚ö†Ô∏è  Batch upload attempt {attempt + 1} failed: {e}")
@@ -409,18 +383,18 @@
                     print(f"‚ùå Batch upload failed after {max_retries} attempts: {e}")
                     self.upload_stats["failed_uploads"] += len(batch)
                     return False
-    
+
     def _build_embeddings_index(self, embeddings_path: str) -> int:
         """Build embeddings index and return total count."""
         import sqlite3
         import pickle
-        
+
         temp_dir = Path(tempfile.gettempdir()) / "qdrant_upload"
         temp_dir.mkdir(exist_ok=True)
         db_file = temp_dir / "embeddings.db"
-        
+
         print("üìä Building embeddings database...")
-        
+
         # Create database and table
         conn = sqlite3.connect(str(db_file))
         cursor = conn.cursor()
@@ -430,72 +404,73 @@
                 embedding BLOB
             )
         """)
-        
+
         # Stream embeddings to database
         count = 0
         duplicates_handled = 0
-        
+
         for company_id, embedding in self.stream_embeddings(embeddings_path):
             embedding_blob = pickle.dumps(embedding)
-            
+
             # Check if company_id already exists
             cursor.execute("SELECT COUNT(*) FROM embeddings WHERE company_id = ?", (company_id,))
             exists = cursor.fetchone()[0] > 0
-            
-            cursor.execute("INSERT OR REPLACE INTO embeddings (company_id, embedding) VALUES (?, ?)", 
-                         (company_id, embedding_blob))
-            
+
+            cursor.execute("INSERT OR REPLACE INTO embeddings (company_id, embedding) VALUES (?, ?)", (company_id, embedding_blob))
+
             if exists:
                 duplicates_handled += 1
-            
+
             count += 1
-            
+
             if count % 10000 == 0:
                 print(f"   üìä Indexed {count:,} embeddings... (duplicates handled: {duplicates_handled})")
                 conn.commit()  # Commit every 10K to prevent memory buildup
-        
+
         conn.commit()
         conn.close()
         print(f"üìä Indexed {count:,} embeddings total")
         print(f"üìä Duplicates handled: {duplicates_handled}")
-        
+
         return count
-    
+
     def upload_to_qdrant(self, embeddings_path: str, payload_path: str, resume: bool = True) -> None:
         """Upload embeddings and payloads to Qdrant with resume capability."""
         print("üöÄ Starting Qdrant Upload")
         print("=" * 50)
-        
+
         # Display file information
         import os
+
         embeddings_size = os.path.getsize(embeddings_path)
         payload_size = os.path.getsize(payload_path)
-        
+
         def format_size(size_bytes):
             """Format bytes to human-readable size."""
-            for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
+            for unit in ["B", "KB", "MB", "GB", "TB"]:
                 if size_bytes < 1024.0:
                     return f"{size_bytes:.2f} {unit}"
                 size_bytes /= 1024.0
             return f"{size_bytes:.2f} PB"
-        
+
         # Count payload records (estimate from file size for speed)
         print(f"\nüìÅ Input Files:")
         print(f"   Embeddings: {embeddings_path}")
         print(f"   Size: {format_size(embeddings_size)}")
-        
+
         # Estimate embeddings count from file size (average line ~69KB)
         avg_line_size = 69008  # Bytes per line (from earlier analysis)
         estimated_embeddings = embeddings_size // avg_line_size
         print(f"   Estimated records: ~{estimated_embeddings:,}")
-        
+
         print(f"\n   Payloads: {payload_path}")
         print(f"   Size: {format_size(payload_size)}")
-        
+
         # Count payload records quickly by sampling
         try:
             import json
-            with open(payload_path, 'r', encoding='utf-8') as f:
+
+            with open(payload_path, "r", encoding="utf-8") as f:
                 # Sample first few lines to estimate average line size
                 sample_lines = []
                 for i, line in enumerate(f):
@@ -511,31 +486,32 @@
                     print(f"   Records: Counting...")
         except Exception as e:
             print(f"   Records: Unable to estimate ({e})")
-        
+
         print()
-        
+
         # Ensure collection exists
         self.ensure_collection()
-        
+
         # Check for existing checkpoint
         checkpoint = self.load_checkpoint() if resume else None
         start_batch = 0
-        
+
         if checkpoint:
             print(f"üîÑ Resuming from checkpoint: batch {checkpoint['batch_index']}")
-            start_batch = checkpoint['batch_index']
-            self.upload_stats = checkpoint['upload_stats']
+            start_batch = checkpoint["batch_index"]
+            self.upload_stats = checkpoint["upload_stats"]
         else:
             print("üÜï Starting fresh upload")
-        
+
         # Check if we need to build index or can skip straight to upload
         temp_dir = Path(tempfile.gettempdir()) / "qdrant_upload"
         db_file = temp_dir / "embeddings.db"
-        
+
         if db_file.exists():
             print("üìä Found existing embeddings database, skipping indexing phase...")
             # Count total records from database
             import sqlite3
+
             conn = sqlite3.connect(str(db_file))
             cursor = conn.cursor()
             cursor.execute("SELECT COUNT(*) FROM embeddings")
@@ -545,15 +521,15 @@
             print("üìä No existing database found, building index first...")
             # Build index and count records
             total_records = self._build_embeddings_index(embeddings_path)
-        
+
         # Count payload records for display (estimate from file size for speed)
         print(f"\nüìä Processing Summary:")
         print(f"   Embeddings indexed: {total_records:,}")
-        
+
         # Estimate payload count from file size (faster than counting lines)
         try:
             # Sample first 100 lines to get average line size
-            with open(payload_path, 'r', encoding='utf-8') as f:
+            with open(payload_path, "r", encoding="utf-8") as f:
                 sample_sizes = []
                 for i, line in enumerate(f):
                     if i < 100:
@@ -568,91 +544,91 @@
                     print(f"   Payload records: Unable to estimate")
         except Exception as e:
             print(f"   Payload records: Unable to estimate ({e})")
-            payload_count = estimated_payloads if 'estimated_payloads' in locals() else 0
-        
+            payload_count = estimated_payloads if "estimated_payloads" in locals() else 0
+
         batch_size = self.config["batch_size"]
         total_batches = (total_records + batch_size - 1) // batch_size
-        
+
         print(f"\nüì¶ Upload Configuration:")
         print(f"   Batch size: {batch_size}")
         print(f"   Total batches: {total_batches:,}")
         print()
-        
+
         # Upload in batches
         batch = []
         batch_index = 0
         start_time = time.time()
         last_progress_time = start_time
-        
+
         for company_id, payload, embedding in self.merge_data_streams(embeddings_path, payload_path):
             batch.append((company_id, payload, embedding))
-            
+
             if len(batch) >= batch_size:
                 if batch_index >= start_batch:
                     # Calculate progress
                     progress = (batch_index + 1) / total_batches * 100
                     elapsed = time.time() - start_time
-                    
+
                     # Calculate ETA
                     if batch_index > start_batch:
                         avg_time_per_batch = elapsed / (batch_index - start_batch + 1)
                         remaining_batches = total_batches - batch_index - 1
                         eta_seconds = remaining_batches * avg_time_per_batch
-                        eta_str = f"ETA: {int(eta_seconds//60)}m {int(eta_seconds%60)}s"
+                        eta_str = f"ETA: {int(eta_seconds // 60)}m {int(eta_seconds % 60)}s"
                     else:
                         eta_str = "ETA: calculating..."
-                    
+
                     # Progress bar
                     bar_length = 30
                     filled_length = int(bar_length * progress / 100)
                     bar = "‚ñà" * filled_length + "‚ñë" * (bar_length - filled_length)
-                    
-                    print(f"üì§ Batch {batch_index+1:4d}/{total_batches} |{bar}| {progress:5.1f}% | {eta_str}")
+
+                    print(f"üì§ Batch {batch_index + 1:4d}/{total_batches} |{bar}| {progress:5.1f}% | {eta_str}")
                     print(f"   üìä Uploading {len(batch):,} points...")
-                    
+
                     batch_start = time.time()
                     success = self.upload_batch(batch)
                     batch_time = time.time() - batch_start
-                    
+
                     if success:
                         print(f"   ‚úÖ Success in {batch_time:.1f}s")
                     else:
                         print(f"   ‚ùå Failed after {batch_time:.1f}s")
-                
+
                 batch = []
                 batch_index += 1
-                
+
                 # Save checkpoint every 10 batches
                 if batch_index % 10 == 0:
                     self.save_checkpoint(batch_index, total_batches)
-                
+
                 # Force garbage collection every 50 batches
                 if batch_index % 50 == 0:
                     gc.collect()
-        
+
         # Upload remaining batch
         if batch and batch_index >= start_batch:
             progress = 100.0
             bar = "‚ñà" * 30
-            print(f"üì§ Final Batch {batch_index+1:4d}/{total_batches} |{bar}| {progress:5.1f}% | Final batch")
+            print(f"üì§ Final Batch {batch_index + 1:4d}/{total_batches} |{bar}| {progress:5.1f}% | Final batch")
             print(f"   üìä Uploading {len(batch):,} points...")
-            
+
             batch_start = time.time()
             success = self.upload_batch(batch)
             batch_time = time.time() - batch_start
-            
+
             if success:
                 print(f"   ‚úÖ Final batch completed in {batch_time:.1f}s")
             else:
                 print(f"   ‚ùå Final batch failed after {batch_time:.1f}s")
-        
+
         end_time = time.time()
         duration = end_time - start_time
-        
+
         # Clean up checkpoint
         for checkpoint_file in self.checkpoint_dir.glob("upload_checkpoint_*.json"):
             checkpoint_file.unlink()
-        
+
         # Print final statistics
         print(f"\n‚úÖ Upload Complete!")
         print(f"‚è±Ô∏è  Duration: {duration:.2f} seconds")
@@ -660,10 +636,12 @@
         print(f"‚úÖ Successful uploads: {self.upload_stats['successful_uploads']}")
         print(f"‚ùå Failed uploads: {self.upload_stats['failed_uploads']}")
         print(f"‚ö†Ô∏è  Skipped points: {self.upload_stats['skipped_points']}")
-        
-        if self.upload_stats['successful_uploads'] > 0:
-            print(f"üéØ Success rate: {self.upload_stats['successful_uploads'] / (self.upload_stats['successful_uploads'] + self.upload_stats['failed_uploads']) * 100:.1f}%")
-        
+
+        if self.upload_stats["successful_uploads"] > 0:
+            print(
+                f"üéØ Success rate: {self.upload_stats['successful_uploads'] / (self.upload_stats['successful_uploads'] + self.upload_stats['failed_uploads']) * 100:.1f}%"
+            )
+
         # Clean up temporary SQLite database
         temp_dir = Path(tempfile.gettempdir()) / "qdrant_upload"
         db_file = temp_dir / "embeddings.db"
@@ -679,23 +657,18 @@
     parser.add_argument("--payloads", required=True, help="Path to payload JSONL file")
     parser.add_argument("--config", default="config/search_recommender/qdrant.yaml", help="Qdrant config file")
     parser.add_argument("--no-resume", action="store_true", help="Don't resume from checkpoint")
-    
+
     args = parser.parse_args()
-    
+
     # Load environment variables
     load_repo_env()
-    
+
     # Initialize uploader
     uploader = QdrantUploader(config_path=args.config)
-    
+
     # Upload to Qdrant
-    uploader.upload_to_qdrant(
-        embeddings_path=args.embeddings,
-        payload_path=args.payloads,
-        resume=not args.no_resume
-    )
+    uploader.upload_to_qdrant(embeddings_path=args.embeddings, payload_path=args.payloads, resume=not args.no_resume)
 
 
 if __name__ == "__main__":
     main()
-

--- experiments/investors_search_recommender/utils/__init__.py
+++ experiments/investors_search_recommender/utils/__init__.py
@@ -3,4 +3,3 @@
 from .env_utils import load_repo_env
 
 __all__ = ["load_repo_env"]
-

--- experiments/investors_search_recommender/utils/env_utils.py
+++ experiments/investors_search_recommender/utils/env_utils.py
@@ -32,4 +32,3 @@
                 key = key.strip()
                 value = value.strip().strip('"').strip("'")
                 os.environ.setdefault(key, value)
-

--- local_token.py
+++ local_token.py
@@ -4,26 +4,28 @@
 from jwcrypto import jwk
 
 # This mimics test fixture
-PRIVATE_KEY = jwk.JWK.generate(kty='RSA', size=2048)
+PRIVATE_KEY = jwk.JWK.generate(kty="RSA", size=2048)
 PRIVATE_PEM = PRIVATE_KEY.export_to_pem(private_key=True, password=None).decode()
-KID = 'test'
-ISS = 'https://fair-basilisk-8.clerk.accounts.dev'
-AUD = 'ardessa-backend-test'
+KID = "test"
+ISS = "https://fair-basilisk-8.clerk.accounts.dev"
+AUD = "ardessa-backend-test"
 
-def make_token(email: str = 'user@example.com', include_email=True):
+
+def make_token(email: str = "user@example.com", include_email=True):
     payload = {
-        'sub': 'user_123',
-        'iss': ISS,
-        'aud': AUD,
-        'iat': int(time.time()),
-        'exp': int(time.time()) + 3600,
-        'public_metadata': {'roles': ['member']},
-        'meta': 'demo'
+        "sub": "user_123",
+        "iss": ISS,
+        "aud": AUD,
+        "iat": int(time.time()),
+        "exp": int(time.time()) + 3600,
+        "public_metadata": {"roles": ["member"]},
+        "meta": "demo",
     }
     if include_email:
-        payload['email'] = email
-    headers = {'kid': KID}
-    return jwt.encode(payload, PRIVATE_PEM, algorithm='RS256', headers=headers)
+        payload["email"] = email
+    headers = {"kid": KID}
+    return jwt.encode(payload, PRIVATE_PEM, algorithm="RS256", headers=headers)
+
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     print(make_token())

--- playground/test_perplexity_halves.py
+++ playground/test_perplexity_halves.py
@@ -22,6 +22,7 @@
 from urllib.parse import urlparse
 
 from dotenv import load_dotenv
+
 load_dotenv()
 
 from app.prompts.data_retrieval_prompts import (
@@ -81,7 +82,7 @@
 
 
 def build_sys_web_half_a(company_name: str, website_url: str, location: str, focus: str) -> str:
-    return f'''
+    return f"""
 You are an OSINT specialist researching {company_name} (website: {website_url}, location: {location}).
 
 **CRITICAL: Your response must be ONLY a valid JSON object with no additional text before or after.**
@@ -206,11 +207,11 @@
 - Prioritize recent information (past 12 months)
 - For fields that can have multiple entries, provide as many results as possible
 - For fields requesting examples, events, activities, or recognition: aim to return multiple recent instances when available rather than single examples - comprehensive coverage is preferred over brevity
-'''
+"""
 
 
 def build_sys_web_half_b(company_name: str, website_url: str, location: str, focus: str) -> str:
-    return f'''
+    return f"""
 You are an OSINT specialist researching {company_name} (website: {website_url}, location: {location}).
 
 **CRITICAL: Your response must be ONLY a valid JSON object with no additional text before or after.**
@@ -323,12 +324,12 @@
 - Prioritize recent information (past 12 months)
 - For fields that can have multiple entries, provide as many results as possible
 - For fields requesting examples, events, activities, or recognition: aim to return multiple recent instances when available rather than single examples - comprehensive coverage is preferred over brevity
-'''
+"""
 
 
 def build_sys_ext_half_a(company_name: str, website_url: str, location: str, focus: str) -> str:
     # Same halves structure as website but for external
-    return f'''
+    return f"""
 Research {company_name} from EXTERNAL news sources only. EXCLUDE site:{website_url}. Location: {location}.
 
 **RESPOND WITH ONLY JSON - NO THINKING OR EXPLANATIONS**
@@ -453,11 +454,11 @@
 - Prioritize recent information (past 12 months)
 - For fields that can have multiple entries, provide as many results as possible
 - For fields requesting examples, events, activities, or recognition: aim to return multiple recent instances when available rather than single examples - comprehensive coverage is preferred over brevity
-'''
+"""
 
 
 def build_sys_ext_half_b(company_name: str, website_url: str, location: str, focus: str) -> str:
-    return f'''
+    return f"""
 Research {company_name} from EXTERNAL news sources only. EXCLUDE site:{website_url}. Location: {location}.
 
 **RESPOND WITH ONLY JSON - NO THINKING OR EXPLANATIONS**
@@ -581,7 +582,7 @@
 - Prioritize recent information (past 12 months)
 - For fields that can have multiple entries, provide as many results as possible
 - For fields requesting examples, events, activities, or recognition: aim to return multiple recent instances when available rather than single examples - comprehensive coverage is preferred over brevity
-'''
+"""
 
 
 async def call_perplexity(system_prompt: str, user_prompt: str, domains: List[str], source: str) -> Dict[str, Any]:
@@ -666,6 +667,7 @@
     - None: ("null", 0, 0)
     - other: ("other", 1, char_len)
     """
+
     def count_leaves(node: Any) -> int:
         if isinstance(node, dict):
             return sum(count_leaves(v) for v in node.values())
@@ -704,11 +706,20 @@
     return out
 
 
-def print_diff_report(website_full_json: Dict[str, Any], web_A_json: Dict[str, Any], web_B_json: Dict[str, Any],
-                      external_full_json: Dict[str, Any], ext_A_json: Dict[str, Any], ext_B_json: Dict[str, Any]):
+def print_diff_report(
+    website_full_json: Dict[str, Any],
+    web_A_json: Dict[str, Any],
+    web_B_json: Dict[str, Any],
+    external_full_json: Dict[str, Any],
+    ext_A_json: Dict[str, Any],
+    ext_B_json: Dict[str, Any],
+):
     """Pretty-print per-field metrics and deltas for Website and External (Full vs Halves)."""
-    def combine_halves(a: Dict[str, Dict[str, Tuple[str,int,int]]], b: Dict[str, Dict[str, Tuple[str,int,int]]]) -> Dict[str, Dict[str, Tuple[str,int,int]]]:
-        combined: Dict[str, Dict[str, Tuple[str,int,int]]] = {}
+
+    def combine_halves(
+        a: Dict[str, Dict[str, Tuple[str, int, int]]], b: Dict[str, Dict[str, Tuple[str, int, int]]]
+    ) -> Dict[str, Dict[str, Tuple[str, int, int]]]:
+        combined: Dict[str, Dict[str, Tuple[str, int, int]]] = {}
         keys = set(a.keys()) | set(b.keys())
         for cat in keys:
             combined[cat] = {}
@@ -723,7 +734,12 @@
                     combined[cat][f] = av or bv
         return combined
 
-    def print_section(title: str, full: Dict[str, Dict[str, Tuple[str,int,int]]], halfA: Dict[str, Dict[str, Tuple[str,int,int]]], halfB: Dict[str, Dict[str, Tuple[str,int,int]]]):
+    def print_section(
+        title: str,
+        full: Dict[str, Dict[str, Tuple[str, int, int]]],
+        halfA: Dict[str, Dict[str, Tuple[str, int, int]]],
+        halfB: Dict[str, Dict[str, Tuple[str, int, int]]],
+    ):
         print(f"\n=== {title} ===")
         combined = combine_halves(halfA, halfB)
         cats = sorted(set(list(full.keys()) + list(combined.keys())))
@@ -787,8 +803,12 @@
     ext_halfB_task = call_perplexity(sys_ext_B, user_prompt, domains, "EXTERNAL")
 
     (web_full, web_A, web_B, ext_full, ext_A, ext_B) = await asyncio.gather(
-        web_full_task, web_halfA_task, web_halfB_task,
-        ext_full_task, ext_halfA_task, ext_halfB_task,
+        web_full_task,
+        web_halfA_task,
+        web_halfB_task,
+        ext_full_task,
+        ext_halfA_task,
+        ext_halfB_task,
     )
 
     # Parse
@@ -832,7 +852,7 @@
             "external_full": ext_full_json,
             "external_A": ext_A_json,
             "external_B": ext_B_json,
-        }
+        },
     }
 
 
@@ -840,7 +860,7 @@
     def fmt(s: Dict[str, Any]) -> str:
         return (
             f"ok={s.get('success')} t={s.get('elapsed_s'):.2f}s in={s.get('input_tokens')} out={s.get('output_tokens')} "
-            f"cats={s.get('metrics',{}).get('present_categories')} leaves={s.get('metrics',{}).get('leaf_fields')}"
+            f"cats={s.get('metrics', {}).get('present_categories')} leaves={s.get('metrics', {}).get('leaf_fields')}"
         )
 
     wfull = summary["website_full"]
@@ -861,8 +881,12 @@
     # Detailed per-field delta report
     p = result["parsed"]
     print_diff_report(
-        p["website_full"], p["website_A"], p["website_B"],
-        p["external_full"], p["external_A"], p["external_B"],
+        p["website_full"],
+        p["website_A"],
+        p["website_B"],
+        p["external_full"],
+        p["external_A"],
+        p["external_B"],
     )
     if args.debug_out:
         with open(args.debug_out, "w", encoding="utf-8") as f:
@@ -883,5 +907,3 @@
 
 if __name__ == "__main__":
     main()
-
-

--- run_orchestrator.py
+++ run_orchestrator.py
@@ -5,6 +5,7 @@
 
 # Load environment variables FIRST, before any imports
 from dotenv import load_dotenv
+
 load_dotenv()
 
 import asyncio
@@ -18,11 +19,12 @@
 # Read from environment/secrets manager instead of hardcoding
 ENABLE_DEBUGGING = get_enable_debugging()
 
+
 async def main():
     """Run the orchestrator in interactive mode with test user ID."""
     print("üöÄ Prospecting Orchestrator - Interactive Mode")
     print("=" * 60)
-    
+
     # Set the test user ID
     TEST_USER_ID = "test_user_123"
     print(f"üë§ Test User ID: {TEST_USER_ID}")
@@ -37,11 +39,12 @@
     print("   ‚Ä¢ General search: 'Find VC firms in London focusing on fintech'")
     print("   ‚Ä¢ Off-topic: 'What's the weather like?'")
     print()
-    
+
     # Initialize and run the orchestrator with debugging flag
     orchestrator = ProspectingOrchestrator(enable_debugging=ENABLE_DEBUGGING)
     await orchestrator.execute_interactive(user_id=TEST_USER_ID)
 
+
 if __name__ == "__main__":
     try:
         asyncio.run(main())
@@ -50,6 +53,7 @@
     except Exception as e:
         print(f"‚ùå Error: {e}")
         import traceback
+
         traceback.print_exc()
         sys.exit(1)
 
@@ -61,4 +65,4 @@
 #     #"Research Trinity Street Asset Management, LLP",
 #     # "Search for private equity firms in New York"
 #     "Search for Frederic Favre at Alpstone Capital Suisse SA"
-# ]
\ No newline at end of file
+# ]

--- scripts/backfill_finding_relevance_scores.py
+++ scripts/backfill_finding_relevance_scores.py
@@ -24,6 +24,7 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 from dotenv import load_dotenv
+
 load_dotenv()
 
 from app.utils.global_db import get_global_db
@@ -33,27 +34,24 @@
 import json
 import time
 
-logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'
-)
+logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(name)s | %(message)s")
 logger = logging.getLogger(__name__)
 
 
 async def backfill_finding_relevance_scores():
     """Score all existing findings for relevance using LLM-based classification."""
-    print("\n" + "="*60)
+    print("\n" + "=" * 60)
     print("BACKFILLING FINDING RELEVANCE SCORES (LLM-Based)")
-    print("="*60)
+    print("=" * 60)
     print(f"Max concurrent scoring tasks: {MAX_CONCURRENT_SCORING}")
     print("This will use the same LLM classification as the live system.")
-    print("="*60 + "\n")
-    
+    print("=" * 60 + "\n")
+
     db = await get_global_db()
     if not db:
         print("‚ùå Database not available")
         return 1
-    
+
     async with db.pool.acquire() as conn:
         # Get all findings with their radar info that need scoring
         # Include findings that have generic categories (profile_match, Tier 2, Tier 3) that need to be re-scored
@@ -76,40 +74,43 @@
             GROUP BY f.finding_id, f.radar_id, f.radar_type, f.finding_data
             ORDER BY f.finding_id
         """)
-        
+
         if not findings:
             print("‚úÖ No findings need scoring (all already scored or no findings exist)")
             return 0
-        
+
         print(f"Found {len(findings)} findings to score\n")
-        
+
         total_scored = 0
         total_errors = 0
         total_users_scored = 0
         start_time = time.time()
-        
+
         for i, finding_row in enumerate(findings, 1):
             finding_id = finding_row["finding_id"]
             radar_id = finding_row["radar_id"]
             radar_type = finding_row["radar_type"]
             finding_data = finding_row["finding_data"]
             user_count = finding_row["user_count"]
-            
+
             print(f"[{i}/{len(findings)}] Finding {finding_id[:16]}... ({user_count} users)...", end=" ", flush=True)
-            
+
             # Get user IDs for this finding
-            user_rows = await conn.fetch("""
+            user_rows = await conn.fetch(
+                """
                 SELECT DISTINCT user_id
                 FROM tamradar_user_findings
                 WHERE finding_id = $1
-            """, finding_id)
-            
+            """,
+                finding_id,
+            )
+
             user_ids = [row["user_id"] for row in user_rows]
-            
+
             if not user_ids:
                 print("‚ö†Ô∏è  (no users)")
                 continue
-            
+
             try:
                 # Parse finding_data if it's a string
                 if isinstance(finding_data, str):
@@ -118,7 +119,7 @@
                     except (json.JSONDecodeError, TypeError):
                         # If parsing fails, keep as string (will be handled by extract_finding_text)
                         pass
-                
+
                 # Score the finding (this will update tamradar_user_findings)
                 # This uses LLM classification with two-phase approach:
                 # 1. Base classification (finding only) - threshold 0.65
@@ -130,34 +131,34 @@
                     radar_type=radar_type,
                     user_ids=user_ids,
                 )
-                
+
                 print("‚úÖ")
                 total_scored += 1
                 total_users_scored += len(user_ids)
-                
+
                 # Small delay to avoid overwhelming the system (semaphore handles concurrency)
                 if i % 10 == 0:
                     await asyncio.sleep(0.5)  # Brief pause every 10 findings
-                
+
             except Exception as e:
                 print(f"‚ùå Error: {e}")
                 total_errors += 1
                 logger.error(f"Failed to score finding {finding_id}: {e}", exc_info=True)
-        
+
         elapsed_time = time.time() - start_time
-        
-        print("\n" + "="*60)
+
+        print("\n" + "=" * 60)
         print("SUMMARY")
-        print("="*60)
+        print("=" * 60)
         print(f"‚úÖ Successfully scored: {total_scored} findings")
         print(f"üë• Total user-finding pairs: {total_users_scored}")
         print(f"‚ùå Failed: {total_errors}")
         print(f"üì¶ Total findings processed: {len(findings)}")
         print(f"‚è±Ô∏è  Time elapsed: {elapsed_time:.1f} seconds")
         if total_scored > 0:
-            print(f"‚ö° Average time per finding: {elapsed_time/total_scored:.2f} seconds")
-        print("="*60)
-        
+            print(f"‚ö° Average time per finding: {elapsed_time / total_scored:.2f} seconds")
+        print("=" * 60)
+
         if total_errors == 0:
             print("‚úÖ All findings scored successfully!")
             return 0
@@ -177,10 +178,10 @@
     except Exception as e:
         print(f"\n‚ùå Fatal error: {e}")
         import traceback
+
         traceback.print_exc()
         sys.exit(1)
 
 
 if __name__ == "__main__":
     asyncio.run(main())
-

--- scripts/benchmark_llm_tier_classification.py
+++ scripts/benchmark_llm_tier_classification.py
@@ -18,7 +18,7 @@
 from datetime import datetime
 
 # Add project root to path
-sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
 
 from langchain_google_genai import ChatGoogleGenerativeAI
 from langchain_openai import ChatOpenAI
@@ -70,299 +70,294 @@
         "name": "Direct allocation mention (tweaked)",
         "finding_text": "Schroders announces $50M commitment to new growth equity fund. The firm has allocated capital to emerging managers in the technology sector, demonstrating their active deployment strategy and ticket size preferences.",
         "expected_tier": 1,
-        "expected_category": "direct_allocation"
+        "expected_category": "direct_allocation",
     },
     {
         "name": "Investment committee meeting (tweaked)",
         "finding_text": "Schroders Investment Committee meeting scheduled for Q1 2026 to review new fund commitments. The committee will evaluate emerging manager programs and make allocation decisions during the fiscal year-end review period.",
         "expected_tier": 1,
-        "expected_category": "ic_meetings"
+        "expected_category": "ic_meetings",
     },
     {
         "name": "Senior investment hire - Portfolio Manager (SPECIFIC)",
         "finding_text": "Oscar Alberte joined Schroders as Portfolio Manager",
         "expected_tier": 1,
-        "expected_category": "senior_investment_hires"
+        "expected_category": "senior_investment_hires",
     },
     {
         "name": "Seeking managers statement (tweaked)",
         "finding_text": "Schroders is actively seeking emerging VC managers in the healthcare and fintech sectors. We are evaluating new fund managers and open to new manager relationships for funds under $500M in size.",
         "expected_tier": 1,
-        "expected_category": "seeking_managers"
+        "expected_category": "seeking_managers",
     },
     {
         "name": "Distribution/re-up discussion (tweaked)",
         "finding_text": "Schroders received significant distributions from existing portfolio funds, creating fresh capital capacity for redeployment. The firm is evaluating re-up decisions with current managers and considering new commitments.",
         "expected_tier": 1,
-        "expected_category": "distribution_reup"
+        "expected_category": "distribution_reup",
     },
-    
     # ===== TIER 2 CASES =====
     {
         "name": "Conference speaking (from real finding - tweaked)",
         "finding_text": "Schroders investment team speaking at ILPA conference on emerging managers and sector strategies. Join us for insights on market outlook and manager evaluation approaches.",
         "expected_tier": 2,
-        "expected_category": "conference_speaking"
+        "expected_category": "conference_speaking",
     },
     {
         "name": "New mandate announcement (tweaked)",
         "finding_text": "Schroders launches new ESG investment mandate with diversity and inclusion commitments. The new focus area includes climate transition strategies and impact investing goals, defining priority consideration for fund managers.",
         "expected_tier": 2,
-        "expected_category": "new_mandate"
+        "expected_category": "new_mandate",
     },
     {
         "name": "Team expansion (tweaked)",
         "finding_text": "Schroders is expanding its private equity team, hiring additional investment professionals to increase deployment capacity and grow allocations to the asset class.",
         "expected_tier": 2,
-        "expected_category": "team_expansion"
+        "expected_category": "team_expansion",
     },
     {
         "name": "Allocation announcement (from real finding - tweaked)",
         "finding_text": "UK advisers are prioritising diversification in client portfolios, boosting allocations to alternatives and emerging markets, the latest Schroders UK Financial Adviser Survey reveals. Schroders is increasing private equity target allocation from 10% to 15%.",
         "expected_tier": 2,
-        "expected_category": "allocation_announcements"
+        "expected_category": "allocation_announcements",
     },
-    
     # ===== TIER 3 CASES =====
     {
         "name": "General thought leadership (tweaked)",
         "finding_text": "Schroders publishes white paper on private equity market trends and investment philosophies. Our research explores industry perspectives on portfolio construction and manager evaluation.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
-    
     # ===== NO MATCH CASES (Real findings kept as-is) =====
     {
         "name": "General market commentary (REAL - no match)",
         "finding_text": "What does big tech's capex splurge mean for valuations? Capex is going through the roof at the hyperscalers and is on track to treble in the three years to 2026. Their premium valuations have partly reflected high returns on invested capital(ROIC)/low capital intensity. A shift towards increased capital intensity presents a risk.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Event announcement (REAL - no match)",
         "finding_text": "Thank you to everyone who joined us for the second private markets event in our three-part series with Schroders Capital. Abigail Dean set the scene and explored how megatrends ‚Äî including climate transition, digitalisation and demographic shifts‚Äîare reshaping real assets. The panel then sparked valuable discussion on the case for real estate, infrastructure and natural capital in forward-looking portfolios.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Podcast sponsorship mention (REAL - no match)",
         "finding_text": "At our recent Allocator Summit, we asked some of our guests what piece of advice they'd give to their younger selves. Thank you to Michael Tiedemann, CEO of AlTi Tiedemann Global, Jack Edmondson, CIO of Oxford Science Enterprises, Deviyani Misra-Godwina, Managing Director of Strategic Value Partners, Mark Wallace, Managing Directors at Rothschild & Co, and our very own Simon Brewer for speaking to us! The Money Maze Podcast is kindly sponsored by Schroders, World Gold Council, IFM Investors, and LSEG.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Internship program (REAL - no match)",
         "finding_text": "As the Campus Brand Ambassador for Schroders, I'm excited to share that two 2026 Summer Internship Programmes are open for penultimate-year students graduating in 2027. Schroders is a global investment manager with ¬£776bn+ AUM, a 200-year heritage, and a strong focus on sustainability, innovation, and long-term investing. These programmes are an excellent chance to gain hands-on experience, learn from industry professionals, and explore a future career in asset management.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "New hire - Head of Indirect Tax (SPECIFIC - should NOT match)",
         "finding_text": "Yijun Liu joined Schroders as Head of Indirect Tax",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Award announcement (REAL - no match)",
         "finding_text": "Last week we had the pleasure of attending the Financial Promoter Awards 2025, where William Blake was proud to sponsor the Platinum Award for In-House Team of the Year. A huge congratulations to all the nominees in our category - BCB Group, Schroders Media Relations Team, eToro, Aon and IFM Investors. Well-deserved congratulations to eToro for taking home the award, and to the Schroders Media Relations Team for being highly commended.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
-    
     # ===== ADDITIONAL REAL FINDINGS (some tweaked) =====
     {
         "name": "Sustainability thought leadership (REAL - could be Tier 3)",
         "finding_text": "Navigating the Complexities of Sustainability in a Polarized World with Marina Severinovsky. We are thrilled to present a thought-provoking conversation with Marina Severinovsky, Head of Sustainability for North America at Schroders, who leads the integration of sustainability and ESG practices across markets. With over 18 years of experience in economic analysis, financial modeling, and client relationship management, Marina brings a wealth of knowledge to the table.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
     {
         "name": "Digital assets publication (REAL - could be Tier 3)",
         "finding_text": "Asset and Wealth Management - Operationalising Tokenised Funds, published now in November 2025 by the Guardian Asset & Wealth Management Industry Group in collaboration with Deutsche Bank, Phillip Securities Pte Ltd, Baker McKenzie, Citi, Drew & Napier LLC, Fidelity International, Franklin Templeton, Interop Labs, the Monetary Authority of Singapore (MAS), Memento Blockchain, Schroders, Swift, the Financial Conduct Authority, & UOB. This report outlines how tokenised funds, especially tokenised funds, are transforming asset management.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
     {
         "name": "Investment Forum participation (REAL - tweaked for Tier 2)",
         "finding_text": "Going back to South Africa feels always so special for me, having spent 9 years of my life there. I am truly delighted to join Kevin Hinton's INVESTMENT FORUM 2026 alongside incredible speakers and guests. I'll be exploring investment strategies and sharing insights on portfolio construction and manager evaluation at this industry conference.",
         "expected_tier": 2,
-        "expected_category": "conference_speaking"
+        "expected_category": "conference_speaking",
     },
     {
         "name": "VC Summit mention (REAL - tweaked for Tier 2)",
         "finding_text": "Ending the year strong, TNB Aura hosted our VC Summit 2025, bringing together our community of LPs, founders, and team members. Schroders investment team participated in panel discussions on emerging managers and sector strategies, sharing insights on market outlook and manager evaluation approaches.",
         "expected_tier": 2,
-        "expected_category": "conference_speaking"
+        "expected_category": "conference_speaking",
     },
     {
         "name": "Allocation survey (REAL - tweaked for Tier 2)",
         "finding_text": "UK advisers are prioritising diversification in client portfolios, boosting allocations to alternatives and emerging markets, the latest Schroders UK Financial Adviser Survey reveals. Schroders is increasing private equity target allocation from 10% to 15% and expanding into growth equity strategies.",
         "expected_tier": 2,
-        "expected_category": "allocation_announcements"
+        "expected_category": "allocation_announcements",
     },
     {
         "name": "Market commentary (REAL - no match)",
         "finding_text": "What does big tech's capex splurge mean for valuations? Further reading in Schroders Equity Lens November 2025.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Podcast sponsorship (REAL - no match)",
         "finding_text": "Sir Nick Clegg, Former UK Deputy PM & Former President of Global Affairs at Meta, on the potential future of Britain & technology. Listen or watch here. The Money Maze Podcast is kindly sponsored by Schroders, World Gold Council, IFM Investors, and LSEG.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Event invitation (REAL - no match)",
         "finding_text": "Join the industry's leading voices on automation and digital transformation in fixed income. As digital bonds, AI, and data standards accelerate across global capital markets, this event brings together peers and international experts to share insights, engage in practical discussions, and explore where your organisation fits within this evolving landscape.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Career post (REAL - no match)",
         "finding_text": "Curious about a career at Schroders? I'm excited to share a quick snapshot of what makes Schroders a great place to grow, whether you're looking for an internship, a graduate role, or a seasoned position.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Charity mention (REAL - no match)",
         "finding_text": "One of our most important legacies is the people we leave behind. I'm passionate about supporting young people - education, both formal and non-formal, is critical to children. More importantly it's critical to our shared future success. I'm fortunate to lead an organisation focussed on the future, and am proud of Schroders collaboration with charities across the world focussed on education for children.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Direct allocation (tweaked from real - Tier 1)",
         "finding_text": "Schroders announces $75M commitment to new venture capital fund focused on healthcare technology. The firm has selected emerging managers in the biotech sector and disclosed ticket sizes ranging from $5M to $15M per fund commitment.",
         "expected_tier": 1,
-        "expected_category": "direct_allocation"
+        "expected_category": "direct_allocation",
     },
     {
         "name": "Senior hire - Head of PE (tweaked - Tier 1)",
         "finding_text": "Schroders has appointed Sarah Johnson as Head of Private Equity, bringing extensive experience in fund manager selection and portfolio management. The new hire will lead the alternative investments team and serve on the Investment Committee.",
         "expected_tier": 1,
-        "expected_category": "senior_investment_hires"
+        "expected_category": "senior_investment_hires",
     },
     {
         "name": "Seeking managers (tweaked - Tier 1)",
         "finding_text": "Schroders is actively seeking emerging growth equity managers in the fintech and healthcare sectors. We are evaluating new fund managers and open to new manager relationships for funds between $200M and $500M in size, with a focus on US and European markets.",
         "expected_tier": 1,
-        "expected_category": "seeking_managers"
+        "expected_category": "seeking_managers",
     },
     {
         "name": "Distribution announcement (tweaked - Tier 1)",
         "finding_text": "Schroders received $150M in distributions from existing portfolio funds, creating fresh capital capacity for redeployment. The firm is evaluating re-up decisions with current managers and actively considering new commitments to growth equity and venture capital funds.",
         "expected_tier": 1,
-        "expected_category": "distribution_reup"
+        "expected_category": "distribution_reup",
     },
     {
         "name": "IC meeting (tweaked - Tier 1)",
         "finding_text": "Schroders Investment Committee meeting scheduled for March 2026 to review new fund commitments and make allocation decisions. The committee will evaluate emerging manager programs during the fiscal year-end review period and finalize commitment timelines.",
         "expected_tier": 1,
-        "expected_category": "ic_meetings"
+        "expected_category": "ic_meetings",
     },
     {
         "name": "New ESG mandate (tweaked - Tier 2)",
         "finding_text": "Schroders launches new ESG investment mandate with diversity and inclusion commitments, climate transition strategies, and impact investing goals. The new focus area defines priority consideration for fund managers and includes specific criteria for fund size, stage preferences, and sector focus.",
         "expected_tier": 2,
-        "expected_category": "new_mandate"
+        "expected_category": "new_mandate",
     },
     {
         "name": "Team expansion announcement (tweaked - Tier 2)",
         "finding_text": "Schroders is expanding its venture capital team, hiring three additional investment professionals to increase deployment capacity and grow allocations to the asset class. The expansion signals increased commitment to early-stage technology investments.",
         "expected_tier": 2,
-        "expected_category": "team_expansion"
+        "expected_category": "team_expansion",
     },
-    
     # ===== ADDITIONAL REAL FINDINGS FROM DATABASE =====
     {
         "name": "Asia Pacific Capital Markets event (REAL - no match)",
         "finding_text": "I'm looking forward to attending the upcoming Savills 45-minute strategic briefing on the 2026 Asia Pacific Capital Markets outlook. The panel features experts from Oxford Economics, Schroders Capital, Rava Partners and Savills. The session includes a presentation from Oxford Economics followed by a panel discussion focused on liquidity and targeted growth strategies.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Equity Lens publication (REAL - no match)",
         "finding_text": "Schroders - Equity Lens November 2025. What does big tech's capex splurge mean for valuations?",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "AgroStar funding announcement (REAL - no match)",
         "finding_text": "AgroStar Raises $30 Million From Just Climate to Accelerate Sustainable Farming Across India. We're thrilled to announce that we've secured $30 million in funding from global investor, Just Climate (established by Generation Investment Management) and our existing investors. This marks Just Climate's first investment in India from their global Natural Climate Solutions strategy fund.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "AI-driven search commentary (REAL - no match)",
         "finding_text": "The rise of AI-driven search is reducing clicks to external websites, according to Schroders' Michael White, CFA. As a result, Google and YouTube have reportedly committed R688m to support South African media.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Clearstream Roadshow panel (REAL - tweaked for Tier 2)",
         "finding_text": "What an incredible turnout at the Clearstream Roadshow 2025 in Singapore! I was especially honored to moderate the panel session on 'Beyond 60/40: The Role of Alternatives in Wealth Management.' Schroders investment team member Joyce Lim, CFA, CAIA participated in the panel discussion, sharing insights on portfolio construction and alternative investment strategies.",
         "expected_tier": 2,
-        "expected_category": "conference_speaking"
+        "expected_category": "conference_speaking",
     },
     {
         "name": "Energy conference panel (REAL - tweaked for Tier 2)",
         "finding_text": "The Energyear Europe 2025 conference will take place next November 25th and 26th in London. Schroders investment team member Lee Moscovitch from Schroders Greencoat LLP will participate as a speaker in the panel: 'Project Finance & Structured Deals: Financing the Next Wave of Green Projects', alongside leading industry experts discussing renewable energy infrastructure financing.",
         "expected_tier": 2,
-        "expected_category": "conference_speaking"
+        "expected_category": "conference_speaking",
     },
     {
         "name": "Property awards sponsorship (REAL - no match)",
         "finding_text": "We're pleased to announce that Orbit Developments is sponsoring the Office Agency team award at the Insider Media North West Property Awards 2026. We're looking forward to another fantastic night and cannot wait to celebrate with the rest of the sponsors, including Schroders Capital.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Green hydrogen project (REAL - no match)",
         "finding_text": "Plug Power has been selected for an equipment supply and long-term service agreement to provide 55 MW of electrolyser capacity for three green hydrogen projects in the UK. The Barrow-in-Furness project, developed by a joint venture between Carlton Power Limited and Schroders Greencoat LLP, will use six 5 MW GenEco proton exchange membrane (PEM) electrolysers powered by renewable electricity to produce hydrogen.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "OCIO podcast (REAL - tweaked for Tier 3)",
         "finding_text": "New Episode of The CIO Chair Podcast: What does it really take to evolve from a traditional Chief Investment Officer to an OCIO leading complex institutional portfolios? In our latest episode, we sit down with Chetan Ghosh, OCIO at Schroders Solutions, to explore how the OCIO model is transforming investment governance and the balance between process and intuition in decision-making.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
     {
         "name": "Private equity market commentary (REAL - tweaked for Tier 3)",
         "finding_text": "Private equity remains in a period of recalibration, with fundraising, deal activity, and exits still below pre-2022 levels. For investors, this creates opportunities in the form of pricing dislocations and reduced competition, especially in less efficient segments where capital is scarce. Schroders investment team shares insights on market trends and portfolio construction strategies.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
     {
         "name": "Impact investment research (REAL - tweaked for Tier 3)",
         "finding_text": "A wonderful evening with Cazenove Capital - sharing their research. Can impact-investments outperform the market? A study of 257 companies by Catherine Macaulay, and Oxford professor Amir Amel-Zadeh suggests that companies with higher revenue alignment to measurable impact themes generate superior financial returns while also exhibiting lower volatility and greater resilience. Schroders investment team discusses these findings and their implications for portfolio construction.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
     {
         "name": "Internship program (REAL - no match)",
         "finding_text": "Schroders internships are open. You can apply for an opportunity to gain work experience at one of the UK's leading investment managers. They have $1 billion+ in assets and 6000+ employees. Apply here for various internship and placement opportunities.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "ULI UK NEXT Forum (REAL - no match)",
         "finding_text": "What a turnout for the inaugural ULI UK NEXT Forum! It was fantastic to welcome more than 30 participants on Tuesday. From the start, the room was alive with conversation and engagement. The panel brought sharp insights and lively discussion as we delved into operational real estate: Annette Kr√∂ger (PIMCO Prime Real Estate), Paddy Allen (The Dot Group), Charlotte Jacques (Schroders Capital), and others.",
         "expected_tier": None,
-        "expected_category": None
+        "expected_category": None,
     },
     {
         "name": "Tokenised funds publication (REAL - tweaked for Tier 3)",
         "finding_text": "Asset and Wealth Management - Operationalising Tokenised Funds, published now in November 2025 by the Guardian Asset & Wealth Management Industry Group in collaboration with Deutsche Bank, Schroders, and other leading institutions. This report outlines how tokenised funds are transforming asset management. Schroders investment team contributed research and analysis on the implications for portfolio construction and manager evaluation.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
     {
         "name": "Sustainability webinar (REAL - tweaked for Tier 3)",
         "finding_text": "Navigating the Complexities of Sustainability in a Polarized World with Marina Severinovsky. We are thrilled to present a thought-provoking conversation with Marina Severinovsky, Head of Sustainability for North America at Schroders, who leads the integration of sustainability and ESG practices across markets. With over 18 years of experience in economic analysis, financial modeling, and client relationship management, Marina brings a wealth of knowledge to the table.",
         "expected_tier": 3,
-        "expected_category": "thought_leadership"
+        "expected_category": "thought_leadership",
     },
 ]
 
@@ -377,6 +372,7 @@
         # Fallback to rough estimation
         return len(text) // 4
 
+
 def count_tokens_gemini(text: str) -> int:
     """Count tokens for Gemini models (rough estimation: 1 token ‚âà 4 characters)."""
     return len(text) // 4
@@ -385,18 +381,18 @@
 async def classify_with_openai(finding_text: str, model_name: str, company_name: str = "Schroders plc") -> Dict[str, Any]:
     """
     Classify finding using OpenAI models (GPT-4o-mini, GPT-4.1-mini).
-    
+
     Args:
         finding_text: The finding text to classify
         model_name: Model identifier (gpt-4o-mini, gpt-4.1-mini)
         company_name: Company name for the prompt
-        
+
     Returns:
         Dict with tier, category, confidence, cost, latency, tokens
     """
     if model_name not in PRICING:
         raise ValueError(f"Unknown model: {model_name}. Available: {list(PRICING.keys())}")
-    
+
     api_key = secrets_manager.get_secret("OPENAI_API_KEY")
     if not api_key:
         logger.warning("OpenAI API key not configured")
@@ -411,26 +407,21 @@
             "output_tokens": 0,
             "raw_response": "",
         }
-    
+
     start_time = time.time()
-    
+
     try:
         # Get prompt from data_process_prompts
         prompt = get_base_classification_prompt(finding_text, company_name)
-        
+
         # Count input tokens using tiktoken
         input_tokens = count_tokens_openai(prompt, model_name)
-        
-        llm = ChatOpenAI(
-            model=model_name,
-            temperature=0.1,
-            max_tokens=200,
-            openai_api_key=api_key
-        )
-        
+
+        llm = ChatOpenAI(model=model_name, temperature=0.1, max_tokens=200, openai_api_key=api_key)
+
         response = await llm.ainvoke([("user", prompt)])
         content = response.content.strip()
-        
+
         # Remove markdown code blocks if present
         if content.startswith("```json"):
             content = content[7:]
@@ -439,21 +430,18 @@
         if content.endswith("```"):
             content = content[:-3]
         content = content.strip()
-        
+
         # Count output tokens using tiktoken
         output_tokens = count_tokens_openai(content, model_name)
-        
+
         # Calculate cost
-        cost = (
-            (input_tokens / 1_000_000) * PRICING[model_name]["input"] +
-            (output_tokens / 1_000_000) * PRICING[model_name]["output"]
-        )
-        
+        cost = (input_tokens / 1_000_000) * PRICING[model_name]["input"] + (output_tokens / 1_000_000) * PRICING[model_name]["output"]
+
         latency = time.time() - start_time
-        
+
         # Parse JSON response
         result = json.loads(content)
-        
+
         return {
             "tier": result.get("tier"),
             "category": result.get("category"),
@@ -464,7 +452,7 @@
             "output_tokens": output_tokens,
             "raw_response": content,
         }
-    
+
     except json.JSONDecodeError as e:
         logger.error(f"Failed to parse OpenAI response as JSON: {e}, response: {content[:200]}")
         return {
@@ -476,7 +464,7 @@
             "error": f"JSON decode error: {e}",
             "input_tokens": input_tokens,
             "output_tokens": 0,
-            "raw_response": content[:200] if 'content' in locals() else "",
+            "raw_response": content[:200] if "content" in locals() else "",
         }
     except Exception as e:
         logger.error(f"Failed to classify with OpenAI: {e}", exc_info=True)
@@ -501,28 +489,23 @@
     api_key = secrets_manager.get_secret("GEMINI_API_KEY") or secrets_manager.get_secret("GOOGLE_API_KEY")
     if not api_key:
         raise ValueError("Google/Gemini API key not configured (need GEMINI_API_KEY or GOOGLE_API_KEY)")
-    
+
     if model_name not in PRICING:
         raise ValueError(f"Unknown model: {model_name}. Available: {list(PRICING.keys())}")
-    
-    llm = ChatGoogleGenerativeAI(
-        model=model_name,
-        temperature=0.1,
-        max_tokens=200,
-        google_api_key=api_key
-    )
-    
+
+    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.1, max_tokens=200, google_api_key=api_key)
+
     # Use prompt from data_process_prompts.py with company_name
     prompt = get_base_classification_prompt(finding_text, company_name)
-    
+
     # Count input tokens (Gemini estimation)
     input_tokens = count_tokens_gemini(prompt)
-    
+
     start_time = time.time()
     try:
         response = await llm.ainvoke([("user", prompt)])
         latency = time.time() - start_time
-        
+
         # Parse response
         content = response.content.strip()
         # Remove markdown code blocks if present
@@ -533,18 +516,15 @@
         if content.endswith("```"):
             content = content[:-3]
         content = content.strip()
-        
+
         # Count output tokens (Gemini estimation)
         output_tokens = count_tokens_gemini(content)
-        
+
         result = json.loads(content)
-        
+
         # Calculate cost using actual token counts
-        cost = (
-            (input_tokens / 1_000_000) * PRICING[model_name]["input"] +
-            (output_tokens / 1_000_000) * PRICING[model_name]["output"]
-        )
-        
+        cost = (input_tokens / 1_000_000) * PRICING[model_name]["input"] + (output_tokens / 1_000_000) * PRICING[model_name]["output"]
+
         return {
             "tier": result.get("tier"),
             "category": result.get("category"),
@@ -553,7 +533,7 @@
             "cost": cost,
             "input_tokens": input_tokens,
             "output_tokens": output_tokens,
-            "raw_response": content
+            "raw_response": content,
         }
     except json.JSONDecodeError as e:
         latency = time.time() - start_time
@@ -566,8 +546,8 @@
             "cost": (input_tokens / 1_000_000) * PRICING[model_name]["input"],
             "input_tokens": input_tokens,
             "output_tokens": 0,
-            "raw_response": content[:200] if 'content' in locals() else "",
-            "error": f"JSON decode error: {e}"
+            "raw_response": content[:200] if "content" in locals() else "",
+            "error": f"JSON decode error: {e}",
         }
     except Exception as e:
         latency = time.time() - start_time
@@ -581,81 +561,75 @@
             "input_tokens": input_tokens,
             "output_tokens": 0,
             "raw_response": "",
-            "error": str(e)
+            "error": str(e),
         }
 
 
 async def benchmark_model(model_name: str, model_id: str, test_cases: List[Dict]) -> Dict[str, Any]:
     """Benchmark a model on test cases."""
-    logger.info(f"\n{'='*80}")
+    logger.info(f"\n{'=' * 80}")
     logger.info(f"Benchmarking {model_name} ({model_id})")
-    logger.info(f"{'='*80}")
-    
+    logger.info(f"{'=' * 80}")
+
     results = []
     total_cost = 0.0
     total_latency = 0.0
     correct_tier = 0
     correct_category = 0
-    
+
     for i, test_case in enumerate(test_cases, 1):
         logger.info(f"\n[{i}/{len(test_cases)}] {test_case['name']}")
         logger.info(f"Finding: {test_case['finding_text'][:100]}...")
-        
+
         # Determine which classification function to use based on model
         company_name = "Schroders plc"  # Default company for test cases
         if model_id.startswith("gpt-"):
-            result = await classify_with_openai(test_case['finding_text'], model_id, company_name)
+            result = await classify_with_openai(test_case["finding_text"], model_id, company_name)
         else:
-            result = await classify_with_gemini(test_case['finding_text'], model_id, company_name)
-        results.append({
-            "test_case": test_case,
-            "result": result
-        })
-        
+            result = await classify_with_gemini(test_case["finding_text"], model_id, company_name)
+        results.append({"test_case": test_case, "result": result})
+
         total_cost += result["cost"]
         total_latency += result["latency"]
-        
+
         # Check accuracy
         tier_match = result["tier"] == test_case["expected_tier"]
         category_match = result["category"] == test_case.get("expected_category")
-        
+
         if tier_match:
             correct_tier += 1
         if category_match:
             correct_category += 1
-        
+
         # Check if confidence threshold would filter this result
         meets_threshold = result.get("confidence", 0.0) >= CONFIDENCE_THRESHOLD
         threshold_status = "‚úÖ PASS" if meets_threshold else "‚ùå BELOW"
-        
+
         logger.info(f"  Tier: {result['tier']} (expected: {test_case['expected_tier']}) {'‚úÖ' if tier_match else '‚ùå'}")
         logger.info(f"  Category: {result['category']} {'‚úÖ' if category_match else '‚ùå'}")
         logger.info(f"  Confidence: {result['confidence']:.2f} (threshold: {CONFIDENCE_THRESHOLD}) {threshold_status}")
         logger.info(f"  Latency: {result['latency']:.3f}s")
         logger.info(f"  Cost: ${result['cost']:.6f}")
-        
+
         # Small delay to avoid rate limits
         await asyncio.sleep(0.5)
-    
+
     avg_latency = total_latency / len(test_cases)
     avg_cost = total_cost / len(test_cases)
     tier_accuracy = correct_tier / len(test_cases)
     category_accuracy = correct_category / len(test_cases)
-    
+
     # Calculate accuracy with confidence threshold
     correct_with_threshold = sum(
-        1 for r in results
-        if r["result"].get("tier") == r["test_case"]["expected_tier"]
-        and r["result"].get("confidence", 0.0) >= CONFIDENCE_THRESHOLD
+        1
+        for r in results
+        if r["result"].get("tier") == r["test_case"]["expected_tier"] and r["result"].get("confidence", 0.0) >= CONFIDENCE_THRESHOLD
     )
     tier_accuracy_with_threshold = correct_with_threshold / len(test_cases)
-    
+
     # Count how many would be filtered by threshold
-    filtered_by_threshold = sum(
-        1 for r in results
-        if r["result"].get("confidence", 0.0) < CONFIDENCE_THRESHOLD
-    )
-    
+    filtered_by_threshold = sum(1 for r in results if r["result"].get("confidence", 0.0) < CONFIDENCE_THRESHOLD)
+
     return {
         "model": model_name,
         "total_cost": total_cost,
@@ -667,19 +641,19 @@
         "category_accuracy": category_accuracy,
         "filtered_by_threshold": filtered_by_threshold,
         "confidence_threshold": CONFIDENCE_THRESHOLD,
-        "results": results
+        "results": results,
     }
 
 
 async def main():
     """Run benchmark for multiple Gemini models."""
-    logger.info("="*80)
+    logger.info("=" * 80)
     logger.info("LLM TIER CLASSIFICATION BENCHMARK - Multiple Gemini Models")
-    logger.info("="*80)
+    logger.info("=" * 80)
     logger.info(f"\nTest cases: {len(TEST_CASES)}")
     logger.info(f"Confidence threshold: {CONFIDENCE_THRESHOLD}")
     logger.info(f"Models: {', '.join(PRICING.keys())}")
-    
+
     # Benchmark all models
     all_results = {}
     models_to_test = [
@@ -690,72 +664,68 @@
         ("GPT-4o Mini", "gpt-4o-mini"),
         ("GPT-4.1 Mini", "gpt-4.1-mini"),
     ]
-    
+
     for model_name, model_id in models_to_test:
         all_results[model_id] = await benchmark_model(model_name, model_id, TEST_CASES)
-    
+
     # Summary comparison
-    logger.info("\n" + "="*80)
+    logger.info("\n" + "=" * 80)
     logger.info("BENCHMARK SUMMARY - COMPARISON")
-    logger.info("="*80)
-    
+    logger.info("=" * 80)
+
     logger.info(f"\n{'Model':<30} {'Tier Acc':<12} {'Cat Acc':<12} {'Avg Cost':<15} {'Avg Latency':<15}")
     logger.info("-" * 80)
     for model_id, results in all_results.items():
         logger.info(
             f"{model_id:<30} "
-            f"{results['tier_accuracy']*100:>6.1f}%    "
-            f"{results['category_accuracy']*100:>6.1f}%    "
+            f"{results['tier_accuracy'] * 100:>6.1f}%    "
+            f"{results['category_accuracy'] * 100:>6.1f}%    "
             f"${results['avg_cost_per_finding']:>8.6f}    "
             f"{results['avg_latency']:>6.3f}s"
         )
-    
+
     # Detailed metrics for each model
     for model_id, results in all_results.items():
-        logger.info(f"\n{'='*80}")
+        logger.info(f"\n{'=' * 80}")
         logger.info(f"Detailed Metrics: {model_id}")
-        logger.info(f"{'='*80}")
-        
+        logger.info(f"{'=' * 80}")
+
         logger.info(f"\nPerformance Metrics:")
-        logger.info(f"  Tier accuracy (all): {results['tier_accuracy']*100:.1f}%")
+        logger.info(f"  Tier accuracy (all): {results['tier_accuracy'] * 100:.1f}%")
         if CONFIDENCE_THRESHOLD > 0.0:
-            logger.info(f"  Tier accuracy (confidence >= {CONFIDENCE_THRESHOLD}): {results['tier_accuracy_with_threshold']*100:.1f}%")
+            logger.info(f"  Tier accuracy (confidence >= {CONFIDENCE_THRESHOLD}): {results['tier_accuracy_with_threshold'] * 100:.1f}%")
             logger.info(f"  Findings filtered by threshold: {results['filtered_by_threshold']}/{len(TEST_CASES)}")
-        logger.info(f"  Category accuracy: {results['category_accuracy']*100:.1f}%")
-        
+        logger.info(f"  Category accuracy: {results['category_accuracy'] * 100:.1f}%")
+
         logger.info(f"\nCost Metrics:")
         logger.info(f"  Average cost per finding: ${results['avg_cost_per_finding']:.6f}")
         logger.info(f"  Total cost ({len(TEST_CASES)} findings): ${results['total_cost']:.6f}")
         logger.info(f"  Estimated monthly cost (1000 findings/day): ${results['avg_cost_per_finding'] * 30000:.2f}")
-        
+
         logger.info(f"\nLatency Metrics:")
         logger.info(f"  Average latency per finding: {results['avg_latency']:.3f}s")
         logger.info(f"  Total latency ({len(TEST_CASES)} findings): {results['total_latency']:.2f}s")
         logger.info(f"  Estimated throughput: {60 / results['avg_latency']:.1f} findings/minute")
-        
+
         # Confidence analysis
         confidences = [r["result"].get("confidence", 0.0) for r in results["results"]]
         if confidences:
             logger.info(f"\nConfidence Analysis:")
             logger.info(f"  Min confidence: {min(confidences):.2f}")
             logger.info(f"  Max confidence: {max(confidences):.2f}")
-            logger.info(f"  Avg confidence: {sum(confidences)/len(confidences):.2f}")
+            logger.info(f"  Avg confidence: {sum(confidences) / len(confidences):.2f}")
             above_threshold = sum(1 for c in confidences if c >= CONFIDENCE_THRESHOLD)
-            logger.info(f"  Above threshold ({CONFIDENCE_THRESHOLD}): {above_threshold}/{len(confidences)} ({above_threshold/len(confidences)*100:.1f}%)")
-    
+            logger.info(
+                f"  Above threshold ({CONFIDENCE_THRESHOLD}): {above_threshold}/{len(confidences)} ({above_threshold / len(confidences) * 100:.1f}%)"
+            )
+
     # Save detailed results
     output_file = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
-    with open(output_file, 'w') as f:
-        json.dump({
-            "models": all_results,
-            "test_cases": TEST_CASES,
-            "confidence_threshold": CONFIDENCE_THRESHOLD,
-            "pricing": PRICING
-        }, f, indent=2)
-    
+    with open(output_file, "w") as f:
+        json.dump({"models": all_results, "test_cases": TEST_CASES, "confidence_threshold": CONFIDENCE_THRESHOLD, "pricing": PRICING}, f, indent=2)
+
     logger.info(f"\nDetailed results saved to: {output_file}")
 
 
 if __name__ == "__main__":
     asyncio.run(main())
-

--- scripts/generate_complete_schema_migration.py
+++ scripts/generate_complete_schema_migration.py
@@ -32,7 +32,8 @@
 
 # Load .env file FIRST
 from dotenv import load_dotenv
-env_path = Path(__file__).parent.parent / '.env'
+
+env_path = Path(__file__).parent.parent / ".env"
 load_dotenv(env_path)
 
 # Disable AWS Secrets Manager for local development
@@ -108,12 +109,13 @@
         WHERE schemaname = 'public'
         ORDER BY tablename
     """)
-    return [row['tablename'] for row in rows]
+    return [row["tablename"] for row in rows]
 
 
 async def get_table_columns(conn: asyncpg.Connection, table_name: str) -> List[dict]:
     """Get all columns for a table."""
-    rows = await conn.fetch("""
+    rows = await conn.fetch(
+        """
         SELECT
             a.attname AS column_name,
             pg_catalog.format_type(a.atttypid, a.atttypmod) AS data_type,
@@ -129,7 +131,9 @@
           AND a.attnum > 0
           AND NOT a.attisdropped
         ORDER BY a.attnum
-    """, table_name)
+    """,
+        table_name,
+    )
     return [dict(row) for row in rows]
 
 
@@ -175,7 +179,7 @@
           AND NOT idx.indisprimary
         ORDER BY rel.relname, indrel.relname
     """)
-    return [row['index_def'] for row in rows]
+    return [row["index_def"] for row in rows]
 
 
 def quote_ident(identifier: str) -> str:
@@ -193,7 +197,7 @@
         f"MINVALUE {seq['min_value']}",
         f"MAXVALUE {seq['max_value']}",
         f"CACHE {seq['cache_size']}",
-        "CYCLE" if seq['cycle'] else "NO CYCLE"
+        "CYCLE" if seq["cycle"] else "NO CYCLE",
     ]
     return " ".join(parts) + ";"
 
@@ -203,15 +207,14 @@
     col_defs = []
     for col in columns:
         col_def = f"    {quote_ident(col['column_name'])} {col['data_type']}"
-        if col['not_null']:
+        if col["not_null"]:
             col_def += " NOT NULL"
-        if col['default_value']:
+        if col["default_value"]:
             col_def += f" DEFAULT {col['default_value']}"
         col_defs.append(col_def)
-    
-    return f"CREATE TABLE IF NOT EXISTS {quote_ident(table_name)} (\n" + \
-           ",\n".join(col_defs) + "\n);"
 
+    return f"CREATE TABLE IF NOT EXISTS {quote_ident(table_name)} (\n" + ",\n".join(col_defs) + "\n);"
+
 
 async def generate_migration(
     output_file: Optional[str] = None,
@@ -219,16 +222,16 @@
     port: Optional[int] = None,
     database: Optional[str] = None,
     user: Optional[str] = None,
-    password: Optional[str] = None
+    password: Optional[str] = None,
 ) -> str:
     """Generate complete schema migration file."""
-    
+
     db_host = host or get_postgres_host()
     db_port = port or get_postgres_port()
     db_name = database or get_postgres_db()
     db_user = user or get_postgres_user()
     db_password = password or get_postgres_password()
-    
+
     print("=" * 60)
     print("Database Schema Migration Generator")
     print("=" * 60)
@@ -238,26 +241,20 @@
     print(f"User: {db_user}")
     print("=" * 60)
     print()
-    
-    conn = await asyncpg.connect(
-        host=db_host,
-        port=db_port,
-        database=db_name,
-        user=db_user,
-        password=db_password
-    )
-    
+
+    conn = await asyncpg.connect(host=db_host, port=db_port, database=db_name, user=db_user, password=db_password)
+
     try:
         # Generate output filename
         if not output_file:
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
             output_file = f"migrations/versions/{timestamp}_complete_schema.sql"
-        
+
         output_path = Path(output_file)
         output_path.parent.mkdir(parents=True, exist_ok=True)
-        
+
         print("Extracting schema components...")
-        
+
         # Extract all schema components
         extensions = await get_extensions(conn)
         sequences = await get_sequences(conn)
@@ -265,65 +262,73 @@
         tables = await get_tables(conn)
         constraints = await get_constraints(conn)
         indexes = await get_indexes(conn)
-        
+
         print(f"  ‚úì {len(extensions)} extensions")
         print(f"  ‚úì {len(sequences)} sequences")
         print(f"  ‚úì {len(tables)} tables")
         print(f"  ‚úì {len(constraints)} constraints")
         print(f"  ‚úì {len(indexes)} indexes")
         print()
-        
+
         # Build SQL content
         sql_lines = []
-        
+
         # Header
-        sql_lines.extend([
-            "-- =====================================================",
-            "-- COMPLETE DATABASE SCHEMA MIGRATION",
-            "-- =====================================================",
-            f"-- Generated: {datetime.now().isoformat()}",
-            f"-- Source Database: {db_name}",
-            f"-- Host: {db_host}:{db_port}",
-            "-- Schema: public",
-            "-- =====================================================",
-            "",
-            "-- This script is idempotent - safe to run multiple times.",
-            "-- All CREATE statements use IF NOT EXISTS.",
-            "",
-            "BEGIN;",
-            "",
-        ])
-        
-        # Extensions
-        if extensions:
-            sql_lines.extend([
+        sql_lines.extend(
+            [
+                "-- =====================================================",
+                "-- COMPLETE DATABASE SCHEMA MIGRATION",
                 "-- =====================================================",
-                "-- SECTION 1: EXTENSIONS",
+                f"-- Generated: {datetime.now().isoformat()}",
+                f"-- Source Database: {db_name}",
+                f"-- Host: {db_host}:{db_port}",
+                "-- Schema: public",
                 "-- =====================================================",
                 "",
-            ])
+                "-- This script is idempotent - safe to run multiple times.",
+                "-- All CREATE statements use IF NOT EXISTS.",
+                "",
+                "BEGIN;",
+                "",
+            ]
+        )
+
+        # Extensions
+        if extensions:
+            sql_lines.extend(
+                [
+                    "-- =====================================================",
+                    "-- SECTION 1: EXTENSIONS",
+                    "-- =====================================================",
+                    "",
+                ]
+            )
             for ext in extensions:
                 sql_lines.append(f"CREATE EXTENSION IF NOT EXISTS {ext['extname']};")
             sql_lines.append("")
-        
+
         # Sequences
         if sequences:
-            sql_lines.extend([
-                "-- =====================================================",
-                "-- SECTION 2: SEQUENCES",
-                "-- =====================================================",
-                "",
-            ])
+            sql_lines.extend(
+                [
+                    "-- =====================================================",
+                    "-- SECTION 2: SEQUENCES",
+                    "-- =====================================================",
+                    "",
+                ]
+            )
             for seq in sequences:
                 sql_lines.append(format_sequence(seq))
             sql_lines.append("")
-            
+
             # Sequence ownership
             if sequence_ownership:
-                sql_lines.extend([
-                    "-- Sequence ownership",
-                    "",
-                ])
+                sql_lines.extend(
+                    [
+                        "-- Sequence ownership",
+                        "",
+                    ]
+                )
                 for ownership in sequence_ownership:
                     sql_lines.append(
                         f"ALTER SEQUENCE {quote_ident(ownership['sequence_name'])} "
@@ -331,48 +336,47 @@
                         f"{quote_ident(ownership['column_name'])};"
                     )
                 sql_lines.append("")
-        
+
         # Tables
-        sql_lines.extend([
-            "-- =====================================================",
-            "-- SECTION 3: TABLES",
-            "-- =====================================================",
-            "",
-        ])
+        sql_lines.extend(
+            [
+                "-- =====================================================",
+                "-- SECTION 3: TABLES",
+                "-- =====================================================",
+                "",
+            ]
+        )
         for table in tables:
             columns = await get_table_columns(conn, table)
             table_def = format_table_definition(table, columns)
             sql_lines.append(f"-- Table: {table}")
             sql_lines.append(table_def)
             sql_lines.append("")
-        
+
         # Constraints (grouped by type)
-        constraint_types = {
-            'p': ('PRIMARY KEYS', []),
-            'u': ('UNIQUE CONSTRAINTS', []),
-            'c': ('CHECK CONSTRAINTS', []),
-            'f': ('FOREIGN KEYS', [])
-        }
-        
+        constraint_types = {"p": ("PRIMARY KEYS", []), "u": ("UNIQUE CONSTRAINTS", []), "c": ("CHECK CONSTRAINTS", []), "f": ("FOREIGN KEYS", [])}
+
         for constraint in constraints:
-            const_type = constraint['constraint_type']
+            const_type = constraint["constraint_type"]
             # Handle bytes type (PostgreSQL returns constraint_type as bytes)
             if isinstance(const_type, bytes):
-                const_type = const_type.decode('utf-8')
+                const_type = const_type.decode("utf-8")
             if const_type in constraint_types:
                 constraint_types[const_type][1].append(constraint)
-        
+
         # Write constraint sections
         section_num = 4
-        for const_type in ['p', 'u', 'c', 'f']:  # Process in order
+        for const_type in ["p", "u", "c", "f"]:  # Process in order
             section_name, const_list = constraint_types[const_type]
             if const_list:
-                sql_lines.extend([
-                    f"-- =====================================================",
-                    f"-- SECTION {section_num}: {section_name}",
-                    f"-- =====================================================",
-                    "",
-                ])
+                sql_lines.extend(
+                    [
+                        f"-- =====================================================",
+                        f"-- SECTION {section_num}: {section_name}",
+                        f"-- =====================================================",
+                        "",
+                    ]
+                )
                 for constraint in const_list:
                     sql_lines.append(
                         f"ALTER TABLE {quote_ident(constraint['table_name'])} "
@@ -381,15 +385,17 @@
                     )
                 sql_lines.append("")
                 section_num += 1
-        
+
         # Indexes
         if indexes:
-            sql_lines.extend([
-                f"-- =====================================================",
-                f"-- SECTION {section_num}: INDEXES",
-                f"-- =====================================================",
-                "",
-            ])
+            sql_lines.extend(
+                [
+                    f"-- =====================================================",
+                    f"-- SECTION {section_num}: INDEXES",
+                    f"-- =====================================================",
+                    "",
+                ]
+            )
             for index_def in indexes:
                 # Replace CREATE INDEX with CREATE INDEX IF NOT EXISTS
                 if index_def.startswith("CREATE INDEX"):
@@ -398,20 +404,22 @@
                     index_def = index_def.replace("CREATE UNIQUE INDEX", "CREATE UNIQUE INDEX IF NOT EXISTS", 1)
                 sql_lines.append(index_def)
             sql_lines.append("")
-        
+
         # Footer
-        sql_lines.extend([
-            "COMMIT;",
-            "",
-            "-- =====================================================",
-            "-- END OF SCHEMA MIGRATION",
-            "-- =====================================================",
-        ])
-        
+        sql_lines.extend(
+            [
+                "COMMIT;",
+                "",
+                "-- =====================================================",
+                "-- END OF SCHEMA MIGRATION",
+                "-- =====================================================",
+            ]
+        )
+
         # Write to file
         sql_content = "\n".join(sql_lines)
-        output_path.write_text(sql_content, encoding='utf-8')
-        
+        output_path.write_text(sql_content, encoding="utf-8")
+
         print(f"‚úÖ Schema migration written to: {output_path}")
         print(f"   File size: {len(sql_content)} bytes")
         print()
@@ -420,69 +428,38 @@
         print()
         print("Or using Docker:")
         print(f"  docker exec -i <postgres_container> psql -U <user> -d <database> < {output_path}")
-        
+
         return str(output_path)
-        
+
     finally:
         await conn.close()
 
 
 async def main():
     """Main entry point."""
-    parser = argparse.ArgumentParser(
-        description="Generate a complete database schema migration from PostgreSQL database"
-    )
-    parser.add_argument(
-        "--output", "-o",
-        type=str,
-        help="Output file path (default: migrations/versions/YYYYMMDD_HHMMSS_complete_schema.sql)"
-    )
-    parser.add_argument(
-        "--host",
-        type=str,
-        help="PostgreSQL host (default: from config)"
-    )
-    parser.add_argument(
-        "--port",
-        type=int,
-        help="PostgreSQL port (default: from config)"
-    )
-    parser.add_argument(
-        "--database", "-d",
-        type=str,
-        help="PostgreSQL database name (default: from config)"
-    )
-    parser.add_argument(
-        "--user", "-u",
-        type=str,
-        help="PostgreSQL user (default: from config)"
-    )
-    parser.add_argument(
-        "--password", "-p",
-        type=str,
-        help="PostgreSQL password (default: from config)"
-    )
-    
+    parser = argparse.ArgumentParser(description="Generate a complete database schema migration from PostgreSQL database")
+    parser.add_argument("--output", "-o", type=str, help="Output file path (default: migrations/versions/YYYYMMDD_HHMMSS_complete_schema.sql)")
+    parser.add_argument("--host", type=str, help="PostgreSQL host (default: from config)")
+    parser.add_argument("--port", type=int, help="PostgreSQL port (default: from config)")
+    parser.add_argument("--database", "-d", type=str, help="PostgreSQL database name (default: from config)")
+    parser.add_argument("--user", "-u", type=str, help="PostgreSQL user (default: from config)")
+    parser.add_argument("--password", "-p", type=str, help="PostgreSQL password (default: from config)")
+
     args = parser.parse_args()
-    
+
     try:
         output_file = await generate_migration(
-            output_file=args.output,
-            host=args.host,
-            port=args.port,
-            database=args.database,
-            user=args.user,
-            password=args.password
+            output_file=args.output, host=args.host, port=args.port, database=args.database, user=args.user, password=args.password
         )
         print(f"\n‚úÖ Success! Migration file: {output_file}")
-        
+
     except Exception as e:
         print(f"\n‚ùå Error: {e}", file=sys.stderr)
         import traceback
+
         traceback.print_exc()
         sys.exit(1)
 
 
 if __name__ == "__main__":
     asyncio.run(main())
-

--- scripts/test-advanced-functionality.py
+++ scripts/test-advanced-functionality.py
@@ -29,9 +29,10 @@
 from typing import Dict, Any, Optional
 from datetime import datetime
 
+
 class AdvancedAppRunnerTester:
     def __init__(self, app_url: str, admin_email: str, admin_password: str, test_user_email: str):
-        self.app_url = app_url.rstrip('/')
+        self.app_url = app_url.rstrip("/")
         self.admin_email = admin_email
         self.admin_password = admin_password
         self.test_user_email = test_user_email
@@ -45,18 +46,12 @@
 
         if self.user_jwt_token:
             self.session.headers.update({"Authorization": f"Bearer {self.user_jwt_token}"})
-        
+
     def log_test(self, test_name: str, success: bool, details: str = "", response_data: Any = None):
         """Log test results with details."""
-        result = {
-            "test": test_name,
-            "success": success,
-            "timestamp": datetime.now().isoformat(),
-            "details": details,
-            "response_data": response_data
-        }
+        result = {"test": test_name, "success": success, "timestamp": datetime.now().isoformat(), "details": details, "response_data": response_data}
         self.test_results.append(result)
-        
+
         status = "‚úÖ PASS" if success else "‚ùå FAIL"
         print(f"{status} {test_name}")
         if details:
@@ -68,33 +63,20 @@
     def test_admin_pre_approval(self):
         """Test admin pre-approval of test user."""
         print("üîç Testing Admin Pre-Approval...")
-        
+
         if not self.admin_jwt_token:
             self.log_test("Admin Pre-Approval", False, "Admin authentication required first")
             return
-            
+
         try:
             headers = {"Authorization": f"Bearer {self.admin_jwt_token}"}
-            response = self.session.post(
-                f"{self.app_url}/api/admin/approved-users",
-                json={"email": self.test_user_email},
-                headers=headers
-            )
-            
+            response = self.session.post(f"{self.app_url}/api/admin/approved-users", json={"email": self.test_user_email}, headers=headers)
+
             if response.status_code == 200 or response.status_code == 409:  # 409 if already exists
-                self.log_test(
-                    "Admin Pre-Approval",
-                    True,
-                    f"Test user pre-approved ({response.status_code})"
-                )
+                self.log_test("Admin Pre-Approval", True, f"Test user pre-approved ({response.status_code})")
             else:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
-                self.log_test(
-                    "Admin Pre-Approval",
-                    False,
-                    f"Failed to pre-approve user: {response.status_code}",
-                    data
-                )
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
+                self.log_test("Admin Pre-Approval", False, f"Failed to pre-approve user: {response.status_code}", data)
         except Exception as e:
             self.log_test("Admin Pre-Approval", False, f"Request failed: {e}")
 
@@ -167,209 +149,142 @@
     def test_session_management(self):
         """Test session start functionality."""
         print("üîç Testing Session Management...")
-        
+
         if not self.user_jwt_token:
             self.log_test("Session Management", False, "User authentication required first")
             return
-            
+
         try:
             headers = {"Authorization": f"Bearer {self.user_jwt_token}"}
             response = self.session.post(f"{self.app_url}/api/prospecting/session/start", headers=headers)
-            
+
             if response.status_code == 200:
                 data = response.json()
                 if "session_id" in data:
                     self.session_id = data["session_id"]
-                    self.log_test(
-                        "Session Management",
-                        True,
-                        f"Session started successfully: {self.session_id}"
-                    )
+                    self.log_test("Session Management", True, f"Session started successfully: {self.session_id}")
                 else:
-                    self.log_test(
-                        "Session Management",
-                        True,
-                        "Session started but no session_id in response"
-                    )
+                    self.log_test("Session Management", True, "Session started but no session_id in response")
             else:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
-                self.log_test(
-                    "Session Management",
-                    False,
-                    f"Session start failed: {response.status_code}",
-                    data
-                )
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
+                self.log_test("Session Management", False, f"Session start failed: {response.status_code}", data)
         except Exception as e:
             self.log_test("Session Management", False, f"Request failed: {e}")
 
     def test_basic_prospecting_workflow(self):
         """Test basic prospecting workflow initiation."""
         print("üîç Testing Basic Prospecting Workflow...")
-        
+
         if not self.user_jwt_token:
             self.log_test("Basic Prospecting Workflow", False, "User authentication required first")
             return
-            
+
         try:
             headers = {"Authorization": f"Bearer {self.user_jwt_token}"}
             response = self.session.post(
-                f"{self.app_url}/api/prospecting/process/start",
-                json={"prompt": "Research Sequoia Capital"},
-                headers=headers
+                f"{self.app_url}/api/prospecting/process/start", json={"prompt": "Research Sequoia Capital"}, headers=headers
             )
-            
+
             if response.status_code == 200:
                 data = response.json()
                 if "run_id" in data:
                     self.current_run_id = data["run_id"]
-                    self.log_test(
-                        "Basic Prospecting Workflow",
-                        True,
-                        f"Workflow started successfully: {self.current_run_id}"
-                    )
+                    self.log_test("Basic Prospecting Workflow", True, f"Workflow started successfully: {self.current_run_id}")
                 else:
-                    self.log_test(
-                        "Basic Prospecting Workflow",
-                        True,
-                        "Workflow started but no run_id in response"
-                    )
+                    self.log_test("Basic Prospecting Workflow", True, "Workflow started but no run_id in response")
             elif response.status_code == 429:
-                self.log_test(
-                    "Basic Prospecting Workflow",
-                    True,
-                    "Rate limit hit (expected for repeated tests)"
-                )
+                self.log_test("Basic Prospecting Workflow", True, "Rate limit hit (expected for repeated tests)")
             else:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
-                self.log_test(
-                    "Basic Prospecting Workflow",
-                    False,
-                    f"Workflow start failed: {response.status_code}",
-                    data
-                )
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
+                self.log_test("Basic Prospecting Workflow", False, f"Workflow start failed: {response.status_code}", data)
         except Exception as e:
             self.log_test("Basic Prospecting Workflow", False, f"Request failed: {e}")
 
     def test_workflow_status_polling(self):
         """Test workflow status polling."""
         print("üîç Testing Workflow Status Polling...")
-        
+
         if not self.user_jwt_token or not self.current_run_id:
             self.log_test("Workflow Status Polling", False, "Run ID required first")
             return
-            
+
         try:
             headers = {"Authorization": f"Bearer {self.user_jwt_token}"}
-            response = self.session.get(
-                f"{self.app_url}/api/prospecting/process/status/{self.current_run_id}",
-                headers=headers
-            )
-            
+            response = self.session.get(f"{self.app_url}/api/prospecting/process/status/{self.current_run_id}", headers=headers)
+
             if response.status_code == 200:
                 data = response.json()
                 status = data.get("status", "unknown")
-                self.log_test(
-                    "Workflow Status Polling",
-                    True,
-                    f"Status retrieved successfully: {status}"
-                )
+                self.log_test("Workflow Status Polling", True, f"Status retrieved successfully: {status}")
             else:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
-                self.log_test(
-                    "Workflow Status Polling",
-                    False,
-                    f"Status polling failed: {response.status_code}",
-                    data
-                )
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
+                self.log_test("Workflow Status Polling", False, f"Status polling failed: {response.status_code}", data)
         except Exception as e:
             self.log_test("Workflow Status Polling", False, f"Request failed: {e}")
 
     def test_profile_management(self):
         """Test user profile retrieval and update."""
         print("üîç Testing Profile Management...")
-        
+
         if not self.user_jwt_token:
             self.log_test("Profile Management", False, "User authentication required first")
             return
-            
+
         try:
             headers = {"Authorization": f"Bearer {self.user_jwt_token}"}
-            
+
             # Test profile retrieval
             response = self.session.get(f"{self.app_url}/api/auth/profile", headers=headers)
-            
+
             if response.status_code == 200:
                 data = response.json()
-                self.log_test(
-                    "Profile Management (GET)",
-                    True,
-                    "Profile retrieved successfully"
-                )
-                
+                self.log_test("Profile Management (GET)", True, "Profile retrieved successfully")
+
                 # Test profile update
                 update_data = {
                     "firm_description": "Updated test investment firm description",
                     "key_differentiators": "Updated test differentiators",
-                    "key_objectives": "Updated test objectives"
+                    "key_objectives": "Updated test objectives",
                 }
-                
+
                 update_response = self.session.put(f"{self.app_url}/api/auth/profile", json=update_data, headers=headers)
-                
+
                 if update_response.status_code == 200:
-                    self.log_test(
-                        "Profile Management (PUT)",
-                        True,
-                        "Profile updated successfully"
-                    )
+                    self.log_test("Profile Management (PUT)", True, "Profile updated successfully")
                 else:
-                    update_data_resp = update_response.json() if update_response.headers.get('content-type', '').startswith('application/json') else update_response.text
-                    self.log_test(
-                        "Profile Management (PUT)",
-                        False,
-                        f"Profile update failed: {update_response.status_code}",
-                        update_data_resp
+                    update_data_resp = (
+                        update_response.json()
+                        if update_response.headers.get("content-type", "").startswith("application/json")
+                        else update_response.text
                     )
+                    self.log_test("Profile Management (PUT)", False, f"Profile update failed: {update_response.status_code}", update_data_resp)
             else:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
-                self.log_test(
-                    "Profile Management (GET)",
-                    False,
-                    f"Profile retrieval failed: {response.status_code}",
-                    data
-                )
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
+                self.log_test("Profile Management (GET)", False, f"Profile retrieval failed: {response.status_code}", data)
         except Exception as e:
             self.log_test("Profile Management", False, f"Request failed: {e}")
 
     def test_admin_functionality(self):
         """Test admin-specific functionality."""
         print("üîç Testing Admin Functionality...")
-        
+
         if not self.admin_jwt_token:
             self.log_test("Admin Functionality", False, "Admin authentication required first")
             return
-            
+
         try:
             headers = {"Authorization": f"Bearer {self.admin_jwt_token}"}
-            
+
             # Test approved users list
             response = self.session.get(f"{self.app_url}/api/admin/approved-users", headers=headers)
-            
+
             if response.status_code == 200:
                 data = response.json()
                 approved_count = len(data) if isinstance(data, list) else 0
-                self.log_test(
-                    "Admin Functionality (Approved Users)",
-                    True,
-                    f"Retrieved approved users list ({approved_count} users)"
-                )
+                self.log_test("Admin Functionality (Approved Users)", True, f"Retrieved approved users list ({approved_count} users)")
             else:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
-                self.log_test(
-                    "Admin Functionality (Approved Users)",
-                    False,
-                    f"Failed to retrieve approved users: {response.status_code}",
-                    data
-                )
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
+                self.log_test("Admin Functionality (Approved Users)", False, f"Failed to retrieve approved users: {response.status_code}", data)
         except Exception as e:
             self.log_test("Admin Functionality", False, f"Request failed: {e}")
 
@@ -377,7 +292,7 @@
         """Run all advanced tests in sequence."""
         print(f"üöÄ Starting Advanced App Runner Tests for: {self.app_url}")
         print("=" * 80)
-        
+
         # Run tests in logical order
         self.test_admin_authentication()
         self.test_admin_pre_approval()
@@ -388,29 +303,29 @@
         self.test_basic_prospecting_workflow()
         self.test_workflow_status_polling()
         self.test_admin_functionality()
-        
+
         print("=" * 80)
         print("üìä Advanced Test Summary")
         print("=" * 80)
-        
+
         passed = sum(1 for result in self.test_results if result["success"])
         total = len(self.test_results)
-        
+
         print(f"Tests Passed: {passed}/{total}")
-        print(f"Success Rate: {(passed/total)*100:.1f}%")
-        
+        print(f"Success Rate: {(passed / total) * 100:.1f}%")
+
         if passed == total:
             print("\nüéâ All advanced tests passed! Your deployment is fully functional.")
         else:
-            print(f"\n‚ö†Ô∏è  {total-passed} test(s) failed. Check the details above.")
-            
+            print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed. Check the details above.")
+
         # Show failed tests
         failed_tests = [result for result in self.test_results if not result["success"]]
         if failed_tests:
             print("\n‚ùå Failed Tests:")
             for test in failed_tests:
                 print(f"   - {test['test']}: {test['details']}")
-        
+
         # Show successful authentications
         if self.admin_jwt_token:
             print(f"\nüîë Admin JWT obtained: {self.admin_jwt_token[:20]}...")
@@ -420,22 +335,24 @@
             print(f"üì± Session ID: {self.session_id}")
         if self.current_run_id:
             print(f"üèÉ Current Run ID: {self.current_run_id}")
-        
+
         return passed == total
 
+
 def main():
     parser = argparse.ArgumentParser(description="Test Ardessa Agent App Runner advanced functionality")
     parser.add_argument("--app-url", required=True, help="App Runner URL (e.g., https://abc123.region.awsapprunner.com)")
     parser.add_argument("--admin-email", required=True, help="Admin email for testing")
     parser.add_argument("--admin-password", required=True, help="Admin password for testing")
     parser.add_argument("--test-user-email", required=True, help="Test user email for registration testing")
-    
+
     args = parser.parse_args()
-    
+
     tester = AdvancedAppRunnerTester(args.app_url, args.admin_email, args.admin_password, args.test_user_email)
     success = tester.run_all_tests()
-    
+
     exit(0 if success else 1)
 
+
 if __name__ == "__main__":
-    main() 
+    main()

--- scripts/test-app-runner-deployment.py
+++ scripts/test-app-runner-deployment.py
@@ -25,27 +25,22 @@
 from typing import Dict, Any, Optional
 from datetime import datetime
 
+
 class AppRunnerTester:
     def __init__(self, app_url: str):
-        self.app_url = app_url.rstrip('/')
+        self.app_url = app_url.rstrip("/")
         self.session = requests.Session()
         self.test_results = []
         self.session_id = None
         self.clerk_test_jwt = os.getenv("CLERK_TEST_JWT")
         if self.clerk_test_jwt:
             self.session.headers.update({"Authorization": f"Bearer {self.clerk_test_jwt}"})
-        
+
     def log_test(self, test_name: str, success: bool, details: str = "", response_data: Any = None):
         """Log test results with details."""
-        result = {
-            "test": test_name,
-            "success": success,
-            "timestamp": datetime.now().isoformat(),
-            "details": details,
-            "response_data": response_data
-        }
+        result = {"test": test_name, "success": success, "timestamp": datetime.now().isoformat(), "details": details, "response_data": response_data}
         self.test_results.append(result)
-        
+
         status = "‚úÖ PASS" if success else "‚ùå FAIL"
         print(f"{status} {test_name}")
         if details:
@@ -57,17 +52,17 @@
     def test_basic_health(self):
         """Test basic health check endpoints."""
         print("üîç Testing Basic Health & Connectivity...")
-        
+
         # Test root endpoint
         try:
             response = self.session.get(f"{self.app_url}/")
             success = response.status_code == 200
-            data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
+            data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
             self.log_test(
                 "Root Endpoint Health Check",
                 success,
                 f"Status: {response.status_code}" + (f", Response time: {response.elapsed.total_seconds():.2f}s" if success else ""),
-                data if not success else None
+                data if not success else None,
             )
         except Exception as e:
             self.log_test("Root Endpoint Health Check", False, f"Connection failed: {e}")
@@ -76,12 +71,12 @@
         try:
             response = self.session.get(f"{self.app_url}/health")
             success = response.status_code == 200
-            data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
+            data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
             self.log_test(
                 "Health Endpoint Check",
                 success,
                 f"Status: {response.status_code}" + (f", Response time: {response.elapsed.total_seconds():.2f}s" if success else ""),
-                data if not success else None
+                data if not success else None,
             )
         except Exception as e:
             self.log_test("Health Endpoint Check", False, f"Connection failed: {e}")
@@ -89,33 +84,27 @@
     def test_database_connectivity(self):
         """Test database connectivity through API endpoints."""
         print("üîç Testing Database Connectivity...")
-        
+
         # Test an endpoint that requires database (approved users list - should require admin auth)
         try:
             response = self.session.get(f"{self.app_url}/api/admin/approved-users")
-            
+
             # We expect 401 (unauthorized) which means the endpoint is working but needs auth
             # If we get 500, it likely means database connection issues
             if response.status_code == 401:
                 self.log_test(
-                    "Database Connectivity (via Admin Endpoint)",
-                    True,
-                    "Endpoint accessible (401 Unauthorized as expected - needs admin auth)"
+                    "Database Connectivity (via Admin Endpoint)", True, "Endpoint accessible (401 Unauthorized as expected - needs admin auth)"
                 )
             elif response.status_code == 500:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
                 self.log_test(
                     "Database Connectivity (via Admin Endpoint)",
                     False,
                     f"Server error (likely database connection issue): {response.status_code}",
-                    data
+                    data,
                 )
             else:
-                self.log_test(
-                    "Database Connectivity (via Admin Endpoint)",
-                    True,
-                    f"Unexpected but successful response: {response.status_code}"
-                )
+                self.log_test("Database Connectivity (via Admin Endpoint)", True, f"Unexpected but successful response: {response.status_code}")
         except Exception as e:
             self.log_test("Database Connectivity (via Admin Endpoint)", False, f"Connection failed: {e}")
 
@@ -158,117 +147,77 @@
     def test_prospecting_endpoints(self):
         """Test prospecting endpoint accessibility."""
         print("üîç Testing Prospecting Endpoints...")
-        
+
         # Test session start endpoint (should require auth)
         try:
             response = self.session.post(f"{self.app_url}/api/prospecting/session/start")
-            
+
             if response.status_code == 401:
-                self.log_test(
-                    "Prospecting Session Endpoint",
-                    True,
-                    "Endpoint accessible (401 Unauthorized as expected)"
-                )
+                self.log_test("Prospecting Session Endpoint", True, "Endpoint accessible (401 Unauthorized as expected)")
             elif response.status_code == 500:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
-                self.log_test(
-                    "Prospecting Session Endpoint",
-                    False,
-                    f"Server error: {response.status_code}",
-                    data
-                )
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
+                self.log_test("Prospecting Session Endpoint", False, f"Server error: {response.status_code}", data)
             else:
-                self.log_test(
-                    "Prospecting Session Endpoint",
-                    True,
-                    f"Unexpected response (but endpoint working): {response.status_code}"
-                )
+                self.log_test("Prospecting Session Endpoint", True, f"Unexpected response (but endpoint working): {response.status_code}")
         except Exception as e:
             self.log_test("Prospecting Session Endpoint", False, f"Connection failed: {e}")
 
     def test_cors_and_security_headers(self):
         """Test CORS configuration and security headers."""
         print("üîç Testing CORS and Security Headers...")
-        
+
         try:
             response = self.session.get(f"{self.app_url}/health")
-            
+
             # Check for security headers
             headers_to_check = [
                 ("Strict-Transport-Security", "HSTS header"),
                 ("X-Content-Type-Options", "Content type options header"),
                 ("X-Frame-Options", "Frame options header"),
-                ("Referrer-Policy", "Referrer policy header")
+                ("Referrer-Policy", "Referrer policy header"),
             ]
-            
+
             missing_headers = []
             for header, description in headers_to_check:
                 if header not in response.headers:
                     missing_headers.append(description)
-            
+
             if not missing_headers:
-                self.log_test(
-                    "Security Headers",
-                    True,
-                    "All security headers present"
-                )
+                self.log_test("Security Headers", True, "All security headers present")
             else:
-                self.log_test(
-                    "Security Headers",
-                    False,
-                    f"Missing headers: {', '.join(missing_headers)}"
-                )
-                
+                self.log_test("Security Headers", False, f"Missing headers: {', '.join(missing_headers)}")
+
         except Exception as e:
             self.log_test("Security Headers", False, f"Connection failed: {e}")
 
     def test_environment_configuration(self):
         """Test that environment variables and secrets are properly configured."""
         print("üîç Testing Environment Configuration...")
-        
+
         # Test an endpoint that would use external APIs (should fail gracefully with proper error)
         try:
-            response = self.session.post(f"{self.app_url}/api/prospecting/process/start", json={
-                "prompt": "test prompt"
-            })
-            
+            response = self.session.post(f"{self.app_url}/api/prospecting/process/start", json={"prompt": "test prompt"})
+
             # We expect 401 (unauthorized) since we're not authenticated
             # If we get other errors, it might indicate configuration issues
             if response.status_code == 401:
-                self.log_test(
-                    "Environment Configuration (API Access)",
-                    True,
-                    "Endpoint properly requires authentication"
-                )
+                self.log_test("Environment Configuration (API Access)", True, "Endpoint properly requires authentication")
             elif response.status_code == 500:
-                data = response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
+                data = response.json() if response.headers.get("content-type", "").startswith("application/json") else response.text
                 error_msg = str(data)
-                
+
                 # Check for common configuration error patterns
-                config_errors = [
-                    "secret", "api", "key", "connection", "timeout", "configuration"
-                ]
-                
+                config_errors = ["secret", "api", "key", "connection", "timeout", "configuration"]
+
                 if any(error in error_msg.lower() for error in config_errors):
-                    self.log_test(
-                        "Environment Configuration (API Access)",
-                        False,
-                        f"Possible configuration error: {response.status_code}",
-                        data
-                    )
+                    self.log_test("Environment Configuration (API Access)", False, f"Possible configuration error: {response.status_code}", data)
                 else:
                     self.log_test(
-                        "Environment Configuration (API Access)",
-                        True,
-                        f"Server error but not configuration-related: {response.status_code}"
+                        "Environment Configuration (API Access)", True, f"Server error but not configuration-related: {response.status_code}"
                     )
             else:
-                self.log_test(
-                    "Environment Configuration (API Access)",
-                    True,
-                    f"Unexpected response (endpoint working): {response.status_code}"
-                )
-                
+                self.log_test("Environment Configuration (API Access)", True, f"Unexpected response (endpoint working): {response.status_code}")
+
         except Exception as e:
             self.log_test("Environment Configuration (API Access)", False, f"Connection failed: {e}")
 
@@ -276,48 +225,50 @@
         """Run all tests in sequence."""
         print(f"üöÄ Starting App Runner Deployment Tests for: {self.app_url}")
         print("=" * 80)
-        
+
         self.test_basic_health()
-        self.test_database_connectivity() 
+        self.test_database_connectivity()
         self.test_authentication_endpoints()
         self.test_prospecting_endpoints()
         self.test_cors_and_security_headers()
         self.test_environment_configuration()
-        
+
         print("=" * 80)
         print("üìä Test Summary")
         print("=" * 80)
-        
+
         passed = sum(1 for result in self.test_results if result["success"])
         total = len(self.test_results)
-        
+
         print(f"Tests Passed: {passed}/{total}")
-        print(f"Success Rate: {(passed/total)*100:.1f}%")
-        
+        print(f"Success Rate: {(passed / total) * 100:.1f}%")
+
         if passed == total:
             print("\nüéâ All tests passed! Your App Runner deployment looks good.")
         else:
-            print(f"\n‚ö†Ô∏è  {total-passed} test(s) failed. Check the details above.")
-            
+            print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed. Check the details above.")
+
         # Show failed tests
         failed_tests = [result for result in self.test_results if not result["success"]]
         if failed_tests:
             print("\n‚ùå Failed Tests:")
             for test in failed_tests:
                 print(f"   - {test['test']}: {test['details']}")
-        
+
         return passed == total
 
+
 def main():
     parser = argparse.ArgumentParser(description="Test Ardessa Agent App Runner deployment")
     parser.add_argument("--app-url", required=True, help="App Runner URL (e.g., https://abc123.region.awsapprunner.com)")
-    
+
     args = parser.parse_args()
-    
+
     tester = AppRunnerTester(args.app_url)
     success = tester.run_all_tests()
-    
+
     exit(0 if success else 1)
 
+
 if __name__ == "__main__":
-    main() 
+    main()

--- scripts/view_tier_findings.py
+++ scripts/view_tier_findings.py
@@ -15,6 +15,7 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 from dotenv import load_dotenv
+
 load_dotenv()
 
 from app.utils.global_db import get_global_db
@@ -32,42 +33,42 @@
             finding_data = json.loads(finding_data)
         except (json.JSONDecodeError, TypeError):
             return finding_data
-    
+
     if not isinstance(finding_data, dict):
         return str(finding_data)
-    
-    if radar_type in ['company_mentions', 'company_social_posts', 'company_social_posts_cxo']:
-        content = finding_data.get('mention_content') or finding_data.get('post_content', 'N/A')
-        author = finding_data.get('author', {})
+
+    if radar_type in ["company_mentions", "company_social_posts", "company_social_posts_cxo"]:
+        content = finding_data.get("mention_content") or finding_data.get("post_content", "N/A")
+        author = finding_data.get("author", {})
         if isinstance(author, dict):
             author_info = []
             # Build comprehensive author info
-            if author.get('name'):
+            if author.get("name"):
                 author_info.append(f"Name: {author['name']}")
-            elif author.get('first_name') or author.get('last_name'):
-                name_parts = [author.get('first_name', ''), author.get('last_name', '')]
+            elif author.get("first_name") or author.get("last_name"):
+                name_parts = [author.get("first_name", ""), author.get("last_name", "")]
                 author_info.append(f"Name: {' '.join(filter(None, name_parts))}")
-            
-            if author.get('profile_type'):
+
+            if author.get("profile_type"):
                 author_info.append(f"Type: {author['profile_type']}")
-            if author.get('title'):
+            if author.get("title"):
                 author_info.append(f"Title: {author['title']}")
-            if author.get('company_name'):
+            if author.get("company_name"):
                 author_info.append(f"Company: {author['company_name']}")
-            if author.get('company_domain'):
+            if author.get("company_domain"):
                 author_info.append(f"Domain: {author['company_domain']}")
-            if author.get('country'):
+            if author.get("country"):
                 author_info.append(f"Country: {author['country']}")
-            if author.get('profile_url'):
+            if author.get("profile_url"):
                 author_info.append(f"Profile: {author['profile_url']}")
-            
+
             if author_info:
                 return f"Author Info:\n{chr(10).join('  ' + info for info in author_info)}\n\nContent:\n{content}"
         return content
-    elif radar_type == 'company_new_hires':
-        title = finding_data.get('title', 'N/A')
-        full_name = finding_data.get('full_name', 'N/A')
-        department = finding_data.get('department', 'N/A')
+    elif radar_type == "company_new_hires":
+        title = finding_data.get("title", "N/A")
+        full_name = finding_data.get("full_name", "N/A")
+        department = finding_data.get("department", "N/A")
         return f"Title: {title}\nName: {full_name}\nDepartment: {department}"
     else:
         return json.dumps(finding_data, indent=2)
@@ -75,21 +76,21 @@
 
 async def view_tier_findings(user_email: str = None):
     """Display all tier findings with full details.
-    
+
     Args:
         user_email: Optional email address to filter findings for a specific user
     """
-    print("\n" + "="*80)
+    print("\n" + "=" * 80)
     print("TIER FINDINGS - FULL DETAILS")
     if user_email:
         print(f"Filtered for user: {user_email}")
-    print("="*80 + "\n")
-    
+    print("=" * 80 + "\n")
+
     db = await get_global_db()
     if not db:
         print("‚ùå Database not available")
         return 1
-    
+
     async with db.pool.acquire() as conn:
         # Build query with optional email filter
         query = """
@@ -114,39 +115,39 @@
         if user_email:
             query += " AND u.email = $1"
             params.append(user_email)
-        
+
         query += " ORDER BY uf.priority_tier, uf.relevance_score DESC"
-        
+
         findings = await conn.fetch(query, *params)
-        
+
         if not findings:
             print("No findings with tier classifications found.")
             return 0
-        
+
         print(f"Found {len(findings)} user-finding pairs with tier classifications\n")
-        
+
         current_tier = None
         for i, row in enumerate(findings, 1):
-            tier = row['priority_tier']
-            confidence = row['relevance_score']
-            category = row['category'] or 'N/A'
-            radar_type = row['radar_type']
-            finding_data = row['finding_data']
-            discovered_at = row['discovered_at']
-            finding_id = row['finding_id']
-            user_email = row['user_email']
-            company_name = row['company_name']
-            domain = row['domain']
-            
+            tier = row["priority_tier"]
+            confidence = row["relevance_score"]
+            category = row["category"] or "N/A"
+            radar_type = row["radar_type"]
+            finding_data = row["finding_data"]
+            discovered_at = row["discovered_at"]
+            finding_id = row["finding_id"]
+            user_email = row["user_email"]
+            company_name = row["company_name"]
+            domain = row["domain"]
+
             # Print tier header when tier changes
             if current_tier != tier:
                 if current_tier is not None:
-                    print("\n" + "="*80 + "\n")
-                print(f"\n{'='*80}")
+                    print("\n" + "=" * 80 + "\n")
+                print(f"\n{'=' * 80}")
                 print(f"TIER {tier} FINDINGS")
-                print(f"{'='*80}\n")
+                print(f"{'=' * 80}\n")
                 current_tier = tier
-            
+
             print(f"[{i}] Finding ID: {finding_id}")
             print(f"    User: {user_email}")
             print(f"    Company: {company_name} ({domain})")
@@ -156,30 +157,31 @@
             print(f"    Confidence: {confidence:.3f}")
             print(f"    Discovered: {discovered_at}")
             print(f"\n    Content:")
-            print("    " + "-"*76)
-            
+            print("    " + "-" * 76)
+
             # Format and display content
             content = format_finding_content(finding_data, radar_type)
-            for line in content.split('\n'):
+            for line in content.split("\n"):
                 print(f"    {line}")
-            
-            print("    " + "-"*76)
+
+            print("    " + "-" * 76)
             print()
-        
-        print("\n" + "="*80)
+
+        print("\n" + "=" * 80)
         print(f"Total: {len(findings)} user-finding pairs across all tiers")
-        print("="*80 + "\n")
-        
+        print("=" * 80 + "\n")
+
         return 0
 
 
 async def main():
     """Main entry point."""
     import argparse
-    parser = argparse.ArgumentParser(description='View tier findings with full details')
-    parser.add_argument('--email', type=str, help='Filter findings for a specific user email')
+
+    parser = argparse.ArgumentParser(description="View tier findings with full details")
+    parser.add_argument("--email", type=str, help="Filter findings for a specific user email")
     args = parser.parse_args()
-    
+
     try:
         exit_code = await view_tier_findings(user_email=args.email)
         sys.exit(exit_code)
@@ -189,10 +191,10 @@
     except Exception as e:
         print(f"\n‚ùå Fatal error: {e}")
         import traceback
+
         traceback.print_exc()
         sys.exit(1)
 
 
 if __name__ == "__main__":
     asyncio.run(main())
-

--- services/radar_webhook_fanout/lambda_function.py
+++ services/radar_webhook_fanout/lambda_function.py
@@ -40,7 +40,7 @@
 def validate_tamradar_secret(event: Dict[str, Any]) -> bool:
     """
     Validate TAMradar webhook secret from query parameters or headers.
-    
+
     TAMradar sends secret in query parameter: ?secret=xxx
     Or in header: X-TAMradar-Secret: xxx
     """
@@ -48,26 +48,26 @@
     query_params = event.get("queryStringParameters") or {}
     if query_params.get("secret") == TAMRADAR_WEBHOOK_SECRET:
         return True
-    
+
     # Check headers
     headers = event.get("headers") or {}
     # API Gateway lowercases header names
     secret_header = headers.get("x-tamradar-secret") or headers.get("X-TAMradar-Secret")
     if secret_header == TAMRADAR_WEBHOOK_SECRET:
         return True
-    
+
     return False
 
 
 def get_backend_urls() -> List[str]:
     """
     Get list of backend URLs to fan-out to.
-    
+
     Returns:
         List of internal webhook endpoint URLs
     """
     urls = []
-    
+
     # Always include dev, stage, prod if configured
     if DEV_BACKEND_URL:
         urls.append(f"{DEV_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
@@ -75,7 +75,7 @@
         urls.append(f"{STAGE_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
     if PROD_BACKEND_URL:
         urls.append(f"{PROD_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
-    
+
     # Include local if enabled and configured
     if ENABLE_LOCAL_ROUTING and LOCAL_BACKEND_URL:
         # Local URL might already include the path, so check
@@ -83,21 +83,21 @@
             urls.append(LOCAL_BACKEND_URL)
         else:
             urls.append(f"{LOCAL_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
-    
+
     return urls
 
 
 def send_to_backend(url: str, payload: Dict[str, Any], timeout: float) -> Dict[str, Any]:
     """
     Send webhook payload to a single backend environment.
-    
+
     Uses synchronous httpx client for Lambda compatibility.
-    
+
     Args:
         url: Backend internal webhook URL
         payload: Webhook payload to send
         timeout: Request timeout in seconds
-    
+
     Returns:
         Dict with status, url, and any error message
     """
@@ -106,11 +106,11 @@
         parsed_url = urlparse(url)
         query_params = parse_qs(parsed_url.query)
         query_params["secret"] = [INTERNAL_WEBHOOK_SECRET]
-        
+
         # Reconstruct URL with secret
         new_query = "&".join([f"{k}={v[0]}" for k, v in query_params.items()])
         url_with_secret = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"
-        
+
         # Use synchronous client for Lambda
         with httpx.Client(timeout=timeout) as client:
             response = client.post(
@@ -118,7 +118,7 @@
                 json=payload,
                 headers={"Content-Type": "application/json"},
             )
-            
+
             return {
                 "url": url,
                 "status": response.status_code,
@@ -144,7 +144,7 @@
 def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
     """
     Lambda handler function.
-    
+
     Event structure (from API Gateway):
     {
         "httpMethod": "POST",
@@ -155,7 +155,7 @@
     }
     """
     logger.info("Webhook fan-out function invoked", extra={"event": event})
-    
+
     # 1. Validate TAMradar secret
     if not validate_tamradar_secret(event):
         logger.warning("Invalid TAMradar secret", extra={"event": event})
@@ -164,7 +164,7 @@
             "headers": {"Content-Type": "application/json"},
             "body": json.dumps({"error": "Invalid webhook secret"}),
         }
-    
+
     # 2. Parse request body
     try:
         body = event.get("body", "{}")
@@ -179,19 +179,17 @@
             "headers": {"Content-Type": "application/json"},
             "body": json.dumps({"error": "Invalid JSON payload"}),
         }
-    
+
     # 3. Extract radar_id for logging
     radar_id = payload.get("data", {}).get("radar_id", "unknown")
     event_id = payload.get("event_id", "unknown")
     event_type = payload.get("event_type", "unknown")
-    
-    logger.info(
-        f"Processing webhook: event_id={event_id}, radar_id={radar_id}, type={event_type}"
-    )
-    
+
+    logger.info(f"Processing webhook: event_id={event_id}, radar_id={radar_id}, type={event_type}")
+
     # 4. Get list of backend URLs to fan-out to
     backend_urls = get_backend_urls()
-    
+
     if not backend_urls:
         logger.error("No backend URLs configured")
         return {
@@ -199,55 +197,48 @@
             "headers": {"Content-Type": "application/json"},
             "body": json.dumps({"error": "No backend URLs configured"}),
         }
-    
+
     logger.info(f"Fanning out to {len(backend_urls)} backends: {backend_urls}")
-    
+
     # 5. Fan-out to all backends in parallel (fire-and-forget)
     # Use ThreadPoolExecutor for parallel requests since Lambda doesn't handle async well
     results = []
     with ThreadPoolExecutor(max_workers=len(backend_urls)) as executor:
         # Submit all requests
-        future_to_url = {
-            executor.submit(send_to_backend, url, payload, HTTP_TIMEOUT): url
-            for url in backend_urls
-        }
-        
+        future_to_url = {executor.submit(send_to_backend, url, payload, HTTP_TIMEOUT): url for url in backend_urls}
+
         # Collect results as they complete
         for future in as_completed(future_to_url):
             url = future_to_url[future]
             try:
                 result = future.result()
                 results.append(result)
-                
+
                 if result["success"]:
-                    logger.info(
-                        f"Successfully sent to {result['url']}: HTTP {result['status']}"
-                    )
+                    logger.info(f"Successfully sent to {result['url']}: HTTP {result['status']}")
                 else:
-                    logger.warning(
-                        f"Failed to send to {result['url']}: {result.get('error', 'Unknown error')}"
-                    )
+                    logger.warning(f"Failed to send to {result['url']}: {result.get('error', 'Unknown error')}")
             except Exception as e:
-                logger.error(
-                    f"Error sending to {url}: {e}",
-                    exc_info=True
+                logger.error(f"Error sending to {url}: {e}", exc_info=True)
+                results.append(
+                    {
+                        "url": url,
+                        "status": None,
+                        "success": False,
+                        "error": str(e),
+                    }
                 )
-                results.append({
-                    "url": url,
-                    "status": None,
-                    "success": False,
-                    "error": str(e),
-                })
-    
+
     # 6. Return 200 to TAMradar immediately (don't wait for all backends)
     # This ensures TAMradar doesn't retry if one backend is slow/down
     return {
         "statusCode": 200,
         "headers": {"Content-Type": "application/json"},
-        "body": json.dumps({
-            "status": "ok",
-            "message": "Webhook received and fanned out",
-            "backends_contacted": len(backend_urls),
-        }),
+        "body": json.dumps(
+            {
+                "status": "ok",
+                "message": "Webhook received and fanned out",
+                "backends_contacted": len(backend_urls),
+            }
+        ),
     }
-

--- services/sec_ingestion/build_columns_from_schema.py
+++ services/sec_ingestion/build_columns_from_schema.py
@@ -32,13 +32,14 @@
 
 Usage:
     python services/sec_ingestion/build_columns_from_schema.py [options]
-    
+
     To generate all files:
     python services/sec_ingestion/build_columns_from_schema.py --emit-schema --emit-trend --emit-fe-mapping
 
 This tool helps maintain consistency between SEC data schemas and processing
 code, ensuring reliable data handling across the ingestion pipeline.
 """
+
 import argparse
 from pathlib import Path
 import sys
@@ -76,40 +77,28 @@
     if len(df) < 3:
         print("[ERROR] CSV must have at least 3 rows to include 'Display Column' row")
         return 1
-    
+
     display_column_row = df.iloc[2]  # Row 3 (index 2) contains Display Column values
-    
+
     # Write narrow columns.csv: column_name,include,alias,canonical_name
     output_csv.parent.mkdir(parents=True, exist_ok=True)
-    
+
     # Build the data for CSV writing
     csv_data = []
     for col in df.columns:
         display_value = str(display_column_row[col]).strip()
         # Include columns where Display Column is not null/empty
-        if display_value and display_value.lower() not in {'', 'nan', 'none'}:
-            csv_data.append({
-                'column_name': col,
-                'include': 'yes',
-                'alias': '',
-                'canonical_name': col
-            })
-    
+        if display_value and display_value.lower() not in {"", "nan", "none"}:
+            csv_data.append({"column_name": col, "include": "yes", "alias": "", "canonical_name": col})
+
     # Add post-processed columns that are generated during ingestion
-    post_processed_columns = [
-        {
-            'column_name': 'domain',
-            'include': 'yes',
-            'alias': '',
-            'canonical_name': 'domain'
-        }
-    ]
+    post_processed_columns = [{"column_name": "domain", "include": "yes", "alias": "", "canonical_name": "domain"}]
     csv_data.extend(post_processed_columns)
-    
+
     # Write using pandas to handle proper CSV escaping
     if csv_data:
         csv_df = pd.DataFrame(csv_data)
-        csv_df.to_csv(output_csv, index=False, encoding='utf-8')
+        csv_df.to_csv(output_csv, index=False, encoding="utf-8")
     else:
         # Create empty CSV with header if no data
         with open(output_csv, "w", encoding="utf-8") as f:
@@ -134,14 +123,14 @@
 
     # Find rows by their labels in the first column
     labels_raw = df.iloc[:, 0].astype(str).str.strip()
-    
+
     # Find the 'Display Column' row (for inclusion)
     display_col_mask = labels_raw.str.lower() == "display column"
     if not display_col_mask.any():
         print("[ERROR] Could not find 'Display Column' row to build schema.")
         return 1
     display_col_row = df[display_col_mask].iloc[0]
-    
+
     # Try to find the 'Column description' row (optional)
     desc_mask = labels_raw.str.lower() == "column description"
     desc_row = None
@@ -155,14 +144,14 @@
     for col in df.columns[1:]:
         # Check if this column should be included (non-null Display Column value)
         display_col_name = str(display_col_row[col]).strip()
-        if not display_col_name or display_col_name.lower() in {'', 'nan', 'none'}:
+        if not display_col_name or display_col_name.lower() in {"", "nan", "none"}:
             continue
-            
+
         # Get the description (if available)
         desc = ""
         if desc_row is not None:
             desc = str(desc_row[col]).strip()
-        
+
         field = {
             "canonical_name": display_col_name,
             "source_header": display_col_name,
@@ -172,11 +161,7 @@
 
     # Add post-processed columns to schema
     post_processed_fields = [
-        {
-            "canonical_name": "domain",
-            "source_header": "domain",
-            "description": "Extracted domain name from website URL (e.g., 'example.com')."
-        }
+        {"canonical_name": "domain", "source_header": "domain", "description": "Extracted domain name from website URL (e.g., 'example.com')."}
     ]
     fields.extend(post_processed_fields)
 
@@ -214,14 +199,14 @@
 
     # Find the 'Display Column' and 'Historical View (Yes)' rows by label
     labels_raw = df.iloc[:, 0].astype(str).str.strip()
-    
+
     # Get Display Column row (contains the actual column names)
     display_col_mask = labels_raw.str.lower() == "display column"
     if not display_col_mask.any():
         print("[ERROR] Could not find 'Display Column' row to build trend columns.")
         return 1
     display_col_row = df[display_col_mask].iloc[0]
-    
+
     # Get Historical View (Yes) row (determines which columns to include)
     historical_view_mask = labels_raw.str.lower() == "historical view (yes)"
     if not historical_view_mask.any():
@@ -233,14 +218,16 @@
     for col in df.columns:
         # Check if this column should be included in trends
         hv = str(historical_view_row[col]).strip().lower()
-        if hv == 'yes':
+        if hv == "yes":
             # Use the column name from Display Column row
             display_col_name = str(display_col_row[col]).strip()
-            if display_col_name and display_col_name.lower() not in {'', 'nan', 'none'}:
-                cols.append({
-                    "canonical_name": display_col_name,
-                    "source_header": display_col_name,
-                })
+            if display_col_name and display_col_name.lower() not in {"", "nan", "none"}:
+                cols.append(
+                    {
+                        "canonical_name": display_col_name,
+                        "source_header": display_col_name,
+                    }
+                )
 
     data = {
         "version": "v1",
@@ -276,14 +263,14 @@
 
     # Find the 'Display Column' and 'FE Display Name' rows by label
     labels_raw = df.iloc[:, 0].astype(str).str.strip()
-    
+
     # Get Display Column row (contains the column names)
     display_col_mask = labels_raw.str.lower() == "display column"
     if not display_col_mask.any():
         print("[ERROR] Could not find 'Display Column' row to build FE mapping.")
         return 1
     display_col_row = df[display_col_mask].iloc[0]
-    
+
     # Get FE Display Name row (contains the frontend display names)
     fe_display_mask = labels_raw.str.lower() == "fe display name"
     if not fe_display_mask.any():
@@ -297,16 +284,18 @@
         display_col_name = str(display_col_row[col]).strip()
         # Get the frontend display name from FE Display Name row
         fe_display_name = str(fe_display_row[col]).strip()
-        
+
         # Only include columns where both Display Column and FE Display Name are not null/empty
-        if (display_col_name and display_col_name.lower() not in {'', 'nan', 'none'} and
-            fe_display_name and fe_display_name.lower() not in {'', 'nan', 'none'}):
+        if (
+            display_col_name
+            and display_col_name.lower() not in {"", "nan", "none"}
+            and fe_display_name
+            and fe_display_name.lower() not in {"", "nan", "none"}
+        ):
             mappings[display_col_name] = fe_display_name
 
     # Add post-processed columns to FE mapping
-    post_processed_mappings = {
-        "domain": "Domain"
-    }
+    post_processed_mappings = {"domain": "Domain"}
     mappings.update(post_processed_mappings)
 
     data = {
@@ -324,42 +313,54 @@
 def main(argv: list[str] | None = None) -> int:
     p = argparse.ArgumentParser(description="Build columns.csv and optional schema/trend/FE mapping JSON from ria_master_schema.csv")
     # Default to the ria_master_schema.csv file
-    p.add_argument("--input", type=str, default=str(Path(__file__).resolve().parent / "config" / "ria_master_schema.csv"), help="Path to ria_master_schema.csv")
+    p.add_argument(
+        "--input", type=str, default=str(Path(__file__).resolve().parent / "config" / "ria_master_schema.csv"), help="Path to ria_master_schema.csv"
+    )
     p.add_argument("--output", type=str, default=str(Path(__file__).resolve().parent / "config" / "columns.csv"), help="Output columns.csv path")
-    p.add_argument("--schema-output", type=str, default=str(Path(__file__).resolve().parent / "config" / "schema.v1.json"), help="Output schema JSON path")
+    p.add_argument(
+        "--schema-output", type=str, default=str(Path(__file__).resolve().parent / "config" / "schema.v1.json"), help="Output schema JSON path"
+    )
     p.add_argument("--emit-schema", action="store_true", help="Also build schema.v1.json from the wide CSV")
     p.add_argument("--emit-trend", action="store_true", help="Also build trend_columns.v1.json from the 'Historical View (Yes)' row")
-    p.add_argument("--trend-output", type=str, default=str(Path(__file__).resolve().parent / "config" / "trend_columns.v1.json"), help="Output trend columns JSON path")
+    p.add_argument(
+        "--trend-output",
+        type=str,
+        default=str(Path(__file__).resolve().parent / "config" / "trend_columns.v1.json"),
+        help="Output trend columns JSON path",
+    )
     p.add_argument("--trend-horizon-years", type=int, default=5, help="Years of history to precompute for trends")
     p.add_argument("--emit-fe-mapping", action="store_true", help="Also build fe_column_mapping.json from 'Original Header' to 'FE Display Name'")
-    p.add_argument("--fe-mapping-output", type=str, default=str(Path(__file__).resolve().parent / "config" / "fe_column_mapping.json"), help="Output FE column mapping JSON path")
+    p.add_argument(
+        "--fe-mapping-output",
+        type=str,
+        default=str(Path(__file__).resolve().parent / "config" / "fe_column_mapping.json"),
+        help="Output FE column mapping JSON path",
+    )
     args = p.parse_args(argv)
-    
+
     # Always build columns.csv (required)
     rc = build_columns_csv(Path(args.input), Path(args.output))
     if rc != 0:
         return rc
-    
+
     # Optional outputs
     if args.emit_schema:
         s_rc = build_schema_json(Path(args.input), Path(args.schema_output))
         if s_rc != 0:
             return s_rc
-    
+
     if args.emit_trend:
         t_rc = build_trend_columns_json(Path(args.input), Path(args.trend_output), args.trend_horizon_years)
         if t_rc != 0:
             return t_rc
-    
+
     if args.emit_fe_mapping:
         f_rc = build_fe_column_mapping_json(Path(args.input), Path(args.fe_mapping_output))
         if f_rc != 0:
             return f_rc
-    
+
     return 0
 
 
 if __name__ == "__main__":
     raise SystemExit(main())
-
-

--- services/sec_ingestion/compare_headers.py
+++ services/sec_ingestion/compare_headers.py
@@ -18,156 +18,150 @@
 
 def read_schema_headers(schema_csv: Path) -> Tuple[Set[str], Set[str]]:
     """Read Original Header and Display Column values from schema CSV.
-    
+
     Returns:
         Tuple of (original_headers, display_columns) as sets
     """
     if not schema_csv.exists():
         raise FileNotFoundError(f"Schema CSV not found: {schema_csv}")
-    
+
     df = pd.read_csv(schema_csv, low_memory=False, encoding="utf-8-sig")
-    
+
     # Original Headers are the CSV column names themselves
     original_headers = set(df.columns.astype(str))
-    
-    # Row 2: Display Column values  
+
+    # Row 2: Display Column values
     display_row = df.iloc[2]
     display_columns = set()
     for col in df.columns:
         value = str(display_row[col]).strip()
-        if value and value.lower() not in {'', 'nan', 'none'}:
+        if value and value.lower() not in {"", "nan", "none"}:
             display_columns.add(value)
-    
+
     return original_headers, display_columns
 
 
 def read_excel_headers(excel_file: Path) -> Set[str]:
     """Read column headers from Excel file.
-    
+
     Returns:
         Set of column header names
     """
     if not excel_file.exists():
         raise FileNotFoundError(f"Excel file not found: {excel_file}")
-    
+
     # Read only the first row to get headers
     df = pd.read_excel(excel_file, nrows=0)
     headers = set(df.columns.astype(str))
-    
+
     return headers
 
 
 def compare_headers(schema_headers: Set[str], display_columns: Set[str], excel_headers: Set[str]) -> Dict[str, Dict[str, List[str]]]:
     """Compare schema headers with Excel headers.
-    
+
     Returns:
         Dictionary with comparison results
     """
     results = {
-        'original_headers': {
-            'found': [],
-            'missing': [],
-            'total': len(schema_headers)
-        },
-        'display_columns': {
-            'found': [],
-            'missing': [],
-            'total': len(display_columns)
-        }
+        "original_headers": {"found": [], "missing": [], "total": len(schema_headers)},
+        "display_columns": {"found": [], "missing": [], "total": len(display_columns)},
     }
-    
+
     # Check Original Headers
     for header in schema_headers:
         if header in excel_headers:
-            results['original_headers']['found'].append(header)
+            results["original_headers"]["found"].append(header)
         else:
-            results['original_headers']['missing'].append(header)
-    
+            results["original_headers"]["missing"].append(header)
+
     # Check Display Columns
     for column in display_columns:
         if column in excel_headers:
-            results['display_columns']['found'].append(column)
+            results["display_columns"]["found"].append(column)
         else:
-            results['display_columns']['missing'].append(column)
-    
+            results["display_columns"]["missing"].append(column)
+
     return results
 
 
 def print_results(results: Dict[str, Dict[str, List[str]]], excel_file: Path):
     """Print comparison results in a formatted way."""
-    print(f"\n{'='*80}")
+    print(f"\n{'=' * 80}")
     print(f"HEADER COMPARISON RESULTS")
     print(f"Excel File: {excel_file}")
-    print(f"{'='*80}")
-    
+    print(f"{'=' * 80}")
+
     # Original Headers Results
-    orig = results['original_headers']
+    orig = results["original_headers"]
     print(f"\nüìã ORIGINAL HEADERS:")
     print(f"   Total in schema: {orig['total']}")
     print(f"   Found in Excel:  {len(orig['found'])}")
     print(f"   Missing:         {len(orig['missing'])}")
-    print(f"   Match rate:      {len(orig['found'])/orig['total']*100:.1f}%")
-    
-    if orig['found']:
+    print(f"   Match rate:      {len(orig['found']) / orig['total'] * 100:.1f}%")
+
+    if orig["found"]:
         print(f"\n   ‚úÖ FOUND ({len(orig['found'])}):")
-        for header in sorted(orig['found'])[:10]:  # Show first 10
+        for header in sorted(orig["found"])[:10]:  # Show first 10
             print(f"      - {header}")
-        if len(orig['found']) > 10:
+        if len(orig["found"]) > 10:
             print(f"      ... and {len(orig['found']) - 10} more")
-    
-    if orig['missing']:
+
+    if orig["missing"]:
         print(f"\n   ‚ùå MISSING ({len(orig['missing'])}):")
-        for header in sorted(orig['missing'])[:10]:  # Show first 10
+        for header in sorted(orig["missing"])[:10]:  # Show first 10
             print(f"      - {header}")
-        if len(orig['missing']) > 10:
+        if len(orig["missing"]) > 10:
             print(f"      ... and {len(orig['missing']) - 10} more")
-    
+
     # Display Columns Results
-    disp = results['display_columns']
+    disp = results["display_columns"]
     print(f"\nüìä DISPLAY COLUMNS:")
     print(f"   Total in schema: {disp['total']}")
     print(f"   Found in Excel:  {len(disp['found'])}")
     print(f"   Missing:         {len(disp['missing'])}")
-    print(f"   Match rate:      {len(disp['found'])/disp['total']*100:.1f}%")
-    
-    if disp['found']:
+    print(f"   Match rate:      {len(disp['found']) / disp['total'] * 100:.1f}%")
+
+    if disp["found"]:
         print(f"\n   ‚úÖ FOUND ({len(disp['found'])}):")
-        for column in sorted(disp['found'])[:10]:  # Show first 10
+        for column in sorted(disp["found"])[:10]:  # Show first 10
             print(f"      - {column}")
-        if len(disp['found']) > 10:
+        if len(disp["found"]) > 10:
             print(f"      ... and {len(disp['found']) - 10} more")
-    
-    if disp['missing']:
+
+    if disp["missing"]:
         print(f"\n   ‚ùå MISSING ({len(disp['missing'])}):")
-        for column in sorted(disp['missing'])[:10]:  # Show first 10
+        for column in sorted(disp["missing"])[:10]:  # Show first 10
             print(f"      - {column}")
-        if len(disp['missing']) > 10:
+        if len(disp["missing"]) > 10:
             print(f"      ... and {len(disp['missing']) - 10} more")
-    
-    print(f"\n{'='*80}")
 
+    print(f"\n{'=' * 80}")
+
 
 def save_results_to_csv(results: Dict[str, Dict[str, List[str]]], output_file: Path):
     """Save detailed results to CSV files."""
     output_file.parent.mkdir(parents=True, exist_ok=True)
-    
+
     # Save Original Headers results
     orig_file = output_file.parent / f"{output_file.stem}_original_headers.csv"
-    orig_df = pd.DataFrame({
-        'header': results['original_headers']['found'] + results['original_headers']['missing'],
-        'status': ['found'] * len(results['original_headers']['found']) + 
-                 ['missing'] * len(results['original_headers']['missing'])
-    })
+    orig_df = pd.DataFrame(
+        {
+            "header": results["original_headers"]["found"] + results["original_headers"]["missing"],
+            "status": ["found"] * len(results["original_headers"]["found"]) + ["missing"] * len(results["original_headers"]["missing"]),
+        }
+    )
     orig_df.to_csv(orig_file, index=False)
     print(f"üìÑ Original headers results saved to: {orig_file}")
-    
+
     # Save Display Columns results
     disp_file = output_file.parent / f"{output_file.stem}_display_columns.csv"
-    disp_df = pd.DataFrame({
-        'column': results['display_columns']['found'] + results['display_columns']['missing'],
-        'status': ['found'] * len(results['display_columns']['found']) + 
-                 ['missing'] * len(results['display_columns']['missing'])
-    })
+    disp_df = pd.DataFrame(
+        {
+            "column": results["display_columns"]["found"] + results["display_columns"]["missing"],
+            "status": ["found"] * len(results["display_columns"]["found"]) + ["missing"] * len(results["display_columns"]["missing"]),
+        }
+    )
     disp_df.to_csv(disp_file, index=False)
     print(f"üìÑ Display columns results saved to: {disp_file}")
 
@@ -175,19 +169,17 @@
 def main(argv: list[str] | None = None) -> int:
     """Main function."""
     p = argparse.ArgumentParser(description="Compare schema headers with Excel file headers")
-    p.add_argument("--excel-file", type=str, required=True, 
-                   help="Path to Excel file to compare headers against")
-    p.add_argument("--schema-file", type=str, 
-                   default=str(Path(__file__).resolve().parent / "config" / "ria_master_schema.csv"),
-                   help="Path to schema CSV file")
-    p.add_argument("--output", type=str, 
-                   default=str(Path(__file__).resolve().parent / "config" / "header_comparison"),
-                   help="Output file prefix for CSV results")
-    p.add_argument("--save-csv", action="store_true", 
-                   help="Save detailed results to CSV files")
-    
+    p.add_argument("--excel-file", type=str, required=True, help="Path to Excel file to compare headers against")
+    p.add_argument(
+        "--schema-file", type=str, default=str(Path(__file__).resolve().parent / "config" / "ria_master_schema.csv"), help="Path to schema CSV file"
+    )
+    p.add_argument(
+        "--output", type=str, default=str(Path(__file__).resolve().parent / "config" / "header_comparison"), help="Output file prefix for CSV results"
+    )
+    p.add_argument("--save-csv", action="store_true", help="Save detailed results to CSV files")
+
     args = p.parse_args(argv)
-    
+
     try:
         # Read schema headers
         print("üìñ Reading schema headers...")
@@ -195,27 +187,27 @@
         original_headers, display_columns = read_schema_headers(schema_file)
         print(f"   Found {len(original_headers)} original headers")
         print(f"   Found {len(display_columns)} display columns")
-        
+
         # Read Excel headers
         print("üìä Reading Excel headers...")
         excel_file = Path(args.excel_file)
         excel_headers = read_excel_headers(excel_file)
         print(f"   Found {len(excel_headers)} Excel headers")
-        
+
         # Compare headers
         print("üîç Comparing headers...")
         results = compare_headers(original_headers, display_columns, excel_headers)
-        
+
         # Print results
         print_results(results, excel_file)
-        
+
         # Save to CSV if requested
         if args.save_csv:
             output_file = Path(args.output)
             save_results_to_csv(results, output_file)
-        
+
         return 0
-        
+
     except Exception as e:
         print(f"‚ùå Error: {e}")
         return 1

--- services/sec_ingestion/ingest_runner.py
+++ services/sec_ingestion/ingest_runner.py
@@ -17,6 +17,7 @@
 This is the main processing engine used by run_pipeline.py for both backfill
 and incremental data ingestion workflows.
 """
+
 import argparse
 import os
 import sys
@@ -36,6 +37,7 @@
 import json
 import uuid
 import multiprocessing as mp
+
 try:
     import pyarrow as pa  # type: ignore
 except Exception:
@@ -55,7 +57,7 @@
     # First try to find sec_ingestion.py in current directory (new location)
     current_dir = Path(__file__).resolve().parent
     sec_script = current_dir / "sec_ingestion.py"
-    
+
     if not sec_script.exists():
         try:
             # Fallback: try original structure (going up 2 levels)
@@ -64,7 +66,7 @@
         except IndexError:
             # Final fallback: assume we're in Docker container
             sec_script = current_dir / "sec_ingestion.py"
-    
+
     if not sec_script.exists():
         raise FileNotFoundError(f"sec_ingestion.py not found at {sec_script}")
     spec = importlib.util.spec_from_file_location("sec_ingestion_module", str(sec_script))
@@ -72,10 +74,8 @@
     assert spec.loader is not None
     spec.loader.exec_module(mod)
     return mod
-
 
 
-
 def ensure_columns_csv(repo_root: Path) -> Path:
     """Create services/sec_ingestion/config/columns.csv once from a wide mapping CSV if not present.
     Wide mapping source defaults to repo_root/RIA_Master_Sheet_Schema_marked_with_descriptions.csv
@@ -91,21 +91,20 @@
         # Create a minimal default with three common fields
         mod.ensure_dir(config_dir)
         columns_csv.write_text(
-            "# column_name,include,alias,canonical_name\n"
-            "CRD,yes,,crd\nFirm Name,yes,,firm_name\nWebsite,yes,,website\n",
+            "# column_name,include,alias,canonical_name\nCRD,yes,,crd\nFirm Name,yes,,firm_name\nWebsite,yes,,website\n",
             encoding="utf-8",
         )
         return columns_csv
     # Parse wide mapping: first column holds labels; include where row 'Include in data pull?' == 'Yes'
     map_df = pd.read_csv(src_path, low_memory=False)
     labels = map_df.iloc[:, 0].astype(str).str.strip().str.lower()
-    row_yes = map_df[labels == 'include in data pull?']
-    row_head = map_df[labels == 'heading']
+    row_yes = map_df[labels == "include in data pull?"]
+    row_head = map_df[labels == "heading"]
     include_cols: list[tuple[str, str]] = []  # (col_header, canonical)
     for col in map_df.columns[1:]:
-        val = str(row_yes[col].iloc[0]).strip().lower() if not row_yes.empty else ''
-        if val == 'yes':
-            canonical = str(row_head[col].iloc[0]).strip() if not row_head.empty else ''
+        val = str(row_yes[col].iloc[0]).strip().lower() if not row_yes.empty else ""
+        if val == "yes":
+            canonical = str(row_head[col].iloc[0]).strip() if not row_head.empty else ""
             include_cols.append((col, canonical))
     # Write narrow columns.csv
     mod.ensure_dir(config_dir)
@@ -137,22 +136,26 @@
     soup = BeautifulSoup(html, "html.parser")
     # Collect both XLSX and ZIP links for Registered Investment Advisers
     anchors = []
-    for a in soup.find_all('a'):
-        href = (a.get('href') or '').lower()
-        text = (a.get_text() or '').lower()
-        if 'registered investment advisers' in text and (href.endswith('.xlsx') or href.endswith('.zip')):
+    for a in soup.find_all("a"):
+        href = (a.get("href") or "").lower()
+        text = (a.get_text() or "").lower()
+        if "registered investment advisers" in text and (href.endswith(".xlsx") or href.endswith(".zip")):
             anchors.append(a)
     print(f"[INDEX] Discovered {len(anchors)} RIA link(s) (.xlsx or .zip)")
 
     # Month name mapping
-    month_name_to_num = {m.lower(): i for i, m in enumerate([
-        "", "January", "February", "March", "April", "May", "June",
-        "July", "August", "September", "October", "November", "December"
-    ])}
+    month_name_to_num = {
+        m.lower(): i
+        for i, m in enumerate(
+            ["", "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
+        )
+    }
 
     def parse_year_month(text: str, last_modified: str | None) -> tuple[int, int]:
         if text:
-            m1 = re.search(r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+(20\d{2})", text, flags=re.IGNORECASE)
+            m1 = re.search(
+                r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+(20\d{2})", text, flags=re.IGNORECASE
+            )
             if m1:
                 return int(m1.group(2)), month_name_to_num[m1.group(1).lower()]
             m2 = re.search(r"(20\d{2})[-/ ](\d{1,2})", text)
@@ -161,6 +164,7 @@
         if last_modified:
             try:
                 from email.utils import parsedate_to_datetime
+
                 dt = parsedate_to_datetime(last_modified)
                 return dt.year, dt.month
             except Exception:
@@ -195,8 +199,8 @@
             skipped_count += 1
             continue
         # Handle XLSX and ZIP
-        href_lower = (a.get('href') or '').lower()
-        if href_lower.endswith('.xlsx'):
+        href_lower = (a.get("href") or "").lower()
+        if href_lower.endswith(".xlsx"):
             raw_dir = base / "raw" / "index" / f"year={y}" / f"month={m:02d}"
             mod.ensure_dir(raw_dir)
             filename = Path(mod.requests.utils.urlparse(url).path).name or "index.xlsx"  # type: ignore[attr-defined]
@@ -220,8 +224,8 @@
             # Extract .xlsx into raw/index/year=..../month=....
             try:
                 with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:
-                    xlsx_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith('.xlsx')]
-                    csv_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith('.csv')]
+                    xlsx_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith(".xlsx")]
+                    csv_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith(".csv")]
                     member = None
                     if xlsx_members:
                         member = max(xlsx_members, key=lambda z: z.file_size)
@@ -235,7 +239,7 @@
                     mod.ensure_dir(raw_dir)
                     out_xlsx = raw_dir / Path(member.filename).name
                     print(f"[INDEX] Extracting {member.filename} ‚Üí {out_xlsx}")
-                    with zf.open(member) as src, open(out_xlsx, 'wb') as dst:
+                    with zf.open(member) as src, open(out_xlsx, "wb") as dst:
                         dst.write(src.read())
             except Exception as e:
                 print(f"[INDEX] Failed to extract ZIP {zip_name}: {e}")
@@ -270,13 +274,14 @@
                     df_full = pd.read_excel(out_xlsx, engine="openpyxl", dtype=str)
             # Normalize header names to strings
             df_full.columns = [str(c).strip() for c in df_full.columns]
+
             # Determine columns to include using the generated narrow mapping file
             def _load_mapping(mp: Path):
                 include_specs: list[str] = []
                 alias_map_local: dict[str, list[str]] = {}
                 rename_map_local: dict[str, str] = {}
                 try:
-                    with open(mp, newline='', encoding='utf-8') as f:
+                    with open(mp, newline="", encoding="utf-8") as f:
                         rdr = csv.reader(f)
                         header = None
                         # Fixed positions when header is missing
@@ -284,16 +289,16 @@
                         for row in rdr:
                             if not row:
                                 continue
-                            if row[0].lstrip().startswith('#'):
+                            if row[0].lstrip().startswith("#"):
                                 continue
                             if header is None:
                                 header = [c.strip().lower() for c in row]
                                 # Map expected fields or fallback to fixed positions
-                                if 'column_name' in header and 'include' in header:
-                                    i_name = header.index('column_name')
-                                    i_inc = header.index('include')
-                                    i_alias = header.index('alias') if 'alias' in header else None
-                                    i_canon = header.index('canonical_name') if 'canonical_name' in header else None
+                                if "column_name" in header and "include" in header:
+                                    i_name = header.index("column_name")
+                                    i_inc = header.index("include")
+                                    i_alias = header.index("alias") if "alias" in header else None
+                                    i_canon = header.index("canonical_name") if "canonical_name" in header else None
                                     continue
                                 else:
                                     # Treat this row as data; assume fixed order: name, include, alias, canonical_name
@@ -302,18 +307,18 @@
                             # Data row
                             # Pad/truncate to length
                             while len(row) < len(header):
-                                row.append('')
+                                row.append("")
                             col_name = row[i_name].strip()
-                            include = row[i_inc].strip().lower() == 'yes'
-                            aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ''
-                            canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ''
+                            include = row[i_inc].strip().lower() == "yes"
+                            aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ""
+                            canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ""
                             if not col_name or not include:
                                 continue
                             include_specs.append(col_name)
                             if canonical:
                                 rename_map_local[col_name] = canonical
                             if aliases:
-                                alias_map_local[col_name] = [a.strip() for a in aliases.split('|') if a.strip()]
+                                alias_map_local[col_name] = [a.strip() for a in aliases.split("|") if a.strip()]
                 except Exception as e:
                     raise e
                 return include_specs, alias_map_local, rename_map_local
@@ -335,10 +340,12 @@
                     if not found and want in alias_map:
                         for alias in alias_map[want]:
                             if alias in df_full.columns:
-                                found = alias; break
+                                found = alias
+                                break
                             alias_key = alias.strip()
                             if alias_key in normalized_to_actual:
-                                found = normalized_to_actual[alias_key]; break
+                                found = normalized_to_actual[alias_key]
+                                break
                     if found:
                         # Ensure rename maps canonical to the actual column
                         if found != want and want in rename_map:
@@ -347,7 +354,7 @@
                     else:
                         missing_spec.append(want)
                 if missing_spec:
-                    print(f"[INDEX] Mapping columns missing in Excel (ignored): {missing_spec[:10]}{' ‚Ä¶' if len(missing_spec)>10 else ''}")
+                    print(f"[INDEX] Mapping columns missing in Excel (ignored): {missing_spec[:10]}{' ‚Ä¶' if len(missing_spec) > 10 else ''}")
                 keep = resolved_cols if resolved_cols else []
                 if keep:
                     df_curated = df_full[keep]
@@ -367,10 +374,9 @@
                 else:
                     # Use pandas StringDtype for consistent parquet conversion
                     df_curated[c] = df_curated[c].astype("string").str.strip()
-            
+
             # Add domain extraction if Website Address column exists
             if "Website Address" in df_curated.columns:
-                
                 try:
                     df_curated["domain"] = df_curated["Website Address"].apply(extract_domain_from_url)
                     # Convert to string type for consistency
@@ -379,6 +385,7 @@
                 except Exception as e:
                     print(f"[ERROR] Failed to add domain column: {e}")
                     import traceback
+
                     traceback.print_exc()
                     # Continue without domain column
                     pass
@@ -418,8 +425,7 @@
             else:
                 # Overwrite only if raw is newer than processed
                 raw_mtime = file_path.stat().st_mtime
-                proc_mtime = max(out_parquet.stat().st_mtime if out_parquet.exists() else 0,
-                                 out_csv.stat().st_mtime if out_csv.exists() else 0)
+                proc_mtime = max(out_parquet.stat().st_mtime if out_parquet.exists() else 0, out_csv.stat().st_mtime if out_csv.exists() else 0)
                 if proc_mtime >= raw_mtime:
                     print(f"[INDEX-LOCAL] Up-to-date, skipping {year}-{month:02d}")
                     return
@@ -427,6 +433,7 @@
         # Read file
         try:
             import pandas as pd  # local import safe
+
             if file_path.suffix.lower() == ".csv":
                 try:
                     df_full = pd.read_csv(file_path, low_memory=False)
@@ -454,42 +461,43 @@
             include_cols = None
             rename_map = {}
             try:
+
                 def _load_mapping(mp: Path):
                     include_specs: list[str] = []
                     alias_map_local: dict[str, list[str]] = {}
                     rename_map_local: dict[str, str] = {}
-                    with open(mp, newline='', encoding='utf-8') as f:
+                    with open(mp, newline="", encoding="utf-8") as f:
                         rdr = csv.reader(f)
                         header = None
                         i_name = i_inc = i_alias = i_canon = None
                         for row in rdr:
                             if not row:
                                 continue
-                            if row[0].lstrip().startswith('#'):
+                            if row[0].lstrip().startswith("#"):
                                 continue
                             if header is None:
                                 header = [c.strip().lower() for c in row]
-                                if 'column_name' in header and 'include' in header:
-                                    i_name = header.index('column_name')
-                                    i_inc = header.index('include')
-                                    i_alias = header.index('alias') if 'alias' in header else None
-                                    i_canon = header.index('canonical_name') if 'canonical_name' in header else None
+                                if "column_name" in header and "include" in header:
+                                    i_name = header.index("column_name")
+                                    i_inc = header.index("include")
+                                    i_alias = header.index("alias") if "alias" in header else None
+                                    i_canon = header.index("canonical_name") if "canonical_name" in header else None
                                     continue
                                 else:
                                     i_name, i_inc, i_alias, i_canon = 0, 1, 2, 3
                             while len(row) < len(header):
-                                row.append('')
+                                row.append("")
                             col_name = row[i_name].strip()
-                            include = row[i_inc].strip().lower() == 'yes'
-                            aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ''
-                            canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ''
+                            include = row[i_inc].strip().lower() == "yes"
+                            aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ""
+                            canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ""
                             if not col_name or not include:
                                 continue
                             include_specs.append(col_name)
                             if canonical:
                                 rename_map_local[col_name] = canonical
                             if aliases:
-                                alias_map_local[col_name] = [a.strip() for a in aliases.split('|') if a.strip()]
+                                alias_map_local[col_name] = [a.strip() for a in aliases.split("|") if a.strip()]
                     return include_specs, alias_map_local, rename_map_local
 
                 include_cols, alias_map, rename_map = _load_mapping(mapping_path)
@@ -505,9 +513,11 @@
                     if not found and want in alias_map:
                         for alias in alias_map[want]:
                             if alias in df_full.columns:
-                                found = alias; break
+                                found = alias
+                                break
                             if alias in normalized_to_actual:
-                                found = normalized_to_actual[alias]; break
+                                found = normalized_to_actual[alias]
+                                break
                     if found:
                         if found != want and want in rename_map:
                             rename_map[found] = rename_map.pop(want)
@@ -528,10 +538,9 @@
                     df_curated[c] = pd.to_numeric(df_curated[c], errors="coerce").astype("Int64")
                 else:
                     df_curated[c] = df_curated[c].astype("string").str.strip()
-            
+
             # Add domain extraction if Website Address column exists
             if "Website Address" in df_curated.columns:
-                
                 try:
                     df_curated["domain"] = df_curated["Website Address"].apply(extract_domain_from_url)
                     df_curated["domain"] = df_curated["domain"].astype("string")
@@ -539,10 +548,11 @@
                 except Exception as e:
                     print(f"[ERROR] Failed to add domain column: {e}")
                     import traceback
+
                     traceback.print_exc()
                     # Continue without domain column
                     pass
-            
+
             # Save
             df_curated.to_csv(out_csv, index=False)
             try:
@@ -559,7 +569,8 @@
             # Expect .../raw/index/year=YYYY/month=MM/<file>
             parts = {seg.split("=")[0]: seg.split("=")[-1] for seg in path.parts if "=" in seg}
             try:
-                y = int(parts.get("year", "")); m = int(parts.get("month", ""))
+                y = int(parts.get("year", ""))
+                m = int(parts.get("month", ""))
             except Exception:
                 continue
             _parse_to_processed(path, y, m)
@@ -568,13 +579,14 @@
     for zip_file in sorted(raw_index_zip.rglob("*.zip")):
         parts = {seg.split("=")[0]: seg.split("=")[-1] for seg in zip_file.parts if "=" in seg}
         try:
-            y = int(parts.get("year", "")); m = int(parts.get("month", ""))
+            y = int(parts.get("year", ""))
+            m = int(parts.get("month", ""))
         except Exception:
             continue
         try:
             with zipfile.ZipFile(zip_file, "r") as zf:
-                xlsx_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith('.xlsx')]
-                csv_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith('.csv')]
+                xlsx_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith(".xlsx")]
+                csv_members = [zi for zi in zf.infolist() if zi.filename.lower().endswith(".csv")]
                 member = max(xlsx_members or csv_members, key=lambda z: z.file_size) if (xlsx_members or csv_members) else None
                 if not member:
                     print(f"[INDEX-LOCAL] No xlsx/csv in ZIP {zip_file}")
@@ -582,7 +594,7 @@
                 tmp_dir = base / ".tmp_index_local"
                 tmp_dir.mkdir(parents=True, exist_ok=True)
                 out_tmp = tmp_dir / Path(member.filename).name
-                with zf.open(member) as src, open(out_tmp, 'wb') as dst:
+                with zf.open(member) as src, open(out_tmp, "wb") as dst:
                     dst.write(src.read())
                 _parse_to_processed(out_tmp, y, m)
                 try:
@@ -592,6 +604,7 @@
         except Exception as e:
             print(f"[INDEX-LOCAL] Failed to extract {zip_file}: {e}")
 
+
 class _Args:
     def __init__(self, **kwargs):
         self.__dict__.update(kwargs)
@@ -638,6 +651,7 @@
     # Try pypdf first
     try:
         from pypdf import PdfReader  # type: ignore
+
         reader = PdfReader(str(pdf_path))
         total_pages = len(reader.pages)
         first_page_text = (reader.pages[0].extract_text() or "") if total_pages else ""
@@ -647,6 +661,7 @@
     if not first_page_text:
         try:
             import pdfplumber  # type: ignore
+
             with pdfplumber.open(str(pdf_path)) as pdf:
                 total_pages = max(total_pages, len(pdf.pages))
                 if pdf.pages:
@@ -684,6 +699,7 @@
     else:
         try:
             import pdfplumber  # type: ignore
+
             with pdfplumber.open(str(pdf_path)) as pdf:
                 pages_to_scan = min(max_scan, len(pdf.pages))
                 result = _scan_texts(pdf.pages[i].extract_text() or "" for i in range(pages_to_scan))
@@ -738,14 +754,16 @@
         rel_path = pdf.relative_to(base).as_posix()
         sha = _sha256_file(pdf)
         size_bytes = pdf.stat().st_size
-        records.append({
-            "crd": crd,
-            "doc_type": doc_type,
-            "filing_date": dt.date().isoformat(),
-            "rel_path": rel_path,
-            "sha256": sha,
-            "size_bytes": size_bytes,
-        })
+        records.append(
+            {
+                "crd": crd,
+                "doc_type": doc_type,
+                "filing_date": dt.date().isoformat(),
+                "rel_path": rel_path,
+                "sha256": sha,
+                "size_bytes": size_bytes,
+            }
+        )
 
     # Write inventory
     inv_path = processed_adv / "inventory.csv"
@@ -784,12 +802,14 @@
 
     for crd, doc_map in grouped.items():
         for doc_type, row in doc_map.items():
-            latest_rows.append({
-                "crd": crd,
-                "doc_type": doc_type,
-                "filing_date": row["filing_date"],
-                "rel_path": row["rel_path"],
-            })
+            latest_rows.append(
+                {
+                    "crd": crd,
+                    "doc_type": doc_type,
+                    "filing_date": row["filing_date"],
+                    "rel_path": row["rel_path"],
+                }
+            )
 
     latest_df = pd.DataFrame.from_records(latest_rows)
     latest_path = processed_adv / "latest.csv"
@@ -802,6 +822,7 @@
     if not cfg_path.exists():
         raise FileNotFoundError(f"Trend config not found: {cfg_path}")
     import json
+
     total_crds_seen = set()
     with open(cfg_path, "r", encoding="utf-8") as f:
         return json.load(f)
@@ -832,65 +853,68 @@
 def extract_domain_from_url(url: str) -> str | None:
     """
     Extract domain name from a URL string.
-    
+
     Args:
         url: URL string (e.g., "https://www.example.com/path", "www.firm.com", "firm.org")
-        
+
     Returns:
         Clean domain name (e.g., "example.com") or None if invalid
     """
     # Handle pandas NA values and other non-string types
     try:
         import pandas as pd
+
         if pd.isna(url):
             return None
     except ImportError:
         pass
-    
+
     # Handle None and non-string types
     if url is None:
         return None
-    
+
     if not isinstance(url, str):
         return None
-    
+
     url = url.strip()
     if not url:
         return None
-    
+
     # Clean up common URL issues before parsing
     # Remove common suffixes like [WEBSITE], [URL], etc.
     import re
-    url = re.sub(r'\s*\[.*?\]\s*$', '', url)  # Remove [WEBSITE], [URL], etc. at end
-    url = re.sub(r'\s*\(.*?\)\s*$', '', url)  # Remove (WEBSITE), (URL), etc. at end
+
+    url = re.sub(r"\s*\[.*?\]\s*$", "", url)  # Remove [WEBSITE], [URL], etc. at end
+    url = re.sub(r"\s*\(.*?\)\s*$", "", url)  # Remove (WEBSITE), (URL), etc. at end
     url = url.strip()
-    
+
     # If empty after cleaning, return None
     if not url:
         return None
-    
+
     # Ensure it has a scheme for parsing
     if not url.lower().startswith(("http://", "https://", "ftp://")):
         url_for_parse = "http://" + url
     else:
         url_for_parse = url
-    
+
     try:
         from urllib.parse import urlparse
+
         parsed = urlparse(url_for_parse)
         domain = parsed.netloc.lower()
-        
+
         if not domain:
             return None
-            
+
         # Remove www. prefix
         if domain.startswith("www."):
             domain = domain[4:]
-            
+
         # Basic validation - should have at least one dot
         if "." not in domain:
             return None
-            
+
         return domain
     except Exception as e:
         # Debug: print the error for troubleshooting
@@ -932,6 +956,7 @@
     # Fast path
     if "crd" in df.columns:
         return "crd"
+
     # Normalize headers
     def norm(h: str) -> str:
         return "".join(ch for ch in h.lower() if ch.isalnum())
@@ -1146,8 +1171,10 @@
 
     # Dedicated watch log file (concise, watch-only)
     watch_log_path_env = os.environ.get("WATCH_LOG_PATH")
+
     def _sanitize_filename(s: str) -> str:
         return re.sub(r"[^A-Za-z0-9_.-]", "_", s)
+
     watch_log = None
     if watch_crd is not None:
         if watch_log_path_env:
@@ -1211,38 +1238,38 @@
         if not mp.exists():
             return alias_by_canon
         try:
-            with open(mp, newline='', encoding='utf-8') as f:
+            with open(mp, newline="", encoding="utf-8") as f:
                 rdr = csv.reader(f)
                 header = None
                 i_name = i_inc = i_alias = i_canon = None
                 for row in rdr:
                     if not row:
                         continue
-                    if row[0].lstrip().startswith('#'):
+                    if row[0].lstrip().startswith("#"):
                         continue
                     if header is None:
                         header = [c.strip().lower() for c in row]
-                        if 'column_name' in header and 'include' in header:
-                            i_name = header.index('column_name')
-                            i_inc = header.index('include')
-                            i_alias = header.index('alias') if 'alias' in header else None
-                            i_canon = header.index('canonical_name') if 'canonical_name' in header else None
+                        if "column_name" in header and "include" in header:
+                            i_name = header.index("column_name")
+                            i_inc = header.index("include")
+                            i_alias = header.index("alias") if "alias" in header else None
+                            i_canon = header.index("canonical_name") if "canonical_name" in header else None
                             continue
                         else:
                             i_name, i_inc, i_alias, i_canon = 0, 1, 2, 3
                     while len(row) < len(header):
-                        row.append('')
+                        row.append("")
                     col_name = row[i_name].strip()
-                    include = row[i_inc].strip().lower() == 'yes'
+                    include = row[i_inc].strip().lower() == "yes"
                     if not col_name or not include:
                         continue
-                    canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ''
+                    canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ""
                     canon_key = canonical or col_name
-                    aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ''
+                    aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ""
                     s = alias_by_canon.setdefault(canon_key, set())
                     s.add(col_name)
                     if aliases:
-                        for a in aliases.split('|'):
+                        for a in aliases.split("|"):
                             a = a.strip()
                             if a:
                                 s.add(a)
@@ -1345,6 +1372,7 @@
     generated_at = datetime.now(timezone.utc).isoformat()
 
     import json
+
     total_crds_seen: set[int] = set()
 
     # Per-CRD aggregator: {crd: {canon: {"type": str, "series": [{ym, raw, value|value_norm}]}}}
@@ -1363,7 +1391,8 @@
             replaced = False
             for s in series:
                 if s.get("ym") == ym:
-                    s.clear(); s.update({k: v for k, v in entry.items() if k != "type"})
+                    s.clear()
+                    s.update({k: v for k, v in entry.items() if k != "type"})
                     replaced = True
                     break
             if not replaced:
@@ -1382,7 +1411,7 @@
                     latest_ym = latest.get("ym")
                     try:
                         yy, mm = [int(p) for p in latest_ym.split("-")]
-                        prev_ym = f"{yy-1}-12" if mm == 1 else f"{yy}-{mm-1:02d}"
+                        prev_ym = f"{yy - 1}-12" if mm == 1 else f"{yy}-{mm - 1:02d}"
                     except Exception:
                         prev_ym = None
                     prev = next((s for s in reversed(series) if s.get("ym") == prev_ym and "value" in s), None) if prev_ym else None
@@ -1392,7 +1421,7 @@
                             pct_change_1m = delta_1m / prev["value"]
                     try:
                         yy, mm = [int(p) for p in latest_ym.split("-")]
-                        yoy_ym = f"{yy-1}-{mm:02d}"
+                        yoy_ym = f"{yy - 1}-{mm:02d}"
                     except Exception:
                         yoy_ym = None
                     prev_yoy = next((s for s in series if s.get("ym") == yoy_ym and "value" in s), None) if yoy_ym else None
@@ -1452,7 +1481,7 @@
             dprint(f"[TRENDS] WARN: failed final write for CRD {crd}: {e}")
 
     # Process month by month, chunked
-    for (y, m, idx_path, kind) in month_dirs:
+    for y, m, idx_path, kind in month_dirs:
         ym = f"{y}-{m:02d}"
         dprint(f"[TRENDS] Month {ym} kind={kind} path={idx_path}")
         month_crds_seen: set[int] = set()
@@ -1481,8 +1510,10 @@
             if not quiet_trends:
                 print(f"[TRENDS] CRD column missing in {idx_path}; skipping")
             if watch_crd is not None:
-                focus = next(iter(watch_cols)) if watch_cols else ''
-                wprint(f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus} resolved=False found_row=False value_present=False wrote=False reason=crd_column_missing")
+                focus = next(iter(watch_cols)) if watch_cols else ""
+                wprint(
+                    f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus} resolved=False found_row=False value_present=False wrote=False reason=crd_column_missing"
+                )
             continue
         # Resolve selection map against header with aliases and normalized matching
         sel_cols: dict[str, str] = {}
@@ -1517,8 +1548,10 @@
         if not sel_cols:
             dprint(f"[TRENDS] {ym} resolved 0 columns (after filter), header cols={len(header_cols)}")
             if watch_crd is not None:
-                focus = next(iter(watch_cols)) if watch_cols else ''
-                wprint(f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus} resolved=False found_row=False value_present=False wrote=False reason=column_not_resolved")
+                focus = next(iter(watch_cols)) if watch_cols else ""
+                wprint(
+                    f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus} resolved=False found_row=False value_present=False wrote=False reason=column_not_resolved"
+                )
             continue
         dprint(f"[TRENDS] {ym} resolved {len(sel_cols)} columns ‚Üí {sorted(set(sel_cols.values()))[:12]}")
 
@@ -1578,12 +1611,8 @@
                     print(f"[TRENDS] {ym}: updated 0 CRDs (total {len(total_crds_seen)}), rows 0")
                 return True
 
-            df_polars = df_polars.with_columns(
-                pl.col("__crd").cast(pl.Float64, strict=False).alias("__crd_float")
-            )
-            df_polars = df_polars.with_columns(
-                pl.col("__crd_float").cast(pl.Int64, strict=False).alias("__crd")
-            ).drop("__crd_float")
+            df_polars = df_polars.with_columns(pl.col("__crd").cast(pl.Float64, strict=False).alias("__crd_float"))
+            df_polars = df_polars.with_columns(pl.col("__crd_float").cast(pl.Int64, strict=False).alias("__crd")).drop("__crd_float")
             df_polars = df_polars.filter(pl.col("__crd").is_not_null())
             if only_crd is not None:
                 df_polars = df_polars.filter(pl.col("__crd") == only_crd)
@@ -1600,31 +1629,25 @@
                 variable_name="canonical",
                 value_name="value_raw",
             )
-            long_df = long_df.with_columns([
-                pl.lit(ym).alias("ym"),
-                pl.col("value_raw").cast(pl.Utf8),
-            ])
-            long_df = long_df.filter(
-                pl.col("value_raw").is_not_null()
-                & (pl.col("value_raw").str.strip() != "")
+            long_df = long_df.with_columns(
+                [
+                    pl.lit(ym).alias("ym"),
+                    pl.col("value_raw").cast(pl.Utf8),
+                ]
             )
+            long_df = long_df.filter(pl.col("value_raw").is_not_null() & (pl.col("value_raw").str.strip() != ""))
             if long_df.is_empty():
                 if not quiet_trends:
                     print(f"[TRENDS] {ym}: updated 0 CRDs (total {len(total_crds_seen)}), rows 0")
                 return True
 
-            long_df = long_df.with_columns([
-                pl.col("value_raw").map_elements(_parse_numeric_value, return_dtype=pl.Float64).alias("value_numeric")
-            ])
-            long_df = long_df.with_columns([
-                pl.col("value_raw").map_elements(_categorical_norm_value, return_dtype=pl.Utf8).alias("value_norm_candidate")
-            ])
-            long_df = long_df.with_columns([
-                pl.when(pl.col("value_numeric").is_not_null())
-                .then(pl.lit(None))
-                .otherwise(pl.col("value_norm_candidate"))
-                .alias("value_norm")
-            ]).drop("value_norm_candidate")
+            long_df = long_df.with_columns([pl.col("value_raw").map_elements(_parse_numeric_value, return_dtype=pl.Float64).alias("value_numeric")])
+            long_df = long_df.with_columns(
+                [pl.col("value_raw").map_elements(_categorical_norm_value, return_dtype=pl.Utf8).alias("value_norm_candidate")]
+            )
+            long_df = long_df.with_columns(
+                [pl.when(pl.col("value_numeric").is_not_null()).then(pl.lit(None)).otherwise(pl.col("value_norm_candidate")).alias("value_norm")]
+            ).drop("value_norm_candidate")
 
             month_crds_seen_local: set[int] = set()
             month_updates_local = 0
@@ -1642,24 +1665,14 @@
                     if watch_cols:
                         probe: dict[str, str | None] = {}
                         for canon in watch_cols:
-                            vals = (
-                                df_watch.filter(pl.col("canonical") == canon)
-                                .get_column("value_raw")
-                                .to_list()
-                            )
+                            vals = df_watch.filter(pl.col("canonical") == canon).get_column("value_raw").to_list()
                             probe[canon] = vals[0] if vals else None
                         dprint(f"[WATCH] {ym} values={{{canon: (None if v is None else str(v)) for canon, v in probe.items()}}}")
                     if focus_canon is not None:
-                        watch_value_present = (
-                            df_watch.filter(pl.col("canonical") == focus_canon).height > 0
-                        )
+                        watch_value_present = df_watch.filter(pl.col("canonical") == focus_canon).height > 0
 
-            numeric_rows = long_df.filter(pl.col("value_numeric").is_not_null()).select(
-                ["__crd", "canonical", "value_raw", "value_numeric"]
-            )
-            categorical_rows = long_df.filter(pl.col("value_numeric").is_null()).select(
-                ["__crd", "canonical", "value_raw", "value_norm"]
-            )
+            numeric_rows = long_df.filter(pl.col("value_numeric").is_not_null()).select(["__crd", "canonical", "value_raw", "value_numeric"])
+            categorical_rows = long_df.filter(pl.col("value_numeric").is_null()).select(["__crd", "canonical", "value_raw", "value_norm"])
 
             updates_iter = []
             if numeric_rows.height:
@@ -1707,11 +1720,13 @@
                     watch_wrote = True
 
             if watch_crd is not None and focus_canon is not None:
-                reason = "ok" if watch_wrote else (
-                    "value_blank" if (watch_found_row and watch_col_resolved) else (
-                        "crd_not_found" if not watch_found_row else (
-                            "column_not_resolved" if not watch_col_resolved else "unknown"
-                        )
+                reason = (
+                    "ok"
+                    if watch_wrote
+                    else (
+                        "value_blank"
+                        if (watch_found_row and watch_col_resolved)
+                        else ("crd_not_found" if not watch_found_row else ("column_not_resolved" if not watch_col_resolved else "unknown"))
                     )
                 )
                 wprint(
@@ -1724,25 +1739,14 @@
                         out_path = trends_out_root / f"crd={watch_crd}" / "trends.v1.json"
                         with open(out_path, "r", encoding="utf-8") as _f:
                             doc = json.load(_f)
-                        series = (
-                            doc.get("columns", {})
-                            .get(focus_canon, {})
-                            .get("series", [])
-                        )
+                        series = doc.get("columns", {}).get(focus_canon, {}).get("series", [])
                         present = any(s.get("ym") == ym for s in series)
-                        wprint(
-                            f"[WATCH_VERIFY] ym={ym} crd={watch_crd} col={focus_canon} present={present} size={len(series)}"
-                        )
+                        wprint(f"[WATCH_VERIFY] ym={ym} crd={watch_crd} col={focus_canon} present={present} size={len(series)}")
                     except Exception as _e:
-                        wprint(
-                            f"[WATCH_VERIFY] ym={ym} crd={watch_crd} col={focus_canon} "
-                            f"error={type(_e).__name__}:{_e}"
-                        )
+                        wprint(f"[WATCH_VERIFY] ym={ym} crd={watch_crd} col={focus_canon} error={type(_e).__name__}:{_e}")
 
             if not quiet_trends:
-                print(
-                    f"[TRENDS] {ym}: updated {len(month_crds_seen_local)} CRDs (total {len(total_crds_seen)}), rows {month_updates_local}"
-                )
+                print(f"[TRENDS] {ym}: updated {len(month_crds_seen_local)} CRDs (total {len(total_crds_seen)}), rows {month_updates_local}")
 
             return True
 
@@ -1804,7 +1808,7 @@
                                         if canon in watch_cols:
                                             probe[canon] = row.get(df_col)
                                     if probe:
-                                        dprint(f"[WATCH] {ym} values={ {k: (None if pd.isna(v) else str(v)) for k,v in probe.items()} }")
+                                        dprint(f"[WATCH] {ym} values={ {k: (None if pd.isna(v) else str(v)) for k, v in probe.items()} }")
                                 watch_found_row = True
                                 if focus_actual is not None and focus_canon in updates:
                                     watch_value_present = True
@@ -1815,6 +1819,7 @@
                                 try:
                                     out_path = trends_out_root / f"crd={crd}" / "trends.v1.json"
                                     import json as _json
+
                                     with open(out_path, "r", encoding="utf-8") as _f:
                                         _doc = _json.load(_f)
                                     _series = _doc.get("columns", {}).get(focus_canon, {}).get("series", [])
@@ -1832,10 +1837,22 @@
                             reason_all_values_blank += 1
                 dprint(f"[TRENDS] {ym} wrote {month_updates} row updates")
                 if trace_skip:
-                    dprint(f"[TRENDS] {ym} skip_reasons: parse_crd_fail={reason_parse_crd_fail} only_crd_mismatch={reason_only_crd_mismatch} all_values_blank={reason_all_values_blank} not_active_latest={reason_not_active_latest}")
+                    dprint(
+                        f"[TRENDS] {ym} skip_reasons: parse_crd_fail={reason_parse_crd_fail} only_crd_mismatch={reason_only_crd_mismatch} all_values_blank={reason_all_values_blank} not_active_latest={reason_not_active_latest}"
+                    )
                 if watch_crd is not None and focus_canon is not None:
-                    reason = "ok" if watch_wrote else ("value_blank" if (watch_found_row and watch_col_resolved) else ("crd_not_found" if not watch_found_row else ("column_not_resolved" if not watch_col_resolved else "unknown")))
-                    wprint(f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus_canon} resolved={watch_col_resolved} found_row={watch_found_row} value_present={watch_value_present} wrote={watch_wrote} reason={reason}")
+                    reason = (
+                        "ok"
+                        if watch_wrote
+                        else (
+                            "value_blank"
+                            if (watch_found_row and watch_col_resolved)
+                            else ("crd_not_found" if not watch_found_row else ("column_not_resolved" if not watch_col_resolved else "unknown"))
+                        )
+                    )
+                    wprint(
+                        f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus_canon} resolved={watch_col_resolved} found_row={watch_found_row} value_present={watch_value_present} wrote={watch_wrote} reason={reason}"
+                    )
             else:
                 if pq is None:
                     # Fallback: load whole parquet (may be heavy)
@@ -1850,7 +1867,7 @@
                     watch_wrote = False
                     # iterate in slices of 2000 rows
                     for start in range(0, len(df_full), 2000):
-                        chunk = df_full.iloc[start:start+2000]
+                        chunk = df_full.iloc[start : start + 2000]
                         for _, row in chunk.iterrows():
                             crd_val = row.get(crd_col)
                             if pd.isna(crd_val):
@@ -1893,7 +1910,7 @@
                                             if canon in watch_cols:
                                                 probe[canon] = row.get(df_col)
                                         if probe:
-                                            dprint(f"[WATCH] {ym} values={ {k: (None if pd.isna(v) else str(v)) for k,v in probe.items()} }")
+                                            dprint(f"[WATCH] {ym} values={ {k: (None if pd.isna(v) else str(v)) for k, v in probe.items()} }")
                                     watch_found_row = True
                                     if focus_actual is not None and focus_canon in updates:
                                         watch_value_present = True
@@ -1906,10 +1923,22 @@
                                 reason_all_values_blank += 1
                     dprint(f"[TRENDS] {ym} wrote {month_updates} row updates (parquet fallback)")
                     if trace_skip:
-                        dprint(f"[TRENDS] {ym} skip_reasons: parse_crd_fail={reason_parse_crd_fail} only_crd_mismatch={reason_only_crd_mismatch} all_values_blank={reason_all_values_blank} not_active_latest={reason_not_active_latest}")
+                        dprint(
+                            f"[TRENDS] {ym} skip_reasons: parse_crd_fail={reason_parse_crd_fail} only_crd_mismatch={reason_only_crd_mismatch} all_values_blank={reason_all_values_blank} not_active_latest={reason_not_active_latest}"
+                        )
                     if watch_crd is not None and focus_canon is not None:
-                        reason = "ok" if watch_wrote else ("value_blank" if (watch_found_row and watch_col_resolved) else ("crd_not_found" if not watch_found_row else ("column_not_resolved" if not watch_col_resolved else "unknown")))
-                        wprint(f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus_canon} resolved={watch_col_resolved} found_row={watch_found_row} value_present={watch_value_present} wrote={watch_wrote} reason={reason}")
+                        reason = (
+                            "ok"
+                            if watch_wrote
+                            else (
+                                "value_blank"
+                                if (watch_found_row and watch_col_resolved)
+                                else ("crd_not_found" if not watch_found_row else ("column_not_resolved" if not watch_col_resolved else "unknown"))
+                            )
+                        )
+                        wprint(
+                            f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus_canon} resolved={watch_col_resolved} found_row={watch_found_row} value_present={watch_value_present} wrote={watch_wrote} reason={reason}"
+                        )
                 else:
                     pf = pq.ParquetFile(str(idx_path))
                     month_updates = 0
@@ -1965,7 +1994,7 @@
                                             if canon in watch_cols:
                                                 probe[canon] = row.get(df_col)
                                         if probe:
-                                            dprint(f"[WATCH] {ym} values={ {k: (None if pd.isna(v) else str(v)) for k,v in probe.items()} }")
+                                            dprint(f"[WATCH] {ym} values={ {k: (None if pd.isna(v) else str(v)) for k, v in probe.items()} }")
                                     watch_found_row = True
                                     if focus_actual is not None and focus_canon in updates:
                                         watch_value_present = True
@@ -1978,10 +2007,22 @@
                                 reason_all_values_blank += 1
                     dprint(f"[TRENDS] {ym} wrote {month_updates} row updates (pyarrow)")
                     if trace_skip:
-                        dprint(f"[TRENDS] {ym} skip_reasons: parse_crd_fail={reason_parse_crd_fail} only_crd_mismatch={reason_only_crd_mismatch} all_values_blank={reason_all_values_blank} not_active_latest={reason_not_active_latest}")
+                        dprint(
+                            f"[TRENDS] {ym} skip_reasons: parse_crd_fail={reason_parse_crd_fail} only_crd_mismatch={reason_only_crd_mismatch} all_values_blank={reason_all_values_blank} not_active_latest={reason_not_active_latest}"
+                        )
                     if watch_crd is not None and focus_canon is not None:
-                        reason = "ok" if watch_wrote else ("value_blank" if (watch_found_row and watch_col_resolved) else ("crd_not_found" if not watch_found_row else ("column_not_resolved" if not watch_col_resolved else "unknown")))
-                        wprint(f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus_canon} resolved={watch_col_resolved} found_row={watch_found_row} value_present={watch_value_present} wrote={watch_wrote} reason={reason}")
+                        reason = (
+                            "ok"
+                            if watch_wrote
+                            else (
+                                "value_blank"
+                                if (watch_found_row and watch_col_resolved)
+                                else ("crd_not_found" if not watch_found_row else ("column_not_resolved" if not watch_col_resolved else "unknown"))
+                            )
+                        )
+                        wprint(
+                            f"[WATCH_SUMMARY] ym={ym} crd={watch_crd} col={focus_canon} resolved={watch_col_resolved} found_row={watch_found_row} value_present={watch_value_present} wrote={watch_wrote} reason={reason}"
+                        )
         except Exception as e:
             if not quiet_trends:
                 print(f"[TRENDS] Failed reading {idx_path} in chunks: {e}")
@@ -2034,38 +2075,38 @@
         if not mp.exists():
             return alias_by_canon
         try:
-            with open(mp, newline='', encoding='utf-8') as f:
+            with open(mp, newline="", encoding="utf-8") as f:
                 rdr = csv.reader(f)
                 header = None
                 i_name = i_inc = i_alias = i_canon = None
                 for row in rdr:
                     if not row:
                         continue
-                    if row[0].lstrip().startswith('#'):
+                    if row[0].lstrip().startswith("#"):
                         continue
                     if header is None:
                         header = [c.strip().lower() for c in row]
-                        if 'column_name' in header and 'include' in header:
-                            i_name = header.index('column_name')
-                            i_inc = header.index('include')
-                            i_alias = header.index('alias') if 'alias' in header else None
-                            i_canon = header.index('canonical_name') if 'canonical_name' in header else None
+                        if "column_name" in header and "include" in header:
+                            i_name = header.index("column_name")
+                            i_inc = header.index("include")
+                            i_alias = header.index("alias") if "alias" in header else None
+                            i_canon = header.index("canonical_name") if "canonical_name" in header else None
                             continue
                         else:
                             i_name, i_inc, i_alias, i_canon = 0, 1, 2, 3
                     while len(row) < len(header):
-                        row.append('')
+                        row.append("")
                     col_name = row[i_name].strip()
-                    include = row[i_inc].strip().lower() == 'yes'
+                    include = row[i_inc].strip().lower() == "yes"
                     if not col_name or not include:
                         continue
-                    canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ''
+                    canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ""
                     canon_key = canonical or col_name
-                    aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ''
+                    aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ""
                     s = alias_by_canon.setdefault(canon_key, set())
                     s.add(col_name)
                     if aliases:
-                        for a in aliases.split('|'):
+                        for a in aliases.split("|"):
                             a = a.strip()
                             if a:
                                 s.add(a)
@@ -2140,12 +2181,14 @@
             chosen = None
             for cand in candidates:
                 if cand in head.columns:
-                    chosen = cand; break
+                    chosen = cand
+                    break
             if not chosen:
                 for cand in candidates:
                     n = _norm(cand)
                     if n in normalized_to_actual:
-                        chosen = normalized_to_actual[n]; break
+                        chosen = normalized_to_actual[n]
+                        break
             if chosen:
                 sel_map[chosen] = canon
         if not sel_map:
@@ -2164,7 +2207,7 @@
                     chunk_iter = (batch.to_pandas() for batch in pf.iter_batches(columns=usecols, batch_size=50000))
                 else:
                     df_full = pd.read_parquet(path, columns=usecols)
-                    chunk_iter = (df_full.iloc[i:i+50000] for i in range(0, len(df_full), 50000))
+                    chunk_iter = (df_full.iloc[i : i + 50000] for i in range(0, len(df_full), 50000))
         except Exception as e:
             dprint(f"Failed reading {path}: {e}")
             continue
@@ -2207,6 +2250,7 @@
 
     dprint(f"Complete: total rows written={written_rows:,}")
 
+
 def aggregate_trends_all(output: str, buckets: int = 256, only_bucket: int | None = None, debug: bool = False) -> None:
     base = Path(output).resolve()
     tall_root = base / "processed" / "trends" / "tall"
@@ -2312,7 +2356,7 @@
                         latest_ym = latest.get("ym")
                         try:
                             yy, mm = [int(p) for p in latest_ym.split("-")]
-                            prev_ym = f"{yy-1}-12" if mm == 1 else f"{yy}-{mm-1:02d}"
+                            prev_ym = f"{yy - 1}-12" if mm == 1 else f"{yy}-{mm - 1:02d}"
                         except Exception:
                             prev_ym = None
                         prev = next((s for s in reversed(series) if s.get("ym") == prev_ym and "value" in s), None) if prev_ym else None
@@ -2322,7 +2366,7 @@
                                 pct_change_1m = delta_1m / prev["value"]
                         try:
                             yy, mm = [int(p) for p in latest_ym.split("-")]
-                            yoy_ym = f"{yy-1}-{mm:02d}"
+                            yoy_ym = f"{yy - 1}-{mm:02d}"
                         except Exception:
                             yoy_ym = None
                         prev_yoy = next((s for s in series if s.get("ym") == yoy_ym and "value" in s), None) if yoy_ym else None
@@ -2355,10 +2399,12 @@
                             delta_5y = latest["value"] - prev_5["value"]
                             if prev_5["value"] not in (0, None):
                                 pct_change_5y = delta_5y / prev_5["value"]
+
                         # months since last change
                         def _month_index(ym: str) -> int:
                             y, m = [int(p) for p in ym.split("-")]
                             return y * 12 + m
+
                         try:
                             latest_idx = _month_index(latest_ym)
                             last_diff_idx = None
@@ -2521,38 +2567,38 @@
     mp = Path(mp_env) if mp_env else (Path(__file__).resolve().parent / "config" / "columns.csv")
     if mp.exists():
         try:
-            with open(mp, newline='', encoding='utf-8') as f:
+            with open(mp, newline="", encoding="utf-8") as f:
                 rdr = csv.reader(f)
                 header = None
                 i_name = i_inc = i_alias = i_canon = None
                 for row in rdr:
                     if not row:
                         continue
-                    if row[0].lstrip().startswith('#'):
+                    if row[0].lstrip().startswith("#"):
                         continue
                     if header is None:
                         header = [c.strip().lower() for c in row]
-                        if 'column_name' in header and 'include' in header:
-                            i_name = header.index('column_name')
-                            i_inc = header.index('include')
-                            i_alias = header.index('alias') if 'alias' in header else None
-                            i_canon = header.index('canonical_name') if 'canonical_name' in header else None
+                        if "column_name" in header and "include" in header:
+                            i_name = header.index("column_name")
+                            i_inc = header.index("include")
+                            i_alias = header.index("alias") if "alias" in header else None
+                            i_canon = header.index("canonical_name") if "canonical_name" in header else None
                             continue
                         else:
                             i_name, i_inc, i_alias, i_canon = 0, 1, 2, 3
                     while len(row) < len(header):
-                        row.append('')
+                        row.append("")
                     col_name = row[i_name].strip()
-                    include = row[i_inc].strip().lower() == 'yes'
+                    include = row[i_inc].strip().lower() == "yes"
                     if not col_name or not include:
                         continue
-                    canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ''
+                    canonical = row[i_canon].strip() if (i_canon is not None and i_canon < len(row)) else ""
                     canon_key = canonical or col_name
-                    aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ''
+                    aliases = row[i_alias].strip() if (i_alias is not None and i_alias < len(row)) else ""
                     s = alias_map.setdefault(canon_key, set())
                     s.add(col_name)
                     if aliases:
-                        for a in aliases.split('|'):
+                        for a in aliases.split("|"):
                             a = a.strip()
                             if a:
                                 s.add(a)
@@ -2591,9 +2637,23 @@
                 with open(out_path, "r", encoding="utf-8") as f:
                     doc = json.load(f)
             else:
-                doc = {"version": "v1", "crd": crd, "horizon_years": int(cfg.get("horizon_years", 5)), "generated_at": datetime.now(timezone.utc).isoformat(), "coverage": {"months_total": 0, "months_with_any_value": 0, "gaps": []}, "columns": {}}
+                doc = {
+                    "version": "v1",
+                    "crd": crd,
+                    "horizon_years": int(cfg.get("horizon_years", 5)),
+                    "generated_at": datetime.now(timezone.utc).isoformat(),
+                    "coverage": {"months_total": 0, "months_with_any_value": 0, "gaps": []},
+                    "columns": {},
+                }
         except Exception:
-            doc = {"version": "v1", "crd": crd, "horizon_years": int(cfg.get("horizon_years", 5)), "generated_at": datetime.now(timezone.utc).isoformat(), "coverage": {"months_total": 0, "months_with_any_value": 0, "gaps": []}, "columns": {}}
+            doc = {
+                "version": "v1",
+                "crd": crd,
+                "horizon_years": int(cfg.get("horizon_years", 5)),
+                "generated_at": datetime.now(timezone.utc).isoformat(),
+                "coverage": {"months_total": 0, "months_with_any_value": 0, "gaps": []},
+                "columns": {},
+            }
 
         columns = doc.setdefault("columns", {})
         # Update series for this month
@@ -2611,7 +2671,8 @@
                 replaced = False
                 for s in series:
                     if s.get("ym") == ym:
-                        s.clear(); s.update({"ym": ym, "raw": sval, "value": float(num)})
+                        s.clear()
+                        s.update({"ym": ym, "raw": sval, "value": float(num)})
                         replaced = True
                         break
                 if not replaced:
@@ -2626,7 +2687,8 @@
                 replaced = False
                 for s in series:
                     if s.get("ym") == ym:
-                        s.clear(); s.update({"ym": ym, "raw": sval, "value_norm": val_norm or sval.strip().lower()})
+                        s.clear()
+                        s.update({"ym": ym, "raw": sval, "value_norm": val_norm or sval.strip().lower()})
                         replaced = True
                         break
                 if not replaced:
@@ -2637,6 +2699,7 @@
         def _month_index(ymx: str) -> int:
             y, m = [int(p) for p in ymx.split("-")]
             return y * 12 + m
+
         months_with_any = set()
         for col, colrec in columns.items():
             series = colrec.get("series", [])
@@ -2651,20 +2714,22 @@
             months_since_change = None
             if latest and "value" in latest:
                 latest_ym = latest.get("ym")
+
                 # find lagged values by ym keys
                 def _find_value(target_ym: str):
                     for s in reversed(series):
                         if s.get("ym") == target_ym and "value" in s:
                             return s.get("value")
                     return None
+
                 try:
                     yy, mm = [int(p) for p in latest_ym.split("-")]
-                    prev_ym = f"{yy-1}-12" if mm == 1 else f"{yy}-{mm-1:02d}"
-                    yoy_ym = f"{yy-1}-{mm:02d}"
+                    prev_ym = f"{yy - 1}-12" if mm == 1 else f"{yy}-{mm - 1:02d}"
+                    yoy_ym = f"{yy - 1}-{mm:02d}"
                     m6y = yy if mm > 6 else yy - 1
                     m6m = mm - 6 if mm > 6 else mm + 12 - 6
                     ym6 = f"{m6y}-{m6m:02d}"
-                    ym5 = f"{yy-5}-{mm:02d}"
+                    ym5 = f"{yy - 5}-{mm:02d}"
                 except Exception:
                     prev_ym = yoy_ym = ym6 = ym5 = None
                 prev_val = _find_value(prev_ym) if prev_ym else None
@@ -2757,11 +2822,31 @@
             dprint(f"Write failed for CRD {crd}: {e}")
     dprint(f"Incremental aggregation complete for {wrote}/{total} CRDs in {ym}")
 
+
 def build_cli() -> argparse.ArgumentParser:
     p = argparse.ArgumentParser(description="Containerized SEC ingestion runner")
-    p.add_argument("--mode", required=True, choices=[
-        "ingest", "ingest_with_trends", "backfill_all", "backfill_year", "incremental", "extract_text", "dedup", "coverage_simple", "coverage_per_year", "index_pdfs", "build_trends", "process_index", "process_index_local", "build_trends_tall", "aggregate_trends_all", "aggregate_trends_incremental"
-    ])
+    p.add_argument(
+        "--mode",
+        required=True,
+        choices=[
+            "ingest",
+            "ingest_with_trends",
+            "backfill_all",
+            "backfill_year",
+            "incremental",
+            "extract_text",
+            "dedup",
+            "coverage_simple",
+            "coverage_per_year",
+            "index_pdfs",
+            "build_trends",
+            "process_index",
+            "process_index_local",
+            "build_trends_tall",
+            "aggregate_trends_all",
+            "aggregate_trends_incremental",
+        ],
+    )
     p.add_argument("--output", default=str(Path("output") / "sec_ingestion"))
     p.add_argument("--year", type=int, help="Year for backfill_year mode")
     p.add_argument("--debug", action="store_true")
@@ -2810,5 +2895,3 @@
 
 if __name__ == "__main__":
     raise SystemExit(main())
-
-

--- services/sec_ingestion/run_pipeline.py
+++ services/sec_ingestion/run_pipeline.py
@@ -57,19 +57,19 @@
 
 def setup_cloudwatch_logging() -> logging.Logger:
     """Configure CloudWatch logging for ECS"""
-    logger = logging.getLogger('sec_ingestion')
+    logger = logging.getLogger("sec_ingestion")
     logger.setLevel(logging.INFO)
-    
+
     # Remove existing handlers to avoid duplicates
     for handler in logger.handlers[:]:
         logger.removeHandler(handler)
-    
+
     # CloudWatch handler (ECS will handle log forwarding)
     handler = logging.StreamHandler()
-    formatter = logging.Formatter('%(message)s')  # JSON format
+    formatter = logging.Formatter("%(message)s")  # JSON format
     handler.setFormatter(formatter)
     logger.addHandler(handler)
-    
+
     return logger
 
 
@@ -78,43 +78,54 @@
     """Context manager for timing operations with structured logging"""
     start_time = time.time()
     execution_id = f"exec_{int(start_time)}"
-    
+
     # Log start
-    logger.info(json.dumps({
-        "timestamp": datetime.now(timezone.utc).isoformat(),
-        "level": "INFO",
-        "event": f"{operation_name}_started",
-        "execution_id": execution_id,
-        **context
-    }))
-    
+    logger.info(
+        json.dumps(
+            {
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+                "level": "INFO",
+                "event": f"{operation_name}_started",
+                "execution_id": execution_id,
+                **context,
+            }
+        )
+    )
+
     try:
         yield execution_id
         # Log success
         duration = time.time() - start_time
-        logger.info(json.dumps({
-            "timestamp": datetime.now(timezone.utc).isoformat(),
-            "level": "INFO", 
-            "event": f"{operation_name}_completed",
-            "execution_id": execution_id,
-            "duration_seconds": round(duration, 2),
-            "status": "success"
-        }))
-        
+        logger.info(
+            json.dumps(
+                {
+                    "timestamp": datetime.now(timezone.utc).isoformat(),
+                    "level": "INFO",
+                    "event": f"{operation_name}_completed",
+                    "execution_id": execution_id,
+                    "duration_seconds": round(duration, 2),
+                    "status": "success",
+                }
+            )
+        )
+
     except Exception as e:
         # Log error
         duration = time.time() - start_time
-        logger.error(json.dumps({
-            "timestamp": datetime.now(timezone.utc).isoformat(),
-            "level": "ERROR",
-            "event": f"{operation_name}_failed", 
-            "execution_id": execution_id,
-            "duration_seconds": round(duration, 2),
-            "error": str(e),
-            "error_type": type(e).__name__
-        }))
-        
-        
+        logger.error(
+            json.dumps(
+                {
+                    "timestamp": datetime.now(timezone.utc).isoformat(),
+                    "level": "ERROR",
+                    "event": f"{operation_name}_failed",
+                    "execution_id": execution_id,
+                    "duration_seconds": round(duration, 2),
+                    "error": str(e),
+                    "error_type": type(e).__name__,
+                }
+            )
+        )
+
         raise
 
 
@@ -217,7 +228,7 @@
 
 def run_backfill(debug: bool) -> None:
     logger = setup_cloudwatch_logging()
-    
+
     with log_execution_time(logger, "backfill_pipeline", mode="backfill", debug=debug) as execution_id:
         output_path, s3_uri = _resolve_output()
         _resolve_index_min_year()
@@ -255,24 +266,24 @@
         # Phase 6: S3 sync
         with log_execution_time(logger, "s3_sync", execution_id=execution_id):
             _sync_to_s3(output_path, s3_uri)
-        
+
         # Final summary
-        print("\n" + "="*60)
+        print("\n" + "=" * 60)
         print("[ORCH] BACKFILL RUN SUMMARY")
-        print("="*60)
+        print("=" * 60)
         print("‚úÖ Excel: Historical data ingestion completed")
         print("‚úÖ Trends: Trend analysis built")
         print("‚úÖ PDF: Form ADV PDFs processed")
         print("‚úÖ Inventory: PDF inventory rebuilt")
         print("‚úÖ Cleanup: Data retention cleanup completed")
         print("‚úÖ S3 Sync: Data synchronized to S3")
-        print("="*60)
+        print("=" * 60)
         print("[ORCH] Backfill complete")
 
 
 def run_incremental(debug: bool) -> None:
     logger = setup_cloudwatch_logging()
-    
+
     with log_execution_time(logger, "incremental_pipeline", mode="incremental", debug=debug) as execution_id:
         output_path, s3_uri = _resolve_output()
         _resolve_index_min_year()
@@ -293,24 +304,32 @@
             ingest_runner.ingest_all_excels_if_needed(mod, str(output_path), debug)
             idx_after, _ = _snapshot_month_dirs(base)
             excel_changed = idx_after != idx_before
-            
+
             if excel_changed:
-                logger.info(json.dumps({
-                    "timestamp": datetime.now(timezone.utc).isoformat(),
-                    "level": "INFO",
-                    "event": "excel_data_changed",
-                    "execution_id": execution_id,
-                    "new_months": list(idx_after - idx_before)
-                }))
+                logger.info(
+                    json.dumps(
+                        {
+                            "timestamp": datetime.now(timezone.utc).isoformat(),
+                            "level": "INFO",
+                            "event": "excel_data_changed",
+                            "execution_id": execution_id,
+                            "new_months": list(idx_after - idx_before),
+                        }
+                    )
+                )
                 print(f"[ORCH] Excel data changed: {len(idx_after - idx_before)} new months detected")
             else:
-                logger.info(json.dumps({
-                    "timestamp": datetime.now(timezone.utc).isoformat(),
-                    "level": "INFO",
-                    "event": "excel_data_unchanged",
-                    "execution_id": execution_id,
-                    "message": "No new Excel files found"
-                }))
+                logger.info(
+                    json.dumps(
+                        {
+                            "timestamp": datetime.now(timezone.utc).isoformat(),
+                            "level": "INFO",
+                            "event": "excel_data_unchanged",
+                            "execution_id": execution_id,
+                            "message": "No new Excel files found",
+                        }
+                    )
+                )
                 print("[ORCH] No new Excel files found - skipping Excel processing")
 
         # Phase 3: Trend refresh (conditional)
@@ -319,13 +338,17 @@
                 print("[ORCH] Incremental: refreshing trends")
                 ingest_runner.build_trends(str(output_path), years=trend_years, debug=debug)
         else:
-            logger.info(json.dumps({
-                "timestamp": datetime.now(timezone.utc).isoformat(),
-                "level": "INFO",
-                "event": "trend_refresh_skipped",
-                "execution_id": execution_id,
-                "reason": "no_new_excel_data"
-            }))
+            logger.info(
+                json.dumps(
+                    {
+                        "timestamp": datetime.now(timezone.utc).isoformat(),
+                        "level": "INFO",
+                        "event": "trend_refresh_skipped",
+                        "execution_id": execution_id,
+                        "reason": "no_new_excel_data",
+                    }
+                )
+            )
 
         # Phase 4: PDF processing
         with log_execution_time(logger, "pdf_processing", execution_id=execution_id):
@@ -334,24 +357,32 @@
             mod.cmd_ingest_adv(args_adv)
             _, adv_after = _snapshot_month_dirs(base)
             pdf_changed = adv_after != adv_before
-            
+
             if pdf_changed:
-                logger.info(json.dumps({
-                    "timestamp": datetime.now(timezone.utc).isoformat(),
-                    "level": "INFO",
-                    "event": "pdf_data_changed",
-                    "execution_id": execution_id,
-                    "new_months": list(adv_after - adv_before)
-                }))
+                logger.info(
+                    json.dumps(
+                        {
+                            "timestamp": datetime.now(timezone.utc).isoformat(),
+                            "level": "INFO",
+                            "event": "pdf_data_changed",
+                            "execution_id": execution_id,
+                            "new_months": list(adv_after - adv_before),
+                        }
+                    )
+                )
                 print(f"[ORCH] PDF data changed: {len(adv_after - adv_before)} new months detected")
             else:
-                logger.info(json.dumps({
-                    "timestamp": datetime.now(timezone.utc).isoformat(),
-                    "level": "INFO",
-                    "event": "pdf_data_unchanged",
-                    "execution_id": execution_id,
-                    "message": "No new PDF files found"
-                }))
+                logger.info(
+                    json.dumps(
+                        {
+                            "timestamp": datetime.now(timezone.utc).isoformat(),
+                            "level": "INFO",
+                            "event": "pdf_data_unchanged",
+                            "execution_id": execution_id,
+                            "message": "No new PDF files found",
+                        }
+                    )
+                )
                 print("[ORCH] No new PDF files found - skipping PDF processing")
 
         # Phase 5: PDF inventory (conditional)
@@ -367,34 +398,38 @@
                 ingest_runner.perform_incremental_cleanup(str(output_path))
                 _sync_to_s3(output_path, s3_uri)
         else:
-            logger.info(json.dumps({
-                "timestamp": datetime.now(timezone.utc).isoformat(),
-                "level": "INFO",
-                "event": "no_changes_detected",
-                "execution_id": execution_id,
-                "skipped_operations": ["cleanup", "s3_sync"]
-            }))
+            logger.info(
+                json.dumps(
+                    {
+                        "timestamp": datetime.now(timezone.utc).isoformat(),
+                        "level": "INFO",
+                        "event": "no_changes_detected",
+                        "execution_id": execution_id,
+                        "skipped_operations": ["cleanup", "s3_sync"],
+                    }
+                )
+            )
             print("[ORCH] No changes detected - skipping cleanup and sync")
-        
+
         # Final summary
-        print("\n" + "="*60)
+        print("\n" + "=" * 60)
         print("[ORCH] INCREMENTAL RUN SUMMARY")
-        print("="*60)
+        print("=" * 60)
         if excel_changed:
             print(f"‚úÖ Excel: {len(idx_after - idx_before)} new months processed")
         else:
             print("‚è≠Ô∏è  Excel: No new files found")
-            
+
         if pdf_changed:
             print(f"‚úÖ PDF: {len(adv_after - adv_before)} new months processed")
         else:
             print("‚è≠Ô∏è  PDF: No new files found")
-            
+
         if excel_changed or pdf_changed:
             print("‚úÖ Cleanup and S3 sync: Completed")
         else:
             print("‚è≠Ô∏è  Cleanup and S3 sync: Skipped (no changes)")
-        print("="*60)
+        print("=" * 60)
         print("[ORCH] Incremental complete")
 
 
@@ -430,9 +465,7 @@
         try:
             load_parameters_from_ssm(param_prefix)
         except Exception as exc:  # pragma: no cover
-            raise RuntimeError(
-                "Failed to load parameters from SSM; ensure boto3 is available and IAM permissions are set"
-            ) from exc
+            raise RuntimeError("Failed to load parameters from SSM; ensure boto3 is available and IAM permissions are set") from exc
     debug = args.debug or _strtobool(os.environ.get("DEBUG"), False)
     if args.mode == "backfill":
         run_backfill(debug)

--- services/sec_ingestion/sec_ingestion.py
+++ services/sec_ingestion/sec_ingestion.py
@@ -91,12 +91,13 @@
     # Try to parse a date from nearby text; support both 'YYYY-MM' and 'MonthName YYYY'
     parsed_date = None
     # Case: MonthName YYYY (e.g., 'Registered Investment Advisers, September 2025')
-    month_name_to_num = {m.lower(): i for i, m in enumerate([
-        "", "January", "February", "March", "April", "May", "June",
-        "July", "August", "September", "October", "November", "December"
-    ])}
-    m1 = re.search(r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+(20\d{2})",
-                   text, flags=re.IGNORECASE)
+    month_name_to_num = {
+        m.lower(): i
+        for i, m in enumerate(
+            ["", "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
+        )
+    }
+    m1 = re.search(r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+(20\d{2})", text, flags=re.IGNORECASE)
     if m1:
         mon = month_name_to_num[m1.group(1).lower()]
         yr = int(m1.group(2))
@@ -124,12 +125,13 @@
     if lm:
         try:
             from email.utils import parsedate_to_datetime
+
             lm_dt = parsedate_to_datetime(lm)
             month = lm_dt.date().replace(day=1)
         except Exception:
-            month = (maybe_date or dt.date(dt.date.today().year, 1, 1))
+            month = maybe_date or dt.date(dt.date.today().year, 1, 1)
     else:
-        month = (maybe_date or dt.date(dt.date.today().year, 1, 1))
+        month = maybe_date or dt.date(dt.date.today().year, 1, 1)
     month_dir = output_dir / f"year={month.year}" / f"month={month.month:02d}"
     ensure_dir(month_dir)
     # Preserve original filename if available
@@ -195,11 +197,13 @@
     if not crd_col:
         raise RuntimeError("CRD column not found in Excel after header detection attempts")
 
-    parsed = pd.DataFrame({
-        "crd": pd.to_numeric(df[crd_col], errors="coerce").astype("Int64"),
-        "name": df[name_col].astype(str).str.strip() if name_col else None,
-        "domain_raw": df[website_col].astype(str).str.strip() if website_col else None,
-    })
+    parsed = pd.DataFrame(
+        {
+            "crd": pd.to_numeric(df[crd_col], errors="coerce").astype("Int64"),
+            "name": df[name_col].astype(str).str.strip() if name_col else None,
+            "domain_raw": df[website_col].astype(str).str.strip() if website_col else None,
+        }
+    )
     parsed = parsed.dropna(subset=["crd"]).copy()
     parsed["crd"] = parsed["crd"].astype(int)
     parsed["domain_canonical"] = parsed["domain_raw"].apply(canonicalize_domain)
@@ -242,10 +246,12 @@
 
     # Heuristic: collect all anchors, then filter by href/text patterns likely to be the monthly ZIPs
     anchors = soup.find_all("a")
-    month_name_to_num = {m.lower(): i for i, m in enumerate([
-        "", "January", "February", "March", "April", "May", "June",
-        "July", "August", "September", "October", "November", "December"
-    ])}
+    month_name_to_num = {
+        m.lower(): i
+        for i, m in enumerate(
+            ["", "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
+        )
+    }
 
     def infer_date_from(text: str, href: str) -> Optional[dt.date]:
         txt = (text or "").strip()
@@ -288,11 +294,11 @@
         text = a.get_text(" ", strip=True) or ""
         # Filter by strong hints in href or text
         hint = (
-            href.lower().endswith('.zip') or
-            'advbrochures' in href.lower() or
-            'form adv part 2' in text.lower() or
-            re.search(r"form[-_ ]?adv[-_ ]?part[-_ ]?2", href, flags=re.IGNORECASE) is not None or
-            any(m in text.lower() for m in month_name_to_num.keys() if m)
+            href.lower().endswith(".zip")
+            or "advbrochures" in href.lower()
+            or "form adv part 2" in text.lower()
+            or re.search(r"form[-_ ]?adv[-_ ]?part[-_ ]?2", href, flags=re.IGNORECASE) is not None
+            or any(m in text.lower() for m in month_name_to_num.keys() if m)
         )
         if not hint:
             continue
@@ -327,6 +333,7 @@
     links.sort(key=lambda x: x[1] or dt.date(1970, 1, 1))
     return links
 
+
 def list_foia_part2_zip_urls(html: str, year: Optional[int] = None) -> List[Tuple[str, Optional[dt.date]]]:
     soup = BeautifulSoup(html, "html.parser")
     links: List[Tuple[str, Optional[dt.date]]] = []
@@ -338,13 +345,26 @@
         part2_root = part2_root.parent if part2_root else soup
 
     # Month mapping
-    month_name_to_num = {m.lower(): i for i, m in enumerate([
-        "", "January", "February", "March", "April", "May", "June",
-        "July", "August", "September", "October", "November", "December"
-    ])}
+    month_name_to_num = {
+        m.lower(): i
+        for i, m in enumerate(
+            ["", "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
+        )
+    }
     month_abbrev_to_num = {
-        "jan": 1, "feb": 2, "mar": 3, "apr": 4, "may": 5, "jun": 6,
-        "jul": 7, "aug": 8, "sep": 9, "sept": 9, "oct": 10, "nov": 11, "dec": 12,
+        "jan": 1,
+        "feb": 2,
+        "mar": 3,
+        "apr": 4,
+        "may": 5,
+        "jun": 6,
+        "jul": 7,
+        "aug": 8,
+        "sep": 9,
+        "sept": 9,
+        "oct": 10,
+        "nov": 11,
+        "dec": 12,
     }
 
     def infer_date_from(anchor: any, href: str, text: str) -> Optional[dt.date]:
@@ -360,8 +380,9 @@
             mon = month_name_to_num[m_broch.group("mon").lower()]
             return dt.date(yr, mon, 1)
         # Pattern 2: adv_brochures_YYYY_mon(_N).zip (FOIA style)
-        m_abbr = re.search(r"adv_brochures_(?P<year>20\d{2})_(?P<mon>jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)(?:_\d+)?\.zip",
-                           href, flags=re.IGNORECASE)
+        m_abbr = re.search(
+            r"adv_brochures_(?P<year>20\d{2})_(?P<mon>jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)(?:_\d+)?\.zip", href, flags=re.IGNORECASE
+        )
         if m_abbr:
             yr = int(m_abbr.group("year"))
             mon = month_abbrev_to_num[m_abbr.group("mon").lower()]
@@ -433,6 +454,7 @@
     links.sort(key=lambda x: x[1] or dt.date(1970, 1, 1))
     return links
 
+
 def render_adv_with_playwright(debug_dir: Optional[Path] = None) -> Optional[str]:
     try:
         from playwright.sync_api import sync_playwright
@@ -472,6 +494,8 @@
             context.close()
             browser.close()
     return html
+
+
 def _debug_dump_anchors(html: str, out_path: Path) -> int:
     """Write a TSV of all anchors: index, text, href."""
     soup = BeautifulSoup(html, "html.parser")
@@ -484,7 +508,6 @@
             href = (a.get("href") or "").replace("\t", " ")
             f.write(f"{i}\t{text}\t{href}\n")
     return len(anchors)
-
 
 
 def ingest_adv_zip(zip_source: Path | bytes, month_hint: Optional[dt.date], out_dir: Path) -> List[Path]:
@@ -502,7 +525,7 @@
             if bad:
                 print(f"[WARN] ZIP integrity issue, first bad entry: {bad}")
             for name in zf.namelist():
-                if not name.lower().endswith('.pdf'):
+                if not name.lower().endswith(".pdf"):
                     continue
                 base = os.path.basename(name)
                 m = ZIP_FILENAME_RE.match(base)
@@ -514,7 +537,7 @@
                     d = dt.datetime.strptime(date_str, "%Y%m%d").date()
                     effective_month = d.replace(day=1)
                 except Exception:
-                    effective_month = (month_hint or dt.date.today().replace(day=1))
+                    effective_month = month_hint or dt.date.today().replace(day=1)
 
                 month_dir = out_dir / f"crd={crd}" / f"year={effective_month.year}" / f"month={effective_month.month:02d}"
                 ensure_dir(month_dir)
@@ -545,6 +568,7 @@
     # Prefer pypdf for text-based PDFs; fall back to pdfminer-six if needed
     try:
         from pypdf import PdfReader
+
         reader = PdfReader(str(pdf_path))
         texts = []
         for page in reader.pages:
@@ -556,6 +580,7 @@
         pass
     # Fallback to pdfminer via pdfplumber
     import pdfplumber
+
     with pdfplumber.open(str(pdf_path)) as pdf:
         texts = []
         for page in pdf.pages:
@@ -599,10 +624,7 @@
             f.write(html)
         anchors_path = dbg_dir / "adv_anchors.tsv"
         n_anchors = _debug_dump_anchors(html, anchors_path)
-        print(
-            f"[DEBUG] Saved ADV HTML ‚Üí {dbg_html} (len={len(html)})\n"
-            f"[DEBUG] Anchors dumped ‚Üí {anchors_path} (count={n_anchors})"
-        )
+        print(f"[DEBUG] Saved ADV HTML ‚Üí {dbg_html} (len={len(html)})\n[DEBUG] Anchors dumped ‚Üí {anchors_path} (count={n_anchors})")
     # If the page is dynamically rendered, try fetching a likely static JSON/CSV endpoint as a fallback
     if args.backfill_year:
         links = list_adv_zip_urls(html, year=args.backfill_year)
@@ -619,7 +641,7 @@
                 # Fallback: anchors seem ordered newest‚Üíoldest; take those that share the same year as first, stopping when month changes
                 first_year = None
                 first_group: List[Tuple[str, Optional[dt.date]]] = []
-                for (u, d) in links:
+                for u, d in links:
                     if first_year is None and d is not None:
                         first_year = d.year
                     if d is None or d.year == first_year:
@@ -636,10 +658,7 @@
     # If no links discovered, attempt a secondary strategy: probe month anchors by text
     if not links:
         # Common month labels listed on the page
-        month_labels = [
-            "January", "February", "March", "April", "May", "June",
-            "July", "August", "September", "October", "November", "December"
-        ]
+        month_labels = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
         soup = BeautifulSoup(html, "html.parser")
         anchors = soup.find_all("a")
         if getattr(args, "debug", False):
@@ -681,7 +700,7 @@
         ctype = resp.headers.get("Content-Type", "").lower()
         cd = resp.headers.get("Content-Disposition", "")
         # Accept common ZIP types and octet-stream if URL looks like zip
-        is_zip_by_url = url.lower().endswith('.zip')
+        is_zip_by_url = url.lower().endswith(".zip")
         is_zip_by_hdr = ("zip" in ctype) or (".zip" in cd.lower()) or ("octet-stream" in ctype)
         if not (is_zip_by_url and is_zip_by_hdr):
             print(f"[WARN] Skipping non-zip response from {url} (Content-Type: {ctype})")
@@ -692,6 +711,7 @@
         ensure_dir(month_dir)
         # Preserve basename to avoid overwrites (handles March_1_of_2 vs _2_of_2)
         from urllib.parse import urlparse
+
         basename = os.path.basename(urlparse(url).path) or "form-adv-part2.zip"
         zip_path = month_dir / basename
         if not zip_path.exists():
@@ -800,12 +820,17 @@
 
     # Print summary
     print(f"Scanned {len(groups)} groups, {total_files} files")
-    print(f"Found {len(duplicates_report)} duplicate content hashes; removed/moved {total_dupes} files" if (args.delete or args.move_to) else f"Found {len(duplicates_report)} duplicate content hashes")
+    print(
+        f"Found {len(duplicates_report)} duplicate content hashes; removed/moved {total_dupes} files"
+        if (args.delete or args.move_to)
+        else f"Found {len(duplicates_report)} duplicate content hashes"
+    )
     # Print top N samples
     for i, (h, paths) in enumerate(duplicates_report[:20]):
         names = ", ".join(p.name for p in sorted(paths))
         print(f"[DUPLICATE] sha256={h[:12]}... count={len(paths)} in {paths[0].parent} ‚Üí {names}")
 
+
 def cmd_backfill_foia(args: argparse.Namespace) -> None:
     base = Path(args.output).resolve()
     raw_zip_dir = base / "raw" / "adv_zip"
@@ -843,7 +868,7 @@
         content = resp.content
         ctype = resp.headers.get("Content-Type", "").lower()
         cd = resp.headers.get("Content-Disposition", "")
-        is_zip_by_url = url.lower().endswith('.zip')
+        is_zip_by_url = url.lower().endswith(".zip")
         is_zip_by_hdr = ("zip" in ctype) or (".zip" in cd.lower()) or ("octet-stream" in ctype)
         if not (is_zip_by_url and is_zip_by_hdr):
             print(f"[WARN] Skipping non-zip response from {url} (Content-Type: {ctype})")
@@ -852,11 +877,29 @@
         # Derive year/month from d or from filename
         if not d:
             from urllib.parse import urlparse
+
             basename = os.path.basename(urlparse(url).path)
-            m = re.search(r"ADV_Brochures_(20\d{2})_(January|February|March|April|May|June|July|August|September|October|November|December)", basename, flags=re.IGNORECASE)
+            m = re.search(
+                r"ADV_Brochures_(20\d{2})_(January|February|March|April|May|June|July|August|September|October|November|December)",
+                basename,
+                flags=re.IGNORECASE,
+            )
             if m:
                 yr = int(m.group(1))
-                mon = {"january":1,"february":2,"march":3,"april":4,"may":5,"june":6,"july":7,"august":8,"september":9,"october":10,"november":11,"december":12}[m.group(2).lower()]
+                mon = {
+                    "january": 1,
+                    "february": 2,
+                    "march": 3,
+                    "april": 4,
+                    "may": 5,
+                    "june": 6,
+                    "july": 7,
+                    "august": 8,
+                    "september": 9,
+                    "october": 10,
+                    "november": 11,
+                    "december": 12,
+                }[m.group(2).lower()]
                 d = dt.date(yr, mon, 1)
             else:
                 d = dt.date.today().replace(day=1)
@@ -865,6 +908,7 @@
         month_dir = raw_zip_dir / f"year={y}" / f"month={mth:02d}"
         ensure_dir(month_dir)
         from urllib.parse import urlparse
+
         basename = os.path.basename(urlparse(url).path) or "form-adv-part2.zip"
         zip_path = month_dir / basename
         if not zip_path.exists():
@@ -880,6 +924,7 @@
             print(f"No new PDFs to extract from {url} (ZIP already processed)")
     print(f"Total FOIA PDFs written: {total_written}")
 
+
 def _file_sha256(path: Path, buf_size: int = 1024 * 1024) -> str:
     h = hashlib.sha256()
     with open(path, "rb") as f:
@@ -890,6 +935,7 @@
             h.update(b)
     return h.hexdigest()
 
+
 def _iter_pdf_crds(base: Path, month_filter: Optional[str]) -> set[int]:
     raw_pdf_dir = base / "raw" / "pdfs"
     crds: set[int] = set()
@@ -925,6 +971,7 @@
                     break
     return crds
 
+
 def _load_index_crds(base: Path) -> set[int]:
     processed_index_dir = base / "processed" / "index"
     candidates: list[Path] = []
@@ -936,10 +983,12 @@
                 candidates.append(mon_dir)
     if not candidates:
         return set()
+
     def key(p: Path) -> tuple[int, int]:
         y = int(p.parent.name.split("=", 1)[1])
         m = int(p.name.split("=", 1)[1])
         return (y, m)
+
     latest = max(candidates, key=key)
     pq = latest / "index.parquet"
     df = None
@@ -953,6 +1002,7 @@
     crd_series = pd.to_numeric(df["crd"], errors="coerce").dropna().astype(int)
     return set(crd_series.tolist())
 
+
 def cmd_coverage(args: argparse.Namespace) -> None:
     base = Path(args.output).resolve()
     index_crds = _load_index_crds(base)
@@ -965,6 +1015,7 @@
     missing = sorted(list(index_crds - pdf_crds))
     print(f"Missing CRDs sample (first 50): {missing[:50]}")
 
+
 def cmd_coverage_simple(args: argparse.Namespace) -> None:
     base = Path(args.output).resolve()
     index_crds = _load_index_crds(base)
@@ -989,21 +1040,24 @@
     missing = sorted(list(index_crds - have_pdf))
     print(f"Missing sample (first 50): {missing[:50]}")
 
+
 def _year_dirs_for_crd(crd_dir: Path) -> List[Path]:
     return [p for p in crd_dir.glob("year=*") if p.is_dir()]
 
+
 def _has_any_pdf_in_year(year_dir: Path) -> bool:
     # Fast check: scan month directories and stop at first file
     for mon_dir in year_dir.glob("month=*"):
         try:
             with os.scandir(mon_dir) as it:
                 for entry in it:
-                    if entry.is_file() and entry.name.lower().endswith('.pdf'):
+                    if entry.is_file() and entry.name.lower().endswith(".pdf"):
                         return True
         except FileNotFoundError:
             continue
     return False
 
+
 def cmd_coverage_per_year(args: argparse.Namespace) -> None:
     from concurrent.futures import ThreadPoolExecutor, as_completed
 
@@ -1040,6 +1094,7 @@
 
     # Parallel scan
     year_to_crds: dict[int, set[int]] = {}
+
     def work(item: Tuple[int, Path]) -> Optional[Tuple[int, int]]:
         crd, ydir = item
         try:
@@ -1080,6 +1135,7 @@
             for y, c, t, p in rows:
                 f.write(f"{y},{c},{t},{p:.2f}\n")
 
+
 def build_cli() -> argparse.ArgumentParser:
     p = argparse.ArgumentParser(description="Local SEC ingestion runner (index + ADV PDFs)")
     p.add_argument("--output", default=str(Path("output") / "sec_ingestion"), help="Base output directory")
@@ -1133,5 +1189,3 @@
 
 if __name__ == "__main__":
     raise SystemExit(main())
-
-

--- services/sec_ingestion/test_backfill_incremental.py
+++ services/sec_ingestion/test_backfill_incremental.py
@@ -423,5 +423,3 @@
 
 if __name__ == "__main__":
     raise SystemExit(main())
-
-

--- services/sec_ingestion/test_ria_live.py
+++ services/sec_ingestion/test_ria_live.py
@@ -10,7 +10,7 @@
 
 Required env vars:
   - RIA_LIVE_BASE_URL   ‚Üí e.g. https://<app-runner-service>
-     
+
    Authentication options (choose one):
      - RIA_LIVE_TOKEN      ‚Üí Bearer token for authentication (if you already have one)
      - RIA_LIVE_EMAIL      ‚Üí Email for login authentication
@@ -114,24 +114,18 @@
     token = os.environ.get("RIA_LIVE_TOKEN")
     if token:
         return token
-    
+
     # If no token, try to get credentials and login
     email = os.environ.get("RIA_LIVE_EMAIL")
     password = os.environ.get("RIA_LIVE_PASSWORD")
-    
+
     if not email or not password:
-        pytest.skip(
-            "Skipping live RIA tests: No authentication provided. "
-            "Set either RIA_LIVE_TOKEN or both RIA_LIVE_EMAIL and RIA_LIVE_PASSWORD"
-        )
-    
+        pytest.skip("Skipping live RIA tests: No authentication provided. Set either RIA_LIVE_TOKEN or both RIA_LIVE_EMAIL and RIA_LIVE_PASSWORD")
+
     # Attempt to login and get token
     login_url = f"{base_url}/api/auth/login"
-    login_data = {
-        "email": email,
-        "password": password
-    }
-    
+    login_data = {"email": email, "password": password}
+
     try:
         response = requests.post(login_url, json=login_data, timeout=30)
         response.raise_for_status()
@@ -148,11 +142,8 @@
 def _login_and_get_token(base_url: str, email: str, password: str) -> str:
     """Login and get token for standalone mode (no pytest.skip)."""
     login_url = f"{base_url}/api/auth/login"
-    login_data = {
-        "email": email,
-        "password": password
-    }
-    
+    login_data = {"email": email, "password": password}
+
     try:
         response = requests.post(login_url, json=login_data, timeout=30)
         response.raise_for_status()
@@ -170,17 +161,17 @@
     """Prompt user for credentials if not provided via environment variables."""
     email = os.environ.get("RIA_LIVE_EMAIL")
     password = os.environ.get("RIA_LIVE_PASSWORD")
-    
+
     if not email:
         email = input("Enter your email: ").strip()
         if not email:
             pytest.skip("Skipping live RIA tests: No email provided")
-    
+
     if not password:
         password = getpass.getpass("Enter your password: ")
         if not password:
             pytest.skip("Skipping live RIA tests: No password provided")
-    
+
     return email, password
 
 
@@ -224,50 +215,51 @@
     """Test downloading a PDF file."""
     print(f"üìÑ Testing PDF download: {expected_filename}")
     print(f"üîç Download URL: {download_url}")
-    
+
     try:
         # Make sure we have the full URL
         if download_url.startswith("/"):
             download_url = f"{session.base_url}{download_url}"
             print(f"üîç Converted relative URL to: {download_url}")
-        
+
         print(f"üîç Making HTTP GET request to: {download_url}")
-        
+
         resp = session.get(download_url)
         print(f"üîç HTTP Response - Status: {resp.status_code}")
         print(f"üîç HTTP Response - Headers: {dict(resp.headers)}")
-        
+
         resp.raise_for_status()
         print(f"‚úÖ HTTP request successful")
-        
+
         # Check content type
         content_type = resp.headers.get("content-type", "")
         print(f"üîç Content Type: {content_type}")
         assert "application/pdf" in content_type, f"Expected PDF content type, got: {content_type}"
-        
+
         # Check content length
         content_length = len(resp.content)
         print(f"üîç Content Length: {content_length} bytes")
         assert content_length > 0, "PDF content should not be empty"
-        
+
         # Save PDF for inspection
         out_dir = _output_dir()
         pdf_path = out_dir / f"downloaded_{expected_filename}"
         print(f"üîç Saving PDF to: {pdf_path}")
-        
+
         pdf_path.write_bytes(resp.content)
-        
+
         # Verify file was saved
         if pdf_path.exists():
             saved_size = pdf_path.stat().st_size
             print(f"‚úÖ PDF download successful: {content_length} bytes downloaded, {saved_size} bytes saved to {pdf_path}")
         else:
             print(f"‚ùå PDF file was not saved to {pdf_path}")
-        
+
     except Exception as e:
         print(f"‚ùå PDF download failed: {e}")
         print(f"üîç Exception type: {type(e).__name__}")
         import traceback
+
         print(f"üîç Full traceback: {traceback.format_exc()}")
         raise
 
@@ -275,7 +267,7 @@
 @pytest.mark.integration
 def test_live_index_filter(live_session: requests.Session) -> None:
     """Test the RIA index endpoint with basic filtering.
-    
+
     Description: Tests the basic RIA index endpoint to verify it returns data
     Coverage: Basic index endpoint functionality with state filter
     Expected: Returns list of RIA companies filtered by state (NJ)
@@ -294,7 +286,7 @@
 @pytest.mark.integration
 def test_live_company_detail(live_session: requests.Session) -> None:
     """Test company detail endpoint with CRD lookup and PDF download.
-    
+
     Description: Tests company detail endpoint with CRD 79, includes PDF download test
     Coverage: Company detail endpoint, trend data, PDF metadata and download
     Expected: Returns company data, trends, and PDF information for CRD 79
@@ -312,7 +304,7 @@
     print(f"üîç DEBUG: Response payload keys: {list(payload.keys())}")
     print(f"üîç DEBUG: Response payload crd: {payload.get('crd')}")
     print(f"üîç DEBUG: Response payload pdfs count: {len(payload.get('pdfs', []))}")
-    
+
     assert str(payload.get("crd")) == str(default_crd)
     assert payload.get("index")
     assert payload.get("pdfs") is not None
@@ -322,7 +314,7 @@
     _save_results(f"company_{default_crd}_pdfs", payload.get("pdfs"))
 
     _maybe_download_pdfs(payload.get("pdfs") or [])
-    
+
     # Test PDF download if PDFs are available
     print(f"üîç DEBUG: Starting PDF download test section")
     pdfs = payload.get("pdfs", [])
@@ -330,31 +322,31 @@
     print(f"üîç DEBUG: payload keys: {list(payload.keys())}")
     print(f"üîç DEBUG: payload.get('pdfs') type: {type(payload.get('pdfs'))}")
     print(f"üîç DEBUG: payload.get('pdfs') value: {payload.get('pdfs')}")
-    
+
     if pdfs and len(pdfs) > 0:
         print(f"üîç PDF Download Test - Found {len(pdfs)} PDFs to download")
-        
+
         for i, pdf_record in enumerate(pdfs):
-            print(f"üîç PDF {i+1} record: {pdf_record}")
-            
+            print(f"üîç PDF {i + 1} record: {pdf_record}")
+
             # Extract filename from rel_path (always available in latest.csv)
             rel_path = pdf_record.get("rel_path")
             download_url = pdf_record.get("download_url")
-            
-            print(f"üîç PDF {i+1} Download Test - rel_path: {rel_path}")
-            print(f"üîç PDF {i+1} Download Test - download_url: {download_url}")
-            
+
+            print(f"üîç PDF {i + 1} Download Test - rel_path: {rel_path}")
+            print(f"üîç PDF {i + 1} Download Test - download_url: {download_url}")
+
             if rel_path and download_url:
-                filename = rel_path.split('/')[-1]
-                print(f"üîç PDF {i+1} Download Test - Extracted filename: {filename}")
-                print(f"üîç PDF {i+1} Download Test - Starting download...")
+                filename = rel_path.split("/")[-1]
+                print(f"üîç PDF {i + 1} Download Test - Extracted filename: {filename}")
+                print(f"üîç PDF {i + 1} Download Test - Starting download...")
                 try:
                     _test_pdf_download(live_session, download_url, filename)
-                    print(f"‚úÖ PDF {i+1} download completed successfully")
+                    print(f"‚úÖ PDF {i + 1} download completed successfully")
                 except Exception as pdf_error:
-                    print(f"‚ùå PDF {i+1} download failed: {pdf_error}")
+                    print(f"‚ùå PDF {i + 1} download failed: {pdf_error}")
             else:
-                print(f"‚ùå PDF {i+1} Download Test - Missing required fields:")
+                print(f"‚ùå PDF {i + 1} Download Test - Missing required fields:")
                 print(f"   rel_path: {rel_path}")
                 print(f"   download_url: {download_url}")
     else:
@@ -364,7 +356,7 @@
 @pytest.mark.integration
 def test_live_company_lookup(live_session: requests.Session) -> None:
     """Test company lookup with name search using "AMERICAN INVESTORS CO".
-    
+
     Description: Tests company lookup with name search using "AMERICAN INVESTORS CO"
     Coverage: Company lookup endpoint with fuzzy name matching
     Expected: Returns companies matching "AMERICAN INVESTORS CO" with similarity scores
@@ -383,7 +375,7 @@
 @pytest.mark.integration
 def test_live_ria_index_basic(live_session: requests.Session) -> None:
     """Test basic RIA index endpoint functionality.
-    
+
     Description: Tests basic RIA index endpoint functionality
     Coverage: Basic index endpoint without filters
     Expected: Returns list of RIA companies with basic data
@@ -404,6 +396,7 @@
 # STANDALONE SCRIPT FUNCTIONALITY
 # ============================================================================
 
+
 def _print_banner():
     """Print a welcome banner for standalone mode."""
     print("üöÄ RIA API Integration Test Suite")
@@ -421,7 +414,7 @@
         print(f"Base URL: {base_url}")
     else:
         print(f"üì° Using base URL: {base_url}")
-    
+
     return base_url.rstrip("/")
 
 
@@ -429,19 +422,19 @@
     """Get credentials interactively from user."""
     print("\nüîê Authentication Required")
     print("-" * 30)
-    
+
     email = os.environ.get("RIA_LIVE_EMAIL")
     if not email:
         email = input("Enter your email: ").strip()
         if not email:
             raise ValueError("Email is required")
-    
+
     password = os.environ.get("RIA_LIVE_PASSWORD")
     if not password:
         password = getpass.getpass("Enter your password: ")
         if not password:
             raise ValueError("Password is required")
-    
+
     return email, password
 
 
@@ -450,13 +443,13 @@
     url = f"{session.base_url}{endpoint}"
     print(f"  Testing {description or endpoint}...")
     print(f"üîç DEBUG: _test_endpoint called with endpoint: {endpoint}")
-    
+
     try:
         response = session.get(url, params=params or {}, timeout=30)
         response.raise_for_status()
         data = response.json()
         print(f"    ‚úÖ Success - {response.status_code}")
-        
+
         # Special handling for company detail endpoint to test PDF download
         if "/ria/company/" in endpoint and not endpoint.endswith("/pdf/"):
             print(f"üîç DEBUG: Company detail endpoint detected, checking for PDFs...")
@@ -466,33 +459,33 @@
                 print(f"üîç DEBUG: Testing PDF downloads for all {len(pdfs)} PDFs")
                 # Test PDF download for all PDFs
                 for i, pdf_record in enumerate(pdfs):
-                    print(f"üîç DEBUG: PDF {i+1}: {pdf_record}")
+                    print(f"üîç DEBUG: PDF {i + 1}: {pdf_record}")
                     if pdf_record.get("rel_path") and pdf_record.get("download_url"):
-                        filename = pdf_record["rel_path"].split('/')[-1]
-                        print(f"üîç DEBUG: Attempting PDF {i+1} download for: {filename}")
+                        filename = pdf_record["rel_path"].split("/")[-1]
+                        print(f"üîç DEBUG: Attempting PDF {i + 1} download for: {filename}")
                         try:
                             _test_pdf_download(session, pdf_record["download_url"], filename)
-                            print(f"‚úÖ PDF {i+1} download completed successfully")
+                            print(f"‚úÖ PDF {i + 1} download completed successfully")
                         except Exception as pdf_error:
-                            print(f"‚ùå PDF {i+1} download failed: {pdf_error}")
+                            print(f"‚ùå PDF {i + 1} download failed: {pdf_error}")
                     else:
-                        print(f"‚ùå PDF {i+1} missing required fields (rel_path or download_url)")
-        
+                        print(f"‚ùå PDF {i + 1} missing required fields (rel_path or download_url)")
+
         return {"success": True, "data": data, "status_code": response.status_code}
     except requests.exceptions.RequestException as e:
         print(f"    ‚ùå Failed - {e}")
-        return {"success": False, "error": str(e), "status_code": getattr(e.response, 'status_code', None)}
+        return {"success": False, "error": str(e), "status_code": getattr(e.response, "status_code", None)}
 
 
 def _run_standalone_tests():
     """Run all RIA endpoint tests in standalone mode."""
     print(f"üîç DEBUG: _run_standalone_tests function started")
     _print_banner()
-    
+
     # Get configuration
     base_url = _get_base_url()
     print(f"üîç DEBUG: Base URL: {base_url}")
-    
+
     # Get authentication
     try:
         email, password = _get_credentials_interactive()
@@ -500,7 +493,7 @@
     except ValueError as e:
         print(f"‚ùå Error: {e}")
         return
-    
+
     # Authenticate
     print("\nüîë Authenticating...")
     try:
@@ -510,179 +503,130 @@
     except Exception as e:
         print(f"‚ùå Authentication failed: {e}")
         return
-    
+
     # Create session
     session = requests.Session()
     session.headers.update({"Authorization": f"Bearer {token}"})
     session.base_url = base_url
-    
+
     # Run tests
     print("\nüß™ Running RIA Endpoint Tests")
     print("-" * 40)
-    
+
     results = {}
-    
+
     # Test 1: Basic RIA Index
-    results["ria_index_basic"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/index", 
-        {"limit": 5}, 
-        "RIA Index (basic)"
-    )
-    
+    results["ria_index_basic"] = _test_endpoint(session, "/api/prospecting/ria/index", {"limit": 5}, "RIA Index (basic)")
+
     # Test 2: RIA Index with State Filter
-    results["ria_index_filter"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/index", 
-        {"state": "CA", "limit": 3}, 
-        "RIA Index (CA filter)"
-    )
-    
+    results["ria_index_filter"] = _test_endpoint(session, "/api/prospecting/ria/index", {"state": "CA", "limit": 3}, "RIA Index (CA filter)")
+
     # Test 3: Company Lookup (Name only)
     results["company_lookup"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "AMERICAN INVESTORS CO", "limit": 3}, 
-        "Company Lookup (Name)"
+        session, "/api/prospecting/ria/company/lookup", {"name": "AMERICAN INVESTORS CO", "limit": 3}, "Company Lookup (Name)"
     )
-    
+
     # Test 4: Company Lookup with State Filter
     results["company_lookup_state"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "AMERICAN", "state": "CA", "limit": 3}, 
-        "Company Lookup (Name + State)"
+        session, "/api/prospecting/ria/company/lookup", {"name": "AMERICAN", "state": "CA", "limit": 3}, "Company Lookup (Name + State)"
     )
-    
+
     # Test 5: Company Lookup with AUM Filter
     results["company_lookup_aum"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "NY", "min_aum": 1000000000, "limit": 3}, 
-        "Company Lookup (State + AUM)"
+        session, "/api/prospecting/ria/company/lookup", {"state": "NY", "min_aum": 1000000000, "limit": 3}, "Company Lookup (State + AUM)"
     )
-    
+
     # Test 6: Company Details
     test_crd = os.environ.get("RIA_LIVE_TEST_CRD", "79")
-    results["company_detail"] = _test_endpoint(
-        session, 
-        f"/api/prospecting/ria/company/{test_crd}", 
-        description=f"Company Details (CRD {test_crd})"
-    )
-    
+    results["company_detail"] = _test_endpoint(session, f"/api/prospecting/ria/company/{test_crd}", description=f"Company Details (CRD {test_crd})")
+
     # Test 7: Location Metadata Caching
     print("\nüîç Testing Location Metadata Caching...")
     start_time = time.time()
-    results["location_combined"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/metadata/location-data", 
-        description="Location Metadata (Combined)"
-    )
+    results["location_combined"] = _test_endpoint(session, "/api/prospecting/ria/metadata/location-data", description="Location Metadata (Combined)")
     first_call_time = time.time() - start_time
-    
+
     # Test cache performance
     start_time = time.time()
-    results["location_cached"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/metadata/location-data", 
-        description="Location Metadata (Cached)"
-    )
+    results["location_cached"] = _test_endpoint(session, "/api/prospecting/ria/metadata/location-data", description="Location Metadata (Cached)")
     second_call_time = time.time() - start_time
-    
+
     if second_call_time < first_call_time * 0.5:
         print(f"‚úÖ Cache performance: {second_call_time:.3f}s vs {first_call_time:.3f}s")
     else:
         print(f"‚ö†Ô∏è Cache performance: {second_call_time:.3f}s vs {first_call_time:.3f}s")
-    
+
     # Test 8: Filter Functionality Sample
     print("\nüîç Testing Filter Functionality...")
-    results["state_filter"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "limit": 3}, 
-        "State Filter (CA)"
-    )
-    
-    results["aum_filter"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"min_aum": 1000000000, "limit": 3}, 
-        "AUM Filter (1B+)"
-    )
-    
+    results["state_filter"] = _test_endpoint(session, "/api/prospecting/ria/company/lookup", {"state": "CA", "limit": 3}, "State Filter (CA)")
+
+    results["aum_filter"] = _test_endpoint(session, "/api/prospecting/ria/company/lookup", {"min_aum": 1000000000, "limit": 3}, "AUM Filter (1B+)")
+
     # Test 9: Filter Combinations
     print("\nüîç Testing Filter Combinations...")
     results["combined_filters"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "min_aum": 10000000, "has_hedge_funds": "Y", "limit": 3}, 
-        "Combined Filters (State + AUM + Hedge Funds)"
+        session,
+        "/api/prospecting/ria/company/lookup",
+        {"state": "CA", "min_aum": 10000000, "has_hedge_funds": "Y", "limit": 3},
+        "Combined Filters (State + AUM + Hedge Funds)",
     )
-    
+
     # Test 10: Name Matching Variations
     print("\nüîç Testing Name Matching...")
     results["name_exact"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "AMERICAN INVESTORS CO", "limit": 3}, 
-        "Name Match (Exact)"
+        session, "/api/prospecting/ria/company/lookup", {"name": "AMERICAN INVESTORS CO", "limit": 3}, "Name Match (Exact)"
     )
-    
-    results["name_partial"] = _test_endpoint(
-        session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "AMERICAN", "limit": 3}, 
-        "Name Match (Partial)"
-    )
-    
+
+    results["name_partial"] = _test_endpoint(session, "/api/prospecting/ria/company/lookup", {"name": "AMERICAN", "limit": 3}, "Name Match (Partial)")
+
     # Summary
     print("\nüìä Test Results Summary")
     print("=" * 30)
-    
+
     successful = sum(1 for r in results.values() if r["success"])
     total = len(results)
-    
+
     for test_name, result in results.items():
         status = "‚úÖ PASS" if result["success"] else "‚ùå FAIL"
         print(f"{status} {test_name}")
         if not result["success"]:
             print(f"    Error: {result['error']}")
-    
+
     print(f"\nOverall: {successful}/{total} tests passed")
-    
+
     # Save results
     output_dir = _output_dir()
     print(f"\nüíæ Results saved to: {output_dir}")
-    
+
     # Save individual responses
     for test_name, result in results.items():
         if result["success"]:
             _save_results(f"{test_name}_response", result["data"])
-    
+
     # Save summary
     summary = {
         "timestamp": json.dumps({"$date": {"$numberLong": str(int(__import__("time").time() * 1000))}}),
         "base_url": base_url,
         "total_tests": total,
         "successful_tests": successful,
-        "results": {k: {"success": v["success"], "status_code": v.get("status_code")} for k, v in results.items()}
+        "results": {k: {"success": v["success"], "status_code": v.get("status_code")} for k, v in results.items()},
     }
-    
+
     _save_results("test_summary", summary)
-    
+
     print("\nüéâ Testing complete!")
 
 
 def test_live_location_metadata_caching(live_session):
     """Comprehensive test of location metadata caching functionality.
-    
+
     Description: Test all location metadata endpoints and caching behavior
-    Coverage: 
+    Coverage:
       - Combined endpoint (/ria/metadata/location-data)
       - Individual endpoints (/ria/metadata/countries, /ria/metadata/cities)
       - Cache performance (first call vs subsequent calls)
       - Data consistency between endpoints
-    Expected: 
+    Expected:
       - All endpoints return valid country and city lists
       - Combined endpoint returns both countries and cities
       - Individual endpoints return respective data
@@ -691,61 +635,48 @@
     Validation: Response times, data structure, content consistency
     """
     results = {}
-    
+
     # Test 1: Combined endpoint
     start_time = time.time()
     results["location_combined"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/metadata/location-data", 
-        description="Location Metadata (Combined)"
+        live_session, "/api/prospecting/ria/metadata/location-data", description="Location Metadata (Combined)"
     )
     first_call_time = time.time() - start_time
-    
+
     # Test 2: Individual endpoints
-    results["countries"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/metadata/countries", 
-        description="Countries Metadata"
-    )
-    
-    results["cities"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/metadata/cities", 
-        description="Cities Metadata"
-    )
-    
+    results["countries"] = _test_endpoint(live_session, "/api/prospecting/ria/metadata/countries", description="Countries Metadata")
+
+    results["cities"] = _test_endpoint(live_session, "/api/prospecting/ria/metadata/cities", description="Cities Metadata")
+
     # Test 3: Cache performance (second call to combined endpoint)
     start_time = time.time()
     results["location_combined_cached"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/metadata/location-data", 
-        description="Location Metadata (Cached)"
+        live_session, "/api/prospecting/ria/metadata/location-data", description="Location Metadata (Cached)"
     )
     second_call_time = time.time() - start_time
-    
+
     # Validate cache performance
     if second_call_time < first_call_time * 0.5:  # Should be at least 50% faster
         print(f"‚úÖ Cache performance: {second_call_time:.3f}s vs {first_call_time:.3f}s")
     else:
         print(f"‚ö†Ô∏è Cache performance: {second_call_time:.3f}s vs {first_call_time:.3f}s (expected faster)")
-    
+
     # Validate data consistency
     combined_data = results["location_combined"]
     countries_data = results["countries"]
     cities_data = results["cities"]
-    
-    if (combined_data.get("countries") == countries_data.get("countries") and 
-        combined_data.get("cities") == cities_data.get("cities")):
+
+    if combined_data.get("countries") == countries_data.get("countries") and combined_data.get("cities") == cities_data.get("cities"):
         print("‚úÖ Data consistency: All endpoints return same data")
     else:
         print("‚ùå Data consistency: Endpoints return different data")
-    
+
     _save_results("location_metadata_caching", results)
 
 
 def test_live_filter_functionality_sample(live_session):
     """Test sample of key filter types to verify core filtering works.
-    
+
     Description: Test representative samples of each major filter category
     Coverage:
       - Location filter: state="CA" (dropdown exact match)
@@ -760,61 +691,44 @@
     Validation: Verify all returned records meet filter criteria
     """
     results = {}
-    
+
     # Test 1: Location filter (state)
-    results["state_filter"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "limit": 5}, 
-        "State Filter (CA)"
-    )
-    
+    results["state_filter"] = _test_endpoint(live_session, "/api/prospecting/ria/company/lookup", {"state": "CA", "limit": 5}, "State Filter (CA)")
+
     # Test 2: AUM range filter
     results["aum_range_filter"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"min_aum": 1000000000, "max_aum": 10000000000, "limit": 5}, 
-        "AUM Range Filter (1B-10B)"
+        live_session, "/api/prospecting/ria/company/lookup", {"min_aum": 1000000000, "max_aum": 10000000000, "limit": 5}, "AUM Range Filter (1B-10B)"
     )
-    
+
     # Test 3: Client type filter
     results["client_type_filter"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"has_hedge_funds": "Y", "limit": 5}, 
-        "Asset Class Filter (Hedge Funds)"
+        live_session, "/api/prospecting/ria/company/lookup", {"has_hedge_funds": "Y", "limit": 5}, "Asset Class Filter (Hedge Funds)"
     )
-    
+
     # Test 3b: Individual Clients filter
     results["individual_clients_filter"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"individual_clients": "Y", "limit": 5}, 
-        "Client Type Filter (Individual Clients)"
+        live_session, "/api/prospecting/ria/company/lookup", {"individual_clients": "Y", "limit": 5}, "Client Type Filter (Individual Clients)"
     )
-    
+
     # Test 3c: High Net Worth Clients filter
     results["high_net_worth_clients_filter"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"high_net_worth_clients": "Y", "limit": 5}, 
-        "Client Type Filter (High Net Worth Clients)"
+        live_session,
+        "/api/prospecting/ria/company/lookup",
+        {"high_net_worth_clients": "Y", "limit": 5},
+        "Client Type Filter (High Net Worth Clients)",
     )
-    
+
     # Test 4: Size filter
     results["size_filter"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"min_total_employees": 50, "limit": 5}, 
-        "Size Filter (50+ Employees)"
+        live_session, "/api/prospecting/ria/company/lookup", {"min_total_employees": 50, "limit": 5}, "Size Filter (50+ Employees)"
     )
-    
+
     _save_results("filter_functionality_sample", results)
 
 
 def test_live_filter_combinations_sample(live_session):
     """Test sample of filter combinations to verify multiple filters work together.
-    
+
     Description: Test representative combinations of different filter types
     Coverage:
       - Location + AUM: state="CA" + min_aum=1000000000
@@ -828,45 +742,36 @@
     Validation: Verify each returned record meets ALL applied filters
     """
     results = {}
-    
+
     # Test 1: Location + AUM
     results["location_aum_combo"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "min_aum": 1000000000, "limit": 5}, 
-        "Location + AUM Filter"
+        live_session, "/api/prospecting/ria/company/lookup", {"state": "CA", "min_aum": 1000000000, "limit": 5}, "Location + AUM Filter"
     )
-    
+
     # Test 2: AUM + Client Type
     results["aum_client_combo"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"min_aum": 500000000, "has_hedge_funds": "Y", "limit": 5}, 
-        "AUM + Hedge Funds Filter"
+        live_session, "/api/prospecting/ria/company/lookup", {"min_aum": 500000000, "has_hedge_funds": "Y", "limit": 5}, "AUM + Hedge Funds Filter"
     )
-    
+
     # Test 3: Location + Size
     results["location_size_combo"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "NY", "min_total_employees": 100, "limit": 5}, 
-        "Location + Size Filter"
+        live_session, "/api/prospecting/ria/company/lookup", {"state": "NY", "min_total_employees": 100, "limit": 5}, "Location + Size Filter"
     )
-    
+
     # Test 4: Name + Filters
     results["name_filters_combo"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "AMERICAN", "state": "CA", "min_aum": 1000000000, "limit": 5}, 
-        "Name + Location + AUM Filter"
+        live_session,
+        "/api/prospecting/ria/company/lookup",
+        {"name": "AMERICAN", "state": "CA", "min_aum": 1000000000, "limit": 5},
+        "Name + Location + AUM Filter",
     )
-    
+
     _save_results("filter_combinations_sample", results)
 
 
 def test_live_filter_combinations_performance(live_session):
     """Test performance of complex filter combinations.
-    
+
     Description: Measure response times for complex multi-filter queries
     Coverage:
       - Simple filter: state="CA"
@@ -880,54 +785,50 @@
     """
     results = {}
     performance_results = {}
-    
+
     # Test 1: Simple filter
     start_time = time.time()
     results["simple_filter"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "limit": 10}, 
-        "Simple Filter Performance"
+        live_session, "/api/prospecting/ria/company/lookup", {"state": "CA", "limit": 10}, "Simple Filter Performance"
     )
     simple_time = time.time() - start_time
     performance_results["simple"] = simple_time
-    
+
     # Test 2: Medium complexity
     start_time = time.time()
     results["medium_complexity"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "min_aum": 1000000000, "has_hedge_funds": "Y", "limit": 10}, 
-        "Medium Complexity Performance"
+        live_session,
+        "/api/prospecting/ria/company/lookup",
+        {"state": "CA", "min_aum": 1000000000, "has_hedge_funds": "Y", "limit": 10},
+        "Medium Complexity Performance",
     )
     medium_time = time.time() - start_time
     performance_results["medium"] = medium_time
-    
+
     # Test 3: High complexity
     start_time = time.time()
     results["high_complexity"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "min_aum": 1000000000, "has_hedge_funds": "Y", 
-         "min_total_employees": 50, "private_funds": "Y", "limit": 10}, 
-        "High Complexity Performance"
+        live_session,
+        "/api/prospecting/ria/company/lookup",
+        {"state": "CA", "min_aum": 1000000000, "has_hedge_funds": "Y", "min_total_employees": 50, "private_funds": "Y", "limit": 10},
+        "High Complexity Performance",
     )
     high_time = time.time() - start_time
     performance_results["high"] = high_time
-    
+
     # Validate performance thresholds
     print(f"\nüìä Performance Results:")
     print(f"  Simple filter: {simple_time:.3f}s {'‚úÖ' if simple_time < 0.5 else '‚ùå'} (target: <0.5s)")
     print(f"  Medium complexity: {medium_time:.3f}s {'‚úÖ' if medium_time < 1.0 else '‚ùå'} (target: <1.0s)")
     print(f"  High complexity: {high_time:.3f}s {'‚úÖ' if high_time < 2.0 else '‚ùå'} (target: <2.0s)")
-    
+
     results["performance_metrics"] = performance_results
     _save_results("filter_combinations_performance", results)
 
 
 def test_live_name_matching_validation(live_session):
     """Test name matching accuracy for 'AMERICAN INVESTORS CO' and variations.
-    
+
     Description: Verify fuzzy matching works correctly for known company name
     Coverage:
       - Exact match: "AMERICAN INVESTORS CO"
@@ -945,32 +846,25 @@
     """
     results = {}
     matching_results = {}
-    
+
     test_cases = [
         ("AMERICAN INVESTORS CO", "exact_match", 1.0),
         ("AMERICAN INVESTORS", "partial_match", 0.8),
         ("AMERICAN INVESORS CO", "typo_variation", 0.7),
         ("american investors co", "case_variation", 0.9),
-        ("INVESTORS AMERICAN CO", "word_order", 0.8)
+        ("INVESTORS AMERICAN CO", "word_order", 0.8),
     ]
-    
+
     for search_term, test_name, min_expected_score in test_cases:
         start_time = time.time()
         result = _test_endpoint(
-            live_session, 
-            "/api/prospecting/ria/company/lookup", 
-            {"name": search_term, "limit": 5}, 
-            f"Name Matching ({test_name})"
+            live_session, "/api/prospecting/ria/company/lookup", {"name": search_term, "limit": 5}, f"Name Matching ({test_name})"
         )
         response_time = time.time() - start_time
-        
+
         results[test_name] = result
-        matching_results[test_name] = {
-            "search_term": search_term,
-            "response_time": response_time,
-            "min_expected_score": min_expected_score
-        }
-        
+        matching_results[test_name] = {"search_term": search_term, "response_time": response_time, "min_expected_score": min_expected_score}
+
         # Validate match scores if available
         if result and "matches" in result:
             matches = result["matches"]
@@ -984,14 +878,14 @@
                     print(f"  {test_name}: No match score available")
             else:
                 print(f"  {test_name}: No matches found")
-    
+
     results["matching_analysis"] = matching_results
     _save_results("name_matching_validation", results)
 
 
 def test_live_company_lookup_sorting_verification(live_session):
     """Test that results are sorted correctly based on search type.
-    
+
     Description: Verify proper sorting logic for different search scenarios
     Coverage:
       - Name search: Should be sorted by similarity score (descending)
@@ -1005,83 +899,68 @@
     """
     results = {}
     sorting_results = {}
-    
+
     # Test 1: Name search (should sort by similarity score)
     start_time = time.time()
-    name_result = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "AMERICAN", "limit": 10}, 
-        "Name Search Sorting"
-    )
+    name_result = _test_endpoint(live_session, "/api/prospecting/ria/company/lookup", {"name": "AMERICAN", "limit": 10}, "Name Search Sorting")
     name_time = time.time() - start_time
-    
+
     # Test 2: Filter-only search (should sort by AUM)
     start_time = time.time()
     filter_result = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"state": "CA", "min_aum": 1000000000, "limit": 10}, 
-        "Filter-Only Search Sorting"
+        live_session, "/api/prospecting/ria/company/lookup", {"state": "CA", "min_aum": 1000000000, "limit": 10}, "Filter-Only Search Sorting"
     )
     filter_time = time.time() - start_time
-    
+
     # Test 3: Name + filters (should sort by similarity score)
     start_time = time.time()
     combined_result = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "AMERICAN", "state": "CA", "limit": 10}, 
-        "Name + Filter Search Sorting"
+        live_session, "/api/prospecting/ria/company/lookup", {"name": "AMERICAN", "state": "CA", "limit": 10}, "Name + Filter Search Sorting"
     )
     combined_time = time.time() - start_time
-    
+
     results["name_search"] = name_result
     results["filter_only"] = filter_result
     results["combined_search"] = combined_result
-    
+
     # Validate sorting
     print(f"\nüìä Sorting Validation:")
-    
+
     # Check name search sorting
     if name_result and "matches" in name_result:
         matches = name_result["matches"]
         if len(matches) > 1:
             scores = [m.get("match_score", 0) for m in matches if "match_score" in m]
             if scores:
-                is_sorted = all(scores[i] >= scores[i+1] for i in range(len(scores)-1))
+                is_sorted = all(scores[i] >= scores[i + 1] for i in range(len(scores) - 1))
                 print(f"  Name search sorting: {'‚úÖ' if is_sorted else '‚ùå'} (by similarity score)")
             else:
                 print(f"  Name search sorting: ‚ö†Ô∏è No match scores available")
-    
+
     # Check filter-only sorting
     if filter_result and "matches" in filter_result:
         matches = filter_result["matches"]
         if len(matches) > 1:
             aum_values = [m.get("5F(2)(c)", 0) for m in matches if "5F(2)(c)" in m]
             if aum_values:
-                is_sorted = all(aum_values[i] >= aum_values[i+1] for i in range(len(aum_values)-1))
+                is_sorted = all(aum_values[i] >= aum_values[i + 1] for i in range(len(aum_values) - 1))
                 print(f"  Filter-only sorting: {'‚úÖ' if is_sorted else '‚ùå'} (by AUM)")
             else:
                 print(f"  Filter-only sorting: ‚ö†Ô∏è No AUM values available")
-    
+
     # Check combined search sorting
     if combined_result and "matches" in combined_result:
         matches = combined_result["matches"]
         if len(matches) > 1:
             scores = [m.get("match_score", 0) for m in matches if "match_score" in m]
             if scores:
-                is_sorted = all(scores[i] >= scores[i+1] for i in range(len(scores)-1))
+                is_sorted = all(scores[i] >= scores[i + 1] for i in range(len(scores) - 1))
                 print(f"  Combined search sorting: {'‚úÖ' if is_sorted else '‚ùå'} (by similarity score)")
             else:
                 print(f"  Combined search sorting: ‚ö†Ô∏è No match scores available")
-    
-    sorting_results = {
-        "name_search_time": name_time,
-        "filter_only_time": filter_time,
-        "combined_search_time": combined_time
-    }
-    
+
+    sorting_results = {"name_search_time": name_time, "filter_only_time": filter_time, "combined_search_time": combined_time}
+
     results["sorting_analysis"] = sorting_results
     _save_results("sorting_verification", results)
 
@@ -1089,7 +968,7 @@
 @pytest.mark.integration
 def test_live_company_lookup_pagination(live_session):
     """Test pagination functionality for company lookup.
-    
+
     Description: Tests pagination with different offset/limit combinations to verify
     that results are returned correctly and pagination metadata is accurate
     Coverage: Pagination logic, offset/limit parameters, pagination metadata
@@ -1097,55 +976,46 @@
     Output: tests/integration/output/pagination_test.json
     """
     results = {}
-    
+
     # Test 1: First page (offset=0, limit=5)
     print("üîç Testing pagination - Page 1 (offset=0, limit=5)")
     results["page_1"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "INVESTORS", "limit": 5, "offset": 0}, 
-        "Pagination Page 1 (offset=0, limit=5)"
+        live_session, "/api/prospecting/ria/company/lookup", {"name": "INVESTORS", "limit": 5, "offset": 0}, "Pagination Page 1 (offset=0, limit=5)"
     )
-    
+
     # Test 2: Second page (offset=5, limit=5)
     print("üîç Testing pagination - Page 2 (offset=5, limit=5)")
     results["page_2"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "INVESTORS", "limit": 5, "offset": 5}, 
-        "Pagination Page 2 (offset=5, limit=5)"
+        live_session, "/api/prospecting/ria/company/lookup", {"name": "INVESTORS", "limit": 5, "offset": 5}, "Pagination Page 2 (offset=5, limit=5)"
     )
-    
+
     # Test 3: Third page (offset=10, limit=5)
     print("üîç Testing pagination - Page 3 (offset=10, limit=5)")
     results["page_3"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "INVESTORS", "limit": 5, "offset": 10}, 
-        "Pagination Page 3 (offset=10, limit=5)"
+        live_session, "/api/prospecting/ria/company/lookup", {"name": "INVESTORS", "limit": 5, "offset": 10}, "Pagination Page 3 (offset=10, limit=5)"
     )
-    
+
     # Test 4: Large offset (offset=50, limit=5) - should return empty or fewer results
     print("üîç Testing pagination - Large offset (offset=50, limit=5)")
     results["large_offset"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "INVESTORS", "limit": 5, "offset": 50}, 
-        "Pagination Large Offset (offset=50, limit=5)"
+        live_session,
+        "/api/prospecting/ria/company/lookup",
+        {"name": "INVESTORS", "limit": 5, "offset": 50},
+        "Pagination Large Offset (offset=50, limit=5)",
     )
-    
+
     # Test 5: Different limit (offset=0, limit=3)
     print("üîç Testing pagination - Different limit (offset=0, limit=3)")
     results["different_limit"] = _test_endpoint(
-        live_session, 
-        "/api/prospecting/ria/company/lookup", 
-        {"name": "INVESTORS", "limit": 3, "offset": 0}, 
-        "Pagination Different Limit (offset=0, limit=3)"
+        live_session,
+        "/api/prospecting/ria/company/lookup",
+        {"name": "INVESTORS", "limit": 3, "offset": 0},
+        "Pagination Different Limit (offset=0, limit=3)",
     )
-    
+
     # Verify pagination logic
     print("üîç Verifying pagination logic...")
-    
+
     # Check that all pages have pagination metadata
     for page_name, page_result in results.items():
         if page_result["success"]:
@@ -1153,7 +1023,7 @@
             if "pagination" in data:
                 pagination = data["pagination"]
                 print(f"‚úÖ {page_name} has pagination metadata: {pagination}")
-                
+
                 # Verify pagination fields exist
                 required_fields = ["offset", "limit", "has_more", "next_offset", "showing"]
                 for field in required_fields:
@@ -1163,20 +1033,20 @@
             else:
                 print(f"‚ùå {page_name} missing pagination metadata")
                 page_result["pagination_error"] = "Missing pagination metadata"
-    
+
     # Verify no duplicate results across pages
     if all(results[page]["success"] for page in ["page_1", "page_2", "page_3"]):
         print("üîç Checking for duplicate results across pages...")
-        
+
         page_1_crds = {match["crd"] for match in results["page_1"]["data"]["matches"]}
         page_2_crds = {match["crd"] for match in results["page_2"]["data"]["matches"]}
         page_3_crds = {match["crd"] for match in results["page_3"]["data"]["matches"]}
-        
+
         # Check for overlaps
         overlap_1_2 = page_1_crds & page_2_crds
         overlap_1_3 = page_1_crds & page_3_crds
         overlap_2_3 = page_2_crds & page_3_crds
-        
+
         if overlap_1_2:
             print(f"‚ùå Duplicate CRDs between page 1 and 2: {overlap_1_2}")
             results["duplicate_error"] = f"Page 1-2 overlap: {overlap_1_2}"
@@ -1189,22 +1059,22 @@
         else:
             print("‚úÖ No duplicate results across pages")
             results["no_duplicates"] = True
-    
+
     # Verify total_matches consistency
     if all(results[page]["success"] for page in ["page_1", "page_2", "page_3"]):
         print("üîç Checking total_matches consistency...")
-        
+
         total_matches_1 = results["page_1"]["data"]["total_matches"]
         total_matches_2 = results["page_2"]["data"]["total_matches"]
         total_matches_3 = results["page_3"]["data"]["total_matches"]
-        
+
         if total_matches_1 == total_matches_2 == total_matches_3:
             print(f"‚úÖ Total matches consistent across pages: {total_matches_1}")
             results["total_matches_consistent"] = True
         else:
             print(f"‚ùå Total matches inconsistent: {total_matches_1}, {total_matches_2}, {total_matches_3}")
             results["total_matches_error"] = f"Inconsistent totals: {total_matches_1}, {total_matches_2}, {total_matches_3}"
-    
+
     # Save results
     _save_results("pagination_test", results)
 
@@ -1213,14 +1083,14 @@
     """Save test results to JSON file, overwriting existing files."""
     output_dir = Path("tests/integration/output")
     output_dir.mkdir(parents=True, exist_ok=True)
-    
+
     # Use fixed filename to overwrite on each run
     filename = f"{test_name}.json"
     filepath = output_dir / filename
-    
+
     with open(filepath, "w") as f:
         json.dump(results, f, indent=2, default=str)
-    
+
     print(f"üíæ Results saved to: {filepath}")
 
 
@@ -1238,123 +1108,120 @@
     """Get credentials for authentication."""
     email = os.environ.get("RIA_LIVE_EMAIL")
     password = os.environ.get("RIA_LIVE_PASSWORD")
-    
+
     if not email or not password:
         print("üîê Please enter your credentials:")
         email = input("Email: ").strip()
         password = getpass.getpass("Password: ").strip()
-    
+
     if not email or not password:
         raise ValueError("Email and password are required")
-    
+
     return email, password
 
 
 def _test_pagination(session: requests.Session, search_term: str):
     """Test pagination functionality with detailed validation."""
     print(f"  Testing Pagination for '{search_term}'...")
-    
+
     # Test Page 1
     print("    üìÑ Testing Page 1 (offset=0, limit=5)...")
     page1_response = session.get(
-        f"{session.base_url}/api/prospecting/ria/company/lookup",
-        params={"name": search_term, "limit": 5, "offset": 0},
-        timeout=30
+        f"{session.base_url}/api/prospecting/ria/company/lookup", params={"name": search_term, "limit": 5, "offset": 0}, timeout=30
     )
     page1_response.raise_for_status()
     page1_data = page1_response.json()
-    
+
     # Validate page 1 response structure
     required_fields = ["total_matches", "matches", "pagination"]
     for field in required_fields:
         if field not in page1_data:
             print(f"    ‚ùå Page 1 missing required field: {field}")
             return False
-    
+
     pagination = page1_data["pagination"]
     required_pagination_fields = ["offset", "limit", "has_more", "next_offset", "showing"]
     for field in required_pagination_fields:
         if field not in pagination:
             print(f"    ‚ùå Page 1 pagination missing field: {field}")
             return False
-    
+
     print(f"    ‚úÖ Page 1: {len(page1_data['matches'])} matches, total: {page1_data['total_matches']}")
     print(f"    üìä Page 1 pagination: offset={pagination['offset']}, limit={pagination['limit']}, has_more={pagination['has_more']}")
-    
+
     # Test Page 2 if there are more results
     if pagination["has_more"] and pagination["next_offset"] is not None:
         print(f"    üìÑ Testing Page 2 (offset={pagination['next_offset']}, limit=5)...")
         page2_response = session.get(
             f"{session.base_url}/api/prospecting/ria/company/lookup",
             params={"name": search_term, "limit": 5, "offset": pagination["next_offset"]},
-            timeout=30
+            timeout=30,
         )
         page2_response.raise_for_status()
         page2_data = page2_response.json()
-        
+
         # Validate page 2 response structure
         for field in required_fields:
             if field not in page2_data:
                 print(f"    ‚ùå Page 2 missing required field: {field}")
                 return False
-        
+
         page2_pagination = page2_data["pagination"]
         for field in required_pagination_fields:
             if field not in page2_pagination:
                 print(f"    ‚ùå Page 2 pagination missing field: {field}")
                 return False
-        
+
         print(f"    ‚úÖ Page 2: {len(page2_data['matches'])} matches, total: {page2_data['total_matches']}")
-        print(f"    üìä Page 2 pagination: offset={page2_pagination['offset']}, limit={page2_pagination['limit']}, has_more={page2_pagination['has_more']}")
-        
+        print(
+            f"    üìä Page 2 pagination: offset={page2_pagination['offset']}, limit={page2_pagination['limit']}, has_more={page2_pagination['has_more']}"
+        )
+
         # Validate pagination consistency
         if page1_data["total_matches"] != page2_data["total_matches"]:
             print(f"    ‚ùå Total matches inconsistent: Page 1={page1_data['total_matches']}, Page 2={page2_data['total_matches']}")
             return False
-        
+
         if page2_pagination["offset"] != pagination["next_offset"]:
             print(f"    ‚ùå Page 2 offset mismatch: expected={pagination['next_offset']}, actual={page2_pagination['offset']}")
             return False
-        
+
         # Check for duplicate results between pages
         page1_crds = {match.get("crd") for match in page1_data["matches"]}
         page2_crds = {match.get("crd") for match in page2_data["matches"]}
-        
+
         # Debug: Show sample record structure
         if page1_data["matches"]:
             sample_record = page1_data["matches"][0]
             print(f"    üîç Sample record fields: {list(sample_record.keys())}")
             print(f"    üîç Sample CRD value: {sample_record.get('crd')}")
-        
+
         duplicates = page1_crds.intersection(page2_crds)
         if duplicates:
             print(f"    ‚ùå Found duplicate CRDs between pages: {duplicates}")
             return False
-        
+
         print(f"    ‚úÖ No duplicate results between pages")
         duplicates = set()  # No duplicates found
     else:
         print(f"    ‚ÑπÔ∏è  Only one page of results available (total: {page1_data['total_matches']})")
         duplicates = set()  # No duplicates possible with single page
         page2_data = None
-    
+
     # Save pagination test results
     pagination_results = {
-        "page1": {
-            "params": {"name": search_term, "limit": 5, "offset": 0},
-            "response": page1_data
-        },
+        "page1": {"params": {"name": search_term, "limit": 5, "offset": 0}, "response": page1_data},
         "page2": {
             "params": {"name": search_term, "limit": 5, "offset": pagination["next_offset"]} if pagination["has_more"] else None,
-            "response": page2_data
+            "response": page2_data,
         },
         "validation": {
             "total_matches_consistent": page1_data["total_matches"] == (page2_data["total_matches"] if page2_data else page1_data["total_matches"]),
             "no_duplicates": len(duplicates) == 0,
-            "pagination_metadata_valid": all(field in pagination for field in ["offset", "limit", "has_more", "next_offset", "showing"])
-        }
+            "pagination_metadata_valid": all(field in pagination for field in ["offset", "limit", "has_more", "next_offset", "showing"]),
+        },
     }
-    
+
     # Save to file (overwrites existing)
     _save_results("pagination_test_results", pagination_results)
     print(f"    ‚úÖ Pagination test completed successfully!")
@@ -1373,84 +1240,84 @@
     """Run all RIA endpoint tests in standalone mode."""
     print(f"üîç DEBUG: _run_standalone_tests function started")
     _print_banner()
-    
+
     # Get configuration
     base_url = _get_base_url()
     print(f"üîç DEBUG: Base URL: {base_url}")
-    
+
     # Get authentication
     try:
         email, password = _get_credentials()
         print(f"üîç DEBUG: Got credentials for email: {email}")
-        
+
         # Create session and authenticate
         session = requests.Session()
         session.base_url = base_url
         token = _login_and_get_token(base_url, email, password)
         session.headers.update({"Authorization": f"Bearer {token}"})
         print(f"üîç DEBUG: Authentication successful")
-        
+
     except Exception as e:
         print(f"‚ùå Authentication failed: {e}")
         return
-    
+
     # Run all comprehensive tests
     print(f"üîç DEBUG: Starting comprehensive standalone tests...")
-    
+
     try:
         # Test 1: Basic RIA Index
         print("üîç Running basic RIA index test...")
         test_live_ria_index_basic(session)
-        
+
         # Test 2: Company Lookup
         print("üîç Running company lookup test...")
         test_live_company_lookup(session)
-        
+
         # Test 3: Company Detail
         print("üîç Running company detail test...")
         test_live_company_detail(session)
-        
+
         # Test 4: Index Filter
         print("üîç Running index filter test...")
         test_live_index_filter(session)
-        
+
         # Test 5: Location Metadata Caching
         print("üîç Running location metadata caching test...")
         test_live_location_metadata_caching(session)
-        
+
         # Test 6: Filter Functionality Sample
         print("üîç Running filter functionality sample test...")
         test_live_filter_functionality_sample(session)
-        
+
         # Test 7: Filter Combinations Sample
         print("üîç Running filter combinations sample test...")
         test_live_filter_combinations_sample(session)
-        
+
         # Test 8: Filter Combinations Performance
         print("üîç Running filter combinations performance test...")
         test_live_filter_combinations_performance(session)
-        
+
         # Test 9: Name Matching Validation
         print("üîç Running name matching validation test...")
         test_live_name_matching_validation(session)
-        
+
         # Test 10: Company Lookup Sorting Verification
         print("üîç Running company lookup sorting verification test...")
         test_live_company_lookup_sorting_verification(session)
-        
+
         # Test 11: Pagination test
         print("üîç Running pagination test...")
         _test_pagination(session, "INVESTORS")
-        
+
         print("‚úÖ All 11 comprehensive standalone tests completed successfully!")
-        
+
     except Exception as e:
         print(f"‚ùå Test execution failed: {e}")
         import traceback
+
         traceback.print_exc()
 
 
 if __name__ == "__main__":
     # Run in standalone mode
     _run_standalone_tests()
-

--- services/sec_ingestion/utils/params.py
+++ services/sec_ingestion/utils/params.py
@@ -54,5 +54,3 @@
         if key in os.environ:
             continue
         os.environ[key] = param.get("Value", "")
-
-

--- services/webhook_fanout/lambda_function.py
+++ services/webhook_fanout/lambda_function.py
@@ -21,41 +21,38 @@
 _secrets_cache = {}
 
 # Secrets Manager ARN
-SECRETS_MANAGER_ARN = os.environ.get(
-    "SECRETS_MANAGER_ARN",
-    "arn:aws:secretsmanager:eu-west-1:536984667321:secret:lambda-webhook-jUzhd2"
-)
+SECRETS_MANAGER_ARN = os.environ.get("SECRETS_MANAGER_ARN", "arn:aws:secretsmanager:eu-west-1:536984667321:secret:lambda-webhook-jUzhd2")
 
 
 def get_secrets_client():
     """Get or create Secrets Manager client."""
     global _secrets_client
     if _secrets_client is None:
-        _secrets_client = boto3.client('secretsmanager', region_name='eu-west-1')
+        _secrets_client = boto3.client("secretsmanager", region_name="eu-west-1")
     return _secrets_client
 
 
 def get_secret_from_manager(secret_key: str) -> Optional[str]:
     """
     Get secret value from Secrets Manager with caching.
-    
+
     Secrets are cached for the lifetime of the Lambda execution context.
     """
     # Check cache first
     if secret_key in _secrets_cache:
         return _secrets_cache[secret_key]
-    
+
     try:
         client = get_secrets_client()
         response = client.get_secret_value(SecretId=SECRETS_MANAGER_ARN)
-        
+
         # Parse the secret (assuming JSON format)
-        secret_string = response.get('SecretString', '{}')
+        secret_string = response.get("SecretString", "{}")
         secrets_dict = json.loads(secret_string)
-        
+
         # Cache all secrets
         _secrets_cache.update(secrets_dict)
-        
+
         return secrets_dict.get(secret_key)
     except ClientError as e:
         logger.error(f"Error retrieving secret from Secrets Manager: {e}")
@@ -71,7 +68,7 @@
 def get_secret_value(secret_key: str, env_fallback: str = "") -> str:
     """
     Get secret value from Secrets Manager, with fallback to environment variable.
-    
+
     This allows gradual migration - can use env vars during development,
     Secrets Manager in production.
     """
@@ -79,13 +76,13 @@
     secret_value = get_secret_from_manager(secret_key)
     if secret_value:
         return secret_value
-    
+
     # Fallback to environment variable
     env_value = os.environ.get(secret_key, env_fallback)
     if env_value:
         logger.warning(f"Using environment variable for {secret_key} (Secrets Manager not available)")
         return env_value
-    
+
     return ""
 
 
@@ -111,32 +108,32 @@
     query_params = event.get("queryStringParameters") or {}
     if query_params.get("secret") == TAMRADAR_WEBHOOK_SECRET:
         return True
-    
+
     headers = event.get("headers") or {}
     secret_header = headers.get("x-tamradar-secret") or headers.get("X-TAMradar-Secret")
     if secret_header == TAMRADAR_WEBHOOK_SECRET:
         return True
-    
+
     return False
 
 
 def get_backend_urls() -> List[str]:
     """Get list of backend URLs to fan-out to."""
     urls = []
-    
+
     if DEV_BACKEND_URL:
         urls.append(f"{DEV_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
     if STAGE_BACKEND_URL:
         urls.append(f"{STAGE_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
     if PROD_BACKEND_URL:
         urls.append(f"{PROD_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
-    
+
     if ENABLE_LOCAL_ROUTING and LOCAL_BACKEND_URL:
         if "/api/webhooks/tamradar" in LOCAL_BACKEND_URL:
             urls.append(LOCAL_BACKEND_URL)
         else:
             urls.append(f"{LOCAL_BACKEND_URL.rstrip('/')}/api/webhooks/tamradar/internal")
-    
+
     return urls
 
 
@@ -146,17 +143,17 @@
         parsed_url = urlparse(url)
         query_params = parse_qs(parsed_url.query)
         query_params["secret"] = [INTERNAL_WEBHOOK_SECRET]
-        
+
         new_query = "&".join([f"{k}={v[0]}" for k, v in query_params.items()])
         url_with_secret = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"
-        
+
         with httpx.Client(timeout=timeout) as client:
             response = client.post(
                 url_with_secret,
                 json=payload,
                 headers={"Content-Type": "application/json"},
             )
-            
+
             return {
                 "url": url,
                 "status": response.status_code,
@@ -182,7 +179,7 @@
 def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
     """Lambda handler function."""
     logger.info("Webhook fan-out function invoked")
-    
+
     # 1. Validate TAMradar secret
     if not validate_tamradar_secret(event):
         logger.warning("Invalid TAMradar secret")
@@ -191,7 +188,7 @@
             "headers": {"Content-Type": "application/json"},
             "body": json.dumps({"error": "Invalid webhook secret"}),
         }
-    
+
     # 2. Parse request body
     try:
         body = event.get("body", "{}")
@@ -206,17 +203,17 @@
             "headers": {"Content-Type": "application/json"},
             "body": json.dumps({"error": "Invalid JSON payload"}),
         }
-    
+
     # 3. Extract metadata for logging
     radar_id = payload.get("data", {}).get("radar_id", "unknown")
     event_id = payload.get("event_id", "unknown")
     event_type = payload.get("event_type", "unknown")
-    
+
     logger.info(f"Processing webhook: event_id={event_id}, radar_id={radar_id}, type={event_type}")
-    
+
     # 4. Get backend URLs
     backend_urls = get_backend_urls()
-    
+
     if not backend_urls:
         logger.error("No backend URLs configured")
         return {
@@ -224,43 +221,44 @@
             "headers": {"Content-Type": "application/json"},
             "body": json.dumps({"error": "No backend URLs configured"}),
         }
-    
+
     logger.info(f"Fanning out to {len(backend_urls)} backends: {backend_urls}")
-    
+
     # 5. Fan-out to all backends in parallel
     results = []
     with ThreadPoolExecutor(max_workers=len(backend_urls)) as executor:
-        future_to_url = {
-            executor.submit(send_to_backend, url, payload, HTTP_TIMEOUT): url
-            for url in backend_urls
-        }
-        
+        future_to_url = {executor.submit(send_to_backend, url, payload, HTTP_TIMEOUT): url for url in backend_urls}
+
         for future in as_completed(future_to_url):
             url = future_to_url[future]
             try:
                 result = future.result()
                 results.append(result)
-                
+
                 if result["success"]:
                     logger.info(f"Successfully sent to {result['url']}: HTTP {result['status']}")
                 else:
                     logger.warning(f"Failed to send to {result['url']}: {result.get('error', 'Unknown error')}")
             except Exception as e:
                 logger.error(f"Error sending to {url}: {e}", exc_info=True)
-                results.append({
-                    "url": url,
-                    "status": None,
-                    "success": False,
-                    "error": str(e),
-                })
-    
+                results.append(
+                    {
+                        "url": url,
+                        "status": None,
+                        "success": False,
+                        "error": str(e),
+                    }
+                )
+
     # 6. Return 200 to TAMradar immediately
     return {
         "statusCode": 200,
         "headers": {"Content-Type": "application/json"},
-        "body": json.dumps({
-            "status": "ok",
-            "message": "Webhook received and fanned out",
-            "backends_contacted": len(backend_urls),
-        }),
+        "body": json.dumps(
+            {
+                "status": "ok",
+                "message": "Webhook received and fanned out",
+                "backends_contacted": len(backend_urls),
+            }
+        ),
     }

--- test/conftest.py
+++ test/conftest.py
@@ -37,55 +37,64 @@
 _imported_jwk = None
 _imported_clear_jwks_cache = None
 
+
 def _get_GlobalDB():
     """Get GlobalDB class - import only once and cache."""
     global _imported_GlobalDB
     if _imported_GlobalDB is None:
         from app.utils.global_db import GlobalDB
+
         _imported_GlobalDB = GlobalDB
     return _imported_GlobalDB
 
+
 def _get_jwt():
     """Get jwt module - import only once and cache."""
     global _imported_jwt
     if _imported_jwt is None:
         from jose import jwt
+
         _imported_jwt = jwt
     return _imported_jwt
 
+
 def _get_jwk():
     """Get jwk module - import only once and cache."""
     global _imported_jwk
     if _imported_jwk is None:
         from jwcrypto import jwk
+
         _imported_jwk = jwk
     return _imported_jwk
 
+
 def _get_clear_jwks_cache():
     """Get clear_jwks_cache function - import only once and cache."""
     global _imported_clear_jwks_cache
     if _imported_clear_jwks_cache is None:
         from app.security import clear_jwks_cache
+
         _imported_clear_jwks_cache = clear_jwks_cache
     return _imported_clear_jwks_cache
 
+
 @pytest_asyncio.fixture(scope="session")
 async def _db_session():
     """
     Session-scoped TEMPORARY database for CI/CD - created and destroyed per test run.
-    
+
     IMPORTANT: Creates a temporary test database that is COMPLETELY ISOLATED from production.
     - Database is created at test start with unique name (prospecting_db_test_<timestamp>)
     - Schema is automatically set up via ProspectingDB._create_tables()
     - Database is DROPPED after all tests complete
     - No data persists between test runs
-    
+
     SAFETY: This fixture ensures tests NEVER touch production data by:
     1. Disabling AWS Secrets Manager (prevents production config override)
     2. Creating database with unique name containing "test"
     3. Verifying test database is actually used after initialization
     4. Dropping database after tests complete
-    
+
     REQUIREMENTS: PostgreSQL user needs CREATE DATABASE and DROP DATABASE permissions.
     For CI/CD, use a dedicated test user with these permissions.
     """
@@ -93,12 +102,12 @@
     if _IS_UNIT_TEST:
         yield None
         return
-    
+
     import os
     import asyncpg
     import uuid
     import time
-    
+
     # CRITICAL: Disable AWS Secrets Manager to prevent production config override
     os.environ["DISABLE_AWS_SECRETS"] = "true"
     # CRITICAL: Ensure PostgreSQL storage is enabled for tests
@@ -108,15 +117,10 @@
     # CRITICAL: Disable profile embedding in tests (we use dummy embeddings instead)
     # This prevents background tasks from running and causing "Task was destroyed" warnings
     os.environ["ENABLE_PROFILE_EMBEDDING"] = "false"
-    
+
     # Import config helper functions to get database credentials
-    from app.utils.config import (
-        get_postgres_host,
-        get_postgres_port,
-        get_postgres_user,
-        get_postgres_password
-    )
-    
+    from app.utils.config import get_postgres_host, get_postgres_port, get_postgres_user, get_postgres_password
+
     # Use config.py helper functions to get all DB credentials from .env
     # These will read from POSTGRES_*_LOCAL env vars or .env file
     # We use ALL credentials from config EXCEPT database name (which we create)
@@ -124,22 +128,22 @@
     test_db_port = get_postgres_port()
     test_db_user = get_postgres_user()
     test_db_password = get_postgres_password()
-    
+
     # Create unique test database name with timestamp for isolation
     # This ensures each test run gets a fresh database
     timestamp = int(time.time())
     unique_id = uuid.uuid4().hex[:8]
     test_db_name = f"prospecting_db_test_{timestamp}_{unique_id}"
-    
+
     # Save original env vars to restore later
     original_db = os.environ.get("POSTGRES_DB_LOCAL")
     original_disable_secrets = os.environ.get("DISABLE_AWS_SECRETS")
     original_enable_storage = os.environ.get("ENABLE_POSTGRES_STORAGE")
-    
+
     # Override ONLY the database name to use our temporary test database
     # All other credentials (host, port, user, password) come from config.py
     os.environ["POSTGRES_DB_LOCAL"] = test_db_name
-    
+
     # Connect to PostgreSQL server (not a specific database) to create test database
     admin_conn = None
     try:
@@ -149,21 +153,21 @@
             port=test_db_port,
             user=test_db_user,
             password=test_db_password,
-            database="postgres"  # Connect to default postgres database
+            database="postgres",  # Connect to default postgres database
         )
-        
+
         # Create the test database
         await admin_conn.execute(f'CREATE DATABASE "{test_db_name}"')
         await admin_conn.close()
         admin_conn = None
-        
+
         # Import GlobalDB ONLY ONCE - this is the only place it's imported
         GlobalDB = _get_GlobalDB()
-        
+
         # Initialize once per session - this will create all tables via _create_tables()
         GlobalDB.reset_for_testing()
         await GlobalDB.initialize()
-        
+
         # SAFETY CHECK: Verify test database is actually being used
         actual_db = GlobalDB._instance.database
         if "test" not in actual_db.lower():
@@ -172,20 +176,16 @@
                 f"Expected test database name containing 'test'. "
                 f"This prevents accidental production data access."
             )
-        
+
         yield GlobalDB._instance
-        
+
         # Clean up: Close connection pool first
         await GlobalDB.close()
-        
+
         # Drop the temporary test database
         try:
             admin_conn = await asyncpg.connect(
-                host=test_db_host,
-                port=test_db_port,
-                user=test_db_user,
-                password=test_db_password,
-                database="postgres"
+                host=test_db_host, port=test_db_port, user=test_db_user, password=test_db_password, database="postgres"
             )
             # Terminate any remaining connections to the test database
             await admin_conn.execute(f"""
@@ -199,29 +199,27 @@
             await admin_conn.close()
         except Exception as drop_error:
             import warnings
+
             warnings.warn(f"Failed to drop test database '{test_db_name}': {drop_error}. Manual cleanup may be required.")
-        
+
     except Exception as e:
         # If database creation/initialization fails, yield None
         # Tests that need DB will fail when they try to use it
         # Tests that don't need DB (like security_headers) will continue
         import warnings
+
         warnings.warn(
             f"Temporary database creation/initialization failed: {e}. "
             f"Tests requiring database will fail. "
             f"Ensure PostgreSQL user has CREATE DATABASE permission."
         )
         yield None
-        
+
         # Try to clean up if database was created but initialization failed
         if admin_conn is None:
             try:
                 admin_conn = await asyncpg.connect(
-                    host=test_db_host,
-                    port=test_db_port,
-                    user=test_db_user,
-                    password=test_db_password,
-                    database="postgres"
+                    host=test_db_host, port=test_db_port, user=test_db_user, password=test_db_password, database="postgres"
                 )
                 await admin_conn.execute(f'DROP DATABASE IF EXISTS "{test_db_name}"')
                 await admin_conn.close()
@@ -230,7 +228,7 @@
     finally:
         if admin_conn and not admin_conn.is_closed():
             await admin_conn.close()
-        
+
         # Restore original environment (only what we changed)
         if original_db:
             os.environ["POSTGRES_DB_LOCAL"] = original_db
@@ -265,17 +263,18 @@
     if "unit" in str(request.fspath) or _IS_UNIT_TEST:
         yield
         return
-    
+
     # Skip database setup for tests that don't need it (like security_headers)
     # These tests only check HTTP headers and don't interact with database
     if "test_security_headers" in str(request.fspath):
         yield
         return
-    
+
     # Database is already initialized in _db_session fixture
     # All tests share the same test DB instance (no cleanup between tests)
     yield
 
+
 @pytest_asyncio.fixture
 async def db(request, _db_session):
     """Get the global database instance for tests."""
@@ -283,10 +282,11 @@
     if "unit" in str(request.fspath) or _IS_UNIT_TEST:
         yield None
         return
-    
+
     # Use session-scoped database instance
     yield _db_session
 
+
 @pytest.fixture(autouse=True)
 def reset_app_overrides(request):
     """Reset FastAPI dependency overrides after each test."""
@@ -294,36 +294,46 @@
     if "unit" in str(request.fspath):
         yield
         return
-    
+
     from app.main import app
+
     yield
     app.dependency_overrides = {}
 
+
 @pytest.fixture(autouse=True)
 def mock_redis():
     """Mock Redis to prevent connection errors in tests."""
+
     class MockRedis:
         def __init__(self, *args, **kwargs):
             self._data = {}
+
         async def get(self, key):
             return self._data.get(key)
+
         async def set(self, key, value, *args, **kwargs):
             self._data[key] = str(value)
             return True
+
         async def incr(self, key):
             self._data[key] = str(int(self._data.get(key, 0)) + 1)
             return int(self._data[key])
+
         async def expire(self, key, seconds):
             return True
+
         async def keys(self, pattern):
             import re
+
             regex = re.compile(pattern.replace("*", ".*"))
             return [k for k in self._data if regex.match(k)]
+
         async def delete(self, *keys):
             for k in keys:
                 self._data.pop(k, None)
             return True
-    
+
     with patch("redis.asyncio.Redis", MockRedis):
         yield
 
@@ -332,6 +342,7 @@
 _clerk_signing_material_cache = None
 _lp_clerk_signing_material_cache = None
 
+
 def _generate_clerk_signing_material():
     """Generate Clerk signing material (cached to avoid regeneration)."""
     global _clerk_signing_material_cache
@@ -352,6 +363,7 @@
         }
     return _clerk_signing_material_cache
 
+
 @pytest.fixture(scope="session")
 def _clerk_signing_material(request):
     """Cached Clerk signing material - only generated when needed (lazy)."""
@@ -388,6 +400,7 @@
         }
     return _lp_clerk_signing_material_cache
 
+
 @pytest.fixture(scope="session")
 def _lp_clerk_signing_material(request):
     """Cached LP Clerk signing material - only generated when needed (lazy)."""
@@ -411,7 +424,7 @@
     if _IS_UNIT_TEST:
         yield
         return
-    
+
     import os
     from app.security import clerk_tokens
 
@@ -429,6 +442,7 @@
 
     def fake_get(url, *args, **kwargs):
         if url == jwks_url:
+
             class _Response:
                 status_code = 200
 
@@ -440,6 +454,7 @@
 
             return _Response()
         if url == lp_jwks_url:
+
             class _Response:
                 status_code = 200
 
@@ -454,9 +469,9 @@
 
     # Store original for cleanup
     clerk_tokens.httpx.get = fake_get
-    
+
     yield
-    
+
     # Cleanup: restore original and remove env vars
     clerk_tokens.httpx.get = original_get
     os.environ.pop("CLERK_JWKS_URL", None)
@@ -474,7 +489,7 @@
     if "unit" in str(request.fspath) or _IS_UNIT_TEST:
         yield
         return
-    
+
     clear_jwks_cache = _get_clear_jwks_cache()
     clear_jwks_cache()
     yield
@@ -485,7 +500,7 @@
 def clerk_token_factory(_clerk_signing_material):
     # Get cached jwt module
     jwt = _get_jwt()
-    
+
     private_key = _clerk_signing_material["private_key"]
     issuer = _clerk_signing_material["issuer"]
     audience = _clerk_signing_material["audience"]
@@ -537,7 +552,7 @@
 def lp_clerk_token_factory(_lp_clerk_signing_material):
     # Get cached jwt module
     jwt = _get_jwt()
-    
+
     private_key = _lp_clerk_signing_material["private_key"]
     issuer = _lp_clerk_signing_material["issuer"]
     audience = _lp_clerk_signing_material["audience"]
@@ -602,6 +617,7 @@
     Uses a unit vector where each dimension = 1/sqrt(3072) for valid similarity operations.
     """
     import math
+
     dimension_value = 1.0 / math.sqrt(3072)
     return [dimension_value] * 3072
 
@@ -609,6 +625,7 @@
 def _compute_profile_text_hash(profile_text: str) -> str:
     """Compute SHA256 hash of profile text, matching production code."""
     import hashlib
+
     return hashlib.sha256(profile_text.encode("utf-8")).hexdigest()
 
 
@@ -618,17 +635,17 @@
     This ensures the hash matches what the embedding service would generate.
     """
     sections = []
-    
+
     if firm_description and firm_description.strip():
         sections.append("Firm Description")
         sections.append(firm_description.strip())
         sections.append("")
-    
+
     if key_objectives and key_objectives.strip():
         sections.append("Key Objectives")
         sections.append(key_objectives.strip())
         sections.append("")
-    
+
     return "\n".join(sections).strip()
 
 
@@ -640,16 +657,17 @@
     # Build profile text matching production format
     profile_text = _build_test_profile_text(firm_description, key_differentiators, key_objectives)
     profile_text_hash = _compute_profile_text_hash(profile_text)
-    
+
     # Generate dummy embedding vector
     dummy_embedding = _generate_dummy_embedding_vector()
-    
+
     # Convert to string format for asyncpg VECTOR type: "[0.018,0.018,...]"
     embedding_str = "[" + ",".join(str(v) for v in dummy_embedding) + "]"
-    
+
     async with db.pool.acquire() as conn:
         # Insert embedding for 'investor_search' purpose
-        await conn.execute("""
+        await conn.execute(
+            """
             INSERT INTO profiles_embeddings (
                 user_id, embedding_purpose, format_version, model_name,
                 profile_text, profile_text_hash, embedding, updated_at
@@ -661,15 +679,24 @@
                 profile_text_hash = EXCLUDED.profile_text_hash,
                 embedding = EXCLUDED.embedding,
                 updated_at = NOW()
-        """, user_id, "investor_search", "v1", "text-embedding-3-large", profile_text, profile_text_hash, embedding_str)
-        
+        """,
+            user_id,
+            "investor_search",
+            "v1",
+            "text-embedding-3-large",
+            profile_text,
+            profile_text_hash,
+            embedding_str,
+        )
+
         # Build profile text for feedback analysis (enhanced context)
         # For simplicity, use same text but could be enhanced if needed
         feedback_profile_text = f"Profile Context:\n{profile_text}\n\nKey Differentiators: {key_differentiators or 'N/A'}"
         feedback_profile_text_hash = _compute_profile_text_hash(feedback_profile_text)
-        
+
         # Insert embedding for 'profile_feedback_analysis' purpose
-        await conn.execute("""
+        await conn.execute(
+            """
             INSERT INTO profiles_embeddings (
                 user_id, embedding_purpose, format_version, model_name,
                 profile_text, profile_text_hash, embedding, updated_at
@@ -681,7 +708,15 @@
                 profile_text_hash = EXCLUDED.profile_text_hash,
                 embedding = EXCLUDED.embedding,
                 updated_at = NOW()
-        """, user_id, "profile_feedback_analysis", "v1", "text-embedding-3-large", feedback_profile_text, feedback_profile_text_hash, embedding_str)
+        """,
+            user_id,
+            "profile_feedback_analysis",
+            "v1",
+            "text-embedding-3-large",
+            feedback_profile_text,
+            feedback_profile_text_hash,
+            embedding_str,
+        )
 
 
 @pytest.fixture
@@ -708,7 +743,7 @@
             "key_differentiators": "Test differentiators",
             "key_objectives": "Test objectives",
         }
-        
+
         # Merge with any existing extra_claims
         merged_extra_claims = extra_claims.copy() if extra_claims else {}
         if "public_metadata" in merged_extra_claims:
@@ -729,18 +764,19 @@
         # Make a request to trigger user sync via middleware
         # The middleware will sync the user, then we can get the user_id
         response = await api_client.get("/api/auth/me", headers=headers)
-        
+
         # If user not found, wait a moment for sync to complete and retry once
         if response.status_code == 404:
             import asyncio
+
             await asyncio.sleep(0.1)  # Brief wait for async sync to complete
             response = await api_client.get("/api/auth/me", headers=headers)
-        
+
         assert response.status_code == 200, f"Failed to sync user: {response.status_code} - {response.text}"
         payload = response.json()
-        
+
         user_id = payload["user_id"]
-        
+
         # Populate dummy profile embeddings for this test user
         # This prevents errors when embedding service tries to query the table
         try:
@@ -754,6 +790,7 @@
         except Exception as e:
             # Log but don't fail - embeddings are optional for basic tests
             import logging
+
             logging.warning(f"Failed to populate test profile embeddings for user {user_id}: {e}")
 
         return {
@@ -856,9 +893,10 @@
     # Skip for unit tests
     if _IS_UNIT_TEST:
         return None
-    
+
     # Import app once per session to avoid repeated slow imports
     from app.main import app
+
     return app
 
 
@@ -869,13 +907,14 @@
     if "unit" in str(request.fspath) or _IS_UNIT_TEST:
         yield None
         return
-    
+
     import httpx
-    
+
     # Use session-scoped app to avoid re-importing
     async with httpx.AsyncClient(transport=httpx.ASGITransport(app=_app), base_url="http://test") as client:
         yield client
 
+
 # To bypass email verification (e.g., for CI or local testing without Mailgun),
 # uncomment the following fixture. This will patch send_verification_email to a no-op.
 # By default, real email verification is used.
@@ -885,4 +924,4 @@
 #     from app.services import auth_service
 #     async def dummy_send_verification_email(*args, **kwargs):
 #         return None
-#     monkeypatch.setattr(auth_service, "send_verification_email", dummy_send_verification_email) 
+#     monkeypatch.setattr(auth_service, "send_verification_email", dummy_send_verification_email)

--- test/integration/__init__.py
+++ test/integration/__init__.py
@@ -5,4 +5,3 @@
 All tests use real database connections but mock external services (OpenAI, Perplexity, etc.)
 to ensure fast execution.
 """
-

--- test/integration/test_clerk_authentication.py
+++ test/integration/test_clerk_authentication.py
@@ -21,11 +21,11 @@
         email="auth_test@example.com",
         roles=["member"],
     )
-    
+
     # Verify user can access protected endpoint
     response = await api_client.get("/api/auth/me", headers=user["headers"])
     assert response.status_code == 200
-    
+
     payload = response.json()
     assert payload["user_id"] == user["user_id"]
     assert payload["email"].lower() == "auth_test@example.com"
@@ -41,7 +41,7 @@
         roles=["member"],
         daily_limit_override=25,
     )
-    
+
     # Verify user exists in database
     async with db.pool.acquire() as conn:
         db_user = await conn.fetchrow(
@@ -52,7 +52,7 @@
             """,
             user["user_id"],
         )
-    
+
     assert db_user is not None
     assert db_user["clerk_user_id"] == user["clerk_user_id"]
     assert db_user["email"] == "sync_test@example.com"
@@ -69,16 +69,16 @@
         email="admin_sync@example.com",
         roles=["admin"],
     )
-    
+
     # Verify admin status in database
     async with db.pool.acquire() as conn:
         is_admin = await conn.fetchval(
             "SELECT is_admin FROM users WHERE user_id = $1",
             admin_user["user_id"],
         )
-    
+
     assert is_admin is True
-    
+
     # Verify admin can access admin endpoint
     response = await api_client.get("/api/admin/approved-users", headers=admin_user["headers"])
     assert response.status_code == 200
@@ -95,9 +95,9 @@
             "DELETE FROM approved_users WHERE email = $1",
             unapproved_email.lower(),
         )
-    
+
     headers, _ = auth_headers(email=unapproved_email)
-    
+
     response = await api_client.get("/api/auth/me", headers=headers)
     assert response.status_code == 403
     # Middleware returns {"error": error, "message": message}, not {"detail": error}
@@ -111,12 +111,12 @@
     # clerk_token_factory always sets email, so we need to create token directly
     from jose import jwt
     import time
-    
+
     private_key = _clerk_signing_material["private_key"]
     issuer = _clerk_signing_material["issuer"]
     audience = _clerk_signing_material["audience"]
     kid = _clerk_signing_material["kid"]
-    
+
     now = int(time.time())
     # Create payload WITHOUT email claim
     payload = {
@@ -134,10 +134,10 @@
             "accountStatus": "approved",
         },
     }
-    
+
     headers = {"kid": kid}
     token = jwt.encode(payload, private_key, algorithm="RS256", headers=headers)
-    
+
     response = await api_client.get(
         "/api/auth/me",
         headers={"Authorization": f"Bearer {token}"},
@@ -162,7 +162,7 @@
             }
         },
     )
-    
+
     # Verify profile exists
     async with db.pool.acquire() as conn:
         profile = await conn.fetchrow(
@@ -173,7 +173,7 @@
             """,
             user["user_id"],
         )
-    
+
     assert profile is not None
     assert profile["firm_description"] == "Test description"
 
@@ -193,18 +193,17 @@
             "custom_field": "test_value",
         },
     )
-    
+
     # Verify claims snapshot stored
     async with db.pool.acquire() as conn:
         row = await conn.fetchrow(
             "SELECT claims_snapshot, last_synced_at FROM users WHERE user_id = $1",
             user["user_id"],
         )
-    
+
     assert row is not None
     assert row["last_synced_at"] is not None
-    
+
     snapshot = json.loads(row["claims_snapshot"])
     assert snapshot["public_metadata"]["firmName"] == "Snapshot Firm"
     assert snapshot["custom_field"] == "test_value"
-

--- test/integration/test_prospecting_workflow.py
+++ test/integration/test_prospecting_workflow.py
@@ -26,55 +26,55 @@
 ) -> Dict[str, Any]:
     """
     Poll status endpoint until workflow completes or times out.
-    
+
     Args:
         api_client: FastAPI test client
         run_id: Run ID to poll
         headers: Authentication headers
         timeout: Maximum time to wait in seconds
         poll_interval: Time between polls in seconds
-    
+
     Returns:
         Final status response
-    
+
     Raises:
         TimeoutError: If workflow doesn't complete within timeout
     """
     start_time = asyncio.get_event_loop().time()
-    
+
     while True:
         elapsed = asyncio.get_event_loop().time() - start_time
         if elapsed > timeout:
             raise TimeoutError(f"Workflow did not complete within {timeout} seconds")
-        
+
         response = await api_client.get(
             f"/api/prospecting/process/status/{run_id}",
             headers=headers,
         )
-        
+
         assert response.status_code == 200, f"Status check failed: {response.status_code} - {response.text}"
         status_data = response.json()
-        
+
         status = status_data.get("status", "").lower()
         progress = status_data.get("progress", "unknown")
-        
+
         # Log progress periodically
         if int(elapsed) % 10 == 0:  # Every 10 seconds
             print(f"  Status: {status}, Progress: {progress}, Elapsed: {int(elapsed)}s")
-        
+
         # Stop polling when workflow reaches a terminal state
         if status in ("completed", "failed", "error"):
             print(f"  Workflow reached terminal state: {status}")
             await asyncio.sleep(1.0)  # Brief wait for final DB writes
             return status_data
-        
+
         await asyncio.sleep(poll_interval)
 
 
 async def verify_enrichment_completeness(db, run_id: str):
     """
     Verify company and person enrichment agent output completeness.
-    
+
     Args:
         db: Database connection pool
         run_id: Run ID to check enrichment for
@@ -92,21 +92,21 @@
             """,
             run_id,
         )
-    
+
     assert company_enrichment_result is not None, f"Company enrichment agent result not found for run_id: {run_id}"
-    
+
     enrichment_data = company_enrichment_result.get("enrichment_data")
     if isinstance(enrichment_data, str):
         import json
+
         enrichment_data = json.loads(enrichment_data)
-    
-    assert enrichment_data is not None and isinstance(enrichment_data, dict), \
-        f"Company enrichment data is not a valid dict for run_id: {run_id}"
-    
+
+    assert enrichment_data is not None and isinstance(enrichment_data, dict), f"Company enrichment data is not a valid dict for run_id: {run_id}"
+
     # Calculate completeness based on all fields in SOURCE_MAPPINGS categories
     total_company_fields = 0
     populated_company_fields = 0
-    
+
     def count_enrichment_fields(category_data, category_name=""):
         """Recursively count fields in enrichment data structure."""
         nonlocal total_company_fields, populated_company_fields
@@ -128,14 +128,14 @@
                             populated_company_fields += 1
                     elif field_value:
                         populated_company_fields += 1
-    
+
     for category_name, category_data in enrichment_data.items():
         if category_name == "enrichment_metadata":
             continue
         count_enrichment_fields(category_data, category_name)
-    
+
     company_population_pct = (populated_company_fields / total_company_fields * 100) if total_company_fields > 0 else 0.0
-    
+
     print("\n=== Company Enrichment Agent Output ===")
     print(f"Total fields expected: {total_company_fields}")
     print(f"Populated fields: {populated_company_fields}")
@@ -144,13 +144,14 @@
         if category_name != "enrichment_metadata" and isinstance(category_data, dict):
             category_field_count = len(category_data)
             print(f"  {category_name}: {category_field_count} fields")
-    
+
     print(f"\nCompany enrichment completeness: {company_population_pct:.1f}% ({populated_company_fields}/{total_company_fields} fields populated)")
-    
-    assert company_population_pct >= 60.0, \
-        f"Company enrichment has only {company_population_pct:.1f}% fields populated (required: 60%). " \
+
+    assert company_population_pct >= 60.0, (
+        f"Company enrichment has only {company_population_pct:.1f}% fields populated (required: 60%). "
         f"Populated: {populated_company_fields}/{total_company_fields} fields."
-    
+    )
+
     # Step 2: Verify person enrichment agent output completeness
     async with db.pool.acquire() as conn:
         person_row = await conn.fetchrow(
@@ -162,20 +163,20 @@
             """,
             run_id,
         )
-    
+
     assert person_row is not None, f"Person enrichment result not found for run_id: {run_id}"
-    
+
     enrichment_data = person_row.get("enrichment_data")
     if isinstance(enrichment_data, str):
         import json
+
         enrichment_data = json.loads(enrichment_data)
-    
-    assert enrichment_data is not None and isinstance(enrichment_data, dict), \
-        f"Person enrichment data is not a valid dict for run_id: {run_id}"
-    
+
+    assert enrichment_data is not None and isinstance(enrichment_data, dict), f"Person enrichment data is not a valid dict for run_id: {run_id}"
+
     total_person_fields = 0
     populated_person_fields = 0
-    
+
     def count_person_enrichment_fields(category_data, category_name=""):
         """Recursively count fields in person enrichment data structure."""
         nonlocal total_person_fields, populated_person_fields
@@ -195,12 +196,12 @@
                             populated_person_fields += 1
                     elif field_value:
                         populated_person_fields += 1
-    
+
     for category_name, category_data in enrichment_data.items():
         count_person_enrichment_fields(category_data, category_name)
-    
+
     person_population_pct = (populated_person_fields / total_person_fields * 100) if total_person_fields > 0 else 0.0
-    
+
     print("\n=== Person Enrichment Agent Output ===")
     print(f"Total fields expected: {total_person_fields}")
     print(f"Populated fields: {populated_person_fields}")
@@ -209,12 +210,13 @@
         if isinstance(category_data, dict):
             category_field_count = len(category_data)
             print(f"  {category_name}: {category_field_count} fields")
-    
+
     print(f"\nPerson enrichment completeness: {person_population_pct:.1f}% ({populated_person_fields}/{total_person_fields} fields populated)")
-    
-    assert person_population_pct >= 50.0, \
-        f"Person enrichment has only {person_population_pct:.1f}% fields populated (required: 50%). " \
+
+    assert person_population_pct >= 50.0, (
+        f"Person enrichment has only {person_population_pct:.1f}% fields populated (required: 50%). "
         f"Populated: {populated_person_fields}/{total_person_fields} fields."
+    )
 
 
 @pytest.mark.asyncio
@@ -234,7 +236,7 @@
         email="specific_company_test@example.com",
         roles=["member"],
     )
-    
+
     # Step 1: Start session
     session_response = await api_client.post(
         "/api/prospecting/session/start",
@@ -245,7 +247,7 @@
     session_data = session_response.json()
     session_id = session_data["session_id"]
     assert session_id is not None
-    
+
     # Step 2: Start processing (creates run_id and workflow type)
     process_response = await api_client.post(
         "/api/prospecting/process/start",
@@ -256,31 +258,32 @@
         },
         headers=user["headers"],
     )
-    
+
     assert process_response.status_code == 202  # Accepted
     process_data = process_response.json()
     run_id = process_data["run_id"]
     assert run_id is not None
     assert process_data["status"] == "started"
-    
+
     # Step 3: Execute workflow directly using orchestrator
     # GlobalDB is already initialized with test DB, so no patching needed
     from app.agents.prospecting_orchestrator import ProspectingOrchestrator, SessionContext
-    
+
     print(f"Executing workflow for run_id: {run_id}")
-    
+
     # Create session context
     session_context = SessionContext(session_id=session_id, user_id=user["user_id"])
     # Align session interaction count with provided run_id
     try:
         import re
+
         m = re.search(r"_run_(\d+)$", run_id)
         if m:
             idx = int(m.group(1))
             session_context.interaction_count = max(0, idx - 1)
     except Exception:
         pass
-    
+
     # Execute orchestrator - it will use test DB automatically via get_global_db()
     orchestrator = ProspectingOrchestrator()
     await orchestrator.execute(
@@ -289,9 +292,9 @@
         session_context=session_context,
         run_id=run_id,
     )
-    
+
     print(f"Workflow execution complete for run_id: {run_id}")
-    
+
     # Step 4: Verify workflow completed successfully
     async with db.pool.acquire() as conn:
         run_row = await conn.fetchrow(
@@ -302,27 +305,23 @@
             """,
             run_id,
         )
-    
+
     assert run_row is not None, "Run not found in database"
     status_lower = (run_row["status"] or "").lower()
-    
+
     # Check for errors first
     if status_lower in ("failed", "error"):
-        pytest.fail(
-            f"Workflow failed with status '{status_lower}'. "
-            f"Workflow type: {run_row.get('workflow_type') or 'Unknown'}"
-        )
-    
+        pytest.fail(f"Workflow failed with status '{status_lower}'. Workflow type: {run_row.get('workflow_type') or 'Unknown'}")
+
     # Verify it actually completed
-    assert status_lower == "completed", \
-        f"Workflow did not complete. Status: {run_row['status']}"
-    
+    assert status_lower == "completed", f"Workflow did not complete. Status: {run_row['status']}"
+
     # Step 5: Verify run in database
     assert run_row["user_id"] == user["user_id"], "User ID mismatch"
     assert run_row["session_id"] == session_id, "Session ID mismatch"
     assert run_row["status"] == "completed", f"Run status is not completed: {run_row['status']}"
     assert run_row["workflow_type"] is not None, "Workflow type not determined"
-    
+
     # Step 6: Verify results exist in database with actual data
     async with db.pool.acquire() as conn:
         results = await conn.fetch(
@@ -334,17 +333,16 @@
             """,
             run_id,
         )
-    
+
     # For successful workflows, we should have some results
-    assert len(results) > 0, \
-        f"No results stored for completed workflow. Run ID: {run_id}, Workflow type: {run_row['workflow_type']}"
-    
+    assert len(results) > 0, f"No results stored for completed workflow. Run ID: {run_id}, Workflow type: {run_row['workflow_type']}"
+
     # Verify results have actual data (not empty)
     for result in results:
         assert result["result_data"] is not None, f"Result from {result['agent_name']} has no data"
         result_data_str = str(result["result_data"])
         assert len(result_data_str) > 2, f"Result data is empty: {result_data_str}"  # More than just "{}"
-    
+
     # Step 7-8: Verify enrichment completeness
     await verify_enrichment_completeness(db, run_id)
 
@@ -364,7 +362,7 @@
         email="general_search_test@example.com",
         roles=["member"],
     )
-    
+
     # Start session
     session_response = await api_client.post(
         "/api/prospecting/session/start",
@@ -372,7 +370,7 @@
         headers=user["headers"],
     )
     session_id = session_response.json()["session_id"]
-    
+
     # Start processing
     process_response = await api_client.post(
         "/api/prospecting/process/start",
@@ -383,65 +381,62 @@
         },
         headers=user["headers"],
     )
-    
+
     assert process_response.status_code == 202
     run_id = process_response.json()["run_id"]
-    
+
     # Execute workflow directly using orchestrator
     from app.agents.prospecting_orchestrator import ProspectingOrchestrator, SessionContext
-    
+
     print(f"Executing workflow for run_id: {run_id}")
-    
+
     # Create session context
     session_context = SessionContext(session_id=session_id, user_id=user["user_id"])
     # Align session interaction count with provided run_id
     try:
         import re
+
         m = re.search(r"_run_(\d+)$", run_id)
         if m:
             idx = int(m.group(1))
             session_context.interaction_count = max(0, idx - 1)
     except Exception:
         pass
-    
+
     # Execute orchestrator - it will use test DB automatically via get_global_db()
     orchestrator = ProspectingOrchestrator()
     await orchestrator.execute(
-            prompt="Find venture capital firms in San Francisco",
-            user_id=user["user_id"],
-            session_context=session_context,
-            run_id=run_id,
-        )
-    
+        prompt="Find venture capital firms in San Francisco",
+        user_id=user["user_id"],
+        session_context=session_context,
+        run_id=run_id,
+    )
+
     print(f"Workflow execution complete for run_id: {run_id}")
-    
+
     # Verify completion
     async with db.pool.acquire() as conn:
         run_row = await conn.fetchrow(
             "SELECT status, workflow_type FROM prospecting_runs WHERE run_id = $1",
             run_id,
         )
-    
+
     assert run_row is not None, "Run not found in database"
     status_lower = (run_row["status"] or "").lower()
-    
+
     if status_lower in ("failed", "error"):
-        pytest.fail(
-            f"Workflow failed. "
-            f"Workflow type: {run_row.get('workflow_type') or 'Unknown'}"
-        )
-    
-    assert status_lower == "completed", \
-        f"Workflow did not complete. Status: {run_row['status']}"
-    
+        pytest.fail(f"Workflow failed. Workflow type: {run_row.get('workflow_type') or 'Unknown'}")
+
+    assert status_lower == "completed", f"Workflow did not complete. Status: {run_row['status']}"
+
     # Verify database state
     assert run_row["status"] == "completed", f"Run status is not completed: {run_row['status']}"
     assert run_row["workflow_type"] is not None, "Workflow type not determined"
-    
+
     # Verify results were stored
     # For general_search workflows, results are stored in company_search_results table, not agent_results
     workflow_type = run_row.get("workflow_type", "").lower()
-    
+
     if workflow_type == "general_search":
         # Check company_search_results table for general_search workflows
         async with db.pool.acquire() as conn:
@@ -449,16 +444,16 @@
                 "SELECT COUNT(*) FROM company_search_results WHERE run_id = $1",
                 run_id,
             )
-        
+
         assert results_count > 0, f"No company search results stored for completed general_search workflow. Run ID: {run_id}"
-        
+
         # Also verify the search_results JSONB has data
         async with db.pool.acquire() as conn:
             search_result_row = await conn.fetchrow(
                 "SELECT search_query, search_results FROM company_search_results WHERE run_id = $1 LIMIT 1",
                 run_id,
             )
-        
+
         if search_result_row:
             assert search_result_row["search_results"] is not None, "search_results JSONB is None"
             if isinstance(search_result_row["search_results"], dict):
@@ -469,7 +464,7 @@
                     assert isinstance(companies, list), "companies should be a list"
                     assert len(companies) > 0, "companies list is empty"
                     print(f"‚úÖ Found {len(companies)} companies in search results")
-                    
+
                     # Step: Select first company and trigger enrichment workflow
                     print(f"\nSelecting first company (index 1) for enrichment...")
                     select_response = await api_client.post(
@@ -481,13 +476,12 @@
                         },
                         headers=user["headers"],
                     )
-                    
-                    assert select_response.status_code == 202, \
-                        f"Company selection failed: {select_response.status_code} - {select_response.text}"
-                    
+
+                    assert select_response.status_code == 202, f"Company selection failed: {select_response.status_code} - {select_response.text}"
+
                     enrichment_run_id = select_response.json()["run_id"]
                     print(f"Enrichment workflow started with run_id: {enrichment_run_id}")
-                    
+
                     # Wait for enrichment workflow to complete
                     print("Waiting for enrichment workflow to complete...")
                     try:
@@ -497,15 +491,16 @@
                             user["headers"],
                             timeout=300,  # Longer timeout for enrichment
                         )
-                        
-                        assert final_status["status"].lower() == "completed", \
+
+                        assert final_status["status"].lower() == "completed", (
                             f"Enrichment workflow did not complete. Status: {final_status.get('status')}"
-                        
+                        )
+
                         print(f"‚úÖ Enrichment workflow completed for run_id: {enrichment_run_id}")
-                        
+
                         # Verify enrichment completeness
                         await verify_enrichment_completeness(db, enrichment_run_id)
-                        
+
                     except TimeoutError:
                         pytest.fail(f"Enrichment workflow did not complete within timeout for run_id: {enrichment_run_id}")
     else:
@@ -515,7 +510,7 @@
                 "SELECT COUNT(*) FROM agent_results WHERE run_id = $1",
                 run_id,
             )
-        
+
         assert results_count > 0, f"No results stored for completed workflow. Run ID: {run_id}, Workflow type: {workflow_type}"
 
 
@@ -533,7 +528,7 @@
         api_client,
         email="user2@example.com",
     )
-    
+
     # User1 starts a session
     session_response = await api_client.post(
         "/api/prospecting/session/start",
@@ -541,7 +536,7 @@
         headers=user1["headers"],
     )
     session_id = session_response.json()["session_id"]
-    
+
     # User2 tries to use User1's session
     process_response = await api_client.post(
         "/api/prospecting/process/start",
@@ -566,7 +561,7 @@
         api_client,
         email="invalid_session@example.com",
     )
-    
+
     # Try to start processing with non-existent session
     response = await api_client.post(
         "/api/prospecting/process/start",
@@ -595,7 +590,7 @@
         api_client,
         email="owner2@example.com",
     )
-    
+
     # User1 starts session
     session_response = await api_client.post(
         "/api/prospecting/session/start",
@@ -603,7 +598,7 @@
         headers=user1["headers"],
     )
     session_id = session_response.json()["session_id"]
-    
+
     # User2 tries to end User1's session
     end_response = await api_client.post(
         "/api/prospecting/session/end",
@@ -628,7 +623,7 @@
         email="error_handling_test@example.com",
         roles=["member"],
     )
-    
+
     # Start session
     session_response = await api_client.post(
         "/api/prospecting/session/start",
@@ -636,7 +631,7 @@
         headers=user["headers"],
     )
     session_id = session_response.json()["session_id"]
-    
+
     # Start processing with a prompt that might cause issues
     process_response = await api_client.post(
         "/api/prospecting/process/start",
@@ -647,14 +642,14 @@
         },
         headers=user["headers"],
     )
-    
+
     # Should either accept it (and handle in workflow) or reject it immediately
     assert process_response.status_code in (202, 400)
-    
+
     if process_response.status_code == 202:
         # If accepted, verify error handling in workflow
         run_id = process_response.json()["run_id"]
-        
+
         # Poll with shorter timeout for error case
         try:
             final_status = await poll_status_until_complete(
@@ -663,10 +658,10 @@
                 user["headers"],
                 timeout=60,
             )
-            
+
             # Workflow should complete (even if with error status)
             assert final_status["status"].lower() in ("completed", "failed", "error")
-            
+
         except TimeoutError:
             # Timeout is acceptable for error cases
             pass

--- test/unit/__init__.py
+++ test/unit/__init__.py
@@ -1,2 +1 @@
 """Unit tests - fast, isolated tests with mocked dependencies."""
-

--- test/unit/agents/test_prospecting_orchestrator.py
+++ test/unit/agents/test_prospecting_orchestrator.py
@@ -3,11 +3,7 @@
 import pytest
 from unittest.mock import AsyncMock, Mock, patch, MagicMock
 from datetime import datetime
-from app.agents.prospecting_orchestrator import (
-    ProspectingOrchestrator,
-    SessionContext,
-    PromptAnalyzer
-)
+from app.agents.prospecting_orchestrator import ProspectingOrchestrator, SessionContext, PromptAnalyzer
 
 
 @pytest.fixture
@@ -17,19 +13,21 @@
     mock_conn.fetchrow = AsyncMock()
     mock_conn.fetchval = AsyncMock()
     mock_conn.execute = AsyncMock()
-    
+
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_db = Mock()
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     return mock_db, mock_conn
 
 
@@ -48,7 +46,7 @@
     run_id = session.increment_interaction()
     assert session.interaction_count == 1
     assert run_id == "session123_001"
-    
+
     run_id2 = session.increment_interaction()
     assert session.interaction_count == 2
     assert run_id2 == "session123_002"
@@ -59,7 +57,7 @@
     session = SessionContext("session123", "user123")
     session.increment_interaction()
     data = session.to_dict()
-    
+
     assert data["session_id"] == "session123"
     assert data["user_id"] == "user123"
     assert data["interaction_count"] == 1
@@ -74,10 +72,10 @@
         "start_time": "2024-01-01T00:00:00",
         "interaction_count": 5,
         "status": "active",
-        "last_activity": "2024-01-01T01:00:00"
+        "last_activity": "2024-01-01T01:00:00",
     }
     session = SessionContext.from_dict(data)
-    
+
     assert session.session_id == "session123"
     assert session.user_id == "user123"
     assert session.interaction_count == 5
@@ -86,8 +84,10 @@
 @pytest.mark.asyncio
 async def test_prompt_analyzer_init():
     """Test PromptAnalyzer initialization."""
-    with patch('app.agents.prospecting_orchestrator.get_openai_api_key', return_value='test-key'), \
-         patch('app.agents.prospecting_orchestrator.ChatOpenAI'):
+    with (
+        patch("app.agents.prospecting_orchestrator.get_openai_api_key", return_value="test-key"),
+        patch("app.agents.prospecting_orchestrator.ChatOpenAI"),
+    ):
         analyzer = PromptAnalyzer()
         assert analyzer.llm is not None
 
@@ -99,13 +99,15 @@
     mock_response = Mock()
     mock_response.content = '{"prompt_type": "specific_company", "confidence": 0.9, "extracted_data": {"company_name": "Sequoia Capital"}}'
     mock_llm.ainvoke = AsyncMock(return_value=mock_response)
-    
-    with patch('app.agents.prospecting_orchestrator.get_openai_api_key', return_value='test-key'), \
-         patch('app.agents.prospecting_orchestrator.ChatOpenAI', return_value=mock_llm), \
-         patch('app.agents.prospecting_orchestrator.trace_operation'):
+
+    with (
+        patch("app.agents.prospecting_orchestrator.get_openai_api_key", return_value="test-key"),
+        patch("app.agents.prospecting_orchestrator.ChatOpenAI", return_value=mock_llm),
+        patch("app.agents.prospecting_orchestrator.trace_operation"),
+    ):
         analyzer = PromptAnalyzer()
         result = await analyzer.analyze_prompt("Research Sequoia Capital")
-        
+
         assert result["prompt_type"] == "specific_company"
         assert result["confidence"] == 0.9
         assert "Sequoia Capital" in str(result["extracted_data"])
@@ -118,13 +120,15 @@
     mock_response = Mock()
     mock_response.content = '{"prompt_type": "general_search", "confidence": 0.8, "extracted_data": {}}'
     mock_llm.ainvoke = AsyncMock(return_value=mock_response)
-    
-    with patch('app.agents.prospecting_orchestrator.get_openai_api_key', return_value='test-key'), \
-         patch('app.agents.prospecting_orchestrator.ChatOpenAI', return_value=mock_llm), \
-         patch('app.agents.prospecting_orchestrator.trace_operation'):
+
+    with (
+        patch("app.agents.prospecting_orchestrator.get_openai_api_key", return_value="test-key"),
+        patch("app.agents.prospecting_orchestrator.ChatOpenAI", return_value=mock_llm),
+        patch("app.agents.prospecting_orchestrator.trace_operation"),
+    ):
         analyzer = PromptAnalyzer()
         result = await analyzer.analyze_prompt("Find VC firms in London")
-        
+
         assert result["prompt_type"] == "general_search"
         assert result["confidence"] == 0.8
 
@@ -134,28 +138,32 @@
     """Test prompt analyzer error handling."""
     mock_llm = AsyncMock()
     mock_llm.ainvoke = AsyncMock(side_effect=Exception("LLM error"))
-    
-    with patch('app.agents.prospecting_orchestrator.get_openai_api_key', return_value='test-key'), \
-         patch('app.agents.prospecting_orchestrator.ChatOpenAI', return_value=mock_llm), \
-         patch('app.agents.prospecting_orchestrator.trace_operation'):
+
+    with (
+        patch("app.agents.prospecting_orchestrator.get_openai_api_key", return_value="test-key"),
+        patch("app.agents.prospecting_orchestrator.ChatOpenAI", return_value=mock_llm),
+        patch("app.agents.prospecting_orchestrator.trace_operation"),
+    ):
         analyzer = PromptAnalyzer()
         result = await analyzer.analyze_prompt("Invalid prompt")
-        
+
         assert result["prompt_type"] == "off_topic"
         assert result["confidence"] == 0.0
 
 
 def test_prospecting_orchestrator_init():
     """Test ProspectingOrchestrator initialization."""
-    with patch('app.agents.prospecting_orchestrator.initialize_langsmith'), \
-         patch('app.agents.prospecting_orchestrator.get_enable_debugging', return_value=False), \
-         patch('app.agents.prospecting_orchestrator.CompanySearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.CoreSignalSubAgent'), \
-         patch('app.agents.prospecting_orchestrator.WebResearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.PersonEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.CompanyEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.YouTubeMediaAgent'), \
-         patch('app.agents.prospecting_orchestrator.PromptAnalyzer'):
+    with (
+        patch("app.agents.prospecting_orchestrator.initialize_langsmith"),
+        patch("app.agents.prospecting_orchestrator.get_enable_debugging", return_value=False),
+        patch("app.agents.prospecting_orchestrator.CompanySearchAgent"),
+        patch("app.agents.prospecting_orchestrator.CoreSignalSubAgent"),
+        patch("app.agents.prospecting_orchestrator.WebResearchAgent"),
+        patch("app.agents.prospecting_orchestrator.PersonEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.CompanyEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.YouTubeMediaAgent"),
+        patch("app.agents.prospecting_orchestrator.PromptAnalyzer"),
+    ):
         orchestrator = ProspectingOrchestrator()
         assert orchestrator.db is None
         assert len(orchestrator.sub_agents) == 6
@@ -168,19 +176,21 @@
     """Test starting a new session."""
     db, conn = mock_db
     db.store_session = AsyncMock()
-    
-    with patch('app.agents.prospecting_orchestrator.initialize_langsmith'), \
-         patch('app.agents.prospecting_orchestrator.get_enable_debugging', return_value=False), \
-         patch('app.agents.prospecting_orchestrator.get_global_db', return_value=db), \
-         patch('app.agents.prospecting_orchestrator.CompanySearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.CoreSignalSubAgent'), \
-         patch('app.agents.prospecting_orchestrator.WebResearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.PersonEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.CompanyEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.PromptAnalyzer'):
+
+    with (
+        patch("app.agents.prospecting_orchestrator.initialize_langsmith"),
+        patch("app.agents.prospecting_orchestrator.get_enable_debugging", return_value=False),
+        patch("app.agents.prospecting_orchestrator.get_global_db", return_value=db),
+        patch("app.agents.prospecting_orchestrator.CompanySearchAgent"),
+        patch("app.agents.prospecting_orchestrator.CoreSignalSubAgent"),
+        patch("app.agents.prospecting_orchestrator.WebResearchAgent"),
+        patch("app.agents.prospecting_orchestrator.PersonEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.CompanyEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.PromptAnalyzer"),
+    ):
         orchestrator = ProspectingOrchestrator()
         session = await orchestrator.start_session("user123")
-        
+
         assert session.user_id == "user123"
         assert session.session_id is not None
         # Note: start_session stores in DB but doesn't add to active_sessions dict
@@ -193,26 +203,28 @@
     """Test getting an existing session."""
     mock_db = Mock()
     mock_db.store_session = AsyncMock()
-    
+
     async def mock_get_global_db():
         return mock_db
-    
-    with patch('app.agents.prospecting_orchestrator.initialize_langsmith'), \
-         patch('app.agents.prospecting_orchestrator.get_enable_debugging', return_value=False), \
-         patch('app.agents.prospecting_orchestrator.get_global_db', side_effect=mock_get_global_db), \
-         patch('app.agents.prospecting_orchestrator.CompanySearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.CoreSignalSubAgent'), \
-         patch('app.agents.prospecting_orchestrator.WebResearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.PersonEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.CompanyEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.PromptAnalyzer'):
+
+    with (
+        patch("app.agents.prospecting_orchestrator.initialize_langsmith"),
+        patch("app.agents.prospecting_orchestrator.get_enable_debugging", return_value=False),
+        patch("app.agents.prospecting_orchestrator.get_global_db", side_effect=mock_get_global_db),
+        patch("app.agents.prospecting_orchestrator.CompanySearchAgent"),
+        patch("app.agents.prospecting_orchestrator.CoreSignalSubAgent"),
+        patch("app.agents.prospecting_orchestrator.WebResearchAgent"),
+        patch("app.agents.prospecting_orchestrator.PersonEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.CompanyEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.PromptAnalyzer"),
+    ):
         orchestrator = ProspectingOrchestrator()
         session = await orchestrator.start_session("user123")
         session_id = session.session_id
-        
+
         # Manually add to active_sessions for testing (simulating what would happen in real usage)
         orchestrator.active_sessions[session_id] = session
-        
+
         # Check that session is in active_sessions dict
         assert session_id in orchestrator.active_sessions
         retrieved = orchestrator.active_sessions.get(session_id)
@@ -223,15 +235,16 @@
 @pytest.mark.asyncio
 async def test_prospecting_orchestrator_get_session_not_found():
     """Test getting a non-existent session."""
-    with patch('app.agents.prospecting_orchestrator.initialize_langsmith'), \
-         patch('app.agents.prospecting_orchestrator.get_enable_debugging', return_value=False), \
-         patch('app.agents.prospecting_orchestrator.CompanySearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.CoreSignalSubAgent'), \
-         patch('app.agents.prospecting_orchestrator.WebResearchAgent'), \
-         patch('app.agents.prospecting_orchestrator.PersonEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.CompanyEnrichAgent'), \
-         patch('app.agents.prospecting_orchestrator.PromptAnalyzer'):
+    with (
+        patch("app.agents.prospecting_orchestrator.initialize_langsmith"),
+        patch("app.agents.prospecting_orchestrator.get_enable_debugging", return_value=False),
+        patch("app.agents.prospecting_orchestrator.CompanySearchAgent"),
+        patch("app.agents.prospecting_orchestrator.CoreSignalSubAgent"),
+        patch("app.agents.prospecting_orchestrator.WebResearchAgent"),
+        patch("app.agents.prospecting_orchestrator.PersonEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.CompanyEnrichAgent"),
+        patch("app.agents.prospecting_orchestrator.PromptAnalyzer"),
+    ):
         orchestrator = ProspectingOrchestrator()
         result = orchestrator.active_sessions.get("nonexistent")
         assert result is None
-

--- test/unit/conftest.py
+++ test/unit/conftest.py
@@ -29,6 +29,7 @@
 os.environ.setdefault("LP_CLERK_AUDIENCE", "test")
 os.environ.setdefault("LANGSMITH_API_KEY", "")  # Disable LangSmith
 
+
 # Hook to ensure path is set in pytest-xdist workers
 def pytest_configure(config):
     """Configure pytest - ensures path is set in all workers."""
@@ -36,131 +37,159 @@
     if str(project_root) not in sys.path:
         sys.path.insert(0, str(project_root))
 
+
 # Early mocking at module level - happens before any imports
 # This prevents expensive LangChain imports during test collection
 class MockChatOpenAI:
     def __init__(self, *args, **kwargs):
         pass
+
     async def ainvoke(self, *args, **kwargs):
         mock_response = Mock()
         mock_response.content = '{"prompt_type": "specific_company", "confidence": 0.9}'
         return mock_response
+
     def invoke(self, *args, **kwargs):
         mock_response = Mock()
         mock_response.content = '{"prompt_type": "specific_company", "confidence": 0.9}'
         return mock_response
 
+
 def mock_initialize_langsmith():
     pass
 
+
 class MockTraceOperation:
     def __init__(self, *args, **kwargs):
         pass
+
     def __enter__(self):
         return self
+
     def __exit__(self, *args):
         return None
 
+
 # Patch modules before they're imported
 # Use sys.modules to ensure mocks are in place
 # Create proper mock classes that pydantic can handle
 class MockBaseMessage:
     """Mock BaseMessage that pydantic can serialize."""
+
     def __init__(self, content="", **kwargs):
         self.content = content
         for k, v in kwargs.items():
             setattr(self, k, v)
 
+
 class MockSystemMessage(MockBaseMessage):
     pass
 
+
 class MockHumanMessage(MockBaseMessage):
     pass
 
-if 'langchain_openai' not in sys.modules:
-    sys.modules['langchain_openai'] = MagicMock()
-    sys.modules['langchain_openai'].ChatOpenAI = MockChatOpenAI
-if 'langchain_core.messages' not in sys.modules:
-    sys.modules['langchain_core.messages'] = MagicMock()
-    sys.modules['langchain_core.messages'].BaseMessage = MockBaseMessage
-    sys.modules['langchain_core.messages'].SystemMessage = MockSystemMessage
-    sys.modules['langchain_core.messages'].HumanMessage = MockHumanMessage
-if 'langchain_perplexity' not in sys.modules:
-    sys.modules['langchain_perplexity'] = MagicMock()
-    sys.modules['langchain_perplexity'].ChatPerplexity = MockChatOpenAI
+
+if "langchain_openai" not in sys.modules:
+    sys.modules["langchain_openai"] = MagicMock()
+    sys.modules["langchain_openai"].ChatOpenAI = MockChatOpenAI
+if "langchain_core.messages" not in sys.modules:
+    sys.modules["langchain_core.messages"] = MagicMock()
+    sys.modules["langchain_core.messages"].BaseMessage = MockBaseMessage
+    sys.modules["langchain_core.messages"].SystemMessage = MockSystemMessage
+    sys.modules["langchain_core.messages"].HumanMessage = MockHumanMessage
+if "langchain_perplexity" not in sys.modules:
+    sys.modules["langchain_perplexity"] = MagicMock()
+    sys.modules["langchain_perplexity"].ChatPerplexity = MockChatOpenAI
 # Mock langchain.tools to prevent import errors
-if 'langchain.tools' not in sys.modules:
-    sys.modules['langchain.tools'] = MagicMock()
+if "langchain.tools" not in sys.modules:
+    sys.modules["langchain.tools"] = MagicMock()
+
     # Create a mock tool decorator
     def mock_tool(*args, **kwargs):
         def decorator(func):
             return func
+
         return decorator
-    sys.modules['langchain.tools'].tool = mock_tool
-if 'langchain_core.tools' not in sys.modules:
-    sys.modules['langchain_core.tools'] = MagicMock()
+
+    sys.modules["langchain.tools"].tool = mock_tool
+if "langchain_core.tools" not in sys.modules:
+    sys.modules["langchain_core.tools"] = MagicMock()
 # Mock langchain_google_genai before it's imported (after MockChatOpenAI is defined)
-if 'langchain_google_genai' not in sys.modules:
-    sys.modules['langchain_google_genai'] = MagicMock()
-    sys.modules['langchain_google_genai'].ChatGoogleGenerativeAI = MockChatOpenAI
+if "langchain_google_genai" not in sys.modules:
+    sys.modules["langchain_google_genai"] = MagicMock()
+    sys.modules["langchain_google_genai"].ChatGoogleGenerativeAI = MockChatOpenAI
 
 
 @pytest.fixture(autouse=True, scope="session")
 def mock_global_db():
     """Mock global database for all unit tests to prevent real DB connections (session-scoped for efficiency)."""
+
     # Create proper async context manager
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_conn = Mock()
     mock_conn.fetchrow = AsyncMock()
     mock_conn.fetchval = AsyncMock()
     mock_conn.execute = AsyncMock()
-    
+
     mock_db = Mock()
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     async def mock_get_global_db():
         return mock_db
-    
+
     # Use patch instead of monkeypatch for session scope
-    with patch("app.utils.global_db.get_global_db", mock_get_global_db), \
-         patch("app.utils.global_db.GlobalDB.get_instance", mock_get_global_db), \
-         patch("app.utils.global_db.GlobalDB.initialize", mock_get_global_db):
+    with (
+        patch("app.utils.global_db.get_global_db", mock_get_global_db),
+        patch("app.utils.global_db.GlobalDB.get_instance", mock_get_global_db),
+        patch("app.utils.global_db.GlobalDB.initialize", mock_get_global_db),
+    ):
         yield mock_db
 
 
 @pytest.fixture(autouse=True, scope="session")
 def mock_redis():
     """Mock Redis to prevent connection errors in unit tests (session-scoped for efficiency)."""
+
     class MockRedis:
         def __init__(self, *args, **kwargs):
             self._data = {}
+
         async def get(self, key):
             return self._data.get(key)
+
         async def set(self, key, value, *args, **kwargs):
             self._data[key] = str(value)
             return True
+
         async def incr(self, key):
             self._data[key] = str(int(self._data.get(key, 0)) + 1)
             return int(self._data[key])
+
         async def expire(self, key, seconds):
             return True
+
         async def keys(self, pattern):
             import re
+
             regex = re.compile(pattern.replace("*", ".*"))
             return [k for k in self._data if regex.match(k)]
+
         async def delete(self, *keys):
             for k in keys:
                 self._data.pop(k, None)
             return True
-    
+
     with patch("redis.asyncio.Redis", MockRedis):
         yield
 
@@ -168,16 +197,20 @@
 @pytest.fixture(autouse=True, scope="session")
 def mock_clerk_jwks():
     """Mock Clerk JWKS fetching for unit tests (session-scoped for efficiency)."""
+
     def mock_httpx_get(url, *args, **kwargs):
         class MockResponse:
             status_code = 200
+
             def raise_for_status(self):
                 return None
+
             def json(self):
                 # Return minimal JWKS structure
                 return {"keys": []}
+
         return MockResponse()
-    
+
     with patch("app.security.clerk_tokens.httpx.get", mock_httpx_get):
         yield
 
@@ -185,20 +218,24 @@
 @pytest.fixture(autouse=True, scope="session")
 def mock_secrets_manager():
     """Mock AWS Secrets Manager to prevent slow AWS calls (session-scoped for efficiency)."""
+
     def mock_get_secret(key):
         return None
-    
+
     class MockSecretsManager:
         def __init__(self, *args, **kwargs):
             self._aws_enabled = False
+
         def get_secret(self, key):
             return None
-    
+
     # Patch at module level before any imports happen
     # This prevents hangs when modules try to access secrets at import time
-    with patch("app.utils.config.get_secret", mock_get_secret, create=True), \
-         patch("app.utils.secrets_manager.SecretsManager", MockSecretsManager, create=True), \
-         patch("app.utils.secrets_manager.get_secret", mock_get_secret, create=True):
+    with (
+        patch("app.utils.config.get_secret", mock_get_secret, create=True),
+        patch("app.utils.secrets_manager.SecretsManager", MockSecretsManager, create=True),
+        patch("app.utils.secrets_manager.get_secret", mock_get_secret, create=True),
+    ):
         yield
 
 
@@ -208,12 +245,14 @@
     # Don't patch modules that trigger imports - instead rely on environment variables
     # and module-level mocks that are already in place
     # Only patch functions that are called at runtime, not at import time
-    with patch("app.utils.langsmith_config.initialize_langsmith", mock_initialize_langsmith, create=True), \
-         patch("app.utils.langsmith_config.trace_operation", MockTraceOperation, create=True), \
-         patch("app.tools.search_module.initialize_langsmith", mock_initialize_langsmith, create=True), \
-         patch("app.agents.prospecting_orchestrator.initialize_langsmith", mock_initialize_langsmith, create=True), \
-         patch("app.agents.prospecting_orchestrator.trace_operation", MockTraceOperation, create=True), \
-         patch("app.agents.sub_agents.person_enrich_agent.initialize_langsmith", mock_initialize_langsmith, create=True):
+    with (
+        patch("app.utils.langsmith_config.initialize_langsmith", mock_initialize_langsmith, create=True),
+        patch("app.utils.langsmith_config.trace_operation", MockTraceOperation, create=True),
+        patch("app.tools.search_module.initialize_langsmith", mock_initialize_langsmith, create=True),
+        patch("app.agents.prospecting_orchestrator.initialize_langsmith", mock_initialize_langsmith, create=True),
+        patch("app.agents.prospecting_orchestrator.trace_operation", MockTraceOperation, create=True),
+        patch("app.agents.sub_agents.person_enrich_agent.initialize_langsmith", mock_initialize_langsmith, create=True),
+    ):
         yield
 
 
@@ -224,7 +263,7 @@
     This dramatically reduces import overhead - imports happen once per worker,
     not once per test. Python caches imports per process, so all tests in the
     same worker reuse these cached modules.
-    
+
     Only pre-import simple utility modules that don't trigger expensive langchain imports.
     """
     # Import only simple utility modules - these are fast and commonly used

--- test/unit/middleware/test_clerk_auth_middleware.py
+++ test/unit/middleware/test_clerk_auth_middleware.py
@@ -7,7 +7,7 @@
 def test_extract_email_from_claims():
     """Test extracting email directly from claims."""
     claims = {"email": "test@example.com", "sub": "user_123"}
-    
+
     email = ClerkAuthMiddleware._extract_email(claims)
     assert email == "test@example.com"
 
@@ -15,7 +15,7 @@
 def test_extract_email_lowercases_and_strips():
     """Test that email is lowercased and stripped."""
     claims = {"email": "  TEST@EXAMPLE.COM  ", "sub": "user_123"}
-    
+
     email = ClerkAuthMiddleware._extract_email(claims)
     assert email == "test@example.com"
 
@@ -30,7 +30,7 @@
             {"id": "addr_456", "email_address": "other@example.com"},
         ],
     }
-    
+
     email = ClerkAuthMiddleware._extract_email(claims)
     assert email == "test@example.com"
 
@@ -38,7 +38,7 @@
 def test_extract_email_returns_none_when_missing():
     """Test that _extract_email returns None when email is missing."""
     claims = {"sub": "user_123"}
-    
+
     email = ClerkAuthMiddleware._extract_email(claims)
     assert email is None
 
@@ -49,7 +49,7 @@
         "sub": "user_123",
         "email_addresses": [],
     }
-    
+
     email = ClerkAuthMiddleware._extract_email(claims)
     assert email is None
 
@@ -61,7 +61,7 @@
             "roles": ["admin", "member"],
         },
     }
-    
+
     roles = ClerkAuthMiddleware._extract_roles(claims)
     assert "admin" in roles
     assert "member" in roles
@@ -74,7 +74,7 @@
             "roles": "admin",
         },
     }
-    
+
     roles = ClerkAuthMiddleware._extract_roles(claims)
     assert roles == ["admin"]
 
@@ -82,7 +82,7 @@
 def test_extract_roles_returns_empty_list_when_missing():
     """Test that _extract_roles returns empty list when roles missing."""
     claims = {}
-    
+
     roles = ClerkAuthMiddleware._extract_roles(claims)
     assert roles == []
 
@@ -90,7 +90,7 @@
 def test_extract_roles_handles_missing_public_metadata():
     """Test that _extract_roles handles missing public_metadata."""
     claims = {"sub": "user_123"}
-    
+
     roles = ClerkAuthMiddleware._extract_roles(claims)
     assert roles == []
 
@@ -102,7 +102,7 @@
             "dailyLimitOverride": 42,
         },
     }
-    
+
     override = ClerkAuthMiddleware._extract_daily_override(claims)
     assert override == 42
 
@@ -110,7 +110,7 @@
 def test_extract_daily_override_returns_none_when_missing():
     """Test that _extract_daily_override returns None when missing."""
     claims = {}
-    
+
     override = ClerkAuthMiddleware._extract_daily_override(claims)
     assert override is None
 
@@ -122,7 +122,7 @@
             "dailyLimitOverride": "not_a_number",
         },
     }
-    
+
     override = ClerkAuthMiddleware._extract_daily_override(claims)
     assert override is None
 
@@ -134,7 +134,7 @@
             "dailyLimitOverride": None,
         },
     }
-    
+
     override = ClerkAuthMiddleware._extract_daily_override(claims)
     assert override is None
 
@@ -142,10 +142,11 @@
 def test_forbidden_creates_correct_response():
     """Test that _forbidden creates correct JSONResponse."""
     response = ClerkAuthMiddleware._forbidden("test_error", "Test message")
-    
+
     assert response.status_code == 403
     assert response.body is not None
     import json
+
     content = json.loads(response.body)
     assert content["error"] == "test_error"
     assert content["message"] == "Test message"
@@ -154,12 +155,12 @@
 def test_unauthorized_creates_correct_response():
     """Test that _unauthorized creates correct JSONResponse."""
     response = ClerkAuthMiddleware._unauthorized("test_error", "Test message")
-    
+
     assert response.status_code == 401
     assert response.headers.get("WWW-Authenticate") == "Bearer"
     import json
+
     content = json.loads(response.body)
     assert content["error"] == "test_error"
     assert content["message"] == "Test message"
     assert content["refresh_required"] is False
-

--- test/unit/routers/test_admin_endpoints.py
+++ test/unit/routers/test_admin_endpoints.py
@@ -14,16 +14,18 @@
     """Create a mock database."""
     mock_conn = Mock()
     mock_conn.fetchval = AsyncMock()
-    
+
     # Create a proper async context manager
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_db = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
     return mock_db, mock_conn
@@ -33,7 +35,7 @@
 async def test_get_admin_user_returns_user_when_already_admin(mock_db):
     """Test that get_admin_user returns user when is_admin is already True."""
     mock_db, mock_conn = mock_db
-    
+
     current_user = CurrentUser(
         clerk_user_id="clerk_123",
         email="admin@example.com",
@@ -41,9 +43,9 @@
         roles=["admin", "user"],
         is_admin=True,
     )
-    
+
     result = await get_admin_user(current_user=current_user, db=mock_db)
-    
+
     assert result is current_user
     assert result.is_admin is True
     # Should not query database when already admin
@@ -54,7 +56,7 @@
 async def test_get_admin_user_checks_db_when_not_admin_in_claims(mock_db):
     """Test that get_admin_user checks database when not admin in claims."""
     mock_db, mock_conn = mock_db
-    
+
     current_user = CurrentUser(
         clerk_user_id="clerk_123",
         email="user@example.com",
@@ -62,11 +64,11 @@
         roles=["user"],
         is_admin=False,
     )
-    
+
     mock_conn.fetchval.return_value = True  # Admin in DB
-    
+
     result = await get_admin_user(current_user=current_user, db=mock_db)
-    
+
     assert result.is_admin is True
     assert "admin" in result.roles
     mock_conn.fetchval.assert_called_once()
@@ -76,7 +78,7 @@
 async def test_get_admin_user_raises_when_not_admin(mock_db):
     """Test that get_admin_user raises HTTPException when user is not admin."""
     mock_db, mock_conn = mock_db
-    
+
     current_user = CurrentUser(
         clerk_user_id="clerk_123",
         email="user@example.com",
@@ -84,12 +86,12 @@
         roles=["user"],
         is_admin=False,
     )
-    
+
     mock_conn.fetchval.return_value = False  # Not admin in DB
-    
+
     with pytest.raises(HTTPException) as exc_info:
         await get_admin_user(current_user=current_user, db=mock_db)
-    
+
     assert exc_info.value.status_code == 403
     assert "Admin privileges required" in str(exc_info.value.detail)
 
@@ -98,7 +100,7 @@
 async def test_get_admin_user_raises_when_no_user_id(mock_db):
     """Test that get_admin_user raises HTTPException when user has no user_id."""
     mock_db, mock_conn = mock_db
-    
+
     current_user = CurrentUser(
         clerk_user_id="clerk_123",
         email="user@example.com",
@@ -106,10 +108,10 @@
         roles=["user"],
         is_admin=False,
     )
-    
+
     with pytest.raises(HTTPException) as exc_info:
         await get_admin_user(current_user=current_user, db=mock_db)
-    
+
     assert exc_info.value.status_code == 403
     # Should not query database when no user_id
     mock_conn.fetchval.assert_not_called()
@@ -119,7 +121,7 @@
 async def test_get_admin_user_elevates_roles_when_db_admin(mock_db):
     """Test that get_admin_user elevates roles when admin in database."""
     mock_db, mock_conn = mock_db
-    
+
     current_user = CurrentUser(
         clerk_user_id="clerk_123",
         email="user@example.com",
@@ -127,15 +129,14 @@
         roles=["user", "member"],
         is_admin=False,
     )
-    
+
     mock_conn.fetchval.return_value = True  # Admin in DB
-    
+
     result = await get_admin_user(current_user=current_user, db=mock_db)
-    
+
     assert result.is_admin is True
     # Check that roles were normalized (should include admin and user)
     normalized = normalize_roles(["user", "member"], is_admin=True)
     assert set(result.roles) == set(normalized)
     assert "admin" in result.roles
     assert "user" in result.roles
-

--- test/unit/routers/test_prospecting_api.py
+++ test/unit/routers/test_prospecting_api.py
@@ -9,7 +9,7 @@
     extract_search_context,
     get_session_id_from_request,
     format_search_params_for_storage,
-    CompanySelectionRequest
+    CompanySelectionRequest,
 )
 from app.models.current_user import CurrentUser
 
@@ -18,7 +18,7 @@
     """Test generating unique search ID."""
     search_id1 = generate_search_id()
     search_id2 = generate_search_id()
-    
+
     assert search_id1.startswith("search_")
     assert search_id1 != search_id2
     assert len(search_id1) > 10
@@ -28,7 +28,7 @@
     """Test extracting valid search context from header."""
     mock_request = Mock()
     mock_request.headers = {"X-Search-Context": '{"query": "test", "filters": {}}'}
-    
+
     context = extract_search_context(mock_request)
     assert context is not None
     assert context["query"] == "test"
@@ -38,7 +38,7 @@
     """Test extracting search context when header is missing."""
     mock_request = Mock()
     mock_request.headers = {}
-    
+
     context = extract_search_context(mock_request)
     assert context is None
 
@@ -47,7 +47,7 @@
     """Test extracting search context with invalid JSON."""
     mock_request = Mock()
     mock_request.headers = {"X-Search-Context": "invalid json"}
-    
+
     context = extract_search_context(mock_request)
     assert context is None
 
@@ -56,21 +56,16 @@
     """Test getting session ID from request."""
     mock_request = Mock()
     session_id = get_session_id_from_request(mock_request)
-    
+
     assert session_id.startswith("session_")
     assert len(session_id) > 10
 
 
 def test_format_search_params_for_storage():
     """Test formatting search parameters for storage."""
-    params = {
-        "query": "test",
-        "location": None,
-        "industry": "tech",
-        "limit": 10
-    }
+    params = {"query": "test", "location": None, "industry": "tech", "limit": 10}
     formatted = format_search_params_for_storage(params)
-    
+
     assert "query" in formatted
     assert "location" not in formatted  # None values removed
     assert formatted["industry"] == "tech"
@@ -79,12 +74,9 @@
 
 def test_format_search_params_all_none():
     """Test formatting search parameters when all are None."""
-    params = {
-        "query": None,
-        "location": None
-    }
+    params = {"query": None, "location": None}
     formatted = format_search_params_for_storage(params)
-    
+
     assert len(formatted) == 0
 
 
@@ -92,28 +84,14 @@
 async def test_select_company_for_enrichment_user_mismatch():
     """Test company selection with user ID mismatch."""
     from app.routers.prospecting_api import select_company_for_enrichment
-    
-    request = CompanySelectionRequest(
-        user_id="user123",
-        run_id="run123",
-        selected_company_index=0
-    )
-    
-    current_user = CurrentUser(
-        user_id="user456",
-        clerk_user_id="clerk_user456",
-        email="test@example.com",
-        roles=["user"]
-    )
-    
+
+    request = CompanySelectionRequest(user_id="user123", run_id="run123", selected_company_index=0)
+
+    current_user = CurrentUser(user_id="user456", clerk_user_id="clerk_user456", email="test@example.com", roles=["user"])
+
     with pytest.raises(HTTPException) as exc_info:
-        await select_company_for_enrichment(
-            request=request,
-            db=Mock(),
-            current_user=current_user,
-            background_tasks=None
-        )
-    
+        await select_company_for_enrichment(request=request, db=Mock(), current_user=current_user, background_tasks=None)
+
     assert exc_info.value.status_code == 403
 
 
@@ -121,29 +99,14 @@
 async def test_select_company_for_enrichment_missing_selection():
     """Test company selection without index or company_id."""
     from app.routers.prospecting_api import select_company_for_enrichment
-    
-    request = CompanySelectionRequest(
-        user_id="user123",
-        run_id="run123",
-        selected_company_index=None,
-        company_id=None
-    )
-    
-    current_user = CurrentUser(
-        user_id="user123",
-        clerk_user_id="clerk_user123",
-        email="test@example.com",
-        roles=["user"]
-    )
-    
+
+    request = CompanySelectionRequest(user_id="user123", run_id="run123", selected_company_index=None, company_id=None)
+
+    current_user = CurrentUser(user_id="user123", clerk_user_id="clerk_user123", email="test@example.com", roles=["user"])
+
     with pytest.raises(HTTPException) as exc_info:
-        await select_company_for_enrichment(
-            request=request,
-            db=Mock(),
-            current_user=current_user,
-            background_tasks=None
-        )
-    
+        await select_company_for_enrichment(request=request, db=Mock(), current_user=current_user, background_tasks=None)
+
     assert exc_info.value.status_code == 400
 
 
@@ -152,72 +115,64 @@
     """Test start_processing endpoint request validation."""
     from app.routers.prospecting_api import start_processing
     from app.models.api_models import ProcessingRequest
-    
-    request = ProcessingRequest(
-        user_id="user123",
-        session_id="session123",
-        prompt="Test prompt"
-    )
-    
+
+    request = ProcessingRequest(user_id="user123", session_id="session123", prompt="Test prompt")
+
     mock_db = Mock()
     mock_conn = Mock()
     # Mock session lookup - session exists
-    mock_conn.fetchrow = AsyncMock(side_effect=[
-        {"session_id": "session123", "user_id": "user123", "interaction_count": 0},  # Session lookup
-        {"interaction_count": 0}  # Locked session
-    ])
+    mock_conn.fetchrow = AsyncMock(
+        side_effect=[
+            {"session_id": "session123", "user_id": "user123", "interaction_count": 0},  # Session lookup
+            {"interaction_count": 0},  # Locked session
+        ]
+    )
     mock_conn.fetchval = AsyncMock(return_value=0)  # Max run index
     mock_conn.execute = AsyncMock()
-    
+
     class MockTransaction:
         async def __aenter__(self):
             return self
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_conn.transaction = Mock(return_value=MockTransaction())
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     mock_rate_limiter = Mock()
-    mock_rate_limiter.check_rate_limit = AsyncMock(return_value={
-        "allowed": True,
-        "limit": 10,
-        "remaining": 5,
-        "reset": 3600
-    })
-    
+    mock_rate_limiter.check_rate_limit = AsyncMock(return_value={"allowed": True, "limit": 10, "remaining": 5, "reset": 3600})
+
     mock_orch_instance = Mock()
     mock_orch_instance.prompt_analyzer = Mock()
-    mock_orch_instance.prompt_analyzer.analyze_prompt = AsyncMock(return_value={
-        "prompt_type": "specific_company",
-        "confidence": 0.9,
-        "extracted_data": {}
-    })
-    
-    with patch('app.routers.prospecting_api.RateLimiter', return_value=mock_rate_limiter), \
-         patch('app.routers.prospecting_api.ProspectingOrchestrator', return_value=mock_orch_instance), \
-         patch('app.routers.prospecting_api.BackgroundTasks'), \
-         patch('app.routers.prospecting_api.ProgressStore') as mock_progress, \
-         patch('app.routers.prospecting_api.execute_workflow_background'):
+    mock_orch_instance.prompt_analyzer.analyze_prompt = AsyncMock(
+        return_value={"prompt_type": "specific_company", "confidence": 0.9, "extracted_data": {}}
+    )
+
+    with (
+        patch("app.routers.prospecting_api.RateLimiter", return_value=mock_rate_limiter),
+        patch("app.routers.prospecting_api.ProspectingOrchestrator", return_value=mock_orch_instance),
+        patch("app.routers.prospecting_api.BackgroundTasks"),
+        patch("app.routers.prospecting_api.ProgressStore") as mock_progress,
+        patch("app.routers.prospecting_api.execute_workflow_background"),
+    ):
         mock_progress.instance.return_value.set_progress = AsyncMock()
-        
+
         # Should not raise validation error
         # start_processing returns a Response object, not ProcessingResponse
-        response = await start_processing(
-            request=request,
-            db=mock_db,
-            background_tasks=Mock()
-        )
-        
+        response = await start_processing(request=request, db=mock_db, background_tasks=Mock())
+
         assert response is not None
         assert response.status_code == 202  # Accepted
 
@@ -227,30 +182,17 @@
     """Test start_processing when rate limit is exceeded."""
     from app.routers.prospecting_api import start_processing
     from app.models.api_models import ProcessingRequest
-    
-    request = ProcessingRequest(
-        user_id="user123",
-        session_id="session123",
-        prompt="Test prompt"
-    )
-    
+
+    request = ProcessingRequest(user_id="user123", session_id="session123", prompt="Test prompt")
+
     mock_db = Mock()
     mock_rate_limiter = Mock()
-    mock_rate_limiter.check_rate_limit = AsyncMock(return_value={
-        "allowed": False,
-        "limit": 10,
-        "remaining": 0,
-        "reset": 3600
-    })
-    
-    with patch('app.routers.prospecting_api.RateLimiter', return_value=mock_rate_limiter):
+    mock_rate_limiter.check_rate_limit = AsyncMock(return_value={"allowed": False, "limit": 10, "remaining": 0, "reset": 3600})
+
+    with patch("app.routers.prospecting_api.RateLimiter", return_value=mock_rate_limiter):
         with pytest.raises(HTTPException) as exc_info:
-            await start_processing(
-                request=request,
-                db=mock_db,
-                background_tasks=Mock()
-            )
-        
+            await start_processing(request=request, db=mock_db, background_tasks=Mock())
+
         assert exc_info.value.status_code == 429
 
 
@@ -258,55 +200,46 @@
 async def test_get_processing_status():
     """Test getting processing status."""
     from app.routers.prospecting_api import get_processing_status
-    
-    current_user = CurrentUser(
-        user_id="user123",
-        clerk_user_id="clerk_user123",
-        email="test@example.com",
-        roles=["user"]
-    )
-    
+
+    current_user = CurrentUser(user_id="user123", clerk_user_id="clerk_user123", email="test@example.com", roles=["user"])
+
     mock_db = Mock()
     mock_conn = Mock()
-    mock_conn.fetchrow = AsyncMock(return_value={
-        "run_id": "run123",
-        "status": "completed",
-        "workflow_type": "specific_company",
-        "user_id": "user123"
-    })
-    
+    mock_conn.fetchrow = AsyncMock(
+        return_value={"run_id": "run123", "status": "completed", "workflow_type": "specific_company", "user_id": "user123"}
+    )
+
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     # Mock all required database calls
     mock_conn.fetchval = AsyncMock(return_value="user123")  # Owner check
-    mock_conn.fetchrow = AsyncMock(return_value={
-        "status": "completed",
-        "company_name": "Test Company",
-        "start_time": datetime.now(),
-        "end_time": datetime.now(),
-        "execution_time_ms": 1000,
-        "workflow_type": "specific_company"
-    })
-    
-    with patch('app.routers.prospecting_api.ProgressStore') as mock_progress, \
-         patch('app.routers.prospecting_api.get_db', return_value=mock_db):
+    mock_conn.fetchrow = AsyncMock(
+        return_value={
+            "status": "completed",
+            "company_name": "Test Company",
+            "start_time": datetime.now(),
+            "end_time": datetime.now(),
+            "execution_time_ms": 1000,
+            "workflow_type": "specific_company",
+        }
+    )
+
+    with patch("app.routers.prospecting_api.ProgressStore") as mock_progress, patch("app.routers.prospecting_api.get_db", return_value=mock_db):
         mock_progress.instance.return_value.get_display_progress = AsyncMock(return_value=50)
-        
-        response = await get_processing_status(
-            run_id="run123",
-            db=mock_db,
-            current_user=current_user
-        )
-        
+
+        response = await get_processing_status(run_id="run123", db=mock_db, current_user=current_user)
+
         assert response is not None
         assert response.status == "completed"
 
@@ -315,39 +248,31 @@
 async def test_get_processing_status_not_found():
     """Test getting processing status for non-existent run."""
     from app.routers.prospecting_api import get_processing_status
-    
-    current_user = CurrentUser(
-        user_id="user123",
-        clerk_user_id="clerk_user123",
-        email="test@example.com",
-        roles=["user"]
-    )
-    
+
+    current_user = CurrentUser(user_id="user123", clerk_user_id="clerk_user123", email="test@example.com", roles=["user"])
+
     mock_db = Mock()
     mock_conn = Mock()
-    
+
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     # Mock owner check - returns None (run doesn't exist)
     mock_conn.fetchval = AsyncMock(return_value=None)
     mock_conn.fetchrow = AsyncMock(return_value=None)
-    
-    with patch('app.routers.prospecting_api.get_db', return_value=mock_db):
+
+    with patch("app.routers.prospecting_api.get_db", return_value=mock_db):
         with pytest.raises(HTTPException) as exc_info:
-            await get_processing_status(
-                run_id="nonexistent",
-                db=mock_db,
-                current_user=current_user
-            )
-        
+            await get_processing_status(run_id="nonexistent", db=mock_db, current_user=current_user)
+
         assert exc_info.value.status_code == 404
-

--- test/unit/services/test_clerk_admin_service.py
+++ test/unit/services/test_clerk_admin_service.py
@@ -10,12 +10,12 @@
     """Create a mock httpx client."""
     mock_response = Mock()
     mock_response.status_code = 200
-    
+
     mock_client = AsyncMock()
     mock_client.__aenter__ = AsyncMock(return_value=mock_client)
     mock_client.__aexit__ = AsyncMock(return_value=None)
     mock_client.patch = AsyncMock(return_value=mock_response)
-    
+
     return mock_client, mock_response
 
 
@@ -23,12 +23,14 @@
 async def test_set_roles_success(mock_httpx_client):
     """Test setting roles successfully."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.clerk_admin_service.get_clerk_secret_key', return_value='test-key'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.clerk_admin_service.get_clerk_secret_key", return_value="test-key"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         service = ClerkAdminService()
         await service.set_roles("user123", ["admin", "user"])
-        
+
         mock_client.patch.assert_called_once()
         call_args = mock_client.patch.call_args
         assert "/v1/users/user123" in call_args[0][0]
@@ -38,11 +40,11 @@
 @pytest.mark.asyncio
 async def test_set_roles_no_api_key(mock_httpx_client):
     """Test setting roles when API key is missing."""
-    with patch('app.services.clerk_admin_service.get_clerk_secret_key', return_value=None):
+    with patch("app.services.clerk_admin_service.get_clerk_secret_key", return_value=None):
         service = ClerkAdminService()
         # Should not raise error, just log warning
         await service.set_roles("user123", ["admin"])
-        
+
         # Should not make API call
         assert True  # Test passes if no exception
 
@@ -51,12 +53,14 @@
 async def test_set_roles_normalizes_and_sorts(mock_httpx_client):
     """Test that roles are normalized and sorted."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.clerk_admin_service.get_clerk_secret_key', return_value='test-key'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.clerk_admin_service.get_clerk_secret_key", return_value="test-key"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         service = ClerkAdminService()
         await service.set_roles("user123", ["ADMIN", "  user  ", "member"])
-        
+
         call_args = mock_client.patch.call_args
         payload = call_args[1]["json"]
         roles = payload["public_metadata"]["roles"]
@@ -67,12 +71,14 @@
 async def test_set_roles_empty_list_defaults_to_user(mock_httpx_client):
     """Test that empty role list defaults to ['user']."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.clerk_admin_service.get_clerk_secret_key', return_value='test-key'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.clerk_admin_service.get_clerk_secret_key", return_value="test-key"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         service = ClerkAdminService()
         await service.set_roles("user123", [])
-        
+
         call_args = mock_client.patch.call_args
         payload = call_args[1]["json"]
         roles = payload["public_metadata"]["roles"]
@@ -83,12 +89,14 @@
 async def test_set_roles_filters_empty_strings(mock_httpx_client):
     """Test that empty strings after stripping are included (current behavior)."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.clerk_admin_service.get_clerk_secret_key', return_value='test-key'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.clerk_admin_service.get_clerk_secret_key", return_value="test-key"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         service = ClerkAdminService()
         await service.set_roles("user123", ["admin", "", "user", "  "])
-        
+
         call_args = mock_client.patch.call_args
         payload = call_args[1]["json"]
         roles = payload["public_metadata"]["roles"]
@@ -107,14 +115,16 @@
     mock_client, mock_response = mock_httpx_client
     mock_response.status_code = 400
     mock_response.text = "Bad Request"
-    
-    with patch('app.services.clerk_admin_service.get_clerk_secret_key', return_value='test-key'), \
-         patch('httpx.AsyncClient', return_value=mock_client), \
-         patch('app.services.clerk_admin_service.logger') as mock_logger:
+
+    with (
+        patch("app.services.clerk_admin_service.get_clerk_secret_key", return_value="test-key"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+        patch("app.services.clerk_admin_service.logger") as mock_logger,
+    ):
         service = ClerkAdminService()
         # Should not raise exception, just log error
         await service.set_roles("user123", ["admin"])
-        
+
         mock_logger.error.assert_called_once()
 
 
@@ -123,13 +133,14 @@
     """Test handling network exception when setting roles."""
     mock_client, mock_response = mock_httpx_client
     mock_client.patch = AsyncMock(side_effect=Exception("Network error"))
-    
-    with patch('app.services.clerk_admin_service.get_clerk_secret_key', return_value='test-key'), \
-         patch('httpx.AsyncClient', return_value=mock_client), \
-         patch('app.services.clerk_admin_service.logger') as mock_logger:
+
+    with (
+        patch("app.services.clerk_admin_service.get_clerk_secret_key", return_value="test-key"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+        patch("app.services.clerk_admin_service.logger") as mock_logger,
+    ):
         service = ClerkAdminService()
         # Should not raise exception, just log error
         await service.set_roles("user123", ["admin"])
-        
+
         mock_logger.exception.assert_called_once()
-

--- test/unit/services/test_email_service.py
+++ test/unit/services/test_email_service.py
@@ -12,12 +12,12 @@
     mock_response = Mock()
     mock_response.status_code = 200
     mock_response.text = "OK"
-    
+
     mock_client = AsyncMock()
     mock_client.__aenter__ = AsyncMock(return_value=mock_client)
     mock_client.__aexit__ = AsyncMock(return_value=None)
     mock_client.post = AsyncMock(return_value=mock_response)
-    
+
     return mock_client, mock_response
 
 
@@ -25,14 +25,16 @@
 async def test_send_password_reset_email_success(mock_httpx_client):
     """Test sending password reset email successfully."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.email_service.MAILGUN_API_KEY', 'test-key'), \
-         patch('app.services.email_service.MAILGUN_DOMAIN', 'test.com'), \
-         patch('app.services.email_service.MAILGUN_BASE_URL', 'https://api.eu.mailgun.net/v3/test.com/messages'), \
-         patch('app.services.email_service.APP_NAME', 'TestApp'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.email_service.MAILGUN_API_KEY", "test-key"),
+        patch("app.services.email_service.MAILGUN_DOMAIN", "test.com"),
+        patch("app.services.email_service.MAILGUN_BASE_URL", "https://api.eu.mailgun.net/v3/test.com/messages"),
+        patch("app.services.email_service.APP_NAME", "TestApp"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         await email_service.send_password_reset_email("user@example.com", "123456")
-        
+
         mock_client.post.assert_called_once()
         call_args = mock_client.post.call_args
         assert "test.com" in call_args[0][0] or "test.com" in str(call_args)
@@ -42,8 +44,7 @@
 @pytest.mark.asyncio
 async def test_send_password_reset_email_missing_config():
     """Test sending password reset email with missing config."""
-    with patch('app.services.email_service.MAILGUN_API_KEY', None), \
-         patch('app.services.email_service.MAILGUN_DOMAIN', 'test.com'):
+    with patch("app.services.email_service.MAILGUN_API_KEY", None), patch("app.services.email_service.MAILGUN_DOMAIN", "test.com"):
         with pytest.raises(RuntimeError, match="Mailgun API key or domain not set"):
             await email_service.send_password_reset_email("user@example.com", "123456")
 
@@ -54,11 +55,13 @@
     mock_client, mock_response = mock_httpx_client
     mock_response.status_code = 500
     mock_response.text = "Internal Server Error"
-    
-    with patch('app.services.email_service.MAILGUN_API_KEY', 'test-key'), \
-         patch('app.services.email_service.MAILGUN_DOMAIN', 'test.com'), \
-         patch('app.services.email_service.APP_NAME', 'TestApp'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.email_service.MAILGUN_API_KEY", "test-key"),
+        patch("app.services.email_service.MAILGUN_DOMAIN", "test.com"),
+        patch("app.services.email_service.APP_NAME", "TestApp"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         with pytest.raises(RuntimeError, match="Mailgun email failed"):
             await email_service.send_password_reset_email("user@example.com", "123456")
 
@@ -67,15 +70,17 @@
 async def test_send_verification_email_success(mock_httpx_client):
     """Test sending verification email successfully."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.email_service.MAILGUN_API_KEY', 'test-key'), \
-         patch('app.services.email_service.MAILGUN_DOMAIN', 'test.com'), \
-         patch('app.services.email_service.MAILGUN_BASE_URL', 'https://api.eu.mailgun.net/v3/test.com/messages'), \
-         patch('app.services.email_service.APP_NAME', 'TestApp'), \
-         patch('app.services.email_service.BACKEND_URL', 'https://api.test.com'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.email_service.MAILGUN_API_KEY", "test-key"),
+        patch("app.services.email_service.MAILGUN_DOMAIN", "test.com"),
+        patch("app.services.email_service.MAILGUN_BASE_URL", "https://api.eu.mailgun.net/v3/test.com/messages"),
+        patch("app.services.email_service.APP_NAME", "TestApp"),
+        patch("app.services.email_service.BACKEND_URL", "https://api.test.com"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         await email_service.send_verification_email("user@example.com", "token123")
-        
+
         mock_client.post.assert_called_once()
         call_args = mock_client.post.call_args
         # Just verify the call was made with correct auth
@@ -86,11 +91,13 @@
 async def test_send_tier1_radar_alert_email_success(mock_httpx_client):
     """Test sending Tier 1 radar alert email successfully."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.email_service.MAILGUN_API_KEY', 'test-key'), \
-         patch('app.services.email_service.MAILGUN_DOMAIN', 'test.com'), \
-         patch('app.services.email_service.get_frontend_url', return_value='https://app.test.com'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.email_service.MAILGUN_API_KEY", "test-key"),
+        patch("app.services.email_service.MAILGUN_DOMAIN", "test.com"),
+        patch("app.services.email_service.get_frontend_url", return_value="https://app.test.com"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         await email_service.send_tier1_radar_alert_email(
             to_email="user@example.com",
             company_name="Test Company",
@@ -99,9 +106,9 @@
             finding_preview="Test preview",
             finding_id="finding123",
             radar_id="radar123",
-            user_first_name="John"
+            user_first_name="John",
         )
-        
+
         mock_client.post.assert_called_once()
         call_args = mock_client.post.call_args
         data = call_args[1]["data"]
@@ -114,11 +121,13 @@
 async def test_send_tier1_radar_alert_email_without_radar_id(mock_httpx_client):
     """Test sending Tier 1 radar alert email without radar_id."""
     mock_client, mock_response = mock_httpx_client
-    
-    with patch('app.services.email_service.MAILGUN_API_KEY', 'test-key'), \
-         patch('app.services.email_service.MAILGUN_DOMAIN', 'test.com'), \
-         patch('app.services.email_service.get_frontend_url', return_value='https://app.test.com'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.email_service.MAILGUN_API_KEY", "test-key"),
+        patch("app.services.email_service.MAILGUN_DOMAIN", "test.com"),
+        patch("app.services.email_service.get_frontend_url", return_value="https://app.test.com"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         await email_service.send_tier1_radar_alert_email(
             to_email="user@example.com",
             company_name="Test Company",
@@ -127,9 +136,9 @@
             finding_preview="Test preview",
             finding_id="finding123",
             radar_id=None,
-            user_first_name=None
+            user_first_name=None,
         )
-        
+
         mock_client.post.assert_called_once()
         data = mock_client.post.call_args[1]["data"]
         assert "Hello," in data["text"]  # No first name
@@ -139,33 +148,24 @@
 async def test_send_weekly_wrapup_email_success(mock_httpx_client):
     """Test sending weekly wrap-up email successfully."""
     mock_client, mock_response = mock_httpx_client
-    
+
     week_start = datetime(2024, 1, 1)
     week_end = datetime(2024, 1, 7)
     companies = [
-        {
-            "company_name": "Company A",
-            "domain": "companya.com",
-            "summary": "Test summary",
-            "tier1_count": 2,
-            "tier2_count": 5,
-            "findings": []
-        }
+        {"company_name": "Company A", "domain": "companya.com", "summary": "Test summary", "tier1_count": 2, "tier2_count": 5, "findings": []}
     ]
-    
-    with patch('app.services.email_service.MAILGUN_API_KEY', 'test-key'), \
-         patch('app.services.email_service.MAILGUN_DOMAIN', 'test.com'), \
-         patch('app.services.email_service.APP_NAME', 'TestApp'), \
-         patch('app.services.email_service.get_frontend_url', return_value='https://app.test.com'), \
-         patch('httpx.AsyncClient', return_value=mock_client):
+
+    with (
+        patch("app.services.email_service.MAILGUN_API_KEY", "test-key"),
+        patch("app.services.email_service.MAILGUN_DOMAIN", "test.com"),
+        patch("app.services.email_service.APP_NAME", "TestApp"),
+        patch("app.services.email_service.get_frontend_url", return_value="https://app.test.com"),
+        patch("httpx.AsyncClient", return_value=mock_client),
+    ):
         await email_service.send_weekly_wrapup_email(
-            to_email="user@example.com",
-            user_name="John Doe",
-            week_start=week_start,
-            week_end=week_end,
-            companies_with_summaries=companies
+            to_email="user@example.com", user_name="John Doe", week_start=week_start, week_end=week_end, companies_with_summaries=companies
         )
-        
+
         mock_client.post.assert_called_once()
         data = mock_client.post.call_args[1]["data"]
         assert "Weekly Radar Update" in data["subject"]
@@ -175,9 +175,7 @@
 @pytest.mark.asyncio
 async def test_extract_finding_preview_company_mentions():
     """Test extracting finding preview for company mentions."""
-    finding_data = {
-        "mention_content": "This is a test mention"
-    }
+    finding_data = {"mention_content": "This is a test mention"}
     preview = email_service._extract_finding_preview(finding_data, "company_mentions")
     assert "test mention" in preview
 
@@ -185,10 +183,7 @@
 @pytest.mark.asyncio
 async def test_extract_finding_preview_new_hires():
     """Test extracting finding preview for new hires."""
-    finding_data = {
-        "title": "VP Engineering",
-        "department": "Engineering"
-    }
+    finding_data = {"title": "VP Engineering", "department": "Engineering"}
     preview = email_service._extract_finding_preview(finding_data, "company_new_hires")
     assert "VP Engineering" in preview
     assert "Engineering" in preview
@@ -197,10 +192,7 @@
 @pytest.mark.asyncio
 async def test_extract_finding_preview_job_openings():
     """Test extracting finding preview for job openings."""
-    finding_data = {
-        "title": "Senior Engineer",
-        "description": "Looking for experienced engineer"
-    }
+    finding_data = {"title": "Senior Engineer", "description": "Looking for experienced engineer"}
     preview = email_service._extract_finding_preview(finding_data, "company_job_openings")
     assert "Senior Engineer" in preview
     assert "experienced engineer" in preview
@@ -218,10 +210,7 @@
 @pytest.mark.asyncio
 async def test_extract_finding_preview_truncation():
     """Test that finding preview is truncated to 200 characters."""
-    finding_data = {
-        "mention_content": "A" * 300
-    }
+    finding_data = {"mention_content": "A" * 300}
     preview = email_service._extract_finding_preview(finding_data, "company_mentions")
     assert len(preview) <= 200
     assert preview.endswith("...")
-

--- test/unit/services/test_excel_export_service.py
+++ test/unit/services/test_excel_export_service.py
@@ -10,19 +10,21 @@
     """Create a mock database connection."""
     mock_conn = Mock()
     mock_conn.fetchrow = AsyncMock()
-    
+
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_db = Mock()
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     return mock_db, mock_conn
 
 
@@ -30,29 +32,28 @@
 async def test_generate_company_excel_success(mock_db):
     """Test generating company Excel file successfully."""
     db, conn = mock_db
-    enrichment_data = {
-        "company_name": "Test Company",
-        "domain": "test.com"
-    }
-    
+    enrichment_data = {"company_name": "Test Company", "domain": "test.com"}
+
     # Mock the database method
     db.get_company_enrichment_by_run = AsyncMock(return_value=enrichment_data)
-    
-    with patch('app.services.excel_export_service.convert_company_enrichment_json_to_excel') as mock_convert:
+
+    with patch("app.services.excel_export_service.convert_company_enrichment_json_to_excel") as mock_convert:
         # Create a BytesIO mock that returns bytes
         import io
+
         mock_output = io.BytesIO()
         mock_output.write(b"test excel data")
         mock_output.seek(0)
-        
+
         # Make convert function write to the mock output
         def mock_convert_func(data, output):
             output.write(b"test excel data")
+
         mock_convert.side_effect = mock_convert_func
-        
+
         service = ExcelExportService(db)
         result = await service.generate_company_excel("run123", "user123")
-        
+
         assert result is not None
         assert isinstance(result, bytes)
         mock_convert.assert_called_once()
@@ -63,10 +64,10 @@
     """Test generating company Excel when no data found."""
     db, conn = mock_db
     conn.fetchrow = AsyncMock(return_value=None)
-    
+
     service = ExcelExportService(db)
     result = await service.generate_company_excel("run123", "user123")
-    
+
     assert result is None
 
 
@@ -76,11 +77,11 @@
     db, conn = mock_db
     enrichment_data = {"company_name": "Test Company"}
     conn.fetchrow = AsyncMock(return_value=enrichment_data)
-    
-    with patch('app.services.excel_export_service.convert_company_enrichment_json_to_excel', side_effect=Exception("Conversion error")):
+
+    with patch("app.services.excel_export_service.convert_company_enrichment_json_to_excel", side_effect=Exception("Conversion error")):
         service = ExcelExportService(db)
         result = await service.generate_company_excel("run123", "user123")
-        
+
         assert result is None
 
 
@@ -88,24 +89,21 @@
 async def test_generate_person_excel_success(mock_db):
     """Test generating person Excel file successfully."""
     db, conn = mock_db
-    enrichment_data = {
-        "persons": [
-            {"name": "John Doe", "title": "CEO"}
-        ]
-    }
-    
+    enrichment_data = {"persons": [{"name": "John Doe", "title": "CEO"}]}
+
     # Mock the database method
     db.get_person_enrichment_by_run = AsyncMock(return_value=enrichment_data)
-    
-    with patch('app.services.excel_export_service.convert_person_enrichment_json_to_excel') as mock_convert:
+
+    with patch("app.services.excel_export_service.convert_person_enrichment_json_to_excel") as mock_convert:
         # Make convert function write to the output
         def mock_convert_func(data, output):
             output.write(b"test excel data")
+
         mock_convert.side_effect = mock_convert_func
-        
+
         service = ExcelExportService(db)
         result = await service.generate_person_excel("run123", "user123")
-        
+
         assert result is not None
         assert isinstance(result, bytes)
         mock_convert.assert_called_once()
@@ -116,10 +114,10 @@
     """Test generating person Excel when no data found."""
     db, conn = mock_db
     conn.fetchrow = AsyncMock(return_value=None)
-    
+
     service = ExcelExportService(db)
     result = await service.generate_person_excel("run123", "user123")
-    
+
     assert result is None
 
 
@@ -128,9 +126,8 @@
     """Test generating person Excel when error occurs."""
     db, conn = mock_db
     conn.fetchrow = AsyncMock(side_effect=Exception("Database error"))
-    
+
     service = ExcelExportService(db)
     result = await service.generate_person_excel("run123", "user123")
-    
-    assert result is None
 
+    assert result is None

--- test/unit/services/test_user_sync_service.py
+++ test/unit/services/test_user_sync_service.py
@@ -16,20 +16,22 @@
     mock_conn.fetchrow = AsyncMock()
     mock_conn.fetchval = AsyncMock()
     mock_conn.execute = AsyncMock()
-    
+
     # Create proper async context managers
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     # Mock transaction context manager
     mock_transaction = MockAsyncContextManager(None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
+
     mock_db = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
     return mock_db, mock_conn
@@ -39,8 +41,8 @@
 async def test_sync_from_claims_returns_default_when_no_db(mock_db):
     """Test that sync_from_claims returns defaults when database is unavailable."""
     service = UserSyncService()
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=None):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=None):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="test@example.com",
@@ -48,7 +50,7 @@
             daily_limit_override=None,
             claims={"sub": "clerk_123", "email": "test@example.com"},
         )
-    
+
     assert result["clerk_user_id"] == "clerk_123"
     assert result["email"] == "test@example.com"
     assert result["is_admin"] is False
@@ -62,11 +64,11 @@
     """Test that _is_email_approved returns True for approved email."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     mock_conn.fetchrow.return_value = {"is_active": True}
-    
+
     result = await service._is_email_approved(mock_conn, "approved@example.com")
-    
+
     assert result is True
     mock_conn.fetchrow.assert_called_once()
 
@@ -76,11 +78,11 @@
     """Test that _is_email_approved returns False for unapproved email."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     mock_conn.fetchrow.return_value = None
-    
+
     result = await service._is_email_approved(mock_conn, "unapproved@example.com")
-    
+
     assert result is False
 
 
@@ -89,11 +91,11 @@
     """Test that _is_email_approved handles inactive approved user."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     mock_conn.fetchrow.return_value = {"is_active": False}
-    
+
     result = await service._is_email_approved(mock_conn, "inactive@example.com")
-    
+
     assert result is False
 
 
@@ -102,7 +104,7 @@
     """Test that sync_from_claims creates new user when user doesn't exist."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     # Mock database responses - need to handle multiple fetchrow calls
     async def mock_fetchrow(query, *args):
         # First call: _is_email_approved
@@ -112,17 +114,17 @@
         if "users" in query and "clerk_user_id" in query:
             return None
         return None
-    
+
     mock_conn.fetchrow = AsyncMock(side_effect=mock_fetchrow)
     mock_conn.fetchval = AsyncMock(return_value=False)  # _requires_legacy_password_hash
-    
+
     # Mock transaction context
     mock_transaction = Mock()
     mock_transaction.__aenter__ = AsyncMock(return_value=None)
     mock_transaction.__aexit__ = AsyncMock(return_value=None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=mock_db):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=mock_db):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="newuser@example.com",
@@ -130,7 +132,7 @@
             daily_limit_override=None,
             claims={"sub": "clerk_123", "email": "newuser@example.com"},
         )
-    
+
     assert result["user_id"] is not None
     assert result["approved"] is True
     assert result["is_active"] is True
@@ -141,7 +143,7 @@
     """Test that sync_from_claims updates existing user."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     # Mock database responses
     async def mock_fetchrow(query, *args):
         if "approved_users" in query:
@@ -155,17 +157,17 @@
                 "firm_name": None,
             }
         return None
-    
+
     mock_conn.fetchrow = AsyncMock(side_effect=mock_fetchrow)
     mock_conn.fetchval = AsyncMock(return_value=False)
-    
+
     # Mock transaction
     mock_transaction = Mock()
     mock_transaction.__aenter__ = AsyncMock(return_value=None)
     mock_transaction.__aexit__ = AsyncMock(return_value=None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=mock_db):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=mock_db):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="existing@example.com",
@@ -173,7 +175,7 @@
             daily_limit_override=20,  # New limit
             claims={"sub": "clerk_123", "email": "existing@example.com"},
         )
-    
+
     assert result["user_id"] == "existing_user_123"
     assert result["daily_agent_run_limit"] == 20  # Uses override
     assert result["is_admin"] is True  # Updated from claim
@@ -184,23 +186,23 @@
     """Test that sync_from_claims rejects unapproved email."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     # Mock database responses
     async def mock_fetchrow(query, *args):
         if "approved_users" in query:
             return None  # Email not approved
         return None
-    
+
     mock_conn.fetchrow = AsyncMock(side_effect=mock_fetchrow)
     mock_conn.fetchval = AsyncMock(return_value=False)
-    
+
     # Mock transaction
     mock_transaction = Mock()
     mock_transaction.__aenter__ = AsyncMock(return_value=None)
     mock_transaction.__aexit__ = AsyncMock(return_value=None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=mock_db):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=mock_db):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="unapproved@example.com",
@@ -208,7 +210,7 @@
             daily_limit_override=None,
             claims={"sub": "clerk_123", "email": "unapproved@example.com"},
         )
-    
+
     assert result["approved"] is False
     assert result["is_active"] is False
     assert result["account_status"] == "pending"
@@ -219,22 +221,22 @@
     """Test that sync_from_claims uses daily_limit_override when provided."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     async def mock_fetchrow(query, *args):
         if "approved_users" in query:
             return {"is_active": True}
         return None
-    
+
     mock_conn.fetchrow = AsyncMock(side_effect=mock_fetchrow)
     mock_conn.fetchval = AsyncMock(return_value=False)
-    
+
     # Mock transaction
     mock_transaction = Mock()
     mock_transaction.__aenter__ = AsyncMock(return_value=None)
     mock_transaction.__aexit__ = AsyncMock(return_value=None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=mock_db):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=mock_db):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="test@example.com",
@@ -242,7 +244,7 @@
             daily_limit_override=50,  # Custom limit
             claims={"sub": "clerk_123", "email": "test@example.com"},
         )
-    
+
     assert result["daily_agent_run_limit"] == 50
 
 
@@ -251,7 +253,7 @@
     """Test that sync_from_claims uses existing limit when no override provided."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     async def mock_fetchrow(query, *args):
         if "approved_users" in query:
             return {"is_active": True}
@@ -264,17 +266,17 @@
                 "firm_name": None,
             }
         return None
-    
+
     mock_conn.fetchrow = AsyncMock(side_effect=mock_fetchrow)
     mock_conn.fetchval = AsyncMock(return_value=False)
-    
+
     # Mock transaction
     mock_transaction = Mock()
     mock_transaction.__aenter__ = AsyncMock(return_value=None)
     mock_transaction.__aexit__ = AsyncMock(return_value=None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=mock_db):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=mock_db):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="test@example.com",
@@ -282,7 +284,7 @@
             daily_limit_override=None,  # No override
             claims={"sub": "clerk_123", "email": "test@example.com"},
         )
-    
+
     assert result["daily_agent_run_limit"] == 15  # Uses existing limit
 
 
@@ -291,7 +293,7 @@
     """Test that sync_from_claims sets admin status from claim."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     async def mock_fetchrow(query, *args):
         if "approved_users" in query:
             return {"is_active": True}
@@ -304,17 +306,17 @@
                 "firm_name": None,
             }
         return None
-    
+
     mock_conn.fetchrow = AsyncMock(side_effect=mock_fetchrow)
     mock_conn.fetchval = AsyncMock(return_value=False)
-    
+
     # Mock transaction
     mock_transaction = Mock()
     mock_transaction.__aenter__ = AsyncMock(return_value=None)
     mock_transaction.__aexit__ = AsyncMock(return_value=None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=mock_db):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=mock_db):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="test@example.com",
@@ -322,7 +324,7 @@
             daily_limit_override=None,
             claims={"sub": "clerk_123", "email": "test@example.com"},
         )
-    
+
     assert result["is_admin"] is True  # Set from claim
 
 
@@ -331,7 +333,7 @@
     """Test that sync_from_claims preserves existing admin status in DB."""
     mock_db, mock_conn = mock_db
     service = UserSyncService()
-    
+
     async def mock_fetchrow(query, *args):
         if "approved_users" in query:
             return {"is_active": True}
@@ -344,17 +346,17 @@
                 "firm_name": None,
             }
         return None
-    
+
     mock_conn.fetchrow = AsyncMock(side_effect=mock_fetchrow)
     mock_conn.fetchval = AsyncMock(return_value=False)
-    
+
     # Mock transaction
     mock_transaction = Mock()
     mock_transaction.__aenter__ = AsyncMock(return_value=None)
     mock_transaction.__aexit__ = AsyncMock(return_value=None)
     mock_conn.transaction = Mock(return_value=mock_transaction)
-    
-    with patch('app.services.user_sync_service.get_global_db', return_value=mock_db):
+
+    with patch("app.services.user_sync_service.get_global_db", return_value=mock_db):
         result = await service.sync_from_claims(
             clerk_user_id="clerk_123",
             email="test@example.com",
@@ -362,6 +364,5 @@
             daily_limit_override=None,
             claims={"sub": "clerk_123", "email": "test@example.com"},
         )
-    
-    assert result["is_admin"] is True  # Preserved from DB
 
+    assert result["is_admin"] is True  # Preserved from DB

--- test/unit/utils/test_auth_dependencies.py
+++ test/unit/utils/test_auth_dependencies.py
@@ -61,7 +61,7 @@
         "is_admin": False,
     }
     claims = {"sub": "clerk_123", "iat": 1000, "exp": 2000}
-    
+
     user = _build_current_user(
         clerk_user_id="clerk_123",
         email="test@example.com",
@@ -69,7 +69,7 @@
         overlays=overlays,
         roles=["member"],
     )
-    
+
     assert user.user_id == "user_123"
     assert user.email == "test@example.com"
     assert user.is_active is True
@@ -83,7 +83,7 @@
     """Test that _build_current_user falls back to claims when overlay missing."""
     overlays = {}
     claims = {"sub": "clerk_123", "iat": 1000, "exp": 2000}
-    
+
     user = _build_current_user(
         clerk_user_id="clerk_123",
         email="test@example.com",
@@ -91,7 +91,7 @@
         overlays=overlays,
         roles=["admin"],
     )
-    
+
     assert user.user_id == "clerk_123"  # Falls back to claims["sub"]
     assert user.is_admin is True  # From roles
 
@@ -100,7 +100,7 @@
     """Test that _build_current_user sets is_admin from roles."""
     overlays = {"is_admin": False}
     claims = {"sub": "clerk_123"}
-    
+
     user = _build_current_user(
         clerk_user_id="clerk_123",
         email="test@example.com",
@@ -108,7 +108,7 @@
         overlays=overlays,
         roles=["admin"],
     )
-    
+
     assert user.is_admin is True  # Overrides overlay because "admin" in roles
 
 
@@ -116,7 +116,7 @@
     """Test that _build_current_user normalizes roles."""
     overlays = {}
     claims = {"sub": "clerk_123"}
-    
+
     user = _build_current_user(
         clerk_user_id="clerk_123",
         email="test@example.com",
@@ -124,7 +124,7 @@
         overlays=overlays,
         roles=["admin", "member"],  # Include admin in roles to trigger normalization
     )
-    
+
     assert "user" in user.roles
     assert "admin" in user.roles
     assert "member" in user.roles
@@ -139,7 +139,7 @@
         roles=["user"],
         is_admin=False,
     )
-    
+
     result = _current_user_from_state(user)
     assert result is user
 
@@ -152,7 +152,7 @@
         "is_admin": True,
         "is_active": True,
     }
-    
+
     result = _current_user_from_state(state_dict)
     assert isinstance(result, CurrentUser)
     assert result.user_id == "user_123"
@@ -166,4 +166,3 @@
     assert _current_user_from_state(None) is None
     assert _current_user_from_state("string") is None
     assert _current_user_from_state(123) is None
-

--- test/unit/utils/test_chunking.py
+++ test/unit/utils/test_chunking.py
@@ -24,7 +24,7 @@
         index=0,
         user_id="user123",
         company_id="company123",
-        session_id="session123"
+        session_id="session123",
     )
     assert metadata["section"] == "test_section"
     assert metadata["headline"] == "Test Headline"
@@ -41,11 +41,7 @@
 
 def test_chunk_linkedin_data_dict():
     """Test chunking LinkedIn data from dictionary."""
-    data = {
-        "name": "Test Company",
-        "description": "A test company",
-        "industry": "Technology"
-    }
+    data = {"name": "Test Company", "description": "A test company", "industry": "Technology"}
     chunks = chunk_linkedin_data(data, company_name="Test Company")
     assert len(chunks) == 3
     assert all("text" in chunk for chunk in chunks)
@@ -57,7 +53,7 @@
     data = {
         "name": "Test Company",
         "id": "12345",  # Should be skipped
-        "description": "A test company"
+        "description": "A test company",
     }
     chunks = chunk_linkedin_data(data, denylist={"id"})
     assert len(chunks) == 2
@@ -66,9 +62,7 @@
 
 def test_chunk_linkedin_data_with_list():
     """Test chunking LinkedIn data with list values."""
-    data = {
-        "employees": ["John Doe", "Jane Smith"]
-    }
+    data = {"employees": ["John Doe", "Jane Smith"]}
     chunks = chunk_linkedin_data(data)
     assert len(chunks) == 2
     assert "employees [0]" in chunks[0]["text"]
@@ -101,11 +95,9 @@
     data = {
         "news": [
             {"content": "News item 1", "headline": "Headline 1", "date": "2024-01-01"},
-            {"content": "News item 2", "headline": "Headline 2", "date": "2024-01-02"}
+            {"content": "News item 2", "headline": "Headline 2", "date": "2024-01-02"},
         ],
-        "funding": [
-            {"content": "Funding info", "name": "Round A"}
-        ]
+        "funding": [{"content": "Funding info", "name": "Round A"}],
     }
     chunks = chunk_perplexity_json(data, company_name="Test Company")
     assert len(chunks) == 3
@@ -115,65 +107,43 @@
 
 def test_chunk_perplexity_json_empty_sections():
     """Test chunking Perplexity JSON with empty sections."""
-    data = {
-        "news": [],
-        "funding": [{"content": "Funding info"}]
-    }
+    data = {"news": [], "funding": [{"content": "Funding info"}]}
     chunks = chunk_perplexity_json(data)
     assert len(chunks) == 1
 
 
 def test_chunk_perplexity_json_non_list_section():
     """Test chunking Perplexity JSON with non-list section."""
-    data = {
-        "news": "not a list",
-        "funding": [{"content": "Funding info"}]
-    }
+    data = {"news": "not a list", "funding": [{"content": "Funding info"}]}
     chunks = chunk_perplexity_json(data)
     assert len(chunks) == 1
 
 
 def test_is_relevant_web_chunk_relevant():
     """Test identifying relevant web chunk."""
-    chunk = {
-        "text": "This is a relevant chunk with enough content to pass the minimum length requirement.",
-        "metadata": {}
-    }
+    chunk = {"text": "This is a relevant chunk with enough content to pass the minimum length requirement.", "metadata": {}}
     assert is_relevant_web_chunk(chunk, min_length=30) is True
 
 
 def test_is_relevant_web_chunk_too_short():
     """Test identifying chunk that's too short."""
-    chunk = {
-        "text": "Short",
-        "metadata": {}
-    }
+    chunk = {"text": "Short", "metadata": {}}
     assert is_relevant_web_chunk(chunk, min_length=30) is False
 
 
 def test_is_relevant_web_chunk_navigation():
     """Test identifying navigation chunk."""
-    chunk = {
-        "text": "Home About Contact Privacy Policy",
-        "metadata": {}
-    }
+    chunk = {"text": "Home About Contact Privacy Policy", "metadata": {}}
     assert is_relevant_web_chunk(chunk, min_length=10) is False
 
 
 def test_is_relevant_web_chunk_legal():
     """Test identifying legal chunk."""
-    chunk = {
-        "text": "Terms of Service Copyright All Rights Reserved",
-        "metadata": {}
-    }
+    chunk = {"text": "Terms of Service Copyright All Rights Reserved", "metadata": {}}
     assert is_relevant_web_chunk(chunk, min_length=10) is False
 
 
 def test_is_relevant_web_chunk_with_denylist():
     """Test identifying chunk with denylist keywords."""
-    chunk = {
-        "text": "This is a good chunk but it contains cookie policy information",
-        "metadata": {}
-    }
+    chunk = {"text": "This is a good chunk but it contains cookie policy information", "metadata": {}}
     assert is_relevant_web_chunk(chunk, denylist={"cookie"}, min_length=10) is False
-

--- test/unit/utils/test_config.py
+++ test/unit/utils/test_config.py
@@ -41,120 +41,118 @@
 
 def test_get_config_value_from_env():
     """Test getting config value from environment variable."""
-    with patch.dict(os.environ, {"TEST_KEY": "test_value"}, clear=True), \
-         patch('app.utils.config.is_aws_secrets_enabled', return_value=False):
+    with patch.dict(os.environ, {"TEST_KEY": "test_value"}, clear=True), patch("app.utils.config.is_aws_secrets_enabled", return_value=False):
         result = get_config_value("TEST_KEY")
         assert result == "test_value"
 
 
 def test_get_config_value_default():
     """Test getting config value with default."""
-    with patch.dict(os.environ, {}, clear=True), \
-         patch('app.utils.config.is_aws_secrets_enabled', return_value=False):
+    with patch.dict(os.environ, {}, clear=True), patch("app.utils.config.is_aws_secrets_enabled", return_value=False):
         result = get_config_value("MISSING_KEY", "default_value")
         assert result == "default_value"
 
 
 def test_get_config_value_from_secrets_manager():
     """Test getting config value from secrets manager."""
-    with patch('app.utils.config.is_aws_secrets_enabled', return_value=True), \
-         patch('app.utils.config.get_secret', return_value="secret_value"):
+    with patch("app.utils.config.is_aws_secrets_enabled", return_value=True), patch("app.utils.config.get_secret", return_value="secret_value"):
         result = get_config_value("TEST_KEY")
         assert result == "secret_value"
 
 
 def test_get_config_value_secrets_manager_priority():
     """Test that secrets manager takes priority over env vars."""
-    with patch.dict(os.environ, {"TEST_KEY": "env_value"}, clear=True), \
-         patch('app.utils.config.is_aws_secrets_enabled', return_value=True), \
-         patch('app.utils.config.get_secret', return_value="secret_value"):
+    with (
+        patch.dict(os.environ, {"TEST_KEY": "env_value"}, clear=True),
+        patch("app.utils.config.is_aws_secrets_enabled", return_value=True),
+        patch("app.utils.config.get_secret", return_value="secret_value"),
+    ):
         result = get_config_value("TEST_KEY")
         assert result == "secret_value"
 
 
 def test_get_openai_api_key():
     """Test getting OpenAI API key."""
-    with patch('app.utils.config.get_config_value', return_value="sk-test-key"):
+    with patch("app.utils.config.get_config_value", return_value="sk-test-key"):
         result = get_openai_api_key()
         assert result == "sk-test-key"
 
 
 def test_get_anthropic_api_key():
     """Test getting Anthropic API key."""
-    with patch('app.utils.config.get_config_value', return_value="sk-ant-test-key"):
+    with patch("app.utils.config.get_config_value", return_value="sk-ant-test-key"):
         result = get_anthropic_api_key()
         assert result == "sk-ant-test-key"
 
 
 def test_get_anthropic_model_default():
     """Test getting default Anthropic model."""
-    with patch('app.utils.config.get_config_value', return_value=None):
+    with patch("app.utils.config.get_config_value", return_value=None):
         result = get_anthropic_api_key()
         assert result is None
 
 
 def test_get_use_redis_rate_limiting_true():
     """Test getting Redis rate limiting flag when enabled."""
-    with patch('app.utils.config.get_config_value', return_value="true"):
+    with patch("app.utils.config.get_config_value", return_value="true"):
         result = get_use_redis_rate_limiting()
         assert result is True
 
 
 def test_get_use_redis_rate_limiting_false():
     """Test getting Redis rate limiting flag when disabled."""
-    with patch('app.utils.config.get_config_value', return_value="false"):
+    with patch("app.utils.config.get_config_value", return_value="false"):
         result = get_use_redis_rate_limiting()
         assert result is False
 
 
 def test_get_enable_postgres_storage_true():
     """Test getting PostgreSQL storage flag when enabled."""
-    with patch('app.utils.config.get_config_value', return_value="true"):
+    with patch("app.utils.config.get_config_value", return_value="true"):
         result = get_enable_postgres_storage()
         assert result is True
 
 
 def test_get_enable_postgres_storage_false():
     """Test getting PostgreSQL storage flag when disabled."""
-    with patch('app.utils.config.get_config_value', return_value="false"):
+    with patch("app.utils.config.get_config_value", return_value="false"):
         result = get_enable_postgres_storage()
         assert result is False
 
 
 def test_get_postgres_host_development():
     """Test getting PostgreSQL host in development."""
-    with patch('app.utils.config.get_environment', return_value="development"), \
-         patch('app.utils.config.get_config_value', return_value="localhost"):
+    with patch("app.utils.config.get_environment", return_value="development"), patch("app.utils.config.get_config_value", return_value="localhost"):
         result = get_postgres_host()
         assert result == "localhost"
 
 
 def test_get_postgres_host_production():
     """Test getting PostgreSQL host in production."""
-    with patch('app.utils.config.get_environment', return_value="production"), \
-         patch('app.utils.config.get_config_value', return_value="prod-db.example.com"):
+    with (
+        patch("app.utils.config.get_environment", return_value="production"),
+        patch("app.utils.config.get_config_value", return_value="prod-db.example.com"),
+    ):
         result = get_postgres_host()
         assert result == "prod-db.example.com"
 
 
 def test_get_postgres_port():
     """Test getting PostgreSQL port."""
-    with patch('app.utils.config.get_environment', return_value="development"), \
-         patch('app.utils.config.get_config_value', return_value="5432"):
+    with patch("app.utils.config.get_environment", return_value="development"), patch("app.utils.config.get_config_value", return_value="5432"):
         result = get_postgres_port()
         assert result == 5432
 
 
 def test_get_redis_host():
     """Test getting Redis host."""
-    with patch('app.utils.config.get_config_value', return_value="redis.example.com"):
+    with patch("app.utils.config.get_config_value", return_value="redis.example.com"):
         result = get_redis_host()
         assert result == "redis.example.com"
 
 
 def test_get_redis_port():
     """Test getting Redis port."""
-    with patch('app.utils.config.get_config_value', return_value="6379"):
+    with patch("app.utils.config.get_config_value", return_value="6379"):
         result = get_redis_port()
         assert result == 6379
-

--- test/unit/utils/test_dashboard_cache.py
+++ test/unit/utils/test_dashboard_cache.py
@@ -18,7 +18,7 @@
     # Set cache first
     set_cached_dashboard("user123", {"data": "test"})
     assert get_cached_dashboard("user123") is not None
-    
+
     # Invalidate
     invalidate_user_cache("user123")
     assert get_cached_dashboard("user123") is None
@@ -48,13 +48,13 @@
     """Test getting expired cached dashboard."""
     data = {"findings": [1, 2, 3]}
     set_cached_dashboard("user123", data)
-    
+
     # Mock time to be after TTL
-    with patch('app.utils.dashboard_cache.datetime') as mock_datetime:
+    with patch("app.utils.dashboard_cache.datetime") as mock_datetime:
         mock_now = datetime.now(timezone.utc).replace(tzinfo=None) + timedelta(minutes=3)
         mock_datetime.now.return_value = mock_now
         mock_datetime.timezone.utc = timezone.utc
-        
+
         result = get_cached_dashboard("user123")
         assert result is None
 
@@ -71,10 +71,10 @@
     """Test marking Tier 1/2 finding added."""
     # Set cache first
     set_cached_dashboard("user123", {"findings": [1, 2, 3]})
-    
+
     # Mark finding added
     mark_tier1_2_finding_added("user123")
-    
+
     # Cache should be invalidated on next get
     result = get_cached_dashboard("user123")
     assert result is None
@@ -91,10 +91,10 @@
     # Set multiple caches
     set_cached_dashboard("user123", {"data": "test1"})
     set_cached_dashboard("user456", {"data": "test2"})
-    
+
     # Clear all
     clear_all_cache()
-    
+
     # Both should be gone
     assert get_cached_dashboard("user123") is None
     assert get_cached_dashboard("user456") is None
@@ -109,12 +109,11 @@
     """Test that caches are isolated per user."""
     set_cached_dashboard("user123", {"data": "user123_data"})
     set_cached_dashboard("user456", {"data": "user456_data"})
-    
+
     assert get_cached_dashboard("user123") == {"data": "user123_data"}
     assert get_cached_dashboard("user456") == {"data": "user456_data"}
-    
+
     # Invalidate one
     invalidate_user_cache("user123")
     assert get_cached_dashboard("user123") is None
     assert get_cached_dashboard("user456") == {"data": "user456_data"}
-

--- test/unit/utils/test_db_rate_limit.py
+++ test/unit/utils/test_db_rate_limit.py
@@ -12,19 +12,21 @@
     mock_conn = Mock()
     mock_conn.fetchrow = AsyncMock()
     mock_conn.execute = AsyncMock()
-    
+
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_db = Mock()
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     return mock_db, mock_conn
 
 
@@ -33,10 +35,10 @@
     """Test getting user limit when user has custom limit."""
     db, conn = mock_db
     conn.fetchrow.return_value = {"daily_limit": 20}
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     limit = await limiter.get_user_limit("user123")
     assert limit == 20
     conn.fetchrow.assert_called_once()
@@ -47,10 +49,10 @@
     """Test getting default limit when user not found."""
     db, conn = mock_db
     conn.fetchrow.return_value = None
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     limit = await limiter.get_user_limit("user123")
     assert limit == limiter.default_daily_limit
 
@@ -60,10 +62,10 @@
     """Test setting user limit successfully."""
     db, conn = mock_db
     conn.execute.return_value = "UPDATE 1"
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     result = await limiter.set_user_limit("user123", 25)
     assert result is True
     conn.execute.assert_called_once()
@@ -74,10 +76,10 @@
     """Test setting user limit when user doesn't exist."""
     db, conn = mock_db
     conn.execute.return_value = "UPDATE 0"
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     result = await limiter.set_user_limit("user123", 25)
     # The function returns True if result.startswith("UPDATE"), even if 0 rows updated
     # So "UPDATE 0" still returns True
@@ -89,10 +91,10 @@
     """Test getting request count when count exists."""
     db, conn = mock_db
     conn.fetchrow.return_value = {"count": 5}
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     count = await limiter.get_request_count("user123")
     assert count == 5
 
@@ -102,10 +104,10 @@
     """Test getting request count when no count exists."""
     db, conn = mock_db
     conn.fetchrow.return_value = None
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     count = await limiter.get_request_count("user123")
     assert count == 0
 
@@ -115,10 +117,10 @@
     """Test incrementing request count for new entry."""
     db, conn = mock_db
     conn.fetchrow.return_value = {"count": 1}
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     new_count = await limiter.increment_request_count("user123")
     assert new_count == 1
     conn.fetchrow.assert_called_once()
@@ -129,10 +131,10 @@
     """Test incrementing request count for existing entry."""
     db, conn = mock_db
     conn.fetchrow.return_value = {"count": 6}
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     new_count = await limiter.increment_request_count("user123")
     assert new_count == 6
 
@@ -141,7 +143,7 @@
 async def test_check_rate_limit_allowed(mock_db):
     """Test checking rate limit when request is allowed."""
     db, conn = mock_db
-    
+
     async def fetchrow_side_effect(*args, **kwargs):
         query = args[0] if args else ""
         if "daily_agent_run_limit" in query:
@@ -149,18 +151,18 @@
         elif "daily_request_counts" in query:
             return {"count": 5}
         return None
-    
+
     conn.fetchrow = AsyncMock(side_effect=fetchrow_side_effect)
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
-    with patch('app.utils.db_rate_limit.datetime') as mock_datetime:
+
+    with patch("app.utils.db_rate_limit.datetime") as mock_datetime:
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
-        
+
         result = await limiter.check_rate_limit("user123")
-        
+
         assert result["allowed"] is True
         assert result["limit"] == 10
         assert result["remaining"] == 5
@@ -171,7 +173,7 @@
 async def test_check_rate_limit_exceeded(mock_db):
     """Test checking rate limit when limit is exceeded."""
     db, conn = mock_db
-    
+
     async def fetchrow_side_effect(*args, **kwargs):
         query = args[0] if args else ""
         if "daily_agent_run_limit" in query:
@@ -179,18 +181,18 @@
         elif "daily_request_counts" in query:
             return {"count": 10}
         return None
-    
+
     conn.fetchrow = AsyncMock(side_effect=fetchrow_side_effect)
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
-    with patch('app.utils.db_rate_limit.datetime') as mock_datetime:
+
+    with patch("app.utils.db_rate_limit.datetime") as mock_datetime:
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
-        
+
         result = await limiter.check_rate_limit("user123")
-        
+
         assert result["allowed"] is False
         assert result["limit"] == 10
         assert result["remaining"] == 0
@@ -202,18 +204,19 @@
     db, conn = mock_db
     conn.fetchrow.return_value = {"count": 5}
     conn.execute.return_value = None
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     # Create a mock datetime with timestamp method
     from unittest.mock import MagicMock
+
     mock_now = MagicMock(spec=datetime)
     mock_now.timestamp.return_value = 1705320000.0
-    
-    with patch('app.utils.db_rate_limit.datetime') as mock_datetime:
+
+    with patch("app.utils.db_rate_limit.datetime") as mock_datetime:
         mock_datetime.utcnow = Mock(return_value=mock_now)
-        
+
         result = await limiter.check_window_limit("user123", "/api/test", 10, 60)
         assert result is True
 
@@ -223,18 +226,19 @@
     """Test window-based rate limiting when limit exceeded."""
     db, conn = mock_db
     conn.fetchrow.return_value = {"count": 10}
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     # Create a mock datetime with timestamp method
     from unittest.mock import MagicMock
+
     mock_now = MagicMock(spec=datetime)
     mock_now.timestamp.return_value = 1705320000.0
-    
-    with patch('app.utils.db_rate_limit.datetime') as mock_datetime:
+
+    with patch("app.utils.db_rate_limit.datetime") as mock_datetime:
         mock_datetime.utcnow = Mock(return_value=mock_now)
-        
+
         result = await limiter.check_window_limit("user123", "/api/test", 10, 60)
         assert result is False
 
@@ -244,10 +248,10 @@
     """Test resetting all counters."""
     db, conn = mock_db
     conn.execute.return_value = None
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
+
     result = await limiter.reset_all_counters()
     assert result is True
     conn.execute.assert_called_once()
@@ -258,14 +262,14 @@
     """Test cleaning up old window-based rate limit data."""
     db, conn = mock_db
     conn.execute.return_value = "DELETE 5"
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
-    with patch('app.utils.db_rate_limit.datetime') as mock_datetime:
+
+    with patch("app.utils.db_rate_limit.datetime") as mock_datetime:
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
-        
+
         deleted = await limiter.cleanup_old_windows(24)
         assert deleted == 5
 
@@ -275,14 +279,13 @@
     """Test cleanup when no old windows exist."""
     db, conn = mock_db
     conn.execute.return_value = "DELETE 0"
-    
+
     limiter = DatabaseRateLimiter()
     limiter.db = db
-    
-    with patch('app.utils.db_rate_limit.datetime') as mock_datetime:
+
+    with patch("app.utils.db_rate_limit.datetime") as mock_datetime:
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
-        
+
         deleted = await limiter.cleanup_old_windows(24)
         assert deleted == 0
-

--- test/unit/utils/test_domain_utils.py
+++ test/unit/utils/test_domain_utils.py
@@ -106,7 +106,7 @@
 
 def test_check_domain_reachability_success():
     """Test checking reachable domain."""
-    with patch('socket.gethostbyname', return_value="1.2.3.4"):
+    with patch("socket.gethostbyname", return_value="1.2.3.4"):
         result, error = check_domain_reachability("example.com")
         assert result is True
         assert error is None
@@ -115,7 +115,8 @@
 def test_check_domain_reachability_failure():
     """Test checking unreachable domain."""
     import socket
-    with patch('socket.gethostbyname', side_effect=socket.gaierror("Name or service not known")):
+
+    with patch("socket.gethostbyname", side_effect=socket.gaierror("Name or service not known")):
         result, error = check_domain_reachability("nonexistent.example.com")
         assert result is False
         assert error is not None
@@ -183,4 +184,3 @@
     """Test creating company ID from empty name."""
     result = create_company_id_from_name("")
     assert result == "unknown_company"
-

--- test/unit/utils/test_perplexity_utils.py
+++ test/unit/utils/test_perplexity_utils.py
@@ -86,13 +86,10 @@
 
 def test_merge_perplexity_outputs_both_dicts():
     """Test merging two valid dictionaries."""
-    website = {
-        "core_profile": {"name": {"value": "Company A"}},
-        "investment_strategy_mandate": {"focus": {"value": "Tech"}}
-    }
+    website = {"core_profile": {"name": {"value": "Company A"}}, "investment_strategy_mandate": {"focus": {"value": "Tech"}}}
     external = {
         "core_profile": {"name": {"value": "Company A"}, "description": {"value": "A tech company"}},
-        "fund_information": {"aum": {"value": "100M"}}
+        "fund_information": {"aum": {"value": "100M"}},
     }
     result = merge_perplexity_outputs(website, external)
     assert "core_profile" in result
@@ -146,4 +143,3 @@
     content = '[{"item": "value"}]'
     result = safe_parse_perplexity_content(content)
     assert result == [{"item": "value"}]
-

--- test/unit/utils/test_progress_store.py
+++ test/unit/utils/test_progress_store.py
@@ -12,19 +12,21 @@
     mock_conn = Mock()
     mock_conn.execute = AsyncMock()
     mock_conn.fetchrow = AsyncMock()
-    
+
     class MockAsyncContextManager:
         def __init__(self, return_value):
             self.return_value = return_value
+
         async def __aenter__(self):
             return self.return_value
+
         async def __aexit__(self, exc_type, exc_val, exc_tb):
             return None
-    
+
     mock_db = Mock()
     mock_db.pool = Mock()
     mock_db.pool.acquire = Mock(return_value=MockAsyncContextManager(mock_conn))
-    
+
     return mock_db, mock_conn
 
 
@@ -33,26 +35,26 @@
     """Create a mock Redis instance."""
     mock = AsyncMock()
     mock._data = {}
-    
+
     async def hset(key, mapping=None, **kwargs):
         if mapping:
             mock._data[key] = mapping
         return True
-    
+
     async def hgetall(key):
         return mock._data.get(key, {})
-    
+
     async def hget(key, field):
         data = mock._data.get(key, {})
         return data.get(field)
-    
+
     async def expire(key, seconds):
         return True
-    
+
     async def delete(key):
         mock._data.pop(key, None)
         return True
-    
+
     mock.hset = hset
     mock.hgetall = hgetall
     mock.hget = hget
@@ -73,12 +75,14 @@
 async def test_set_progress_redis(mock_redis, mock_db):
     """Test setting progress with Redis backend."""
     db, conn = mock_db
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.progress_store.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.progress_store.redis.Redis", return_value=mock_redis),
+    ):
         store = ProgressStore()
         await store.set_progress(db, "run123", 50)
-        
+
         # Check Redis was called
         assert "status:run123" in mock_redis._data
         assert mock_redis._data["status:run123"]["progress_pct"] == 50
@@ -88,13 +92,15 @@
 async def test_set_progress_clamped(mock_redis, mock_db):
     """Test that progress is clamped to 0-100."""
     db, conn = mock_db
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.progress_store.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.progress_store.redis.Redis", return_value=mock_redis),
+    ):
         store = ProgressStore()
         await store.set_progress(db, "run123", 150)  # Should clamp to 100
         assert mock_redis._data["status:run123"]["progress_pct"] == 100
-        
+
         await store.set_progress(db, "run456", -10)  # Should clamp to 0
         assert mock_redis._data["status:run456"]["progress_pct"] == 0
 
@@ -103,8 +109,8 @@
 async def test_set_progress_database(mock_db):
     """Test setting progress with database backend."""
     db, conn = mock_db
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=False):
+
+    with patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=False):
         store = ProgressStore()
         await store.set_progress(db, "run123", 50)
         conn.execute.assert_called_once()
@@ -114,12 +120,14 @@
 async def test_reset_progress_redis(mock_redis, mock_db):
     """Test resetting progress with Redis backend."""
     db, conn = mock_db
-    
+
     # Set initial progress
     mock_redis._data["status:run123"] = {"progress_pct": 50}
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.progress_store.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.progress_store.redis.Redis", return_value=mock_redis),
+    ):
         store = ProgressStore()
         await store.reset_progress(db, "run123")
         # Key should be deleted or reset
@@ -131,9 +139,11 @@
     """Test getting progress with Redis backend."""
     db, conn = mock_db
     mock_redis._data["status:run123"] = {"progress_pct": "75"}
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.progress_store.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.progress_store.redis.Redis", return_value=mock_redis),
+    ):
         store = ProgressStore()
         progress = await store.get_progress(db, "run123")
         assert progress == 75
@@ -144,8 +154,8 @@
     """Test getting progress with database backend."""
     db, conn = mock_db
     conn.fetchrow.return_value = {"status_payload": '{"progress_pct": 60}'}
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=False):
+
+    with patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=False):
         store = ProgressStore()
         progress = await store.get_progress(db, "run123")
         assert progress == 60
@@ -156,8 +166,8 @@
     """Test getting progress when run doesn't exist."""
     db, conn = mock_db
     conn.fetchrow.return_value = None
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=False):
+
+    with patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=False):
         store = ProgressStore()
         progress = await store.get_progress(db, "nonexistent")
         assert progress is None
@@ -167,18 +177,20 @@
 async def test_update_subprogress(mock_redis, mock_db):
     """Test updating sub-progress."""
     db, conn = mock_db
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.progress_store.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.progress_store.redis.Redis", return_value=mock_redis),
+    ):
         store = ProgressStore()
         # Set initial sub-progress values
         mock_redis._data["status:run123"] = {
             "web_progress": "0.5",
             "coresignal_progress": "0.3",
             "company_enrichment_progress": "0.0",
-            "person_enrichment_progress": "0.0"
+            "person_enrichment_progress": "0.0",
         }
-        
+
         progress = await store.update_subprogress(db, "run123", "web_progress", 0.8)
         assert progress is not None
         assert progress >= 0
@@ -189,9 +201,11 @@
     """Test getting display progress when hard progress is 100."""
     db, conn = mock_db
     mock_redis._data["status:run123"] = {"progress_pct": "100"}
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.progress_store.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.progress_store.redis.Redis", return_value=mock_redis),
+    ):
         store = ProgressStore()
         display = await store.get_display_progress(db, "run123")
         assert display == 100
@@ -201,17 +215,16 @@
 async def test_update_component_status(mock_redis, mock_db):
     """Test updating component status."""
     db, conn = mock_db
-    
-    with patch('app.utils.progress_store.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.progress_store.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.progress_store.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.progress_store.redis.Redis", return_value=mock_redis),
+    ):
         store = ProgressStore()
         mock_redis._data["status:run123"] = {}
-        
-        await store.update_component_status(
-            db, "run123", "company_enrichment", "running", None, None
-        )
-        
+
+        await store.update_component_status(db, "run123", "company_enrichment", "running", None, None)
+
         status_map = await store.get_status_map(db, "run123")
         assert "component_status" in status_map
         assert "company_enrichment" in status_map["component_status"]
-

--- test/unit/utils/test_rate_limit.py
+++ test/unit/utils/test_rate_limit.py
@@ -11,33 +11,34 @@
     """Create a mock Redis instance."""
     mock = AsyncMock()
     mock._data = {}
-    
+
     async def get(key):
         return mock._data.get(key)
-    
+
     async def set(key, value):
         mock._data[key] = str(value)
         return True
-    
+
     async def incr(key):
         current = int(mock._data.get(key, 0))
         new_value = current + 1
         mock._data[key] = str(new_value)
         return new_value
-    
+
     async def expire(key, seconds):
         return True
-    
+
     async def keys(pattern):
         import re
+
         regex = re.compile(pattern.replace("*", ".*"))
         return [k for k in mock._data.keys() if regex.match(k)]
-    
+
     async def delete(*keys):
         for k in keys:
             mock._data.pop(k, None)
         return True
-    
+
     mock.get = get
     mock.set = set
     mock.incr = incr
@@ -50,8 +51,10 @@
 @pytest.mark.asyncio
 async def test_rate_limiter_init_with_redis(mock_redis):
     """Test RateLimiter initialization with Redis enabled."""
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis):
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+    ):
         limiter = RateLimiter()
         assert limiter.use_redis is True
         assert limiter.redis is not None
@@ -61,7 +64,7 @@
 @pytest.mark.asyncio
 async def test_rate_limiter_init_without_redis():
     """Test RateLimiter initialization with Redis disabled."""
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=False):
+    with patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=False):
         limiter = RateLimiter()
         assert limiter.use_redis is False
         assert limiter.redis is None
@@ -72,9 +75,11 @@
 async def test_get_user_limit_redis(mock_redis):
     """Test getting user limit from Redis."""
     mock_redis._data = {"user_limit:user123": "20"}
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+    ):
         limiter = RateLimiter()
         limit = await limiter.get_user_limit("user123")
         assert limit == 20
@@ -84,9 +89,11 @@
 async def test_get_user_limit_redis_default(mock_redis):
     """Test getting default limit when user limit not set in Redis."""
     mock_redis._data = {}
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+    ):
         limiter = RateLimiter()
         limit = await limiter.get_user_limit("user123")
         assert limit == DEFAULT_DAILY_LIMIT
@@ -97,8 +104,8 @@
     """Test getting user limit from database."""
     mock_db_limiter = AsyncMock()
     mock_db_limiter.get_user_limit = AsyncMock(return_value=15)
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=False):
+
+    with patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=False):
         limiter = RateLimiter()
         limiter.db_limiter = mock_db_limiter
         limit = await limiter.get_user_limit("user123")
@@ -109,8 +116,10 @@
 @pytest.mark.asyncio
 async def test_set_user_limit_redis(mock_redis):
     """Test setting user limit in Redis."""
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis):
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+    ):
         limiter = RateLimiter()
         result = await limiter.set_user_limit("user123", 25)
         assert result is True
@@ -122,8 +131,8 @@
     """Test setting user limit in database."""
     mock_db_limiter = AsyncMock()
     mock_db_limiter.set_user_limit = AsyncMock(return_value=True)
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=False):
+
+    with patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=False):
         limiter = RateLimiter()
         limiter.db_limiter = mock_db_limiter
         result = await limiter.set_user_limit("user123", 25)
@@ -136,9 +145,11 @@
     """Test getting request count from Redis."""
     today = datetime.utcnow().strftime("%Y-%m-%d")
     mock_redis._data = {f"request_count:user123:{today}": "5"}
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+    ):
         limiter = RateLimiter()
         count = await limiter.get_request_count("user123")
         assert count == 5
@@ -148,9 +159,11 @@
 async def test_get_request_count_redis_zero(mock_redis):
     """Test getting request count when no count exists in Redis."""
     mock_redis._data = {}
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+    ):
         limiter = RateLimiter()
         count = await limiter.get_request_count("user123")
         assert count == 0
@@ -161,9 +174,11 @@
     """Test incrementing request count in Redis."""
     today = datetime.utcnow().strftime("%Y-%m-%d")
     mock_redis._data = {f"request_count:user123:{today}": "3"}
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis):
+
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+    ):
         limiter = RateLimiter()
         new_count = await limiter.increment_request_count("user123")
         assert new_count == 4
@@ -173,14 +188,16 @@
 @pytest.mark.asyncio
 async def test_increment_request_count_redis_new(mock_redis):
     """Test incrementing request count when it doesn't exist in Redis."""
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis), \
-         patch('app.utils.rate_limit.datetime') as mock_datetime:
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+        patch("app.utils.rate_limit.datetime") as mock_datetime,
+    ):
         # Mock datetime to return a fixed time
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
         mock_datetime.strftime = Mock(return_value="2024-01-15")
-        
+
         limiter = RateLimiter()
         new_count = await limiter.increment_request_count("user123")
         assert new_count == 1
@@ -190,21 +207,20 @@
 async def test_check_rate_limit_redis_allowed(mock_redis):
     """Test checking rate limit when request is allowed."""
     today = datetime.utcnow().strftime("%Y-%m-%d")
-    mock_redis._data = {
-        "user_limit:user123": "10",
-        f"request_count:user123:{today}": "5"
-    }
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis), \
-         patch('app.utils.rate_limit.datetime') as mock_datetime:
+    mock_redis._data = {"user_limit:user123": "10", f"request_count:user123:{today}": "5"}
+
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+        patch("app.utils.rate_limit.datetime") as mock_datetime,
+    ):
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
         mock_datetime.strftime = Mock(return_value="2024-01-15")
-        
+
         limiter = RateLimiter()
         result = await limiter.check_rate_limit("user123")
-        
+
         assert result["allowed"] is True
         assert result["limit"] == 10
         # Remaining should be limit - count = 10 - 5 = 5
@@ -216,23 +232,25 @@
 @pytest.mark.asyncio
 async def test_check_rate_limit_redis_exceeded(mock_redis):
     """Test checking rate limit when limit is exceeded."""
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis), \
-         patch('app.utils.rate_limit.datetime') as mock_datetime:
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+        patch("app.utils.rate_limit.datetime") as mock_datetime,
+    ):
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
         mock_datetime.strftime = Mock(return_value="2024-01-15")
         today = "2024-01-15"
-        
+
         # Set count to 11 to exceed limit of 10
         mock_redis._data = {
             "user_limit:user123": "10",
-            f"request_count:user123:{today}": "11"  # Exceeded limit
+            f"request_count:user123:{today}": "11",  # Exceeded limit
         }
-        
+
         limiter = RateLimiter()
         result = await limiter.check_rate_limit("user123")
-        
+
         # When count > limit, allowed should be False (count < limit is False)
         assert result["allowed"] is False
         assert result["limit"] == 10
@@ -246,16 +264,18 @@
     mock_redis._data = {
         f"request_count:user123:{today}": "5",
         f"request_count:user456:{today}": "3",
-        "user_limit:user123": "10"  # This should remain
+        "user_limit:user123": "10",  # This should remain
     }
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=mock_redis), \
-         patch('app.utils.rate_limit.datetime') as mock_datetime:
+
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=mock_redis),
+        patch("app.utils.rate_limit.datetime") as mock_datetime,
+    ):
         mock_now = datetime(2024, 1, 15, 12, 0, 0)
         mock_datetime.utcnow = Mock(return_value=mock_now)
         mock_datetime.strftime = Mock(return_value="2024-01-15")
-        
+
         limiter = RateLimiter()
         result = await limiter.reset_all_counters()
         assert result is True
@@ -269,8 +289,8 @@
     """Test window-based rate limiting with database."""
     mock_db_limiter = AsyncMock()
     mock_db_limiter.check_window_limit = AsyncMock(return_value=True)
-    
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=False):
+
+    with patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=False):
         limiter = RateLimiter()
         limiter.db_limiter = mock_db_limiter
         result = await limiter.check_window_limit("user123", "/api/test", 10, 60)
@@ -281,10 +301,11 @@
 @pytest.mark.asyncio
 async def test_check_window_limit_redis():
     """Test window-based rate limiting with Redis (placeholder)."""
-    with patch('app.utils.rate_limit.get_use_redis_rate_limiting', return_value=True), \
-         patch('app.utils.rate_limit.redis.Redis', return_value=AsyncMock()):
+    with (
+        patch("app.utils.rate_limit.get_use_redis_rate_limiting", return_value=True),
+        patch("app.utils.rate_limit.redis.Redis", return_value=AsyncMock()),
+    ):
         limiter = RateLimiter()
         # Redis implementation returns True (placeholder)
         result = await limiter.check_window_limit("user123", "/api/test", 10, 60)
         assert result is True
-

--- test_domain_search_debug.py
+++ test_domain_search_debug.py
@@ -2,6 +2,7 @@
 from app.utils.domain_utils import find_domain_and_generate_company_id
 from app.agents.prospecting_orchestrator import PromptAnalyzer
 
+
 async def main():
     prompt = "Search VisionEdge One"
     print(f"Testing prompt: {prompt}")
@@ -22,5 +23,6 @@
     else:
         print("Prompt was not classified as a specific company search. No domain search performed.")
 
+
 if __name__ == "__main__":
-    asyncio.run(main()) 
\ No newline at end of file
+    asyncio.run(main())

--- test_domain_search_simple.py
+++ test_domain_search_simple.py
@@ -14,7 +14,7 @@
 load_dotenv()
 
 # Add the app directory to the path
-sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))
+sys.path.append(os.path.join(os.path.dirname(__file__), "app"))
 
 from langchain_perplexity import ChatPerplexity
 from app.prompts.data_retrieval_prompts import get_multi_domain_prompt
@@ -22,7 +22,7 @@
 # Test companies list
 TEST_COMPANIES = [
     "1OAK Capital Limited",
-    "ABR Dynamic Funds, LLC", 
+    "ABR Dynamic Funds, LLC",
     "Advent Capital Management, LLC",
     "Alpstone Capital Suisse SA",
     "Chelverton Asset Management Limited",
@@ -39,7 +39,7 @@
     "Kayne Anderson Capital Advisors, LP",
     "Marble Bar Asset Management, LLP",
     "AUM Asset Management Limited",
-    "Robocap Asset Management Limited", 
+    "Robocap Asset Management Limited",
     "Rothschild & Co Bank AG",
     "Selwood Asset Management (France) SAS",
     "Tavira Financial Limited",
@@ -47,9 +47,10 @@
     "Toscafund Asset Management, LLP / Toscafund HK Limited",
     "Trinity Street Asset Management, LLP",
     "Quest Partners, LLC",
-    "Westbeck Capital Management LLP"
+    "Westbeck Capital Management LLP",
 ]
 
+
 async def test_domain_search_direct(company_name: str, location: str = "Global") -> str:
     """
     Direct implementation of domain search using the improved prompt.
@@ -57,31 +58,23 @@
     try:
         # Get the multi-domain prompt with improved variations handling
         user_prompt = get_multi_domain_prompt(company_name, location)
-        
+
         # Initialize ChatPerplexity
-        chat = ChatPerplexity(
-            temperature=0, 
-            model="sonar-pro",
-            api_key=os.getenv("PERPLEXITY_API_KEY")
-        )
-        
+        chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=os.getenv("PERPLEXITY_API_KEY"))
+
         # Make the API call
-        response = await chat.ainvoke([
-            ("user", user_prompt)
-        ], extra_body={
-            "web_search_options": {"search_context_size": "high"}
-        })
-        
+        response = await chat.ainvoke([("user", user_prompt)], extra_body={"web_search_options": {"search_context_size": "high"}})
+
         content = response.content if response else None
-        
+
         if not content:
             return None
-            
+
         # Parse the response to extract domain(s)
         content = content.strip()
-        
+
         # Handle different response formats
-        if content.startswith('[') and content.endswith(']'):
+        if content.startswith("[") and content.endswith("]"):
             # Array format: ["domain"] or [null]
             try:
                 domains = json.loads(content)
@@ -91,87 +84,88 @@
                     return None
             except json.JSONDecodeError:
                 pass
-        
+
         # Handle single domain format: "https://example.com"
         if content.startswith('"') and content.endswith('"'):
             domain = content[1:-1]  # Remove quotes
             return domain if domain and domain != "null" else None
-        
+
         # Handle direct URL format
-        if content.startswith('http'):
+        if content.startswith("http"):
             return content
-            
+
         # Handle null response
-        if content.lower() in ['null', '[null]', 'none']:
+        if content.lower() in ["null", "[null]", "none"]:
             return None
-            
+
         return None
-        
+
     except Exception as e:
         print(f"    üí• Exception: {str(e)}")
         return None
 
+
 async def run_domain_test():
     """Test domain search for all companies and print results."""
     print("üîç Testing Improved Domain Search Prompt (Direct Implementation)")
     print("=" * 80)
     print(f"üìä Testing {len(TEST_COMPANIES)} investment firms")
     print("=" * 80)
-    
+
     results = {}
-    
+
     for i, company_name in enumerate(TEST_COMPANIES, 1):
         print(f"\n[{i:2d}/{len(TEST_COMPANIES)}] üè¢ {company_name}")
         print("-" * 60)
-        
+
         try:
             domain_result = await test_domain_search_direct(company_name)
-            
+
             if domain_result:
                 print(f"‚úÖ FOUND: {domain_result}")
                 results[company_name] = {"status": "found", "domain": domain_result}
             else:
                 print(f"‚ùå NOT FOUND")
                 results[company_name] = {"status": "not_found", "domain": None}
-                
+
         except Exception as e:
             print(f"‚ö†Ô∏è ERROR: {str(e)}")
             results[company_name] = {"status": "error", "domain": None, "error": str(e)}
-    
+
     # Summary report
     print("\n" + "=" * 80)
     print("üìà SUMMARY REPORT")
     print("=" * 80)
-    
+
     found_count = sum(1 for r in results.values() if r["status"] == "found")
     not_found_count = sum(1 for r in results.values() if r["status"] == "not_found")
     error_count = sum(1 for r in results.values() if r["status"] == "error")
-    
-    print(f"‚úÖ Found domains: {found_count}/{len(TEST_COMPANIES)} ({found_count/len(TEST_COMPANIES)*100:.1f}%)")
-    print(f"‚ùå Not found: {not_found_count}/{len(TEST_COMPANIES)} ({not_found_count/len(TEST_COMPANIES)*100:.1f}%)")
-    print(f"‚ö†Ô∏è Errors: {error_count}/{len(TEST_COMPANIES)} ({error_count/len(TEST_COMPANIES)*100:.1f}%)")
-    
+
+    print(f"‚úÖ Found domains: {found_count}/{len(TEST_COMPANIES)} ({found_count / len(TEST_COMPANIES) * 100:.1f}%)")
+    print(f"‚ùå Not found: {not_found_count}/{len(TEST_COMPANIES)} ({not_found_count / len(TEST_COMPANIES) * 100:.1f}%)")
+    print(f"‚ö†Ô∏è Errors: {error_count}/{len(TEST_COMPANIES)} ({error_count / len(TEST_COMPANIES) * 100:.1f}%)")
+
     # Detailed results
     print("\n" + "=" * 80)
     print("üìã DETAILED RESULTS")
     print("=" * 80)
-    
+
     for company_name, result in results.items():
         status_icon = "‚úÖ" if result["status"] == "found" else "‚ùå" if result["status"] == "not_found" else "‚ö†Ô∏è"
         domain_text = result["domain"] if result["domain"] else "None"
         print(f"{status_icon} {company_name:<45} ‚Üí {domain_text}")
-    
+
     # Special focus on key test cases
     print("\n" + "=" * 80)
     print("üéØ KEY TEST CASES")
     print("=" * 80)
-    
+
     key_test_cases = [
         "Tavira Financial Limited",  # Should find tavira.group with improved prompt
-        "ABR Dynamic Funds, LLC",   # Should find abrfunds.com or similar
+        "ABR Dynamic Funds, LLC",  # Should find abrfunds.com or similar
         "Rothschild & Co Bank AG",  # Should find rothschildandco.com or similar
     ]
-    
+
     for test_case in key_test_cases:
         if test_case in results:
             result = results[test_case]
@@ -179,12 +173,13 @@
             domain_text = result["domain"] if result["domain"] else "None"
             print(f"{status_icon} {test_case} ‚Üí {domain_text}")
 
+
 if __name__ == "__main__":
     # Check for required environment variables
     if not os.getenv("PERPLEXITY_API_KEY"):
         print("‚ùå PERPLEXITY_API_KEY not found in environment variables")
         print("   Please set your Perplexity API key in the .env file")
         sys.exit(1)
-    
+
     # Run the test
-    asyncio.run(run_domain_test()) 
\ No newline at end of file
+    asyncio.run(run_domain_test())

--- test_langsmith_model_recognition.py
+++ test_langsmith_model_recognition.py
@@ -15,10 +15,11 @@
 
 # Load environment variables from .env file
 from dotenv import load_dotenv
+
 load_dotenv()
 
 # Add the app directory to the path
-sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))
+sys.path.append(os.path.join(os.path.dirname(__file__), "app"))
 
 from app.utils.langsmith_config import initialize_langsmith, create_main_run
 
@@ -26,42 +27,46 @@
 os.environ["LANGCHAIN_TRACING_V2"] = "true"
 os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGSMITH_PROJECT", "ardessa-agent")
 
+
 def count_tokens_approximate(text: str) -> int:
     """Approximate token count (rough estimate: 1 token ‚âà 4 characters)."""
     return len(text) // 4
 
+
 async def test_langchain_perplexity_call():
     """Test LangChain Perplexity API call with LangSmith tracing."""
     print("üîç Testing LangChain Perplexity API call...")
-    
+
     try:
         from langchain_perplexity import ChatPerplexity
-        
+
         # Initialize LangChain Perplexity with the exact model name from LangSmith config
         chat = ChatPerplexity(
-            temperature=0, 
+            temperature=0,
             model="sonar-pro",  # Changed to match LangSmith config: perplexity-sonar-pro
-            api_key=os.getenv("PERPLEXITY_API_KEY")
+            api_key=os.getenv("PERPLEXITY_API_KEY"),
         )
-        
+
         print(f"üîß Using model: sonar-pro (should match perplexity-sonar-pro in LangSmith)")
-        
+
         # Make the API call through LangChain using tuple format
-        response = await chat.ainvoke([
-            ("system", "You are a helpful assistant that provides comprehensive overviews of companies."),
-            ("user", "Tell me about Sequoia Capital. Provide a brief overview of this venture capital firm.")
-        ])
-        
+        response = await chat.ainvoke(
+            [
+                ("system", "You are a helpful assistant that provides comprehensive overviews of companies."),
+                ("user", "Tell me about Sequoia Capital. Provide a brief overview of this venture capital firm."),
+            ]
+        )
+
         print(f"‚úÖ LangChain Perplexity Response: {response.content[:200]}...")
         print(f"üìä Response length: {len(response.content)} characters")
-        
+
         # Show search results if available
-        if hasattr(response, 'additional_kwargs') and 'search_results' in response.additional_kwargs:
+        if hasattr(response, "additional_kwargs") and "search_results" in response.additional_kwargs:
             search_results = response.additional_kwargs["search_results"][:2]
             print(f"üîç Search results: {len(search_results)} found")
-        
+
         return True
-        
+
     except ImportError:
         print("‚ùå langchain-perplexity not installed. Install with: pip install langchain-perplexity")
         return False
@@ -69,31 +74,30 @@
         print(f"‚ùå LangChain Perplexity error: {e}")
         return False
 
+
 async def test_langchain_openai_call():
     """Test LangChain OpenAI API call with LangSmith tracing."""
     print("üîç Testing LangChain OpenAI API call...")
-    
+
     try:
         from langchain_openai import ChatOpenAI
-        
+
         # Initialize LangChain OpenAI
-        chat = ChatOpenAI(
-            temperature=0.7,
-            model="gpt-4o-mini",
-            api_key=os.getenv("OPENAI_API_KEY")
+        chat = ChatOpenAI(temperature=0.7, model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
+
+        # Make the API call through LangChain using tuple format
+        response = await chat.ainvoke(
+            [
+                ("system", "You are a helpful assistant that provides comprehensive overviews of companies."),
+                ("user", "Tell me about Sequoia Capital. Provide a brief overview of this venture capital firm."),
+            ]
         )
-        
-        # Make the API call through LangChain using tuple format
-        response = await chat.ainvoke([
-            ("system", "You are a helpful assistant that provides comprehensive overviews of companies."),
-            ("user", "Tell me about Sequoia Capital. Provide a brief overview of this venture capital firm.")
-        ])
-        
+
         print(f"‚úÖ LangChain OpenAI Response: {response.content[:200]}...")
         print(f"üìä Response length: {len(response.content)} characters")
-        
+
         return True
-        
+
     except ImportError:
         print("‚ùå langchain-openai not installed. Install with: pip install langchain-openai")
         return False
@@ -101,31 +105,30 @@
         print(f"‚ùå LangChain OpenAI error: {e}")
         return False
 
+
 async def test_langchain_gemini_call():
     """Test LangChain Gemini API call with LangSmith tracing."""
     print("üîç Testing LangChain Gemini API call...")
-    
+
     try:
         from langchain_google_genai import ChatGoogleGenerativeAI
-        
+
         # Initialize LangChain Gemini
-        chat = ChatGoogleGenerativeAI(
-            temperature=0.7,
-            model="gemini-1.5-pro",
-            google_api_key=os.getenv("GOOGLE_API_KEY")
-        )
-        
+        chat = ChatGoogleGenerativeAI(temperature=0.7, model="gemini-1.5-pro", google_api_key=os.getenv("GOOGLE_API_KEY"))
+
         # Make the API call through LangChain using tuple format
-        response = await chat.ainvoke([
-            ("system", "You are a helpful assistant that provides comprehensive overviews of companies."),
-            ("user", "Tell me about Sequoia Capital. Provide a brief overview of this venture capital firm.")
-        ])
-        
+        response = await chat.ainvoke(
+            [
+                ("system", "You are a helpful assistant that provides comprehensive overviews of companies."),
+                ("user", "Tell me about Sequoia Capital. Provide a brief overview of this venture capital firm."),
+            ]
+        )
+
         print(f"‚úÖ LangChain Gemini Response: {response.content[:200]}...")
         print(f"üìä Response length: {len(response.content)} characters")
-        
+
         return True
-        
+
     except ImportError:
         print("‚ùå langchain-google-genai not installed. Install with: pip install langchain-google-genai")
         return False
@@ -133,13 +136,14 @@
         print(f"‚ùå LangChain Gemini error: {e}")
         return False
 
+
 async def test_agent_perplexity_prompts():
     """Test actual Perplexity prompts from the agents."""
     print("üîç Testing Agent Perplexity Prompts...")
-    
+
     try:
         from langchain_perplexity import ChatPerplexity
-        
+
         # Test 1: Company Search Prompt (from CompanySearchAgent)
         print("\nüìã Test 1: Company Search Prompt")
         system_prompt = "You are a helpful assistant that searches for investment firms."
@@ -152,27 +156,20 @@
         - focus_area: Investment focus areas
         - description: Brief company description
         """
-        
+
         # Use exact parameters from CompanySearchAgent
-        chat = ChatPerplexity(
-            temperature=0, 
-            model="sonar-pro",
-            api_key=os.getenv("PERPLEXITY_API_KEY")
-        )
-        
+        chat = ChatPerplexity(temperature=0, model="sonar-pro", api_key=os.getenv("PERPLEXITY_API_KEY"))
+
         # Use tuple format with extra_body for search options
         response = await chat.ainvoke(
-            [
-                ("system", system_prompt),
-                ("user", user_prompt)
-            ],
+            [("system", system_prompt), ("user", user_prompt)],
             extra_body={
                 "search_domain_filter": [],  # No domain filter for company search
-                "web_search_options": {"search_context_size": "high"}
-            }
+                "web_search_options": {"search_context_size": "high"},
+            },
         )
         print(f"‚úÖ Company Search Response: {response.content[:300]}...")
-        
+
         # Test 2: Web Research Prompt (from WebResearchAgent)
         print("\nüìã Test 2: Web Research Prompt")
         system_prompt = "You are an expert OSINT researcher. Conduct comprehensive research and return structured JSON."
@@ -186,41 +183,36 @@
         
         Return structured JSON with detailed findings.
         """
-        
+
         # Use exact parameters from WebResearchAgent with domain filter
         domain_filter = ["sequoiacap.com"]  # Example domain filter
         response = await chat.ainvoke(
-            [
-                ("system", system_prompt),
-                ("user", user_prompt)
-            ],
-            extra_body={
-                "search_domain_filter": domain_filter,
-                "web_search_options": {"search_context_size": "high"}
-            }
+            [("system", system_prompt), ("user", user_prompt)],
+            extra_body={"search_domain_filter": domain_filter, "web_search_options": {"search_context_size": "high"}},
         )
         print(f"‚úÖ Web Research Response: {response.content[:300]}...")
-        
+
         return True
-        
+
     except Exception as e:
         print(f"‚ùå Agent Perplexity Prompts error: {e}")
         return False
 
+
 async def test_agent_gemini_prompts():
     """Test actual Gemini prompts from the agents."""
     print("üîç Testing Agent Gemini Prompts...")
-    
+
     try:
         from langchain_google_genai import ChatGoogleGenerativeAI
-        
+
         # Use exact model from CompanyEnrichAgent
         chat = ChatGoogleGenerativeAI(
             temperature=0.7,
             model="gemini-1.5-flash",  # Exact model from agent
-            google_api_key=os.getenv("GOOGLE_API_KEY")
+            google_api_key=os.getenv("GOOGLE_API_KEY"),
         )
-        
+
         # Test 1: Data Consolidation Prompt (from CompanyEnrichAgent)
         print("\nüìã Test 1: Data Consolidation Prompt")
         system_prompt = """
@@ -228,7 +220,7 @@
         Remove duplicates and near-duplicates, combine complementary information, preserve all unique details, and maintain clear structure.
         Always respond with valid JSON only.
         """
-        
+
         user_prompt = """
         Consolidate the following company information from multiple sources:
 
@@ -242,14 +234,11 @@
         
         Return the consolidated data in JSON format.
         """
-        
+
         # Use tuple format instead of concatenated string
-        response = await chat.ainvoke([
-            ("system", system_prompt),
-            ("user", user_prompt)
-        ])
+        response = await chat.ainvoke([("system", system_prompt), ("user", user_prompt)])
         print(f"‚úÖ Data Consolidation Response: {response.content[:300]}...")
-        
+
         # Test 2: Person Selection Prompt (from PersonEnrichAgent)
         print("\nüìã Test 2: Person Selection Prompt")
         system_prompt = """
@@ -257,7 +246,7 @@
         Consider investment decision authority, board representation, strategic influence, and public presence.
         Always respond with valid JSON only.
         """
-        
+
         user_prompt = """
         Analyze the following executives and identify the most relevant person for investment outreach:
 
@@ -269,50 +258,38 @@
         
         Return the name and title of the most relevant person with brief reasoning in JSON format.
         """
-        
+
         # Use tuple format instead of concatenated string
-        response = await chat.ainvoke([
-            ("system", system_prompt),
-            ("user", user_prompt)
-        ])
+        response = await chat.ainvoke([("system", system_prompt), ("user", user_prompt)])
         print(f"‚úÖ Person Selection Response: {response.content[:300]}...")
-        
+
         return True
-        
+
     except Exception as e:
         print(f"‚ùå Agent Gemini Prompts error: {e}")
         return False
 
+
 async def test_direct_api_calls():
     """Test direct API calls (these won't show token counts in LangSmith)."""
     print("\nüîç Testing Direct API calls (no token tracking)...")
-    
+
     # Test direct OpenAI call
     try:
         client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
         response = client.chat.completions.create(
-            model="gpt-4o-mini",
-            messages=[{"role": "user", "content": "Tell me about Sequoia Capital briefly."}],
-            max_tokens=100
+            model="gpt-4o-mini", messages=[{"role": "user", "content": "Tell me about Sequoia Capital briefly."}], max_tokens=100
         )
         print(f"‚úÖ Direct OpenAI Response: {response.choices[0].message.content[:100]}...")
     except Exception as e:
         print(f"‚ùå Direct OpenAI error: {e}")
-    
+
     # Test direct Perplexity call
     try:
         async with aiohttp.ClientSession() as session:
             headers = {"Authorization": f"Bearer {os.getenv('PERPLEXITY_API_KEY')}"}
-            data = {
-                "model": "sonar-pro",
-                "messages": [{"role": "user", "content": "Tell me about Sequoia Capital briefly."}],
-                "max_tokens": 100
-            }
-            async with session.post(
-                "https://api.perplexity.ai/chat/completions",
-                headers=headers,
-                json=data
-            ) as response:
+            data = {"model": "sonar-pro", "messages": [{"role": "user", "content": "Tell me about Sequoia Capital briefly."}], "max_tokens": 100}
+            async with session.post("https://api.perplexity.ai/chat/completions", headers=headers, json=data) as response:
                 result = await response.json()
                 if response.status == 200:
                     content = result["choices"][0]["message"]["content"]
@@ -322,38 +299,39 @@
     except Exception as e:
         print(f"‚ùå Direct Perplexity error: {e}")
 
+
 async def main():
     """Main test function."""
     print("üöÄ Starting LangSmith Model Recognition Test")
     print("=" * 50)
-    
+
     # Initialize LangSmith
     initialize_langsmith()
-    
+
     # Create main run context
     with create_main_run("test_langsmith_model_recognition", "test_user_123", "test_session_abc") as run:
         print(f"üìù Created main run: {run.id}")
-        
+
         # Test LangChain integrations (should show token counts)
         print("\n" + "=" * 50)
         print("üîó Testing LangChain Integrations (should show token counts)")
         print("=" * 50)
-        
+
         await test_langchain_openai_call()
         await test_langchain_perplexity_call()
         await test_langchain_gemini_call()
-        
+
         # Test actual agent prompts
         print("\n" + "=" * 50)
         print("ü§ñ Testing Actual Agent Prompts")
         print("=" * 50)
-        
+
         await test_agent_perplexity_prompts()
         await test_agent_gemini_prompts()
-        
+
         # Test direct API calls (won't show token counts)
         await test_direct_api_calls()
-        
+
         print("\n" + "=" * 50)
         print("üìä Summary:")
         if run:
@@ -364,5 +342,6 @@
         print("‚ùå Direct API calls will not show token counts (manual traces only)")
         print("=" * 50)
 
+
 if __name__ == "__main__":
-    asyncio.run(main()) 
\ No newline at end of file
+    asyncio.run(main())

