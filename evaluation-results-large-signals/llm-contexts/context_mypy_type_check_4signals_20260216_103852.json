{
  "_debug_metadata": {
    "timestamp": "20260216_103852",
    "tool_id": "mypy",
    "signal_type": "type_check",
    "num_signals": 4,
    "signal_files": [
      "authentication/clerk.py",
      "ria/services/query_service.py",
      "ria/services/search_service.py"
    ],
    "system_prompt_length": 10660,
    "user_prompt_length": 15169
  },
  "context": {
    "group": {
      "tool_id": "mypy",
      "signal_type": "type_check",
      "group_size": 4
    },
    "signals": [
      {
        "signal": {
          "tool_id": "mypy",
          "signal_type": "type_check",
          "severity": "high",
          "rule_code": "arg-type",
          "message": "Argument \"issuer\" to \"ClerkSettings\" has incompatible type \"str | None\"; expected \"str\"",
          "docs_url": "https://mypy.readthedocs.io/en/stable/error_code_list.html#arg-type",
          "file_path": "authentication/clerk.py",
          "span": {
            "start": {
              "row": 81,
              "column": 52
            },
            "end": {
              "row": 81,
              "column": 52
            }
          }
        },
        "file_read_error": null,
        "code_context": {
          "window": {
            "file_path": "authentication/clerk.py",
            "start_row": 51,
            "end_row": 111,
            "text": "            (\"CLERK_JWKS_URL\", jwks_url),\n            (\"CLERK_ISSUER\", issuer),\n        ) if not value]\n        if missing:\n            raise ClerkConfigurationError(\n                f\"Missing Clerk configuration value(s): {', '.join(missing)}\"\n            )\n\n        return cls(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\ndef get_clerk_settings() -> ClerkSettings:\n    \"\"\"Load Clerk token validation settings from config helpers.\"\"\"\n    return ClerkSettings.from_config()\n\n\ndef get_lp_clerk_settings() -> ClerkSettings:\n    jwks_url = get_lp_clerk_jwks_url()\n    issuer = get_lp_clerk_issuer()\n    audience = get_lp_clerk_audience()\n\n    missing = [name for name, value in (\n        (\"LP_CLERK_JWKS_URL\", jwks_url),\n        (\"LP_CLERK_ISSUER\", issuer),\n    ) if not value]\n    if missing:\n        raise ClerkConfigurationError(\n            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n        )\n\n    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\n_JWKS_CACHE: Dict[str, Dict[str, Any]] = {}\n\n\ndef _fetch_jwks(\n    settings: ClerkSettings,\n    *,\n    http_get: Optional[Callable[[str], Dict[str, Any]]] = None,\n) -> Dict[str, Any]:\n    \"\"\"Fetch Clerk JWKS document with naive in-memory caching.\"\"\"\n    now = time.time()\n\n    cache_entry = _JWKS_CACHE.get(settings.jwks_url)\n    if cache_entry and cache_entry[\"expires_at\"] > now:\n        return cache_entry[\"payload\"]\n\n    fetcher: Callable[[str], Dict[str, Any]]\n    if http_get is not None:\n        fetcher = http_get\n    else:\n        def fetcher(url: str) -> Dict[str, Any]:\n            logger.info(f\"\ud83d\udd0d Fetching JWKS from: {url}\")\n            try:\n                response = httpx.get(url, timeout=5.0)\n                logger.info(f\"\u2705 JWKS response status: {response.status_code}\")\n                response.raise_for_status()\n                return response.json()\n            except httpx.ConnectError as e:\n                logger.error(f\"\u274c JWKS connection error: {e}\")\n"
          },
          "imports": null,
          "enclosing_function": {
            "file_path": "authentication/clerk.py",
            "start_row": 65,
            "end_row": 83,
            "text": "\n\ndef get_lp_clerk_settings() -> ClerkSettings:\n    jwks_url = get_lp_clerk_jwks_url()\n    issuer = get_lp_clerk_issuer()\n    audience = get_lp_clerk_audience()\n\n    missing = [name for name, value in (\n        (\"LP_CLERK_JWKS_URL\", jwks_url),\n        (\"LP_CLERK_ISSUER\", issuer),\n    ) if not value]\n    if missing:\n        raise ClerkConfigurationError(\n            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n        )\n\n    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\n"
          },
          "try_except_block": null,
          "class_definition": null,
          "type_aliases": null,
          "related_function": null,
          "module_constants": null
        },
        "edit_snippet": {
          "file_path": "authentication/clerk.py",
          "start_row": 74,
          "end_row": 88,
          "text": "        (\"LP_CLERK_ISSUER\", issuer),\n    ) if not value]\n    if missing:\n        raise ClerkConfigurationError(\n            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n        )\n\n    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\n_JWKS_CACHE: Dict[str, Dict[str, Any]] = {}\n\n\ndef _fetch_jwks(\n    settings: ClerkSettings,\n",
          "original_text": "        (\"LP_CLERK_ISSUER\", issuer),\n    ) if not value]\n    if missing:\n        raise ClerkConfigurationError(\n            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n        )\n\n    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\n_JWKS_CACHE: Dict[str, Dict[str, Any]] = {}\n\n\ndef _fetch_jwks(\n    settings: ClerkSettings,\n",
          "error_line": 81,
          "error_line_in_snippet": 8,
          "snippet_length": 15,
          "base_indent": ""
        },
        "edit_window_type": "lines",
        "fix_context": {
          "exists": false
        }
      },
      {
        "signal": {
          "tool_id": "mypy",
          "signal_type": "type_check",
          "severity": "high",
          "rule_code": "assignment",
          "message": "Incompatible types in assignment (expression has type \"datetime\", target has type \"str\")",
          "docs_url": "https://mypy.readthedocs.io/en/stable/error_code_list.html#assignment",
          "file_path": "ria/services/query_service.py",
          "span": {
            "start": {
              "row": 103,
              "column": 46
            },
            "end": {
              "row": 103,
              "column": 46
            }
          }
        },
        "file_read_error": null,
        "code_context": {
          "window": {
            "file_path": "ria/services/query_service.py",
            "start_row": 73,
            "end_row": 133,
            "text": "            \n        except Exception as e:\n            logger.error(\n                f\"Failed to store RIA activity: {str(e)}\",\n                extra={\"user_id\": user_id, \"activity_type\": activity_type},\n                exc_info=True\n            )\n            raise\n\n    @staticmethod\n    async def get_ria_activity(\n        user_id: Optional[str] = None,\n        search_id: Optional[str] = None,\n        activity_type: Optional[str] = None,\n        from_date: Optional[datetime] = None,\n        to_date: Optional[datetime] = None,\n        limit: int = 50,\n        offset: int = 0\n    ) -> Dict[str, Any]:\n        try:\n            # Build query filters\n            filters = {}\n            \n            if user_id:\n                filters['user_id'] = user_id\n            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n            queryset = RIAUserActivity.objects.filter(**filters).order_by('search_id', 'created_at')\n            records = []\n            \n            async for activity in queryset[offset:offset + limit]:\n                records.append({\n                    \"id\": activity.id,\n                    \"user_id\": activity.user_id,\n                    \"session_id\": activity.session_id,\n                    \"activity_type\": activity.activity_type,\n                    \"search_id\": activity.search_id,\n                    \"search_params\": activity.search_params,\n                    \"search_timestamp\": activity.search_timestamp.isoformat() if activity.search_timestamp else None,\n                    \"company_crd\": activity.company_crd,\n                    \"company_name\": activity.company_name,\n                    \"company_action\": activity.company_action,\n                    \"ip_address\": activity.ip_address,\n                    \"created_at\": activity.created_at.isoformat() if activity.created_at else None,\n                })\n            \n            return {\n                \"records\": records,\n                \"total_count\": total_count,\n                \"limit\": limit,\n"
          },
          "imports": {
            "file_path": "ria/services/query_service.py",
            "start_row": 1,
            "end_row": 16,
            "text": "\"\"\"\nQuery Service\n\nThis service handles all database operations for RIA using Django ORM.\nReplaces asyncpg database calls with Django ORM queries.\n\"\"\"\n\nfrom typing import Optional, Dict, Any, List\nfrom datetime import datetime\n\nfrom django.db import transaction\nfrom django.utils import timezone\n\nfrom ria.models import RIAUserActivity\nfrom common.utils.logging_config import get_logger\n\n"
          },
          "enclosing_function": {
            "file_path": "ria/services/query_service.py",
            "start_row": 81,
            "end_row": 148,
            "text": "\n    @staticmethod\n    async def get_ria_activity(\n        user_id: Optional[str] = None,\n        search_id: Optional[str] = None,\n        activity_type: Optional[str] = None,\n        from_date: Optional[datetime] = None,\n        to_date: Optional[datetime] = None,\n        limit: int = 50,\n        offset: int = 0\n    ) -> Dict[str, Any]:\n        try:\n            # Build query filters\n            filters = {}\n            \n            if user_id:\n                filters['user_id'] = user_id\n            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n            queryset = RIAUserActivity.objects.filter(**filters).order_by('search_id', 'created_at')\n            records = []\n            \n            async for activity in queryset[offset:offset + limit]:\n                records.append({\n                    \"id\": activity.id,\n                    \"user_id\": activity.user_id,\n                    \"session_id\": activity.session_id,\n                    \"activity_type\": activity.activity_type,\n                    \"search_id\": activity.search_id,\n                    \"search_params\": activity.search_params,\n                    \"search_timestamp\": activity.search_timestamp.isoformat() if activity.search_timestamp else None,\n                    \"company_crd\": activity.company_crd,\n                    \"company_name\": activity.company_name,\n                    \"company_action\": activity.company_action,\n                    \"ip_address\": activity.ip_address,\n                    \"created_at\": activity.created_at.isoformat() if activity.created_at else None,\n                })\n            \n            return {\n                \"records\": records,\n                \"total_count\": total_count,\n                \"limit\": limit,\n                \"offset\": offset\n            }\n            \n        except Exception as e:\n            logger.error(\n                f\"Failed to get RIA activity: {str(e)}\",\n                extra={\n                    \"user_id\": user_id,\n                    \"search_id\": search_id,\n                    \"activity_type\": activity_type\n                },\n                exc_info=True\n            )\n            raise\n\n"
          },
          "try_except_block": null,
          "class_definition": null,
          "type_aliases": null,
          "related_function": null,
          "module_constants": null
        },
        "edit_snippet": {
          "file_path": "ria/services/query_service.py",
          "start_row": 98,
          "end_row": 108,
          "text": "if search_id:\n    filters['search_id'] = search_id\nif activity_type:\n    filters['activity_type'] = activity_type\nif from_date:\n    filters['created_at__gte'] = from_date\nif to_date:\n    filters['created_at__lte'] = to_date\n            \n# Get total count\ntotal_count = await RIAUserActivity.objects.filter(**filters).acount()\n",
          "original_text": "            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n",
          "error_line": 103,
          "error_line_in_snippet": 6,
          "snippet_length": 11,
          "base_indent": "            "
        },
        "edit_window_type": "lines",
        "fix_context": {
          "exists": false
        }
      },
      {
        "signal": {
          "tool_id": "mypy",
          "signal_type": "type_check",
          "severity": "high",
          "rule_code": "assignment",
          "message": "Incompatible types in assignment (expression has type \"datetime\", target has type \"str\")",
          "docs_url": "https://mypy.readthedocs.io/en/stable/error_code_list.html#assignment",
          "file_path": "ria/services/query_service.py",
          "span": {
            "start": {
              "row": 105,
              "column": 46
            },
            "end": {
              "row": 105,
              "column": 46
            }
          }
        },
        "file_read_error": null,
        "code_context": {
          "window": {
            "file_path": "ria/services/query_service.py",
            "start_row": 75,
            "end_row": 135,
            "text": "            logger.error(\n                f\"Failed to store RIA activity: {str(e)}\",\n                extra={\"user_id\": user_id, \"activity_type\": activity_type},\n                exc_info=True\n            )\n            raise\n\n    @staticmethod\n    async def get_ria_activity(\n        user_id: Optional[str] = None,\n        search_id: Optional[str] = None,\n        activity_type: Optional[str] = None,\n        from_date: Optional[datetime] = None,\n        to_date: Optional[datetime] = None,\n        limit: int = 50,\n        offset: int = 0\n    ) -> Dict[str, Any]:\n        try:\n            # Build query filters\n            filters = {}\n            \n            if user_id:\n                filters['user_id'] = user_id\n            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n            queryset = RIAUserActivity.objects.filter(**filters).order_by('search_id', 'created_at')\n            records = []\n            \n            async for activity in queryset[offset:offset + limit]:\n                records.append({\n                    \"id\": activity.id,\n                    \"user_id\": activity.user_id,\n                    \"session_id\": activity.session_id,\n                    \"activity_type\": activity.activity_type,\n                    \"search_id\": activity.search_id,\n                    \"search_params\": activity.search_params,\n                    \"search_timestamp\": activity.search_timestamp.isoformat() if activity.search_timestamp else None,\n                    \"company_crd\": activity.company_crd,\n                    \"company_name\": activity.company_name,\n                    \"company_action\": activity.company_action,\n                    \"ip_address\": activity.ip_address,\n                    \"created_at\": activity.created_at.isoformat() if activity.created_at else None,\n                })\n            \n            return {\n                \"records\": records,\n                \"total_count\": total_count,\n                \"limit\": limit,\n                \"offset\": offset\n            }\n"
          },
          "imports": {
            "file_path": "ria/services/query_service.py",
            "start_row": 1,
            "end_row": 16,
            "text": "\"\"\"\nQuery Service\n\nThis service handles all database operations for RIA using Django ORM.\nReplaces asyncpg database calls with Django ORM queries.\n\"\"\"\n\nfrom typing import Optional, Dict, Any, List\nfrom datetime import datetime\n\nfrom django.db import transaction\nfrom django.utils import timezone\n\nfrom ria.models import RIAUserActivity\nfrom common.utils.logging_config import get_logger\n\n"
          },
          "enclosing_function": {
            "file_path": "ria/services/query_service.py",
            "start_row": 81,
            "end_row": 148,
            "text": "\n    @staticmethod\n    async def get_ria_activity(\n        user_id: Optional[str] = None,\n        search_id: Optional[str] = None,\n        activity_type: Optional[str] = None,\n        from_date: Optional[datetime] = None,\n        to_date: Optional[datetime] = None,\n        limit: int = 50,\n        offset: int = 0\n    ) -> Dict[str, Any]:\n        try:\n            # Build query filters\n            filters = {}\n            \n            if user_id:\n                filters['user_id'] = user_id\n            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n            queryset = RIAUserActivity.objects.filter(**filters).order_by('search_id', 'created_at')\n            records = []\n            \n            async for activity in queryset[offset:offset + limit]:\n                records.append({\n                    \"id\": activity.id,\n                    \"user_id\": activity.user_id,\n                    \"session_id\": activity.session_id,\n                    \"activity_type\": activity.activity_type,\n                    \"search_id\": activity.search_id,\n                    \"search_params\": activity.search_params,\n                    \"search_timestamp\": activity.search_timestamp.isoformat() if activity.search_timestamp else None,\n                    \"company_crd\": activity.company_crd,\n                    \"company_name\": activity.company_name,\n                    \"company_action\": activity.company_action,\n                    \"ip_address\": activity.ip_address,\n                    \"created_at\": activity.created_at.isoformat() if activity.created_at else None,\n                })\n            \n            return {\n                \"records\": records,\n                \"total_count\": total_count,\n                \"limit\": limit,\n                \"offset\": offset\n            }\n            \n        except Exception as e:\n            logger.error(\n                f\"Failed to get RIA activity: {str(e)}\",\n                extra={\n                    \"user_id\": user_id,\n                    \"search_id\": search_id,\n                    \"activity_type\": activity_type\n                },\n                exc_info=True\n            )\n            raise\n\n"
          },
          "try_except_block": null,
          "class_definition": null,
          "type_aliases": null,
          "related_function": null,
          "module_constants": null
        },
        "edit_snippet": {
          "file_path": "ria/services/query_service.py",
          "start_row": 100,
          "end_row": 110,
          "text": "if activity_type:\n    filters['activity_type'] = activity_type\nif from_date:\n    filters['created_at__gte'] = from_date\nif to_date:\n    filters['created_at__lte'] = to_date\n            \n# Get total count\ntotal_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n# Get paginated results\n",
          "original_text": "            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n",
          "error_line": 105,
          "error_line_in_snippet": 6,
          "snippet_length": 11,
          "base_indent": "            "
        },
        "edit_window_type": "lines",
        "fix_context": {
          "exists": false
        }
      },
      {
        "signal": {
          "tool_id": "mypy",
          "signal_type": "type_check",
          "severity": "high",
          "rule_code": "arg-type",
          "message": "Argument \"key\" to \"sort\" of \"list\" has incompatible type \"Callable[[dict[str, object]], object]\"; expected \"Callable[[dict[str, object]], SupportsDunderLT[Any] | SupportsDunderGT[Any]]\"",
          "docs_url": "https://mypy.readthedocs.io/en/stable/error_code_list.html#arg-type",
          "file_path": "ria/services/search_service.py",
          "span": {
            "start": {
              "row": 96,
              "column": 26
            },
            "end": {
              "row": 96,
              "column": 26
            }
          }
        },
        "file_read_error": null,
        "code_context": {
          "window": {
            "file_path": "ria/services/search_service.py",
            "start_row": 66,
            "end_row": 126,
            "text": "        .alias(\"__domain\")\n    )\n\n\ndef to_serializable_records(df: pl.DataFrame, limit: int) -> List[Dict[str, Any]]:\n    \"\"\"Convert DataFrame to list of dictionaries, limited to specified count.\"\"\"\n    if df.height == 0:\n        return []\n    take = min(limit, df.height)\n    return df.head(take).to_dicts()\n\n\ndef get_top_3_private_funds(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 private fund types by count for a company.\"\"\"\n    fund_counts = []\n    \n    for field_name, column_name in PRIVATE_FUND_COUNT_COLUMNS.items():\n        if column_name in company_record and company_record[column_name] is not None:\n            try:\n                count = int(company_record[column_name])\n                if count > 0:\n                    display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)\n                    fund_counts.append({\n                        \"category\": display_name,\n                        \"count\": count\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by count (highest first) and return top 3\n    fund_counts.sort(key=lambda x: x[\"count\"], reverse=True)\n    return fund_counts[:3]\n\n\ndef get_top_3_allocations(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 SMA allocation categories by percentage for a company.\"\"\"\n    allocations = []\n    \n    for field_name, column_name in SMA_ALLOCATION_COLUMNS.items():\n        if column_name in company_record and company_record[column_name] is not None:\n            try:\n                percentage = float(company_record[column_name])\n                if percentage > 0:\n                    display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)\n                    allocations.append({\n                        \"category\": display_name,\n                        \"percentage\": percentage\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by percentage (highest first) and return top 3\n    allocations.sort(key=lambda x: x[\"percentage\"], reverse=True)\n    return allocations[:3]\n\n\ndef get_total_clients(company_record: Dict[str, Any]) -> int:\n    \"\"\"Get total count of all clients for a company by summing all client type columns.\"\"\"\n    total_count = 0\n    \n    for column_name in ALL_CLIENT_TYPE_COLUMNS:\n"
          },
          "imports": {
            "file_path": "ria/services/search_service.py",
            "start_row": 1,
            "end_row": 14,
            "text": "\"\"\"\nSearch Service\n\nThis service handles all search and filtering operations for RIA data.\nIncludes intelligent fuzzy matching and comprehensive filtering logic.\n\"\"\"\n\nfrom typing import Dict, Any, List, Optional\nfrom urllib.parse import urlparse\n\nimport polars as pl\n\nfrom ria.services.index_cache_service import get_index_cache\nfrom ria.utils.constants import (\n"
          },
          "enclosing_function": {
            "file_path": "ria/services/search_service.py",
            "start_row": 76,
            "end_row": 99,
            "text": "\n\ndef get_top_3_private_funds(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 private fund types by count for a company.\"\"\"\n    fund_counts = []\n    \n    for field_name, column_name in PRIVATE_FUND_COUNT_COLUMNS.items():\n        if column_name in company_record and company_record[column_name] is not None:\n            try:\n                count = int(company_record[column_name])\n                if count > 0:\n                    display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)\n                    fund_counts.append({\n                        \"category\": display_name,\n                        \"count\": count\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by count (highest first) and return top 3\n    fund_counts.sort(key=lambda x: x[\"count\"], reverse=True)\n    return fund_counts[:3]\n\n\n"
          },
          "try_except_block": null,
          "class_definition": null,
          "type_aliases": {
            "file_path": "ria/services/search_service.py",
            "start_row": 26,
            "end_row": 26,
            "text": "def canonicalize_domain(value: Optional[str]) -> Optional[str]:\n"
          },
          "related_function": null,
          "module_constants": null
        },
        "edit_snippet": {
          "file_path": "ria/services/search_service.py",
          "start_row": 89,
          "end_row": 103,
          "text": "                        \"category\": display_name,\n                        \"count\": count\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by count (highest first) and return top 3\n    fund_counts.sort(key=lambda x: x[\"count\"], reverse=True)\n    return fund_counts[:3]\n\n\ndef get_top_3_allocations(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 SMA allocation categories by percentage for a company.\"\"\"\n    allocations = []\n    \n",
          "original_text": "                        \"category\": display_name,\n                        \"count\": count\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by count (highest first) and return top 3\n    fund_counts.sort(key=lambda x: x[\"count\"], reverse=True)\n    return fund_counts[:3]\n\n\ndef get_top_3_allocations(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 SMA allocation categories by percentage for a company.\"\"\"\n    allocations = []\n    \n",
          "error_line": 96,
          "error_line_in_snippet": 8,
          "snippet_length": 15,
          "base_indent": ""
        },
        "edit_window_type": "lines",
        "fix_context": {
          "exists": false
        }
      }
    ],
    "merged_snippet_groups": [
      {
        "signal_indices": [
          1,
          2
        ],
        "edit_snippet": {
          "file_path": "ria/services/query_service.py",
          "start_row": 98,
          "end_row": 110,
          "text": "if search_id:\n    filters['search_id'] = search_id\nif activity_type:\n    filters['activity_type'] = activity_type\nif from_date:\n    filters['created_at__gte'] = from_date\nif to_date:\n    filters['created_at__lte'] = to_date\n            \n# Get total count\ntotal_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n# Get paginated results\n",
          "original_text": "            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n",
          "error_line": 103,
          "error_line_in_snippet": 6,
          "snippet_length": 13,
          "base_indent": "            "
        }
      }
    ],
    "standalone_signal_indices": [
      0,
      3
    ]
  },
  "prompts": {
    "system_prompt": "You are an expert code repair agent. Your task is to make MINIMAL, SURGICAL fixes to code errors.\n\n## CRITICAL PRINCIPLE: MINIMAL CHANGES ONLY\n\nYour job is to fix ONLY the specific error mentioned. You are NOT improving, refactoring, or cleaning up code.\n\n**THE GOLDEN RULE**: Every line in your output that is NOT directly part of the fix MUST be IDENTICAL to the input - same content, same whitespace, same everything.\n\n## How This Works\n\nFor each error signal, you will receive:\n\n1. **Error Information**: Type, message, severity, and rule code\n2. **Edit Snippet**: A small code snippet (~13 lines) containing the error\n   - This is what you need to FIX and RETURN\n   - The error location within the snippet is indicated (e.g., \"Error on line 4 of 13\")\n3. **Context Window**: A larger code window (~30 lines) around the error for understanding\n   - Use this to understand the surrounding code, but DON'T return it\n\n4. **Tailored Context** (varies by signal type - optimized to reduce token usage):\n   - **Imports**: The file's import block (for type definitions, dependencies)\n     - Included for: type errors, undefined names, most issues\n     - Excluded for: import-only errors, bare except blocks\n   - **Enclosing Function**: The function containing the error (for understanding scope/logic)\n     - Included for: type errors, docstring errors, function-level issues\n     - Excluded for: import errors (global scope), bare except (sent separately)\n   - **Try/Except Block**: The try/except block containing the error\n     - Included for: bare except errors (E722)\n     - Excluded for: most other errors\n\nNote: Context is carefully selected to provide what you need while minimizing token usage.\nNot all context is present for every error - you'll only receive relevant context.\n\n## Response Format\n\nFor EACH signal/snippet you receive, respond with this EXACT format:\n\n```\n===== FIX FOR: <file_path> =====\nCONFIDENCE: <0.0-1.0>\nREASONING: <brief explanation of the fix>\n\n```FIXED_CODE\n<complete fixed snippet - ALL lines from edit_snippet, with ONLY the fix applied>\n```\n\nWARNINGS: <any caveats, or \"None\">\n===== END FIX =====\n```\n\n## CRITICAL Rules - READ CAREFULLY\n\n1. **MINIMAL CHANGES ONLY** - Change ONLY what is necessary to fix the specific error. Nothing more.\n\n2. **PRESERVE EVERYTHING ELSE EXACTLY** - Every line that is NOT part of the fix must be returned EXACTLY as it appeared in the input, character-for-character, including:\n   - Comments and documentation\n   - Blank lines (including leading and trailing blank lines)\n   - All whitespace (including trailing newlines at the end of the snippet)\n   - Other variable declarations\n   - Function definitions\n   - String content (including multi-line strings)\n\n   **CRITICAL**: If the input snippet ends with blank lines or newlines, your output MUST end with the exact same number of blank lines/newlines. Do NOT trim trailing whitespace.\n\n3. **DO NOT under any circumstances**:\n   - Delete lines that aren't related to the fix\n   - Add code that isn't required for the fix\n   - \"Clean up\" or \"improve\" surrounding code\n   - Modify comments or documentation\n   - Change formatting on unrelated lines\n   - Remove blank lines\n   - Add blank lines (unless the fix specifically requires it)\n   - Modify string literals or docstrings\n   - Delete function definitions\n   - Remove dictionary entries or list items unrelated to the fix\n\n4. **Maintain RELATIVE indentation** - The snippet has had its base indentation removed. Preserve relative indentation exactly. If a line has 4 spaces in the input, it must have 4 spaces in your output.\n\n5. **One fix block per signal** - If multiple signals, provide multiple fix blocks\n\n6. **Use context for understanding only** - The context window, imports, and enclosing function help you understand the code, but you only return the fixed edit_snippet\n\n## Example - Correct Minimal Fix\n\nInput:\n- Error: \"Need type annotation for 'cache'\" on line 173\n- Edit Snippet (error on line 4 of 7, lines 170-176):\n```\n}\n\nDEPRIORITIZE_QUERIES = {}\ncache = {}\n\n# Model configuration\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n```\n(Note: The snippet ends with a newline after the last line - this MUST be preserved)\n\nCORRECT Response (only `cache = {}` changed to `cache: dict = {}`):\n```\n===== FIX FOR: app/config/tier_queries.py =====\nCONFIDENCE: 0.95\nREASONING: Added dict type annotation to cache variable to satisfy mypy\n\n```FIXED_CODE\n}\n\nDEPRIORITIZE_QUERIES = {}\ncache: dict = {}\n\n# Model configuration\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n```\n\nWARNINGS: None\n===== END FIX =====\n```\n(Note: The output preserves the exact same trailing newline as the input)\n\n## Example - Preserving Trailing Whitespace\n\nInput snippet ending with blank lines (note the TWO newlines after the return statement):\n```\ndef get_clerk_settings() -> ClerkSettings:\n    # Load Clerk token validation settings from config helpers\n    return ClerkSettings.from_config()\n\n\n```\n\nCORRECT - Preserves both trailing newlines:\n```FIXED_CODE\ndef get_clerk_settings() -> ClerkSettings:\n    # Load Clerk token validation settings from config helpers\n    return ClerkSettings.from_config()\n\n\n```\n(Output MUST have the same TWO trailing newlines)\n\nWRONG - Trimming trailing newlines:\n```FIXED_CODE\ndef get_clerk_settings() -> ClerkSettings:\n    # Load Clerk token validation settings from config helpers\n    return ClerkSettings.from_config()\n```\n^ This is WRONG because it removed trailing newlines. The snippet MUST end with the same whitespace as the input.\n\n## Example - WRONG (Deleting Unrelated Content)\n\nIf the input snippet contains documentation or other code, you MUST preserve it:\n\nWRONG - Deleting unrelated lines:\n```FIXED_CODE\ncache: dict = {}\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n```\n^ This is WRONG because it deleted the closing brace, blank lines, DEPRIORITIZE_QUERIES, and the comment.\n\n## Confidence Guidelines\n- High (>0.8): Simple fixes like type annotations, obvious corrections\n- Medium (0.5-0.8): Logic changes, type guards, refactoring\n- Low (<0.5): Complex changes, unclear intent - add detailed warnings\n\n\n\n## MyPy Type Error Fixing - Specialized Guidance\n\nYou are fixing TYPE CHECKING errors from MyPy. These require careful handling of\nvalidation logic, Optional types, and type contracts.\n\n\u26a0\ufe0f CRITICAL - Validation Logic Preservation:\n\nMany type errors occur in code that INTENTIONALLY validates values before use.\nThis validation is NOT a bug - it's defensive programming and security practice.\n\nNEVER bypass validation by adding default values like:\n\u274c `validated_url = get_url(); use(validated_url or \"\")` - Bypasses validation!\n\u274c `if not api_key: raise Error; use(api_key or \"\")` - Makes validation useless!\n\u274c `password = get_password() or \"default\"` - Security risk!\n\nHow to recognize validation patterns:\n- Code that checks \"if not value: raise Exception\"\n- Code that validates required configuration\n- Code in security-related files (auth, tokens, crypto)\n- Code that explicitly checks for None before proceeding\n\nType Error Fixing Strategies (in priority order):\n\n1. **Type Annotations** - Add missing type hints (SAFEST)\n   \u2705 `x = []` \u2192 `x: list[str] = []`\n   \u2705 `CONSTANT = {}` \u2192 `CONSTANT: dict = {}`\n   Use REPLACE to avoid duplicating code\n\n2. **Type Guards / Narrowing** - Help type checker understand flow\n   \u2705 `if value: use(value)` \u2192 `if value is not None: use(value)`\n   \u2705 Add `assert value is not None` when default ensures non-None\n\n3. **Optional Return Types** - If None is legitimately possible\n   \u2705 `def get() -> str:` \u2192 `def get() -> Optional[str]:`\n   Only when the function semantically can return None\n\n4. **Fix Type Contract** - Align function signature with actual usage\n   \u2705 If function requires non-None, callers should guarantee non-None\n   \u2705 Use type guards at call site: `if x is not None: func(x)`\n\n5. **Preserve Validation, Add Type Assertion** - For validated code paths\n   \u2705 Keep validation: `if not url: raise Error`\n   \u2705 After validation: `assert url is not None  # Validated above`\n   \u2705 Then use: `return cls(url=url)`\n\nNEVER do these:\n\u274c Add `or \"\"` to bypass validation checks\n\u274c Add `or 0` to numeric values that shouldn't default to zero\n\u274c Change validation logic just to satisfy type checker\n\u274c Remove validation that raises exceptions\n\u274c Weaken security checks (JWT URLs, API keys, credentials)\n\nSpecial Considerations:\n\nFor Optional[str] \u2192 str conversions:\n- If there's validation: preserve it, add type assertion after\n- If no validation: consider if None is actually possible\n  - If yes: make return type Optional\n  - If no: add assertion or default (only if semantically correct)\n\nFor function arguments:\n- If function signature says str, caller must provide str\n- Fix at call site with type guards, not with ` or \"\"`\n- If argument can be None, change function signature to Optional[str]\n\nFor return types:\n- Match what function actually returns\n- If all paths return non-None, use str not Optional[str]\n- If some paths return None, use Optional[str]\n\nConfidence Guidelines:\n- High confidence (>0.8): Simple type annotations, obvious narrowing\n- Medium confidence (0.5-0.8): Type guards, Optional additions\n- Low confidence (<0.5): Complex validation, unclear intent, security code\n  - Add detailed warnings for human review\n\nExamples:\n\nGOOD - Type annotation:\n```python\n# Before: Need type annotation for \"cache\"\ncache = {}\n# After:\ncache: dict = {}\n```\n\nGOOD - Type guard preserving validation:\n```python\n# Before: Argument 1 to \"process\" has incompatible type \"str | None\"; expected \"str\"\nconfig_value = get_config(\"KEY\")\nif config_value:\n    process(config_value)  # Type error: str | None\n# After:\nconfig_value = get_config(\"KEY\")\nif config_value is not None:\n    process(config_value)  # Type checker satisfied\n```\n\nGOOD - Assertion after validation:\n```python\n# Before: Incompatible type at return\ndef from_config(cls):\n    url = get_url()\n    if not url:\n        raise ConfigError(\"URL required\")\n    return cls(url=url)  # Type error: url is Optional[str]\n# After:\ndef from_config(cls):\n    url = get_url()\n    if not url:\n        raise ConfigError(\"URL required\")\n    assert url is not None  # Validated above\n    return cls(url=url)\n```\n\nBAD - Bypassing validation:\n```python\n# Before: Validation ensures non-None\njwks_url = get_jwks_url()\nif not jwks_url:\n    raise SecurityError(\"JWKS URL required for JWT validation\")\nreturn Settings(jwks_url=jwks_url)  # Type error\n# DON'T DO THIS:\nreturn Settings(jwks_url=jwks_url or \"\")  # \u274c Bypasses security check!\n```\n\nRemember: Type errors in validation code are usually CONTRACT mismatches,\nnot validation bugs. Fix the contract, don't break the validation.\n",
    "user_prompt": "Tool: mypy\nSignal Type: type_check\nNumber of Signals: 4\n\n============================================================\nSIGNAL 1\n============================================================\n\n## Error Information\n- File: authentication/clerk.py\n- Message: Argument \"issuer\" to \"ClerkSettings\" has incompatible type \"str | None\"; expected \"str\"\n- Rule Code: arg-type\n- Severity: high\n- Location: Line 81, Column 52\n\n## Edit Snippet (FIX AND RETURN THIS)\nLines 74-88 (error on line 8 of 15)\n```python\n        (\"LP_CLERK_ISSUER\", issuer),\n    ) if not value]\n    if missing:\n        raise ClerkConfigurationError(\n            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n        )\n\n    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\n_JWKS_CACHE: Dict[str, Dict[str, Any]] = {}\n\n\ndef _fetch_jwks(\n    settings: ClerkSettings,\n\n```\n\n## Context Window (for understanding, DO NOT return)\nLines 51-111\n```python\n            (\"CLERK_JWKS_URL\", jwks_url),\n            (\"CLERK_ISSUER\", issuer),\n        ) if not value]\n        if missing:\n            raise ClerkConfigurationError(\n                f\"Missing Clerk configuration value(s): {', '.join(missing)}\"\n            )\n\n        return cls(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\ndef get_clerk_settings() -> ClerkSettings:\n    \"\"\"Load Clerk token validation settings from config helpers.\"\"\"\n    return ClerkSettings.from_config()\n\n\ndef get_lp_clerk_settings() -> ClerkSettings:\n    jwks_url = get_lp_clerk_jwks_url()\n    issuer = get_lp_clerk_issuer()\n    audience = get_lp_clerk_audience()\n\n    missing = [name for name, value in (\n        (\"LP_CLERK_JWKS_URL\", jwks_url),\n        (\"LP_CLERK_ISSUER\", issuer),\n    ) if not value]\n    if missing:\n        raise ClerkConfigurationError(\n            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n        )\n\n    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\n_JWKS_CACHE: Dict[str, Dict[str, Any]] = {}\n\n\ndef _fetch_jwks(\n    settings: ClerkSettings,\n    *,\n    http_get: Optional[Callable[[str], Dict[str, Any]]] = None,\n) -> Dict[str, Any]:\n    \"\"\"Fetch Clerk JWKS document with naive in-memory caching.\"\"\"\n    now = time.time()\n\n    cache_entry = _JWKS_CACHE.get(settings.jwks_url)\n    if cache_entry and cache_entry[\"expires_at\"] > now:\n        return cache_entry[\"payload\"]\n\n    fetcher: Callable[[str], Dict[str, Any]]\n    if http_get is not None:\n        fetcher = http_get\n    else:\n        def fetcher(url: str) -> Dict[str, Any]:\n            logger.info(f\"\ud83d\udd0d Fetching JWKS from: {url}\")\n            try:\n                response = httpx.get(url, timeout=5.0)\n                logger.info(f\"\u2705 JWKS response status: {response.status_code}\")\n                response.raise_for_status()\n                return response.json()\n            except httpx.ConnectError as e:\n                logger.error(f\"\u274c JWKS connection error: {e}\")\n\n```\n\n## Enclosing Function\nLines 65-83\n```python\n\n\ndef get_lp_clerk_settings() -> ClerkSettings:\n    jwks_url = get_lp_clerk_jwks_url()\n    issuer = get_lp_clerk_issuer()\n    audience = get_lp_clerk_audience()\n\n    missing = [name for name, value in (\n        (\"LP_CLERK_JWKS_URL\", jwks_url),\n        (\"LP_CLERK_ISSUER\", issuer),\n    ) if not value]\n    if missing:\n        raise ClerkConfigurationError(\n            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n        )\n\n    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n\n\n\n```\n\n\n============================================================\nSIGNALS 2-3 (shared edit region)\n============================================================\n\n## Error 1\n- File: ria/services/query_service.py\n- Message: Incompatible types in assignment (expression has type \"datetime\", target has type \"str\")\n- Rule Code: assignment\n- Severity: high\n- Location: Line 103, Column 46\n\n## Error 2\n- File: ria/services/query_service.py\n- Message: Incompatible types in assignment (expression has type \"datetime\", target has type \"str\")\n- Rule Code: assignment\n- Severity: high\n- Location: Line 105, Column 46\n\n## Edit Snippet (FIX ALL ERRORS ABOVE AND RETURN THIS)\nLines 98-110 (2 errors in 13 lines)\n```python\nif search_id:\n    filters['search_id'] = search_id\nif activity_type:\n    filters['activity_type'] = activity_type\nif from_date:\n    filters['created_at__gte'] = from_date\nif to_date:\n    filters['created_at__lte'] = to_date\n            \n# Get total count\ntotal_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n# Get paginated results\n\n```\n\n## Context Window (for understanding, DO NOT return)\nLines 73-133\n```python\n            \n        except Exception as e:\n            logger.error(\n                f\"Failed to store RIA activity: {str(e)}\",\n                extra={\"user_id\": user_id, \"activity_type\": activity_type},\n                exc_info=True\n            )\n            raise\n\n    @staticmethod\n    async def get_ria_activity(\n        user_id: Optional[str] = None,\n        search_id: Optional[str] = None,\n        activity_type: Optional[str] = None,\n        from_date: Optional[datetime] = None,\n        to_date: Optional[datetime] = None,\n        limit: int = 50,\n        offset: int = 0\n    ) -> Dict[str, Any]:\n        try:\n            # Build query filters\n            filters = {}\n            \n            if user_id:\n                filters['user_id'] = user_id\n            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n            queryset = RIAUserActivity.objects.filter(**filters).order_by('search_id', 'created_at')\n            records = []\n            \n            async for activity in queryset[offset:offset + limit]:\n                records.append({\n                    \"id\": activity.id,\n                    \"user_id\": activity.user_id,\n                    \"session_id\": activity.session_id,\n                    \"activity_type\": activity.activity_type,\n                    \"search_id\": activity.search_id,\n                    \"search_params\": activity.search_params,\n                    \"search_timestamp\": activity.search_timestamp.isoformat() if activity.search_timestamp else None,\n                    \"company_crd\": activity.company_crd,\n                    \"company_name\": activity.company_name,\n                    \"company_action\": activity.company_action,\n                    \"ip_address\": activity.ip_address,\n                    \"created_at\": activity.created_at.isoformat() if activity.created_at else None,\n                })\n            \n            return {\n                \"records\": records,\n                \"total_count\": total_count,\n                \"limit\": limit,\n\n```\n\n## Imports\n```python\n\"\"\"\nQuery Service\n\nThis service handles all database operations for RIA using Django ORM.\nReplaces asyncpg database calls with Django ORM queries.\n\"\"\"\n\nfrom typing import Optional, Dict, Any, List\nfrom datetime import datetime\n\nfrom django.db import transaction\nfrom django.utils import timezone\n\nfrom ria.models import RIAUserActivity\nfrom common.utils.logging_config import get_logger\n\n\n```\n\n## Enclosing Function\nLines 81-148\n```python\n\n    @staticmethod\n    async def get_ria_activity(\n        user_id: Optional[str] = None,\n        search_id: Optional[str] = None,\n        activity_type: Optional[str] = None,\n        from_date: Optional[datetime] = None,\n        to_date: Optional[datetime] = None,\n        limit: int = 50,\n        offset: int = 0\n    ) -> Dict[str, Any]:\n        try:\n            # Build query filters\n            filters = {}\n            \n            if user_id:\n                filters['user_id'] = user_id\n            if search_id:\n                filters['search_id'] = search_id\n            if activity_type:\n                filters['activity_type'] = activity_type\n            if from_date:\n                filters['created_at__gte'] = from_date\n            if to_date:\n                filters['created_at__lte'] = to_date\n            \n            # Get total count\n            total_count = await RIAUserActivity.objects.filter(**filters).acount()\n            \n            # Get paginated results\n            queryset = RIAUserActivity.objects.filter(**filters).order_by('search_id', 'created_at')\n            records = []\n            \n            async for activity in queryset[offset:offset + limit]:\n                records.append({\n                    \"id\": activity.id,\n                    \"user_id\": activity.user_id,\n                    \"session_id\": activity.session_id,\n                    \"activity_type\": activity.activity_type,\n                    \"search_id\": activity.search_id,\n                    \"search_params\": activity.search_params,\n                    \"search_timestamp\": activity.search_timestamp.isoformat() if activity.search_timestamp else None,\n                    \"company_crd\": activity.company_crd,\n                    \"company_name\": activity.company_name,\n                    \"company_action\": activity.company_action,\n                    \"ip_address\": activity.ip_address,\n                    \"created_at\": activity.created_at.isoformat() if activity.created_at else None,\n                })\n            \n            return {\n                \"records\": records,\n                \"total_count\": total_count,\n                \"limit\": limit,\n                \"offset\": offset\n            }\n            \n        except Exception as e:\n            logger.error(\n                f\"Failed to get RIA activity: {str(e)}\",\n                extra={\n                    \"user_id\": user_id,\n                    \"search_id\": search_id,\n                    \"activity_type\": activity_type\n                },\n                exc_info=True\n            )\n            raise\n\n\n```\n\n\n============================================================\nSIGNAL 4\n============================================================\n\n## Error Information\n- File: ria/services/search_service.py\n- Message: Argument \"key\" to \"sort\" of \"list\" has incompatible type \"Callable[[dict[str, object]], object]\"; expected \"Callable[[dict[str, object]], SupportsDunderLT[Any] | SupportsDunderGT[Any]]\"\n- Rule Code: arg-type\n- Severity: high\n- Location: Line 96, Column 26\n\n## Edit Snippet (FIX AND RETURN THIS)\nLines 89-103 (error on line 8 of 15)\n```python\n                        \"category\": display_name,\n                        \"count\": count\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by count (highest first) and return top 3\n    fund_counts.sort(key=lambda x: x[\"count\"], reverse=True)\n    return fund_counts[:3]\n\n\ndef get_top_3_allocations(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 SMA allocation categories by percentage for a company.\"\"\"\n    allocations = []\n    \n\n```\n\n## Context Window (for understanding, DO NOT return)\nLines 66-126\n```python\n        .alias(\"__domain\")\n    )\n\n\ndef to_serializable_records(df: pl.DataFrame, limit: int) -> List[Dict[str, Any]]:\n    \"\"\"Convert DataFrame to list of dictionaries, limited to specified count.\"\"\"\n    if df.height == 0:\n        return []\n    take = min(limit, df.height)\n    return df.head(take).to_dicts()\n\n\ndef get_top_3_private_funds(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 private fund types by count for a company.\"\"\"\n    fund_counts = []\n    \n    for field_name, column_name in PRIVATE_FUND_COUNT_COLUMNS.items():\n        if column_name in company_record and company_record[column_name] is not None:\n            try:\n                count = int(company_record[column_name])\n                if count > 0:\n                    display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)\n                    fund_counts.append({\n                        \"category\": display_name,\n                        \"count\": count\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by count (highest first) and return top 3\n    fund_counts.sort(key=lambda x: x[\"count\"], reverse=True)\n    return fund_counts[:3]\n\n\ndef get_top_3_allocations(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 SMA allocation categories by percentage for a company.\"\"\"\n    allocations = []\n    \n    for field_name, column_name in SMA_ALLOCATION_COLUMNS.items():\n        if column_name in company_record and company_record[column_name] is not None:\n            try:\n                percentage = float(company_record[column_name])\n                if percentage > 0:\n                    display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)\n                    allocations.append({\n                        \"category\": display_name,\n                        \"percentage\": percentage\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by percentage (highest first) and return top 3\n    allocations.sort(key=lambda x: x[\"percentage\"], reverse=True)\n    return allocations[:3]\n\n\ndef get_total_clients(company_record: Dict[str, Any]) -> int:\n    \"\"\"Get total count of all clients for a company by summing all client type columns.\"\"\"\n    total_count = 0\n    \n    for column_name in ALL_CLIENT_TYPE_COLUMNS:\n\n```\n\n## Imports\n```python\n\"\"\"\nSearch Service\n\nThis service handles all search and filtering operations for RIA data.\nIncludes intelligent fuzzy matching and comprehensive filtering logic.\n\"\"\"\n\nfrom typing import Dict, Any, List, Optional\nfrom urllib.parse import urlparse\n\nimport polars as pl\n\nfrom ria.services.index_cache_service import get_index_cache\nfrom ria.utils.constants import (\n\n```\n\n## Enclosing Function\nLines 76-99\n```python\n\n\ndef get_top_3_private_funds(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"Get top 3 private fund types by count for a company.\"\"\"\n    fund_counts = []\n    \n    for field_name, column_name in PRIVATE_FUND_COUNT_COLUMNS.items():\n        if column_name in company_record and company_record[column_name] is not None:\n            try:\n                count = int(company_record[column_name])\n                if count > 0:\n                    display_name = DISPLAY_NAME_MAPPING.get(column_name, column_name)\n                    fund_counts.append({\n                        \"category\": display_name,\n                        \"count\": count\n                    })\n            except (ValueError, TypeError):\n                continue\n    \n    # Sort by count (highest first) and return top 3\n    fund_counts.sort(key=lambda x: x[\"count\"], reverse=True)\n    return fund_counts[:3]\n\n\n\n```\n\n## Type Aliases\nLines 26-26\n```python\ndef canonicalize_domain(value: Optional[str]) -> Optional[str]:\n\n```\n\n\nPlease provide fixes for the above signals using the specified response format."
  },
  "llm_response": [
    "===== FIX FOR: authentication/clerk.py =====\n",
    "CONFIDENCE: 0.95\n",
    "REASONING: Added type assertion after validation check to inform type checker that issuer is not None after the validation passes\n",
    "\n",
    "```FIXED_CODE\n",
    "        (\"LP_CLERK_ISSUER\", issuer),\n",
    "    ) if not value]\n",
    "    if missing:\n",
    "        raise ClerkConfigurationError(\n",
    "            f\"Missing LP Clerk configuration value(s): {', '.join(missing)}\"\n",
    "        )\n",
    "\n",
    "    assert issuer is not None  # Validated above\n",
    "    return ClerkSettings(jwks_url=jwks_url, issuer=issuer, audience=audience)\n",
    "\n",
    "\n",
    "_JWKS_CACHE: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "\n",
    "def _fetch_jwks(\n",
    "    settings: ClerkSettings,\n",
    "\n",
    "```\n",
    "\n",
    "WARNINGS: None\n",
    "===== END FIX =====\n",
    "\n",
    "===== FIX FOR: ria/services/query_service.py =====\n",
    "CONFIDENCE: 0.90\n",
    "REASONING: The Django ORM expects datetime objects for date filtering, not strings. The from_date and to_date parameters are already datetime objects, so they can be used directly in the filters.\n",
    "\n",
    "```FIXED_CODE\n",
    "if search_id:\n",
    "    filters['search_id'] = search_id\n",
    "if activity_type:\n",
    "    filters['activity_type'] = activity_type\n",
    "if from_date:\n",
    "    filters['created_at__gte'] = from_date\n",
    "if to_date:\n",
    "    filters['created_at__lte'] = to_date\n",
    "            \n",
    "# Get total count\n",
    "total_count = await RIAUserActivity.objects.filter(**filters).acount()\n",
    "            \n",
    "# Get paginated results\n",
    "\n",
    "```\n",
    "\n",
    "WARNINGS: None\n",
    "===== END FIX =====\n",
    "\n",
    "===== FIX FOR: ria/services/search_service.py =====\n",
    "CONFIDENCE: 0.85\n",
    "REASONING: Added type annotation to the lambda function to specify that it returns an int, which satisfies the SupportsDunderLT requirement for sorting\n",
    "\n",
    "```FIXED_CODE\n",
    "                        \"category\": display_name,\n",
    "                        \"count\": count\n",
    "                    })\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "    \n",
    "    # Sort by count (highest first) and return top 3\n",
    "    fund_counts.sort(key=lambda x: int(x[\"count\"]), reverse=True)\n",
    "    return fund_counts[:3]\n",
    "\n",
    "\n",
    "def get_top_3_allocations(company_record: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get top 3 SMA allocation categories by percentage for a company.\"\"\"\n",
    "    allocations = []\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "WARNINGS: None\n",
    "===== END FIX ====="
  ]
}